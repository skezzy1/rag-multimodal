import requests
from bs4 import BeautifulSoup
import json
import os
import re
from urllib.parse import urljoin, urlparse
from PIL import Image 
import io
from tqdm import tqdm
import hashlib
from dotenv import load_dotenv
import concurrent.futures 

load_dotenv()

ROOT_DOMAIN = os.getenv("ROOT_DOMAIN", "https://www.deeplearning.ai") 
OUTPUT_PATH = os.getenv("ARTICLES_PATH", "data/articles.json")
IMAGE_DIR = os.getenv("IMAGE_DIR", "images") 

if os.path.dirname(OUTPUT_PATH):
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
os.makedirs(IMAGE_DIR, exist_ok=True)


def download_image(image_url, image_dir, article_slug):
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–∞ URL —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î –π–æ–≥–æ –ª–æ–∫–∞–ª—å–Ω–æ.
    –ì–µ–Ω–µ—Ä—É—î —É–Ω—ñ–∫–∞–ª—å–Ω–µ —ñ–º'—è —Ñ–∞–π–ª—É, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ slug —Å—Ç–∞—Ç—Ç—ñ —Ç–∞ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ —ñ–º'—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è.
    –û–±—Ä–æ–±–ª—è—î —Ä—ñ–∑–Ω—ñ —Ä–µ–∂–∏–º–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –ø—Ä–æ–∑–æ—Ä—ñ—Å—Ç—å).
    """
    try:
        response = requests.get(image_url, timeout=10)
        response.raise_for_status()

        image_name_raw = os.path.basename(urlparse(image_url).path)
        image_name_sanitized = re.sub(r'[^\w.-]', '_', image_name_raw)
        url_hash = hashlib.md5(image_url.encode('utf-8')).hexdigest()[:8]
        final_image_name = f"{article_slug}_{url_hash}_{image_name_sanitized}"

        if not '.' in final_image_name or final_image_name.endswith(('.', '_')):
            content_type = response.headers.get('Content-Type', '')
            if 'jpeg' in content_type or 'jpg' in content_type:
                final_image_name += '.jpg'
            elif 'png' in content_type:
                final_image_name += '.png'
            elif 'gif' in content_type:
                final_image_name += '.gif'
            elif 'webp' in content_type:
                final_image_name += '.webp'
            else:
                final_image_name += '.unknown'

        image_path = os.path.join(image_dir, final_image_name)

        if os.path.exists(image_path) and os.path.getsize(image_path) > 0:
            return final_image_name

        img = Image.open(io.BytesIO(response.content))
        if img.mode == 'P' and 'transparency' in img.info:
            img = img.convert('RGBA')
        else:
            img = img.convert('RGB')

        save_format = final_image_name.split('.')[-1].upper()
        if save_format == 'JPG':
            save_format = 'JPEG'
        img.save(image_path, format=save_format) 
        print(f"üñºÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {final_image_name}")
        return final_image_name

    except requests.exceptions.RequestException as e:
        print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è {image_url}: {e}")
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è {image_url}: {e}")
        return None

def get_all_issue_urls():
    """
    –í–∏—è–≤–ª—è—î –≤—Å—ñ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ URL-–∞–¥—Ä–µ—Å–∏ –≤–∏–ø—É—Å–∫—ñ–≤ The Batch, —ñ—Ç–µ—Ä—É—é—á–∏ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó.
    """
    issue_urls = set()
    page_num = 1

    while True:
        paginated_url = f"{ROOT_DOMAIN}/the-batch/page/{page_num}/"
        print(f"üìÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó: {paginated_url}")
        try:
            response = requests.get(paginated_url, timeout=15)
            response.raise_for_status() 
        except requests.exceptions.RequestException as e:
            print(f"‚ùå –°—Ç–æ—Ä—ñ–Ω–∫–∞ {paginated_url} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞ –∞–±–æ –±—ñ–ª—å—à–µ –Ω–µ–º–∞—î —Å—Ç–æ—Ä—ñ–Ω–æ–∫: {e}")
            break 

        soup = BeautifulSoup(response.text, "html.parser")
        
        issues = soup.select("a[href*='/the-batch/issue-'][data-sentry-component='Link']")
        if not issues:
            issues = soup.select("a[href*='/the-batch/issue-']")

        if not issues and page_num > 1:
            print(f"‚ö†Ô∏è –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ –≤–∏–ø—É—Å–∫—É –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {paginated_url}. –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è.")
            break

        found_on_page = False
        for a in issues:
            href = a["href"]
            full_url = urljoin(ROOT_DOMAIN, href)
            if full_url.startswith(f"{ROOT_DOMAIN}/the-batch/issue-") and full_url not in issue_urls:
                issue_urls.add(full_url)
                found_on_page = True
        
        if not found_on_page and page_num > 1:
            print(f"‚ö†Ô∏è –ù–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {paginated_url} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–∏—Ö –≤–∏–ø—É—Å–∫—ñ–≤. –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è.")
            break

        page_num += 1

    print(f"üîé –í–∏—è–≤–ª—è—î–º–æ {len(issue_urls)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö URL-–∞–¥—Ä–µ—Å –≤–∏–ø—É—Å–∫—ñ–≤.")
    return list(issue_urls)

def parse_single_issue_page(url):
    """
    –ü–∞—Ä—Å–∏—Ç—å –æ–¥–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É –≤–∏–ø—É—Å–∫—É The Batch (—è–∫–∞, —è–∫ –ø—Ä–∞–≤–∏–ª–æ, –º—ñ—Å—Ç–∏—Ç—å –æ–¥–Ω—É –≤–µ–ª–∏–∫—É —Å—Ç–∞—Ç—Ç—é/—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω–∏–π –±—é–ª–µ—Ç–µ–Ω—å).
    """
    print(f"\n--- –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –≤–∏–ø—É—Å–∫—É: {url} ---")
    try:
        response = requests.get(url, timeout=15)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ {url}: {e}")
        return None 

    soup = BeautifulSoup(response.text, "html.parser")

    article_data = {
        "url": url, 
        "title": "",
        "date": "",
        "reading_time": "",
        "content": "", 
        "images": [] 
    }
    
    title_element = soup.select_one("h1.post-full-title") or \
                    soup.select_one("h1.article-title") or \
                    soup.select_one("h1") or \
                    soup.select_one("meta[property='og:title']")
    if title_element:
        article_data["title"] = title_element['content'] if title_element.name == 'meta' else title_element.get_text(strip=True)
        print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫: '{article_data['title']}'")
    else:
        print(f"‚ö†Ô∏è –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –¥–ª—è {url}")

    date_time_elements = soup.select("time.post-full-meta-date, span.byline-date, meta[property='article:published_time'], p, div, span, li")
    
    for elem in date_time_elements:
        text = elem.get_text(strip=True)
        
        date_match = re.search(r"(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{1,2},\s+\d{4}", text)
        if date_match and not article_data["date"]:
            article_data["date"] = date_match.group(0)
            print(f"DEBUG: –ó–Ω–∞–π–¥–µ–Ω–æ –¥–∞—Ç—É: '{article_data['date']}' –∑ —Ç–µ–∫—Å—Ç—É: '{text}'")
        
        time_match = re.search(r"(\d+)\s+min read", text, re.IGNORECASE)
        if time_match and not article_data["reading_time"]:
            article_data["reading_time"] = time_match.group(0)
            print(f"DEBUG: –ó–Ω–∞–π–¥–µ–Ω–æ —á–∞—Å —á–∏—Ç–∞–Ω–Ω—è: '{article_data['reading_time']}' –∑ —Ç–µ–∫—Å—Ç—É: '{text}'")
        
        if article_data["date"] and article_data["reading_time"]:
            break 

    main_article_content_div = soup.find('div', class_='prose--styled justify-self-center post_postContent__wGZtc')
    
    if not main_article_content_div:
        content_div_selectors = [
            'div.gh-content',               
            'div.kg-card-markdown',         
            'section.post-full-content',    
            'div.entry-content',            
            'div.post-content',             
            'div.article-body',             
            'div.blog-post-content',        
            'div[itemprop="articleBody"]',  
            'div#article-content',          
            'div.article-content',          
            'div.content-main',             
            'article',                      
            'main'                          
        ]
        for selector in content_div_selectors:
            main_article_content_div = soup.select_one(selector)
            if main_article_content_div:
                print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ –æ—Å–Ω–æ–≤–Ω–∏–π div –∫–æ–Ω—Ç–µ–Ω—Ç—É –∑–∞ –∑–∞–ø–∞—Å–Ω–∏–º —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: '{selector}'")
                break

    if not main_article_content_div:
        print(f"üö´ –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–∏–π div –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–ª—è {url}. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Å—Ç–∞—Ç—Ç—é.")
        return None

    article_content_parts = []
    images_in_content = []
    article_slug = re.sub(r'[^\w-]', '', article_data["title"].lower()[:50] or hashlib.md5(url.encode('utf-8')).hexdigest()[:8])

    for child in main_article_content_div.children:
        if child.name == 'h2' and child.get('id') == 'news':
            print("INFO: –ó–Ω–∞–π–¥–µ–Ω–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ 'News'. –ü—Ä–∏–ø–∏–Ω—è—î–º–æ –ø–∞—Ä—Å–∏–Ω–≥ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É.")
            break 
        
        if child.name == 'p' and not child.get('class'): 
            text = child.get_text(separator='\n', strip=True)
            if text:
                article_content_parts.append(text)
        
        elif child.name == 'figure' and 'kg-card' in child.get('class', []) and 'kg-image-card' in child.get('class', []):
            img_tag = child.find('img')
            if img_tag and img_tag.get('src'):
                src = img_tag.get('src')
                if not src or src.startswith('data:'):
                    continue
                full_img_url = urljoin(url, src)
                
                if any(x in full_img_url.lower() for x in ['icons', 'logo', 'avatar', '.svg', 'ads']):
                    continue
                
                downloaded_name = download_image(full_img_url, IMAGE_DIR, article_slug)
                if downloaded_name:
                    images_in_content.append(downloaded_name)
                    caption_tag = child.find('figcaption')
                    if caption_tag:
                        caption_text = caption_tag.get_text(strip=True)
                        if caption_text:
                            article_content_parts.append(f"Image Caption: {caption_text}")
        
        elif child.name in ['h1', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'div', 'blockquote']:
            text = child.get_text(separator='\n', strip=True)
            if text:
                article_content_parts.append(text)


    article_data["content"] = "\n\n".join(article_content_parts)
    article_data["images"] = images_in_content
    print(f"üñºÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(images_in_content)} –∑–æ–±—Ä–∞–∂–µ–Ω—å.")

    if not article_data["content"].strip():
        print(f"üö´ –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–Ω–∞—á—É—â–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤–º—ñ—Å—Ç—É –¥–ª—è {url}. –¶–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ—Ä–æ–∂–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –∞–±–æ –ø—Ä–æ–±–ª–µ–º–∞ –∑ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏ —Ç–µ–∫—Å—Ç—É/—Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—î—é.")
    else:
        print(f"‚úÖ –ó—ñ–±—Ä–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤–∏–π –≤–º—ñ—Å—Ç (–¥–æ–≤–∂–∏–Ω–∞: {len(article_data['content'])} —Å–∏–º–≤–æ–ª—ñ–≤). –ü–æ—á–∞—Ç–æ–∫ —Ç–µ–∫—Å—Ç—É: {article_data['content'][:500]}...")

    if article_data["title"].strip() or article_data["content"].strip():
        return article_data
    else:
        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É –≤–∏–ø—É—Å–∫—É {url} —á–µ—Ä–µ–∑ –≤—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Ç–∞ –∑–Ω–∞—á–Ω–æ–≥–æ –≤–º—ñ—Å—Ç—É.")
        return None

def main():
    all_articles = []

    if os.path.exists(OUTPUT_PATH):
        os.remove(OUTPUT_PATH)
        print(f"–í–∏–¥–∞–ª–µ–Ω–æ —ñ—Å–Ω—É—é—á–∏–π {OUTPUT_PATH} –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Å–∫—Ä–∞–ø—ñ–Ω–≥—É.")

    if os.path.exists(IMAGE_DIR):
        import shutil
        shutil.rmtree(IMAGE_DIR)
        print(f"–í–∏–¥–∞–ª–µ–Ω–æ —ñ—Å–Ω—É—é—á–∏–π –≤–º—ñ—Å—Ç {IMAGE_DIR} –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Å–∫—Ä–∞–ø—ñ–Ω–≥—É –∑–æ–±—Ä–∞–∂–µ–Ω—å.")
    os.makedirs(IMAGE_DIR, exist_ok=True) 

    issue_urls = get_all_issue_urls()
    if not issue_urls:
        print("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–∏—Ö URL-–∞–¥—Ä–µ—Å –≤–∏–ø—É—Å–∫—ñ–≤. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ ROOT_DOMAIN –∞–±–æ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –Ü–Ω—Ç–µ—Ä–Ω–µ—Ç—É.")
        return

    print(f"üîé –ó–Ω–∞–π–¥–µ–Ω–æ {len(issue_urls)} –≤–∏–ø—É—Å–∫—ñ–≤. –ü–æ—á–∏–Ω–∞—î–º–æ –æ–±—Ä–æ–±–∫—É...")
    successful_articles_count = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor: 
        futures = {executor.submit(parse_single_issue_page, url): url for url in issue_urls}
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="–û–±—Ä–æ–±–∫–∞ –≤–∏–ø—É—Å–∫—ñ–≤ The Batch"):
            article_from_issue = future.result()
            if article_from_issue:
                all_articles.append(article_from_issue)
                successful_articles_count += 1

    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(all_articles, f, indent=2, ensure_ascii=False)
    
    print(f"\n--- –ü—ñ–¥—Å—É–º–æ–∫ —Å–∫—Ä–∞–ø—ñ–Ω–≥—É ---")
    print(f"–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —É—Å–ø—ñ—à–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π: {successful_articles_count}")
    print(f"–î–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É: {OUTPUT_PATH}")
    print(f"–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É: {IMAGE_DIR}")

if __name__ == "__main__":
    main()
