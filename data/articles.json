[
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-248/",
    "title": "issue 248",
    "date": "",
    "reading_time": "",
    "content": "Last week, I spoke about AI and regulation at the U.S. Capitol at an event that was attended by legislative and business leaders. I’m encouraged by the progress the open source community has made fending off regulations that would have stifled innovation. But opponents of open source are continuing to shift their arguments, with the latest worries centering on open source's impact on national security. I hope we’ll all keep protecting open source!\n\nBased on my conversations with legislators, I’m encouraged by the progress the U.S. federal government has made getting a realistic grasp of AI’s risks. To be clear, guardrails are needed. But they should be applied to AI applications, not to general-purpose AI technology.\n\nNonetheless, as Iwrotepreviously, some companies are eager to limit open source, possibly to protect the value of massive investments they’ve made in proprietary models and to deter competitors. It has been fascinating to watch their arguments change over time.\n\nFor instance, about 12 months ago, the Center For AI Safety’s “Statement on AI Risk” warned that AI could cause human extinction and stoked fears of AI taking over. This alarmed leaders in Washington. But many people in AI pointed out that this dystopian science-fiction scenario has little basis in reality. About six months later, when Itestifiedat the U.S. Senate’s AI Insight forum, legislators no longer worried much about an AI takeover.\n\nThen the opponents of open source shifted gears. Their leading argument shifted to the risk of AI helping to create bioweapons. Soon afterward,OpenAIandRANDshowed that current AI does not significantly increase the ability of malefactors to build bioweapons. This fear of AI-enabled bioweapons has diminished. To be sure, the possibility that bad actors could use bioweapons — with or without AI — remains a topic of great international concern.\n\nThe latest argument for blocking open source AI has shifted to national security. AI is useful for both economic competition and warfare, and open source opponents say the U.S. should make sure its adversaries don’t have access to the latest foundation models. While I don’t want authoritarian governments to use AI, particularly to wage unjust wars, the LLM cat is out of the bag, and authoritarian countries will fill the vacuum if democratic nations limit access. When, some day, a child somewhere asks an AI system questions about democracy, the role of a free press, or the function of an independent judiciary in preserving the rule of law, I would like the AI to reflect democratic values rather than favor authoritarian leaders’ goals over, say, human rights.\n\nI came away from Washington optimistic about the progress we’ve made. A  year ago, legislators seemed to me to spend 80% of their time talking about guardrails for AI and 20% about investing in innovation. I was delighted that the ratio has flipped, and there was far more talk of investing in innovation.\n\nLooking beyond the U.S. federal government, there are many jurisdictions globally. Unfortunately, arguments in favor of  regulations that would stifle AI development continue to proliferate. But I’ve learned from my trips to Washington and other nations’ capitals that talking to regulators does have an impact. If you get a chance to talk to a regulator at any level, I hope you’ll do what you can to help governments better understand AI.\n\nKeep learning,Andrew\n\nP.S. Two new short courses!\n\nGitHub Copilot’s latest features are designed to help manage software development from plan to pull request.\n\nWhat’s new:GitHubunveileda preview of Copilot Workspace, a generative development environment that’s designed to encompass entire projects. Users can sign up for awaitlistto gain access to Workspace until the preview ends. Afterward, Copilot Workspace will be available to subscribers to GitHub Copilot (which starts at $10 per month for individuals and $19 per month for businesses).\n\nHow it works:Copilot Workspace is based on GPT-4 Turbo and integrated with GitHub code repositories and libraries. Where GitHub Copilot previously generated code snippets and provided suggestions for editing code segments, Copilot Workspace integrates these tasks within a larger plan.\n\nYes, but:Initial usersnotedthat Copilot Workspace is best at solving straightforward, well defined problems and struggles with more complex ones. Choices can be difficult to unwind later on, and the system is slower than simpler AI coding assistants.\n\nBehind the news:Generative coding assistants quickly have become central tools for software development. Copilot hasattracted1.3 million paid subscribers as of April 2024, including 50,000 businesses. Amazon’sQ Developer(formerly CodeWhisperer), Google’sGemini Code Assist(formerly Duet AI), andCursoroffer coding companions that integrate with or fork popular integrated development environments like Microsoft’s VSCode. On the frontier areagentic toolsthat plan and carry out complex, multi-step coding tasks.Why it matters:Copilot Workspace attempts to extend Copilot’s code-completion and chat capabilities to a wider swath of the software development cycle. Simpler coding assistants have been shown toboostproductivity markedly. Bringing natural-language prompting to tasks like planning, testing, and reading documentation is a natural step.\n\nWe’re thinking:There are many ways to use AI in coding. To learn about a few more, check out our short course, “Pair Programming With a Large Language Model,” taught by Google AI advocate Laurence Moroney.\n\nOpenAI has been making deals with publishers to gain access to high-quality training data. It addedFinancial Timesto the list.\n\nWhat’s new:OpenAIlicensedthe archive of business news owned byFinancial Times(FT) for an undisclosed sum. The agreement lets OpenAI train its models on the publisher’s articles and deliver information gleaned from them. This is OpenAI’s fifth such agreement with major news publishers in the past year.\n\nHow it works:Although the parties didn’t disclose the length of their agreement, OpenAI’s other news licensing deals will end within a few years. The limited commitment suggests that these arrangements are experimental rather than strategic. The deal includes articles behind the publisher’s paywall; that is, not freely available on the open internet. This enables OpenAI to train its models on material that competitors may not have. Other deals have given OpenAI exclusive access, shutting competitors out.\n\nBehind the news:Archives of news articles may be handy if OpenAI proceeds with a rumored search servicereportedby in February byThe Information. Licensing is a way to get such material that is unambiguously legal. Although AI researchers commonly scrape data from the web and use it for training models without obtaining licenses for copyrighted works, whether a license is required to train AI models on works under copyright in the U.S. has yet to be determined. Copyright owners lately have challenged this practice in court. In December 2023,The New York TimessuedOpenAI and Microsoft, claiming that OpenAI infringed its copyrights by training models on its articles. In April 2024, eight U.S. newspapers owned by Alden Global Capital, a hedge fund,fileda lawsuit against the same defendants on similar grounds. Licensing material from publishers gives OpenAI access to their works while offering them incentives to negotiate rather than sue.Why it matters:AI developers need huge amounts of media to train larger and larger models. News publishers have huge archives with high-quality text, relatively well written and fact-checked, that’s relevant to current events of interest to a broad audience. Licensing those archives gives developers access to what they need without incurring legal risk. Furthermore, making news archives available for retrieval augmented generation makes chatbots more capable and reliable.\n\nWe’re thinking:We support efforts to clarify the legal status of training AI models on data scraped from the web. It makes sense to treat the open web pages and paywalled content differently, but we advocate that AI models be free to learn from the open internet just as humans can.\n\nExplore the newest additions to our short courses with “Quantization in Depth,” where you’ll build a quantizer in PyTorch, and “Building Agentic RAG with LlamaIndex,” which teaches how to build agents capable of tool use, reasoning, and making decisions based on your data.Sign up now!\n\nAn AI system is scouring battlefields for landmines and other unexploded ordnance, enabling specialists to defuse them.\n\nWhat’s new:The military hardware firm Safe Pro Group developed Spotlight AI, a computer vision system that identifies mines based on aerial imagery,IEEE Spectrumreported. Nongovernmental organizations that remove landmines, including the Norwegian People's Aid and the HALO Trust, are using the system in Ukraine.\n\nHow it works:SpotlightAI processes visual-light imagery taken by flying drones. The system provides centimeter-resolution maps that guide mine-removal teams through the territory.\n\nBehind the news:In addition to drones, satellites can help machine learning models to find deadly remnants of warfare. In 2020, Ohio State University researchersestimatedthe number of undetonated explosives in Cambodia by collating bomb craters in satellite images identified by a computer vision model with records of U.S. military bombing campaigns in that country in the 1960s and 1970s.\n\nWhy it matters:Unexploded mines, bombs, and other types of munitionskilled or injuredmore than 4,700 people — 85 percent of them civilians and half of them children where military status and age were known — in 2022 alone. Efforts to remove every last mine from a former battlefield likely will continue to rely on traditional methods — manual analysis of overhead imagery along with sweeps by human specialists and explosive-sniffing dogs — but machine learning can significantly reduce the hazard and accelerate the work.\n\nWe’re thinking:Although this system locates unexploded mines and shells, removing them often still falls to a brave human. We hope for speedy progress in robots that can take on this work as well.\n\nIt’s not necessary to activate all parts of a large language model to process a given input. Using only the necessary parts saves processing.\n\nWhat’s new:Zichang Liu and collaborators at Rice University, Zhe Jiang University, Stanford, University of California San Diego, ETH Zürich, Adobe, Meta, and Carnegie Mellon proposedDeja Vu, an algorithm that accelerates inferencing of large language models (LLMs) by using small vanilla neural networks to predict which parts of it to use.\n\nKey insight:Transformer-based neural networks can save a lot of time at inference by activating only a fraction of (i) attention heads and (ii) neurons in fully connected layers. But it’s necessary to activate the right neurons, because different parts of the network learn about different patterns of inputs. By using the input to decide which parts of the network to activate, the network can maintain accuracy using only the parts relevant for the current input.\n\nHow it works:The authors used pretrainedOPTmodels of various sizes (175, 66, and 30 billion parameters). They built a dataset by feeding examples fromOpenBookQAandWiki-Textto the OPTs and recording the outputs of all attention heads and fully-connected-layer neurons. By activating various portions of these networks, they learned that, for a given input, they could discard most of an OPT’s lowest-output attention heads and fully-connected-layer neurons without degrading its performance.\n\nResults:Deja Vu (175 billion parameters) produced a sequence of 128 tokens in 20 milliseconds, while an Nvidia implementation of OPT of the same size needed 40 milliseconds and a Hugging Face implementation of OPT of the same size needed 105 milliseconds. Moreover, Deja Vu achieved these speedups without reducing accuracy. OnWikiTextandC4, Deja Vu’s ability to predict the next word held steady while activating 25 percent of attention heads and fully-connected-layer neurons. On datasets such asWinoGrandeandOpenBookQA, it maintained its accuracy while activating 35 percent of attention heads and fully-connected-layer neurons.\n\nWhy it matters:Efficient use of processing power becomes increasingly important as models become larger. Moreover, faster token generation benefits agentic workflows, which can consume large numbers of tokens.\n\nWe’re thinking:Deja Vu’s design is in the spirit of the mixture of experts (MoE) architecture: For each transformer layer, MoE uses a neural-network layer to choose which fully connected layer to use. In contrast, for each attention head and fully-connected-layer neuron, Deja Vu uses small neural networks to decide which to activate.\n\nNew course with Qualcomm coming soon! In “Introduction to On-Device AI,” you’ll learn how to deploy AI models on edge devices using local computation for faster inference and privacy. Join the next wave of AI as models go beyond the cloud.Sign up for the waitlist!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-08T152327.782-1.png",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-08T152519.838-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed--58-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/05/4--3-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-08T164752.068.png",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-08T164933.555.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/05/V4_Waitlist_DeepLearning_Qualcomm_C1_Banner_2070x1080.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-202/",
    "title": "issue 202",
    "date": "",
    "reading_time": "",
    "content": "I spent Sunday through Tuesday at the CVPR computer vision conference in Vancouver, Canada, along with over 4,000 other attendees. With the easing of the pandemic, it’s fantastic that large conferences are being held in person again!\n\nThere’s a lot of energy in computer vision right now. As I recall, the natural language processing community was buzzing about transformers a couple of years before ChatGPT revolutionized the field more publicly. At CVPR, I sensed similar excitement in the air with respect to computer vision. It feels like major breakthroughs are coming.\n\nIt is impossible to summarize hundreds of papers into a single letter, but I want to share some trends that I’m excited about:\n\nVision transformers:The Batchhas covered vision transformers extensively, and it feels like they’re still gaining momentum. The vision transformerpaperwas published in 2020, and already this architecture has become a solid alternative to the convolutional neural network. There are complexities still to be worked out, however. For example, whereas turning a piece of text into a sequence of tokens is relatively straightforward, many decisions need to be made (such as splitting an image into patches, masking, and so on) to turn an image processing problem into a token prediction problem. Many researchers are exploring different alternatives.\n\nImage generation:Algorithms for generating images have been a growing part of CVPR since the emergence of GANs and then diffusion models. This year, I saw a lot of creative work on editing images and giving users more fine-grained control over what such models generate. I also saw a lot of work on generating faces, which is not surprising, since faces interest people.\n\nNeRF:This approach to generating a 3D scene from a set of 2D images has been taking off for a while (and also covered extensively inThe Batch). Still, I was surprised at the large number of papers on NeRF. Researchers are working to scale up NeRF to larger scenes, make it run more efficiently, handle moving scenes, work with a smaller number of input images, and so on.\n\nAlthough it was less pronounced than excitement around the topics above, I also noticed increased interest in multimodal models. Specifically, given that a transformer can convert either an image or a piece of text into a sequence of tokens, you can feed both types of tokens into the same transformer model to have it process inputs that include both images and text. Many teams are exploring architectures like this.\n\nLastly, even though the roadmap to self-driving cars has been longer than many people expected, there remains a lot of research in this area. I think the rise of large, pretrained transformers will help kickstart breakthroughs in self-driving.\n\nI also spoke at the CVPR conference’s workshop onComputer Vision in the Wildabout Landing AI’s work on making computer vision easy, withvisual promptingas a key component. (Thank you Jianwei Yang, Jianfeng Gao, and the other organizers for inviting me!) After my presentation, speaking with many users of computer vision, it struck me that there’s still a gap between the problems studied/benchmarks used in academic research and commercial practice. For example,test setsare more important in academic research than in practical applications; I will write more about this topic in the future.\n\nTo everyone I met in person at CVPR: Thank you! Meeting so many people made this trip a real highlight for me.Keep learning!\n\nAndrew\n\nGenerative AI could add between $2.6 trillion and $4.4 trillion to the global economy annually (roughly 2 percent to 4 percent of the world’s combined gross domestic product this year), according to a new report.\n\nWhat's new:The management consultancy McKinseyprojectedgenerative AI’s impacts on productivity, automation, and the workforce in a new report.\n\nHow it works:The authors examined adoption scenarios between 2040 and 2060 and their effect on labor productivity through 2040. They evaluated the business impact of generative AI use cases — for instance, large language models applied to customer service — and estimated the economic value those cases would create if they were applied globally. They also assessed the technology’s potential to automate tasks in roughly850 occupationsbased on an occupation’s sensory, cognitive, physical, language, and social requirements.\n\nBehind the news:Generative AI’s potential to displace human workers is causing substantial anxiety among the general public. A recent CNBC survey of 8,874 U.S. workersfoundthat 24 percent of respondents were “very worried” or “somewhat worried” that AI would make their jobs obsolete. Respondents were more likely to worry if they were younger (32 percent of respondents of age 18 to 24 compared to 14 percent of those 65 or older), identified as part of a minority (38 percent of Asian respondents, 35 percent of Hispanic respondents, and 32 percent of black respondents versus 19 percent of white respondents), or earned a relatively low income (30 percent of respondents who earn less than $50,000 annually versus 16 percent of those who earn more than $150,000).\n\nYes, but:As the saying goes, it’s difficult to make predictions, especially about the future. A decade after a 2013 Oxford University studypredictedthat 47 percent of U.S. jobs were at risk of automation, the U.S. unemployment rate is nearly at recordlows. A 2022 study found that employment rates haverisenin occupations previously believed to be at risk from AI and robotics.\n\nWhy it matters:Generative AI already is having a noticeableeffecton venture investments. This analysis indicates that current changes may herald disruptive impacts to come.\n\nWe're thinking:Prospective economic gains are good news, but they should be considered in a broader context. We see a realriskthat AI may become so good at automating human work that many people will find themselves unable to generate substantial economic value. The best path forward is to democratize the technology so everyone can benefit and make sensible decisions together.\n\nWhy it matters:Generative AI already is having a noticeableeffecton venture investments. This analysis indicates that current changes may herald disruptive impacts to come.\n\nWe're thinking:Prospective economic gains are good news, but they should be considered in a broader context. We see a realriskthat AI may become so good at automating human work that many people will find themselves unable to generate substantial economic value. The best path forward is to democratize the technology so everyone can benefit and make sensible decisions together.\n\nTesla cars operating semi-autonomously have had many more collisions than previously reported, government data shows.\n\nWhat's new:Tesla vehicles operating in the so-called Autopilot or Full Self-Driving mode were involved in 736 U.S. crashes between sometime in 2019 and May 2023, according to data gathered by the United States National Highway Traffic Safety Administration (NHTSA),The Washington Postreported.Earlier datashowed that Teslas had been involved in 273 reported crashes between July 2021 and July 2022. The latest data is available at the bottom of thislink.\n\nHow it works:Tesla offers two semi-autonomous driving modes.\n\nThe crashes:The NHTSA data is difficult to interpret, since it omits crucial variables such as miles driven and which of Tesla’s two modes was involved in any given crash. Moreover, the earlier and recent crash tallies are difficult to compare due to the difference in their time frames.\n\nBehind the news:Since August 2021, NHTSA hasopenednumerous probes into Tesla’s autonomous systems. Repeated incidents under investigation include abrupt braking in the path of following vehicles; collisions with emergency vehicles; and allegations that, in multiple crashes, Autopilot disengaged less than a second before the collision, giving drivers little time to react.\n\nWhy it matters:Tesla has claimed repeatedly that its autonomous driving capability is far safer than human drivers. Without knowing which mode was involved in how many crashes over how many miles, that claim is impossible to verify. Meanwhile, there are indications that Tesla may have deliberatelymisledthe public about its self-driving capabilities in the past.\n\nWe're thinking:Engineers who work on systems that are critical to safety have a special responsibility to make sure their products are safe and well understood by users. We urge Tesla engineers to shed more light on the performance of these potentially life-threatening systems.\n\nWant to build computer vision into your applications? Train a model in LandingLens (get started for free), then use theLanding AI SDKto easily build custom applications that leverage your model!\n\nA United States federal judge threw ChatGPT’s legal research out of court.\n\nWhat’s new:An attorney who used ChatGPT to generate a legal brief faces disciplinary action after opposing lawyers discovered that the brief referred to fictional cases and quotations invented by the chatbot,The New York Timesreported.Citation situation:The lawyer, Steven A. Schwartz, was assisting in a personal injury lawsuit on the plaintiff’s side in a federal court in New York City. When the defendant appealed to have the case dismissed, Schwartz countered with a brief based on results from a ChatGPT query.\n\nRipple effects:In the case’s wake of this case, a federal judge in Texasdecreedthat lawyers in cases before him may use generative AI to write their briefs only if they file paperwork stating that they manually verified the output for accuracy.\n\nWhy it matters:Within the AI community, it may be common knowledge that large language models sometimes confidently state falsehoods as though they were true. Among the general public, though, this fact may not be so well understood. Schwartz’s mishap is a painfully public demonstration of what can happen when people trust such models to supply facts.We’re thinking:People outside the AI community might reasonably assume that the technology is qualified to assist in legal research.  After all, in April, GPT-4, the large language model behind the most powerful version of ChatGPT, reportedlyranked in the 90th percentileon a U.S. bar exam. (A recent reappraisalrevisedGPT-4’s score downward to between the 68th and 48th percentiles.) This goes to show that AI performance on these tests doesn’t necessarily map well to human performance, since any junior law student would know not to invent cases. There’s important work to be done to apply LLMs to legal work. Meanwhile, we urge researchers who are testing LLMs’ ability to meet real-world qualifications to resist hype when reporting their results.\n\nA pretrained text-to-image generator enabled researchers to see — roughly — what other people looked at based on brain scans.\n\nWhat's new:Yu Takagi and Shinji Nishimoto developed amethodthat uses Stable Diffusion to reconstruct images viewed by test subjects from scans of their brains that were taken while they were looking at the images.\n\nDiffusion model basics:During training, a text-to-image generator based on diffusion takes a noisy image and a text description. A model embeds the description, and a diffusion model learns to use the embedding to remove the noise in successive steps. At inference, the system starts with pure noise and a text description, and iteratively removes noise according to the text to generate an image. A variant known as alatent diffusion modelsaves computation by embedding the image as well and removing noise from noisy versions of the embedding instead of a noisy image.\n\nKey insight:Stable Diffusion, like other latent diffusion text-to-image generators, uses separate embeddings of corresponding images and text descriptions to generate an image. In an analogous way, the region of the human brain that processes input from the eyes can be divided into areas that process the input’s purely sensory and semantic aspects respectively. In brain scans produced by functional magnetic resonance imaging (fMRI), which depicts cortical blood oxygenation and thus indicates neuron activity, these areas can be embedded separately to substitute for the usual image and text embeddings. Given these embeddings, Stable Diffusion can generate an image similar to what a person was looking at when their brain was scanned.\n\nHow it works:The authors trained a simple system to produce input for Stable Diffusion based on fMRI. They trained a separate version of the system for each of four subjects whose brains were scanned as they looked at 10,000 images ofnatural scenes.\n\nResults:The authors concluded that their approach differed so much from previousworkthat quantitative comparisons weren’t helpful. Qualitatively, the generated images for all four subjects depict roughly the same scenes as the ground-truth images, though the details differ. For instance, compared to the ground-truth image of an airplane, the generated images appear to show something airplane-shaped but with oddly shaped windows, a cloudier sky, and blurred edges.\n\nWhy it matters:Previous efforts to reproduce visual images from brain scans required training a large neural network. In this case, the authors trained a pair of simple linear models and used a large pretrained model to do the heavy lifting.\n\nWe’re thinking:The generated images from models trained on brain scans of different subjects showed different details. The authors suggest that this disagreement may have arisen from differences in the subjects’ perceptions or differences in data quality. On the contrary, they may relate to the noise added during image generation.\n\nWhat stands in the way of Nvidia competitors? NvidiaDespite attempts by companies like AMD to introduce new AI chips, Nvidia's market presence and technological advantage have widened the gap in sales and performance. (Financial Times)Google Shopping launched AI featuresNewly available to U.S. users: A generative AI-powered virtual try-on feature and the option to refine a product search with visual matching. (The Verge)European Parliament approved the AI ActThe vote marks a milestone in an ongoing process. Negotiations among EU representatives will determine the final version of the proposal, which is expected to be finalized by the end of the year. (The New York Times)EU launch of Google's Bard delayed over privacy issuesThe chatbot is available in 180 new countries except for EU nations due to the company's failure to address privacy concerns raised by the Irish Data Protection Commission. (The Verge)Paul McCartney is using AI to produce new Beatles recordDuring an interview, the singer said that his team is using AI tools to extract John Lennon's voice from an old demo and incorporate it into an upcoming track. (The Verge)Adobe enhances Illustrator with generative AIA new Generative Recolor feature is available in beta. It allows users to use text prompts to change colors and fonts in graphics. (Forbes)OpenAI sued for defamation after ChatGPT fabricated accusationsA radio host filed a lawsuit against OpenAI in a Georgia state super court. He alleges that the chatbot made false allegations against him when a journalist used it to summarize a different, federal court case. (The Verge)Media giants discussing potential protections against generative AINews organizations including The New York Times and NBC News are holding talks about safeguards and rules to protect their content against aggregation and misrepresentation by generative AI tools. (CNBC)Artist used Stable Diffusion to generate illustrations that contain hidden QR codesThe artist trained custom Stable Diffusion ControlNet models to develop functional QR codes that masquerade as anime and other Asian art styles. (Ars Technica)Japanese city adopted ChatGPT in administrative operationsAfter a successful one-month trial, Yokosuka will start using the tool to summarize meetings, edit documents, and other tasks, aiming to improve efficiency and shorten business hours. (Japan Times)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/06/ezgif.com-webp-to-jpg--13--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/06/unnamed--31-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/06/TESLA-Crashes-increase2_1200px.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/06/The-Batch-ads-and-exclusive-banners--35-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/06/GPTLAW_Scroll_600px--2-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/06/unnamed--66-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-222/",
    "title": "issue 222",
    "date": "",
    "reading_time": "",
    "content": "The past week has been an unusual split-screen time in AI. On one side, I see rapidly developing innovations from OpenAI, as well as Elon Musk's Grok and Kai-Fu Lee's open source Yi-34B large language model. On the other side, theWhite Houseand participants in last week’s AI Safety Summit are making regulations that may slow down AI by stifling innovation and limiting open source.\n\nAs you can read below, OpenAI held a developer day in which it announced numerous tools for developers. I found the sheer volume of new features impressive. OpenAI is continuing to demonstrate that it is a fast moving company capable of shipping quickly!\n\nThe speed with which OpenAI moves is a lesson for other businesses. We live in an era when tools available to developers are improving quickly, and thus the set of things we can build with generative AI growing fast. As I describe in my presentation onOpportunities in AI, AI tools often get more attention, but ultimately AI applications have to generate more revenue than tools for the market to succeed.\n\nIn my experience, speed in decision-making and execution is a huge predictor of startup success. Bearing in mind the importance of responsible AI, I respect leaders and teams that make decisions and execute quickly. In contrast, I’ve also seen companies where shipping a feature can require 3 months for legal, marketing, and privacy review. Systemically forcing yourself to make a decision quickly rather than calling another meeting to talk about a topic some more (unless it’s really necessary) can push an organization to move faster.\n\nWhen you move fast, not everything will work out. For example, when OpenAI launches so many new features (including highly innovative ones like the GPT store, where developers can distribute special-purpose chatbots as though they were mobile apps), it’s possible that not every one of them will take off. But the sheer speed and volume of execution makes it likely that some of these bets will pay off handsomely. And not only for OpenAI, but also for developers who use the new features to build new products.\n\nIf you’re a developer, the rapidly growing set of tools at your disposal gives you an opportunity to execute quickly and build things that have never existed before, largely because the tools to build them didn’t exist. I believe this is the time to keep building quickly — and responsibly — and accelerate the pace at which we bring new applications to everyone.\n\nKeep learning!\n\nAndrew\n\nP.S. Vector databases are a backbone of large language model (LLM) search and data-retrieval systems, for example in retrieval augmented generation (RAG). In our new short course, created with Weaviate and taught by Sebastian Witalec, you’ll learn the technical foundations of how vector databases work and how to incorporate them in your LLM applications. You’ll also learn to build RAG and search applications. I invite you to sign up for “Vector Databases: from Embeddings to Applications”!\n\nOpenAI added new features designed to help developers build applications using its generative models.What’s new:OpenAIintroduceda plethora of capabilities at its first developer conference in San Francisco.\n\nUpgrades and more:The company rolled out the upgraded GPT-4 Turbo (which now underpins ChatGPT). It extended API access to its DALL·E 3 image generator, text-to-speech engine, speech recognition, and agent-style capabilities. And it showed off a new concept in chatbots called GPTs.\n\nWhy it matters:OpenAI is enabling developers to build intelligence into an ever wider range of applications. GPT-4 Turbo's 128,000-token context window makes possible applications that require tracking information across huge volumes of input. The expanded APIs open up language, vision, and multimodal capabilities as well as agent-style applications that respond to changing conditions and behave in complex ways. The opportunities for developers are immense.We’re thinking:It’s amazing to see cutting-edge AI developments become widely available so quickly. Early on, OpenAI withheld its work out of fear that it could be misused. But that policy clearly no longer holds. “We believe that gradual iterative deployment is the best way to address safety challenges of AI,” OpenAI CEO Sam Altman said in hiskeynote. Based on the evidence to date, we agree.\n\nAn international conference of political leaders and tech executives agreed to regulate AI.\n\nWhat’s new:28 countries including China and the United States as well as the European Union signed adeclarationaimed at mitigating AI risks.\n\nHow it works:The declaration kicked off the United Kingdom’s first AI Safety Summit at Bletchley Park, a country house outside London, where Alan Turing and others cracked Germany’s Enigma code during World War II.\n\nMore to come:The AI Safety Summit is set to be the first in a series. South Korea willhosta follow-up in six months. France will host a third summit six months later.\n\nYes, but:Critics found the conference wanting. Some researcherscriticizedit for failing to endorse concrete limits on AI. Othersblamedthe speakers for promoting fear, particularly UK prime minister Rishi Sunak, whocomparedthe AI risks to a global pandemic or nuclear war.\n\nWhy it matters:AI is developing rapidly, and regulatory frameworks are already emerging in China, Europe, and the U.S. The summit is an effort to lay groundwork for a coherent international framework.\n\nWe’re thinking:We applaud approaches that engage leaders in government, industry, and research. But we remain concerned that exaggerated fear of risks may lead to regulations that stifle innovation, especially by limiting open source development. UK Deputy Prime Minister Oliver Dowdenspokeabout the value of open source and said there should be a very high bar to restrict open source in any way. We heartily agree!\n\nLearn how to use vector databases with large language models to build applications that include hybrid and multilingual searches! Take our new course, “Vector Databases: from Embeddings to Applications.”Enroll for free\n\nLarge language models may help psychiatrists resolve unanswered questions about mental illness.\n\nWhat’s new:Researchers from University College London, Beijing Normal University, and Lisbon’s Champalimaud Centre for the Unknown used a large language model tomeasuredifferences in the ways people with schizophrenia use words.Key insight:Neuroscientists theorize that schizophrenia disturbs the brain’s ability to represent concepts. When given a task like “name as many animals as you can in five minutes,” patients with schizophrenia would propose names in a less-predictable order than people who haven’t. In general, the consecutive names produced by people with schizophrenia would be less semantically related than those produced by others.\n\nHow it works:The authors asked 26 people who had been diagnosed with schizophrenia and 26 people who hadn’t to name as many animals as they could in five minutes. They also asked the subjects to name as many words that start with the letter “P” as they could in five minutes.\n\nResults:Responses by subjects with schizophrenia had greater randomness. To control for variations in the contents of various patients’ lists, the researchers expressed the degree of randomness as astandard score, where 0 indicates complete randomness, and the lower the negative number, the more optimal the order. On average, people with schizophrenia achieved -5.81, while people without schizophrenia achieved -7.02.\n\nWhy it matters:The fastText model’s embeddings helped the authors demonstrate a relationship between cognitive activity and psychiatric symptoms that previously was purely theoretical. Such a relationship has been difficult to establish through brain imaging or traditional testing.\n\nWe’re thinking:It’s important to note that the authors don’t propose using their method as a diagnostic tool to determine whether or not a patient has schizophrenia. Unlike diagnosing, say, a cancerous tumor, establishing ground truth in mental illness is extremely complicated. The fact that AI-based measurements agree with doctors’ assessments is a very positive sign.\n\nGenerated images can be more effective than real ones in training a vision model to classify images.\n\nWhat's new:Yonglong Tian, Lijie Fan, and colleagues at Google and MIT introducedStableRep, a self-supervised method that trains vision transformers on images generated by Stability.AI’s Stable Diffusion image generator.\n\nKey insight:Models that employ a contrastive loss learn to represent examples as more or less similar. For example, images that depict a particular object are more similar to each other, and images that depict other objects are less similar to the first group. The training method known asSimCLRuses a contrastive loss with two augmented (cropped, rotated, flipped, and so on) versions of each image, so a model learns that augmented versions of one image, which is closely related but different, are similar to one another — but not to augmented versions of other images. Given a prompt, an image generator produces images that are closely related but significantly more different than augmented versions of the same image. This makes for greater variety among similar examples, which can lead to more effective learning using a contrastive loss.\n\nHow it works:The authors generated images and trained a vision transformer on them using a contrastive loss.\n\nResults:The authors compared the ViT-B/16 trained using StableRep to two models of the same architecture trained using SimCLR (one using generated images, the other using images from Conceptual Captions). They also compared it to two CLIP models that produced matching embeddings for images and their paired captions, one trained on generated images and their prompts, the other on real images and their captions. For each of 11 computer vision datasets, the authors trained a linear classifier on top of each model without changing the model’s weights. Comparing the classifiers’ performance, StableRep achieved the best results on 9 of them. For example, onFGVC-Aircraft(10,000 images of 100 different aircraft), StableRep achieved 57.6 percent accuracy, while the best competing model, CLIP pretrained on generated images, scored 53.5 percent.\n\nWhy it matters:The fact that text-to-image generators can produce images of similar things that are quite different in appearance makes them a powerful resource for training vision models. And they provide a practically unlimited source of such images!\n\nWe're thinking:Different foundation models understand different aspects of the world. It’s exciting that a large diffusion model, which is good at generating images, can be used to train a large vision transformer, which is good at analyzing images!\n\nJoin us for two live workshops! Learn to leverage large language models in these interactive, hands-on sessions. Team registrations are available.Register here\n\nTikTok explores AI-powered product recognition for e-commerce integrationThe social video platform is testing an AI tool that recognizes products in videos and suggests similar items available on TikTok Shop. The product recommendation system, which currently works without notifying the video’s creator, is currently in the experimental stage with limited availability in the U.S. and UK. (Business Insider)\n\nElon Musk’s startup xAI introduced its first model, GrokThe chatbot, now available to X Premium+ subscribers, boasts real-time knowledge from the X platform. Grok also claims to answer unconventional questions that many other AI systems might reject. Further details about Grok's capabilities and features are yet to be specified. (DW)\n\nLibrarians embrace AI as the next chapter in academic supportThere may be no universal AI guidelines for libraries, but librarians are finding ways to educate students about responsible AI use and citation practices. Despite the challenges, librarians across institutions see opportunities for AI to enhance information literacy and educate students on verifying and evaluating information sources.Many librarians, educators, and students view AI as an opportunity to shape the future of academic research. (Inside Higher Ed)\n\nRecruitment agencies leverage AI to enhance talent acquisitionRecruiters are using AI to sift through a vast number of applications and engage with a wider range of candidates, with the goal of exploring diverse talent pools. Chatbots also enhance the initial interaction with job seekers, allowing recruiters to concentrate on building meaningful relationships with advanced candidates. However, increased use of AI also poses ethical and practical problems, such as poor data quality and the risk of perpetuating human biases. (Financial Times)\n\nResearch: The increasing energy footprint of AI may not be sustainableThe surge in generative AI raises concerns about AI’s significant energy consumption. New research argues that AI’s current trajectory could lead to annual electricity consumption equivalent to the entire country of Ireland. Human self-regulation and institutions carefully evaluating AI integration into various solutions will be pivotal in curbing energy consumption and limiting environmental impact. (IEEE Spectrum)\n\nThe United Nations uses AI to analyze Israeli-Palestinian conflictThe UN partnered with AI company CulturePulse to build a virtual simulation of Israel and the Palestinian territories  to analyze the long-standing conflict.  The model employs a multi-agent system to simulate the region, factoring in numerous variables, helping experts pinpoint the root causes of the conflict. The UN hopes this approach will enable a deeper understanding of the complex issue and uncover potential solutions. (Wired)\n\nAI-aided song marks the last Beatles song featuring John Lennon's voiceThe song, “Now and Then,” billed as the last Beatles track, will be part of a double A-side single alongside the band's debut UK single from 1962, \"Love Me Do.\" AI was used to isolate instruments and vocals from the original tape of \"Now and Then,\" recorded by Lennon as a home demo in the late 1970s. (Reuters)\n\nJury finds Tesla's autopilot not responsible for fatal 2019 crashThe case centered on the death of Micah Lee, who was using Autopilot features in his Tesla when it veered off the road, collided with a palm tree, and caught fire. The jury absolved the carmaker of responsibility for the accident. This verdict is significant for Tesla, which is facing multiple lawsuits related to incidents involving its driver-assistance software. (The Washington Post)\n\nBaidu introduces paid version of Ernie BotThe Chinese giant’s chatbot is charging 59.9 yuan (approximately $8.18) per month for access to its premium version, expanding Baidu’s AI offerings in the Chinese market. (Reuters)\n\nJudge reduces artists’ copyright lawsuitagainst Midjourney and Stability AIU.S. District Judge William Orrick dismissed some claims in the proposed class action copyright infringement suit, including all allegations against Midjourney and DeviantArt, but allowed visual artists to file an amended complaint. (Reuters)\n\nA comprehensive resource for teaching and learning with text generation technologiesThe \"TextGenEd\" collection offers a range of assignments and other resources to help writing teachers integrate text generation technologies into their courses. The collection is open access, allowing teachers to adapt and use the assignments, and provides insights from instructors who have implemented them in their classes. The collection also offers a historical perspective on automated and computational writing, from Leibniz's cryptographic machine to modern AI-generated texts. (WAC Clearinghouse)\n\nMicrosoft launches investigation after controversial AI poll on an article about a woman's deathThe controversy arose when an autogenerated poll accompanied an article by The Guardian about a woman's death, asking readers to speculate about the cause of her death. The Guardian's CEO, Anna Bateson, demanded that Microsoft publicly acknowledge its responsibility for the poll and called for safeguards against the inappropriate application of AI technologies in journalism. (Axios)\n\nDeepMind's latest AlphaFold model improves drug discovery with precise molecule predictionsThe Google-affiliated research lab unveiled the latest iteration of its AlphaFold model, which improved its capability to predict molecular structures. This updated version of AlphaFold can generate predictions for a wide range of molecules, including proteins, ligands, nucleic acids, and post-translational modifications. (TechCrunchandGoogle DeepMind)\n\nScarlett Johansson takes legal action against unauthorized use of her image in AI-generated adThe 22-second ad used real footage of Johansson to create a fabricated image and dialogue. Johansson's representatives confirmed that she is not affiliated with the company. The actress had previously voiced concerns about her image being used in deepfakes without consent. (The Guardian)\n\nAI adds layer of uncertainty to Israel-Hamas conflictThe Israel-Hamas conflict has been marked not only by the actual events but also by the presence of potentially manipulated AI-generated media. This erosion of trust in digital content, coupled with the ease of creating persuasive deepfakes, is raising concerns about the impact of synthetic media on public perception. (The New York Times)\n\nCollins Dictionary names “A”' as the most notable word of 2023Collins selected the word from a list of new terms that reflect the evolving language and concerns of its users, including \"greedflation,\" \"debanking,\" \"nepo baby,\" and \"deinfluencing.\" The annual word of the year is determined by lexicographers who monitor various sources to capture the zeitgeist. (The Guardian)\n\nDawn, the supercomputer powered by Intel and DellDell Technologies, in collaboration with Intel and the University of Cambridge, deployed the Dawn Phase 1 supercomputer, which is part of the UK's AI Research Resource (AIRR) initiative. The system is built on Dell PowerEdge XE9640 servers, optimized for AI and HPC workloads, and features direct liquid cooling technology for enhanced efficiency. (Intel)\n\nResearch: AI outperforms biopsy in assessing aggressiveness of certain cancersA study conducted by the Royal Marsden NHS foundation trust and the Institute of Cancer Research (ICR) found that an AI algorithm outperformed biopsies in grading the aggressiveness of sarcomas, a form of cancer that develops in the body's connective tissues. AI's enhanced accuracy can lead to quicker diagnosis and more personalized treatment for high-risk patients, while low-risk patients can avoid unnecessary treatments and follow-up scans. (The Guardian)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/11/SplitScreen_RoadClosed-5_1200px.webp",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--69-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--70-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch-ads-and-exclusive-banners--77-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--71-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--97--1.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch-ads-and-exclusive-banners--78-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-31/",
    "title": "issue 31",
    "date": "",
    "reading_time": "",
    "content": "The unfolding Covid-19 crisis calls for individuals and organizations to step up and contribute to the common good. I believe that the tech community has an important role to play in slowing the progress of the virus and shortening the time it takes society to recover.\n\nIn my neighborhood, I’ve been gratified to see people volunteering on a local messaging app (Nextdoor) to shop for groceries or help out the elderly. We all need to pull together and lend a hand wherever we can.And of course, I hope you will take care of yourselves and your family.\n\nStay safe,\n\nAndrew\n\nMachine learning thrives on data, but information about the novel coronavirus and the illness it produces has been either thin or hard to access. Now researchers are pooling resources to share everything we do know.What’s new:TheWhite Houseand researchers from top U.S. AI and health institutions launchedCORD-19, a free, machine-readable dataset of nearly 30,000 scholarly articles on the coronavirus. Kaggle is hosting acompetitionfor text- and data-mining tools that sift this mountain of information for valuable insights.\n\nPromising directions:Lack of data so far has limited AI’s usefulness in combating this outbreak, but stronger data-collection efforts could prove decisive in the next, according toMIT Technology Review. Author Will Douglas Heaven describes three areas to focus on:\n\nBehind the news:AI spotted the disease early, but humans still beat it to the punch. At least oneChinese doctorposted his concerns about what came to be known as Covid-19 on a WeChat group before AI health monitors issued their alerts. He later died of the virus.Why it matters:AI has great potential to combat epidemics, and hopeful news reports bring attention to and support for the field. The community must work diligently while taking care not to encourage wildly inflated expectations and false hopes.We’re thinking:Covid-19 isn’t the first pandemic, and sadly it won’t be the last. The AI community’s efforts to fight this virus will prove critical when the next one emerges. And there’s plenty we can do outside the medical sphere: Machine learning can help manage critical resources, coordinate responses, and optimize logistics. At this moment of international crisis, we face a common foe that is bigger than any of us, and we’re gratified to see so many AI developers eager to pitch in.\n\nA simple linear classifier paired with a self-supervised feature extractor outperformed a supervised deep learning model on ImageNet, according to new research.What’s new:Ting Chen and colleagues at Google Brain devised a self-supervised training algorithm (a task that trains a model on unlabeled data to generate features helpful in performing other tasks).Simple Contrastive Learning(SimCLR) compares original and modified versions of images, so a model learns to extract feature representations that are consistent between the two.Key insight:Images and variations produced by data-augmentation techniques such as rotation have similar features — so similar that they’re more informative than the labels such images also might share. SimCLR trains a model to extract features that are unchanged by such transformations, a technique known as contrastive learning.How it works:Unlike other contrastive learning techniques, SimCLR can be used with any model architecture. It requires only multiple data-augmentation methods (which the researchers specify only for images, the subject of this study).\n\nResults:A ResNet-50(x4) trained with SimCLR extracted features from ImageNet using all labels. A linear classifier trained on the resulting features achieved 76.5 percent top-1 accuracy, 0.1 percent better than a fully supervised ResNet-50. SimCLR achieved similar results on a variety of other image datasets.Why it matters:Self-supervised learning schemes often rely on complicated tasks to extract features from unlabeled data. SimCLR simply extracts similar features from similar examples.We’re thinking:This method seems like it would work well on audio data. We’re curious to see how effective it can be with text and other data types based on alphanumeric characters.\n\nBusinesses around the world outsource their customer service work to the Philippines. Now the Philippines is preparing its workers to outsource some tasks to AI.What’s new:The Philippines’ booming call-center industry aims to train workers for more highly skilled jobs as AI takes over the easier ones, according to theSouth China Morning Post.How it works:AI threatens half of the country’s 1.3 million outsourcing jobs, especially relatively simple work like answering simple inquiries via phone or email, according to the IT and Business Process Association of the Philippines. In response, IBPAP is launching a massive campaign to train workers in professions like data analytics and machine learning.\n\nBehind the news:The Philippines has become India’s top competitor in the overseas call center business thanks in part to government-ledeffortsto improve technological literacy and communications infrastructure beginning in theearly 1990s.Why it matters:Outsourcing accounts for almost 10 percent of the Philippines economy,according to IBPAP. Upskilling programs cultivate workers, half of whom are women, into a garden of homegrown talent.We’re thinking:Technical upskilling is a great way to transform outsourcing spokes into innovation hubs. AI is still young enough that there’s room for many such hubs around the world.\n\nExplore federated learning and learn how to retrain deployed models while maintaining user privacy. Take the new course on advanced deployment scenarios in TensorFlow.Enroll now\n\nQuantum computing has made great strides in recent years, though it still faces significantchallenges. If and when it gets here, machine learning may be ready for it.What’s new:TensorFlow Quantumis a platform for building, training, and deploying neural networks on quantum processors. It was developed by Alphabet, Volkswagen, and the University of Waterloo.How it works:The software works withquantum hardwarelike Google’s Sycamore computer, which has 54 qubits. Each qubit processes multiple calculations at once, theoretically enabling such systems to vastly outperform conventional CPUs.\n\nWhy it matters:Imagine that, instead of living one life, you could live billions of lives simultaneously, and at the end, you would have learned from all of them. Quantum speedups can be enormous for operations in which a single quantum computer can outperform the fastest supercomputer (that is, millions of classical computers working together). Machine learning could be one of those operations.\n\nYes, but:It may be a while before quantum computing is practical outside of research labs. Among other challenges, quantum systems are sosensitivethat the noise they generate can derail their calculations.\n\nBehind the news:Last year, Google claimed that Sycamore had achieved so-calledquantum supremacyby performing a calculation that it deemed impractical for a classical supercomputer. IBMchallengedthe claim by solving the problem using conventional technology. The two tech giants, which are vying for leadership in the field, remain at loggerheads.We’re thinking:Tech giants are always on the lookout for disruptions that may threaten their business. By creating tools for developers, they’re positioning themselves for a quantum future whether or not it arrives. Meanwhile, machine learning engineers have a shiny new toy to play with!\n\nWhich dataset was used to train a given model? A new method makes it possible to see traces of the training corpus in a model’s output.What’s new:Alexandre Sablayrolles and colleagues at Facebook and France’s National Institute for Research in Computer Science and Automation adulterated training data with imperceptible signals. Decisions made by models trained on this so-calledradioactive datashowed signs of the altered corpus.Key insight:Changes in training data can affect the loss in a trained model’s decisions. A small, consistent alteration in the training data affects the loss in a predictable way.How it works:The researchers replaced a portion of images in a training corpus with marked images. After training a model on the whole dataset, they compared the model’s loss on small subsets of marked and unmarked images.\n\nResults:The researchers marked 1 percent of Imagenet and trained a Resnet-18 on the entire dataset. The model’s loss on subsets of radioactive and normal data differed by a statistically significant amount, confirming that the model had been trained on a portion of marked data, while accuracy declined by only 0.1 percent compared to training on standard Imagenet. Different architectures and datasets yielded similar results.Why it matters:As neural networks become enmeshed more deeply in a variety of fields, it becomes more helpful to know how they were trained — say, to understand bias or track use of proprietary datasets. This technique, though nascent, offers a potential path to that goal.\n\nWe’re thinking:Beyond identifying training sets, radioactive data may offer a method to enforce data privacy by making it possible to identify models trained from improperly obtained private data.\n\nMachine learning promises to streamline handling of tomorrow’s bureaucratic drudgery — and, it turns out, that of 2,500 years ago.What’s new:Computer vision is helping researchers at the University of Chicagotranslatea massive collection of ancient records inscribed on clay tablets.How it works:Persian scribes around 500 BCE produced thousands of documents now collected in thePersepolis Fortification Archive. Researchers have been translating the cuneiform characters for decades. Now they hope to speed up the job with help from DeepScribe, a model built by computer scientistSanjay Krishnan.\n\nBehind the news:The archive mostly contains records of government purchases, sales, and transport of food, helping scholars develop a detailed understanding of life in the First Persian Empire. University of Chicago archaeologistsfoundthe tablets in 1933 near the palace sites of early Persian kings. Theyreturnedthe artifacts to Iran in 2019.Why it matters:DeepScribe’s current accuracy is good enough to automate translation of repetitive words and phrases, freeing up human attention for more specialized work like translating place names or deciphering particular words in context. The researchers also believe the model could be useful for filling in gaps on tablets where text has worn away or is indecipherable.We’re thinking:These tablets hold an important lesson for all of us during tax season: Never throw away your receipts.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Covid.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/SimCLR2022020ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Philippines.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-video-to-gif201-2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Quantum-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Radioactive20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Cuneiform.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-255/",
    "title": "issue 255",
    "date": "",
    "reading_time": "",
    "content": "On Monday, a number of large music labelssuedAI music makers Suno and Udio for copyright infringement. Their lawsuit echoesThe New York Times’lawsuitagainst OpenAI in December. The question of what’s fair when it comes to AI software remains a difficult one.\n\nIspokeout in favor of OpenAI’s side in the earlier lawsuit. Humans can learn from online articles and use what they learn to produce novel works, so I’d like to be allowed to use AI to do so. Some people criticized my view as making an unjustifiable equivalence between humans and AI. This made me realize that people have at least two views of AI: I view AI as a tool we can use and direct to our own purposes, while some people see it as akin to a separate species, distinct from us, with its own goals and desires.\n\nIf I’m allowed to build a house, I want to be allowed to use a hammer, saw, drill, or any other tool that might get the job done efficiently. If I’m allowed to read a webpage, I’d like to be allowed to read it with any web browser, and perhaps even have the browser modify the page’s formatting for accessibility. More generally, if we agree that humans are allowed to do certain things — such as read and synthesize information on the web — then my inclination is to let humans direct AI to automate this task.\n\nIn contrast to this view of AI as a tool, if someone thinks humans and AI are akin to separate species, they’ll frame the question differently. Few people today think all species should have identical rights. If a mosquito annoys a human, the mosquito can be evicted (or worse). In this view, there’s no reason to think that, just because humans are allowed to do something, AI should be allowed to do it as well.\n\nTo be clear, just as humans aren’t allowed to reproduce large parts of copyrighted works verbatim (or nearly verbatim) without permission, AI shouldn’t be allowed to do so either. The lawsuit against Suno and Udio points out that, when prompted in a particular way, these services can nearly reproduce pieces of copyrighted music.\n\nBut here, too, there are complex issues. If someone were to use a public cloud to distribute online content in violation of copyright, typically the person who did that would be at fault, not the cloud company (so long as the company took reasonable precautions and didn’t enable copyright infringement deliberately). The plaintiffs in the lawsuit against Suno and Udio managed to write prompts that caused the systems to reproduce copyrighted work. But is this like someone managing to get a public cloud to scrape and distribute content in a way that violates copyright? Or is this — as OpenAIsaid— a rare bug that AI companies are working to eliminate? (Disclaimer: I’m not a lawyer and I’m not giving legal advice.)\n\nHumans and software systems use very different mechanisms for processing information. So in terms of what humans can do — and thus what I’d like to be allowed to use software to help me do — it’s helpful to consider the inputs and outputs. Specifically, if I’m allowed to listen to a lot of music and then compose a novel piece of music, I would like to be allowed to use AI to implement a similar input-to-output mapping. The process for implementing this mapping may be training a neural network on music that’s legally published on the open internet for people to enjoy without encumbrances.\n\nTo acknowledge a weakness of my argument, just because humans are allowed to emit a few pounds of carbon dioxide per day simply by breathing doesn’t mean we should allow machines to emit massively more carbon dioxide without restrictions. Scale can change the nature of an act.\n\nWhen I was a high-school student in an internship job, I spent numerous hours photocopying, and I remember wishing I could automate that repetitive work. Humans do lots of valuable work, and AI, used as a tool to automate what we do, will create lots of value. I hope we can empower people to use tools to automate activities they’re allowed to do, and erect barriers to this only in extraordinary circumstances, when we have clear evidence that it creates more harm than benefit to society.\n\nKeep learning!\n\nAndrew\n\nLearn to reduce the carbon footprints of your AI projects in “Carbon Aware Computing for GenAI Developers,” a new course built in collaboration with Google Cloud. Perform model training and inference jobs with cleaner, low-carbon energy and make your AI development greener!Join today\n\nU.S. antitrust regulators are preparing to investigate a trio of AI giants.\n\nWhat’s new:Two government agencies responsible for enforcing United States anti-monopoly laws agreed to investigate Microsoft, Nvidia, and OpenAI,The New York Timesreported.\n\nHow it works:The Department of Justice (DOJ) will investigate Nvidia, which dominates the market for chips that train and run neural networks. The Federal Trade Commission (FTC) will probe Microsoft and its relationship with OpenAI, which together control the distribution of OpenAI’s popular GPT-series models. In February, FTC chair Lina Khansaidthe agency would look for possible anti-competitive forces in the AI market.\n\nBehind the news:Government attention to top AI companies is rising worldwide. Microsoft’s partnership with OpenAIfacesadditional scrutiny by European Union regulators, who are probing whether the relationship violates EU regulations that govern corporate mergers. U.K. regulators areinvestigatingAmazon’s relationship with Anthropic and Microsoft’s relationship with Mistral and Inflection AI. Last year, French regulatorsraidedan Nvidia office over suspected anti-competitive practices. In 2022, Nvidiawithdrewa bid to acquire chip designer Arm Holdings after the proposal attracted international regulatory scrutiny including an FTC lawsuit.\n\nWhy it matters:Microsoft, Nvidia, and OpenAI have put tens of billions of dollars each into the AI market, and lawsuits, settlements, judgments, or other interventions could shape the fate of those investments. The FTC and DOJ similarlydividedtheir jurisdictions in 2019, resulting in investigations into — and ongoing lawsuits against — Amazon, Apple, Google, and Meta for alleged anti-competitive practices in search, social media, and consumer electronics. Their inquiries into the AI market could have similar impacts.\n\nWe’re thinking:Governments must limit unfair corporate behavior without stifling legitimate activities. Recently, in the U.S. and Europe, the pendulum has swung toward overly aggressive enforcement. For example, government opposition to Adobe’s purchase of Figma had a chilling effect on acquisitions that seems likely to hurt startups. The UK blocked Meta’s acquisition of Giphy, which didn’t seem especially anticompetitive. We appreciate antitrust regulators’ efforts to create a level playing field, and we hope they’ll take a balanced approach to antitrust.\n\nAn AI startup that aims to crack markets in southern Asia launched a multilingual competitor to GPT-4.\n\nWhat’s new:The company known as Two AIoffersSUTRA, a low-cost language model built to be proficient in more than 30 languages, including underserved South Asian languages like Gujarati, Marathi, Tamil, and Telugu. The company also launchedChatSUTRA, a free-to-use web chatbot based on the model.\n\nHow it works:SUTRA comprises two mixture-of-experts transformers: a concept model and an encoder-decoder for translation. Apaperincludes some technical details, but certain details and a description of how the system fits together are either absent or ambiguous.\n\nResults:Onmultilingual MMLU(a machine-translated version of multiple-choice questions that cover a wide variety of disciplines), SUTRA outperformed GPT-4 in four of the 11 languages for which the developer reported the results: Gujarati, Marathi, Tamil, and Telugu. Moreover, SUTRA’s tokenizer is highly efficient, making the model fast and cost-effective. In key languages, it compares favorably to the tokenizer used with GPT-3.5 and GPT-4, and even narrowly outperforms GPT-4o’s improved tokenizer, according to Two AI’s tokenizer comparisonspaceon HuggingFace. In languages such as Hindi and Korean that are written in non-Latin scripts and for which GPT-4 performs better on MMLU, SUTRA’s tokenizer generates less than half as many tokens as the one used with GPT-3.5 and GPT-4, and slightly fewer than GPT-4o’s tokenizer.Yes, but:Multilingual MMLU tests only 11 of SUTRA’s 33 languages, making it difficult to fully evaluate the model’s multilingual performance.\n\nBehind the news:Two AI was founded in 2021 by Pranav Mistry, former president and CEO of Samsung Technology & Advanced Research Labs. The startup has offices in California, South Korea, and India. In 2022, it raised $20 million in seed funding from Indian telecommunications firm Jio and South Korean internet firm Naver. Mistry aims to focus on predominantly non-English-speaking markets such as India, South Korea, Japan, and the Middle East, hetoldAnalytics India.\n\nWhy it matters:Many top models work in a variety of languages, but from a practical standpoint, multilingual models remain a frontier in natural language processing. Although SUTRA doesn’t match GPT-4 in all the languages reported, its low price and comparatively high performance may make it appealing in South Asian markets, especially rural areas where people are less likely to speak English. The languages in which SUTRA excels are spoken by tens of millions of people, and they’re the most widely spoken languages in their respective regions. Users in these places have yet to experience GPT-4-level performance in their native tongues.We’re thinking:Can a newcomer like Two AI compete with OpenAI? If SUTRA continues to improve, or if it can maintain its cost-effective service, it may yet carve out a niche.\n\nAdvances in video generation have spawned a market for lifelike avatars of deceased loved ones.\n\nWhat’s new:Several companies in China produce interactive videos that enable customers to chat with animated likenesses of dead friends and relatives,MIT Technology Reviewreported.\n\nHow it works:Super BrainandSilicon Intelligencehave built such models for several thousand customers. They provide a modern equivalent of portrait photos of deceased relatives and a vivid way to commune with ancestors.\n\nBehind the news:The desire to interact with the dead in the form of an AI-generated avatar is neither new nor limited to China. In the U.S., the startup HereAfter AIbuildschatbots that mimic the deceased based on interviews conducted while they were alive. Another startup, StoryFile, markets similar capabilities to elders (pitchedby 93-year-oldStar Trekstar William Shatner) to keep their memory alive for younger family members. The chatbot appReplikabegan as a project by founder Eugenia Kuyda to virtually resurrect a friend who perished in a car accident in 2015.\n\nYes, but:In China, language models struggle with the variety of dialects spoken by many elders.\n\nWhy it matters:Virtualnewscastersandinfluencersare increasingly visible on the web, but the technology has more poignant uses. People long to feel close to loved ones who are no longer present. AI can foster that sense of closeness and rapport, helping to fulfill a deep need to remember, honor, and consult the dead.\n\nWe’re thinking:No doubt, virtual avatars of the dead can bring comfort to the bereaved. But they also bring the risk that providers might manipulate their customers’ emotional attachments for profit. We urge developers to focus on strengthening relationships among living family and friends.\n\nTool use and planning are key behaviors in agentic workflows that enable large language models (LLMs) to execute complex sequences of steps. New benchmarks measure these capabilities in common workplace tasks.\n\nWhat’s new:Recent benchmarks gauge the ability of a large language model (LLM) to use external tools to manipulate corporate databases and to plan events such as travel and meetings.\n\nTool use:Olly Styles, Sam Miller, and colleagues at Mindsdb, University of Warwick, and University of Glasgow proposedWorkBench, which tests an LLM’s ability to use 26 software tools to operate on five simulated workplace databases: email, calendar, web analytics, projects, and customer relationship management. Tools include deleting emails, looking up calendar events, creating graphs, and looking up tasks in a to-do list.\n\nPlanning:Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, and colleagues at Google publishedNatural Plan, a benchmark that evaluates an LLM’s ability to (i) plan trips, (ii) arrange a series of meeting times and locations, and (iii) schedule a group meeting. Each example has only one solution.\n\nWhy it matters:When buildingagentic workflows, developers must decide on LLM choices, prompting strategies, sequencing of steps to be carried out, tool designs, single- versus multi-agent architectures, and so on. Good benchmarks can reveal which approaches work best.\n\nWe're thinking:These tests have unambiguous right answers, so agent outputs can be evaluated automatically as correct or incorrect. We look forward to further work to evaluate agents that generate free text output.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed--64--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/06/The-Batch-ads-and-exclusive-banners---2024-06-25T085213.553.png",
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-26T151248.547.png",
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-26T152037.544.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed--65-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-26T152214.754.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-8/",
    "title": "issue 8",
    "date": "",
    "reading_time": "",
    "content": "Last week, I saw a lot of social media discussion about apaperusing deep learning to generate artificial comments on news articles. I’m not sure why anyone thinks this is a good idea. At best, it adds noise to the media environment. At worst, it’s a tool for con artists and propagandists.\n\nA few years ago, an acquaintance pulled me aside at a conference to tell me he was building a similar fake comment generator. His project worried me, and I privately discussed it with a few AI colleagues, but none of us knew what to do about it. It was only this year, with the staged release of OpenAI’s GPT-2 language model, that the question went mainstream.\n\nDo we avoid publicizing AI threats to try to slow their spread, as I did after hearing about my acquaintance’s project? Keeping secret the details of biological and nuclear weapon designs has been a major force slowing their proliferation. Alternatively, should we publicize them to encourage defenses, as I’m doing in this letter?\n\nEfforts like the OECD’sPrinciples on AI, which state that “AI should benefit people and the planet,” give useful high-level guidance. But we need to develop guidelines to ethical behavior in practical situations, along with concrete mechanisms to encourage and empower such behavior.\n\nWe should look to other disciplines for inspiration, though these ideas will have to be adapted to AI. For example, in computer security, researchers are expected to report vulnerabilities to software vendors confidentially and give them time to issue a patch. But AI actors are global, so it’s less clear how to report specific AI threats.\n\nOr consider healthcare. Doctors have a duty to care for their patients, and also enjoy legal protections so long as they are working to discharge this duty. In AI, what is the duty of an engineer, and how can we make sure engineers are empowered to act in society’s best interest?\n\nTo this day, I don’t know if I did the right thing years ago, when I did not publicize the threat of AI fake commentary. If ethical use of AI is important to you, I hope you will discuss worrisome uses of AI with trusted colleagues so we can help each other find the best path forward. Together, we can think through concrete mechanisms to increase the odds that this powerful technology will reach its highest potential.\n\nKeep learning!\n\nAndrew\n\nElon Musk has promised a fleet of autonomous Tesla taxis by2020. The company reportedly purchased a computer vision startup to help meet that goal.What’s new:Tesla acquired DeepScale, a Silicon Valley startup that processes computer vision on low-power electronics, according toCNBC. The price was not reported.\n\nBehind the news:Tesla’s stock is down 25 percent this year due to manufacturingproblemsand a drop in demand for electric vehicles. In July, the company lost around 10 percent of its self-driving dev team after Musk expressed displeasure at their inability to adapt its highway-specific autopilot software to urban driving, according to areportinThe Information. The recent debut of Tesla’s Smart Summon feature, which enables cars to drive themselves from a parking space to their waiting owner, was marred byreportsof accidents.Why it matters:Cars operate within tight constraints on electrical power, and self-driving cars consume lots of power-hungry processing. Tesla is betting that leaner processing will help it reach full autonomy within the power budget of an electric vehicle. Fleets of self-driving taxis would certainly bolster the company’s bottom line.We’re thinking:Low-power processing is just one of many things that will make fully self-driving systems practical. There’s widespread skepticism about Tesla’s ability to deliver on its promises on time, but every piece will help.\n\nDrugs undergo rigorous experimentation and clinical trials to gain regulatory approval, while dietary supplements get less scrutiny. Even when a drug study reveals an interaction with supplements, the discovery tends to receive little attention. Consequently, information about interactions between drugs and supplements — and between various supplements — is relatively obscure. A newmodelbrings it to light.What’s new:Lucy Lu Wang and collaborators at the Allen Institute createdsupp.ai, a website that scans medical research for information about such interplay. Users can enter a supplement name to find documented interactions.Key insight:Language describing drug interactions is similar to that describing interactions involving supplements, so an approach that spots drug interactions should work for supplements.How it works:The researchers modified an earlier model that finds drug-to-drug interactions in medical literature to support supplements.\n\nResults:Among 22 million abstracts, the system classified 1.1 million sentences describing interactions. To assess accuracy, the authors hand-labeled 400 sentences that contained references to supplements. On this subset, the system was 87 percent accurate in identifying supplement interactions, compared with 92 percent for drug interactions, the state of the art in that task.Why it matters:Most U.S. adultsuse a dietary supplement, yet their interactions with drugs or one another are virtually unknown. Supp.ai makes it easy for anyone with a web browser to look them up.We’re thinking:The researchers took advantage of the similarity between text discussing drug and supplement interactions to adapt a drug-oriented model for an entirely different, less-scrutinized class of remedies — a clever approach to a difficult problem.\n\nThe French government plans to roll out a national identification service based on face recognition. Critics warn that the new system violates citizens’ privacy.What’s new:Beginning in November, President Emmanual Macron’s administration plans to implement adigital ID programbased on an Android app. While French citizens aren’t required to enroll, the app will be the only digital portal to many government services.How it works:Called Alicem, the app is designed to authenticate the user’s identity for interactions such as filing taxes, applying for pension benefits, and paying utility bills.\n\nBehind the news:France isn’t the first government to use face recognition in this way.Singaporealso offers access to government services via face print.Yes, but:Emilie Seruga-Cau, who heads France’s privacy regulator, says Alicem’s authentication scheme violates the European Union’s General Data Protection Regulation by failing to offer an alternative to face recognition. A privacy group called La Quadrature du Net has sued the government over the issue. France’s Interior Ministry has shown no sign of bowing to such concerns.We’re thinking:Any democratic government aiming to use face recognition for identification must protect its citizens on two fronts. Laws must restrict use of the data to its intended purpose, and due care must be taken to secure the data against hacks.\n\nTrying to understand the latest AI research but struggling to understand the math? Build your intuition for foundational deep learning techniques in the Deep Learning Specialization.Enroll now\n\nEvery home is different. That makes it difficult for domestic robots to translate skills learned in one household — say, fetching a soda from the fridge — into another. Training in virtual reality, where the robot has access to rich information about three-dimensional objects and spaces, can make it easier for robots to generalize skills to the real world.What’s new:Toyota Research Institute built a household robot that users can train using avirtual reality interface. The robot learns a new behavior based on a single instance of VR guidance. Then it responds to voice commands to carry out the behavior in a variety of real-world environments.How it works:Toyota’s robot is pieced together from off-the-shelf parts, including two cameras provide stereoscopic vision. Classical robotics software controls the machine, while convolutional neural networks learn unique embeddings.\n\nResults:The Toyota researchers trained the bot in the virtual environment on three tasks: retrieving a bottle from a refrigerator, removing a cup from a dishwasher, and moving multiple objects to different locations. Then they had the robot perform each task 10 times in two physical homes. They ran the experiments with slight alterations, for instance asking the robot to retrieve a bottle from a higher shelf than the virtual one it was trained on, or doing so with the lights turned off. The robot achieved an 85 percent success rate — though it took an average 20 times longer than a human would.Why it matters:Researchers have given a lot of attention lately to the use of reinforcement learning on robots that are both trained and tested in a simulated environment. Getting such systems to generalize from a simulation to the real world is an important step toward making them useful.We’re thinking:Birth rates have been slowing for decades in Japan, China, the U.S., and much of Europe. The World Health Organizationestimatesthat 22 percent of the world’s population will be over 60 years old by 2050. Who will care for the elderly? Robots may be part of the answer.\n\nSkin conditions are the fourth-largest cause of nonfatal disease worldwide, but access to dermatological care is sparse. A new study shows that a neural network can do front-line diagnostic work.What’s new:Researchers at Google Health, UCSF, MIT, and the Medical University of Graz trained a model to examine patient records and predict the likelihood of 26 common skin diseases. The researchers believe that theirsystemcould improve the diagnostic performance of primary-care centers for skin disease.Key insight:The system is designed to mimic the typical diagnostic process in a teledermatology setting. It accepts a patient’s medical history and up to six images, and returns a differential diagnosis, or a ranked list of likely diagnoses.How it works:Yuan Liu and her colleagues collected anonymized patient histories and images from a dermatology service serving 17 sites across two U.S. states. They trained on data collected a few years ago, and they tested on data generated more recently to approximate real-world conditions. The system includes:\n\nResults:The model classified diseases more accurately than primary care physicians and nurse practitioners. Allowed three guesses, it was more accurate than dermatologists by 10 percent. The system proved robust to skin color and type and its performance remained consistent across variations.Why it matters:Most previous models consider only a single image and classify a single disease. Inspired by current medical practices, this model uses a variable number of input images, makes use of non-visual patient information as well, and classifies a variety of conditions. The research also shows how to establish model robustness by comparing performance across characteristics like skin color, age, and sex.Yes, but:This study drew data from a limited geographic area. It remains to be seen whether the results generalize to other regions or whether such systems need to be trained or fine-tuned to account for specific geographic areas.We’re thinking:Computer vision has been making greatprogressin dermatology. Still, there are many difficult steps between encouraging results and deployment in clinical settings.\n\nAI startups are being scooped up at an accelerating pace, many by companies outside the tech sphere.What’s new:Areportby CB Insights shows that, as of August, 2019 was on track to surpass last year’s record number of AI startup acquisitions. The annual tally has grown an average of 38 percent every year since 2010.Who’s buying:While tech giants buy more startups on average, non-tech companies account for the overwhelming majority of purchases.\n\nWhat they’re paying:Seven AI acquisitions topped a billion dollars. The most recent happened in April, when pharma giant Roche Holdings closed its $1.9 billion purchase of cancer analytics provider Flatiron Health. The report doesn’t provide annual spending totals.Why it matters:The report makes a strong case that AI’s strategic value is rising steadily throughout the economy. AI is still a tech-giant specialty, but it’s becoming essential in industries well beyond the internet and software.We’re thinking:Exciting startups attract talent, and their work leads to acquisitions that supercharge innovation with bigger budgets and wider reach, drawing still more people into the field. The latest numbers show that this virtuous cycle has staying power — enough, perhaps, to overcome the ongoing shortage of machine learning engineers.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Deep20Scale20Resized.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Supplements20Resized.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Alicem.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Course20Ad2016.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Toyota20Robot20Resized.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Skin20Resized.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/CB20Insights20Graph20Resized.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-87/",
    "title": "issue 87",
    "date": "",
    "reading_time": "",
    "content": "Machine learning development is highly iterative. Rather than designing a grand system, spending months to build it, and then launching it and hoping for the best, it’s usually better to build a quick-and-dirty system, get feedback, and use that feedback to improve the system.The iterative aspect of machine learning applies to many steps. For example:Data labeling:It’s hard to come up with fully fleshed-out labeling guidelines that result in clean and consistent labels on your first attempt. It might be better to use an initial set of guidelines to label some data, see what problems arise, and then improve the guidelines.Model training:Building an AI system requires deciding what data, hyperparameters, and model architecture to use. Rather than overthinking these choices, it’s often better to train an initial model, then use error analysis to drive improvements.Deployment and monitoring:When deploying a machine learning system, you might implement dashboards that track various metrics to try to spot concept drift or data drift. For example, if you’re building a product recommendation system, you might track both software metrics such as queries per second and statistical metrics such as how often the system recommends products of different categories. What metrics should we track? Rather than try to design the perfect set of dashboards before launch, I find it more fruitful to pick a very large set of metrics, evolve them, and prune the ones that prove less useful.\n\nIteration is helpful in other phases of machine learning development as well. It make sense to take an empirical, experimental approach to decision making whenever:\n\nThese two properties hold true for many steps in a typical ML project.\n\nOne implication is that, if we can build tools and processes that enable high-throughput experimentation, we can make faster progress. For instance, if you have an MLOps platform that enables you to quickly train and evaluate new models, this will allow you to improve models more quickly.\n\nThis principle applies to other aspects of ML development that are iterative. That’s why time spent optimizing your team's capacity to run many experiments can pay off well.\n\nKeep learning!\n\nAndrew\n\nKey machine learning datasets are riddled with mistakes.\n\nWhat’s new:Several benchmark datasets are shot through with incorrect labels. On average, 3.4 percent of examples in 10 commonly used datasets are mislabeled, according to a newstudy— and the detrimental impact of such errors rises with model size.\n\nThe research:Curtis Northcutt and Anish Athalye at MIT and Jonas Mueller at Amazon trained a model to identify erroneous labels in popular datasets such as ImageNet, Amazon Reviews, and IMDB.\n\nWhy it matters:It’s well known that machine learning datasets contain a fair percentage of errors. Previous inquiries into the problem focused on training rather than test sets, and found that training on a small percentage of incorrect labels didn’t hurt deep learning performance. But accuracy on a test set that’s rife with errors is not a true measure of a model’s ability, and bad labels in the test set have a disproportionate impact on bigger models.\n\nWe’re thinking:It’s time for our community to shift from model-centric to data-centric AI development. Many state-of-the-art models work well enough that tinkering with their architecture yields little gain in many problems, and the most direct path to improved performance is to systematically improve the data your algorithm learns from. You can check out Andrew’s recent talk on the subjecthere.#DataCentricAI\n\nArecentgenerative adversarial network (GAN) produced more coherent images using modified transformers that replaced fully connected layers with convolutional layers. A new GAN achieved a similar end using transformers in their original form.\n\nWhat’s new:Yifan Jiang and collaborators at the University of Texas at Austin and the MIT-IBM Watson AI Lab unveiledTransGAN, a transformer-based GAN that doesn’t use any convolutions.\n\nKey insight:Traditionally, GANs rely on convolutional neural networks, which integrate information in pixels far away from one another only in the later layers. The upshot could be an image of a person with two different eye colors or mismatched earrings. A GAN based on transformers, which use self-attention to determine relationships among various parts of an input, would learn relationships between pixels across an entire image from the get-go. That should enable it to produce more realistic images.\n\nHow it works: Like other GANs, TransGAN includes a generator (which, given a random input, generates a new image) and a discriminator (which, given an image, predicts whether or not it’s generated). Both components contain a sequence of transformer layers, each comprising a fully connected layer and a self-attention layer. The authors trained them simultaneously.\n\nResults:TransGAN set a new state of the art on theSTL-10dataset, which includes relatively few labeled examples and many unlabeled examples in a similar distribution. It achieved a Fréchet Inception Distance — a measure of the difference in distribution between generated images and training data (lower is better) — of 25.32 FID, compared to theprevious state of the art’s 26.98 FID.\n\nYes, but:On theCeleb-Adataset of relatively high-res celebrity faces, TransGAN achieved a Fréchet Inception Distance of 12.23 FID versusHDCGAN, which is designed for higher-res output and scored 8.44 FID.\n\nWhy it matters:The transformer takeover continues! Meanwhile, TransGAN’s expanding training mask gives its output the smooth look of convolutions with better coherence across generated images. Maybe such purposeful training schedules can stand in for certain architectural choices.\n\nWe’re thinking:Transformers, with their roots in language processing, might answer the age-old question of how many words an image is worth.\n\nJoin Fei-Fei Li, Curtis Langlotz, and Andrew Ng to explore the future of AI in healthcare on April 29, 2021, at 10 a.m. Pacific Time. Co-hosted by DeepLearning.AI and Stanford Human-Centered Artificial Intelligence and supported by Stanford Center for Artificial Intelligence in Medicine & Imaging.Sign up and submit questions or topics\n\nA robot inspector is looking over the shoulders of robot welders.\n\nWhat’s new:Farm equipment makerJohn Deeredescribed a computer vision system that spots defective joints, helping to ensure that its heavy machinery leaves the production line ready to roll.\n\nHow it works:Like other manufacturers, John Deere uses robotic welders to assemble metal parts on its machines for farming, forestry, and construction. But industrial-strength welding has a longstanding problem: Bubbles of gas can form inside a joint as it cools, weakening it. Anaction recognition modeldeveloped by Intel spots such defects in real time.\n\nBehind the news:AI-powered quality assurance is gaining ground. Systems fromLanding AI(a sister company to DeepLearning.AI) and others recognize defects in a growing number of manufacturing processes.\n\nWhy it matters:Skilled human inspectors are in short supply, expensive to hire, and not always able to inspect every joint in a factory full of robotic welders, so defects may go unnoticed until after a subpar part has become part of a larger assembly. A single welded part can cost up to $10,000. By spotting errors as they occur, computer vision can save manufacturers time and money.\n\nWe’re thinking:Good to see AI making sure the job is weld done\n\nResearchers unveiled competition for the reigning large language modelGPT-3.\n\nWhat’s new:Four models collectively called Wu Dao were described byBeijing Academy of Artificial Intelligence, a research collective funded by the Chinese government, according toSynced Review.\n\nPower quartet:Wu Dao’s constituent models were developed by over 100 scientists at leading Chinese universities and tech companies. In January, researchers associated with the project toldWiredthat it could help citizens navigate China’s bureaucracy, including the Beijing Motor Vehicles Administration.\n\nBehind the news:The Beijing Academy of Artificial Intelligence was founded in 2018 to help the Chinese government achieve itsgoalof becoming the global center of AI. Its other projects include research into thecognitive roots of neural networks, a proposal forstandardized AI notation, and a program to developAI-specific computer chips.\n\nWhy it matters:This effort reflects China’s growing confidence and capability in AI. Such ambitious projects could also curb China’s AI brain drain, as many of its most talented engineers wind up leaving for work overseas.\n\nWe’re thinking:AI has long been dominated by organizations clustered in a few geographic hotspots. China’s effort to shift AI’s center of gravity away from the west could have far-reaching repercussions into the types of systems that get built and how they’re deployed.\n\nIn some animated games, different characters can perform the same actions — say, walking, jumping, or casting spells. A new system learned from unlabeled data to transfer such motions from one character to another.\n\nWhat’s new:Cinjon Resnick at New York University and colleagues at Nvidia, Technical University of Berlin, and Google developed asystemdesigned to isolate changes in the pose of a two-dimensional figure, or sprite, and apply them to another sprite. While earlier approaches to solving this problem require labeled data, the new system is self-supervised.\n\nKey insight:A 2D animation consists of three elements: a sprite, the sprite’s motion and any special effects, and a background (which remains static in this work). Separate neural networks optimizing a variety of loss terms can learn to disentangle these elements, compute their changes from frame to frame, and recombine them to produce a novel frame.\n\nHow it works:The system comprises four convolutional neural networks: two encoders, a transformation network, and a decoder. It generates a new frame given an image of a target sprite, a background, and two frames of animation showing a source sprite in motion — say, the initial frame and the one showing the pose, position, or other attributes to be mapped onto the target. During training, the images of the target sprite, background, and first frame of the animation were identical. The training and test sets consisted ofseveral hundred animated video game characters performing various motions.\n\nResults:The authors compared their system withVisual Dynamics. It underperformed the competition, achieving a mean squared error of ~20 versus ~16 — but Visual Dynamics is a supervised system that requires labeled training data.\n\nWhy it matters:A collection of networks that study different aspects of a dataset, and then compare and combine the representations they generate, can yield valuable information when labels aren’t available.\n\nWe’re thinking:Possibly a useful tool for animators. Definitely a new toy for remix culture.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-04-14%20at%2010.46.44%20AM%20copy.png?upscale=true&name=Screen%20Shot%202021-04-14%20at%2010.46.44%20AM%20copy.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image-12.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image-13.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image-15.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image-14.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image-16.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image-17.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-41/",
    "title": "issue 41",
    "date": "",
    "reading_time": "",
    "content": "I’m proud to announce that we held the 100th Pie & AI last Friday. Pie & AI is our meetup series that brings together members of the AI community worldwide for education, conversation, and a slice of pie.\n\nPie & AI kicked off in Seattle last year shortly after Pi Day (March 14, or 3.14). Since then, we’ve hosted events in over 68 cities in 38 countries. Friday’s event was streamed from Azerbaijan.\n\nWith social distancing keeping us apart physically, it’s more important than ever for AI to have a strong online community. So we’ve doubled down on making Pie & AI a virtual meetup. No matter where you are, you can attend any of our events, learn from experts, and chat with peers even if they’re thousands of miles away.\n\nI would like to say a special thank you to Pie & AI’s 60 event ambassadors. These extraordinary people organize events locally, share resources and tips, and sometimes speak about how AI applies to local businesses and problems. I am grateful and inspired by your dedication to sharing your knowledge and enthusiasm.\n\nIf Pie & AI has answered your questions, helped you grow, or inspired you, please let us know on Twitter using #PieandAI. You can check out upcoming eventshere.\n\nKeep learning!\n\nAndrew\n\nGenerative adversarial networks don’t just produce pretty pictures. They can build world models, too.What’s new:A GAN generated a fully functional replica of the classic video game Pac-Man. Researchers from Nvidia, MIT, the University of Toronto, and Vector Institute developedGameGANto celebrate the original Pac-Man’s 40th anniversary. The company plans to release the code in a few months.How it works:GameGAN learned to reproduce the game by watching it in action for 50,000 hours. During gameplay, the system synthesizes the action frame by frame using three neural networks.\n\nBehind the news:While Nvidia is the first to use a generative adversarial network to reproduce a video game, other researchers have used machine learning for this purpose.\n\nYes, but:Compared to the original arcade game, Pac-Man’s GAN-driven twin requires orders of magnitudemore computationto run.Why it matters:Autonomous systems such as self-driving cars and robots are often trained in elaborate simulators. Nvidia hopes that GAN-based sims can save time and money.We’re thinking:Fifty thousand hours is an awful lot of Pac-Man — or anything else! Simulation makes it possible to amass training data that would be virtually impossible to collect in the real world. It’s also a crutch that leads researchers to develop algorithms that work well in simulated environments but are hard to generalize to real-world conditions. Until better small-data algorithms emerge, GAN-based simulation looks like an exciting new direction.\n\nThe for-profit research organization OpenAI has a new supercomputer to help achieve its dream of building the world’s most sophisticated AI.What’s new:Microsoft engineered the new hardware network to train immense models on thousands of images, texts, and videos simultaneously.How it works:Hosted on Microsoft’s Azure cloud platform, the system comprises 10,000 GPUs and 285,000 CPUs.\n\nBehind the news:In 2019, Microsoftinvested$1 billion in OpenAI in exchange for the first shot at commercializing the research outfit’s innovations. Built using anundisclosedportion of that investment, the new system ranks among the world’s five most powerful computers.Yes, but:While some experts see AGI on the horizon, others are less sanguine. Prominent researchers includingYann LeCun,Jerome Pesenti,Geoffrey Hinton, and Demis Hassabishave thrown cold water on AGI’s prospects.Why it matters:OpenAI and Microsoft believe that the new supercomputer will open the door to systems capable of running hundreds of language and vision models simultaneously. Microsoftsaidthat techniques developed on it eventually will benefit other Azure customers.\n\nWe’re thinking:We love supercomputers as much as anyone. But if Moore’s Law keeps up, today’s supercomputer will be tomorrow’s wrist watch.\n\nProgress in language models is spawning a new breed of chatbots and, unlike their narrow-domain forebears, they have the gift of gab. Recent research tests the limits of conversational AI.What’s new:Daniel Adiwardana and collaborators at Google Brain propose a human-scored measure, Sensibleness and Specificity Average (SSA), to rate chatbots on important qualities of human dialog. They also offerMeena, a chatbot optimized for open-domain, multi-turn conversation that scores well on the new metric.Key insight:Sensibleness (whether a statement makes logical and contextual sense) and specificity (how specific it is within the established context) are good indicators of performance in general conversation. While these criteria don’t lend themselves to gradient calculations, an existing loss function can serve as a proxy.How it works:Meena is a sequence-to-sequence model with anevolved transformerarchitecture. It comprises 2.6 billion parameters — a large number only a few months ago, lately overshadowed by ever larger models of up to17 billion parameters.\n\nResults:The researchers compared Meena, DialoGPT, Cleverbot, Mitsuku and XiaoIce. For each bot, they scored the SSA of both output transcripts and real-time conversational experiences. Meena showed considerably better performance, 79 percent versus the next-best score of 56 percent. The SSA scores of variously sized Meena implementations correlated with their scores on both human-likeness and perplexity.Why it matters:We’re all for better chatbots, and we’re especially charmed by Meena’s higher-education pun, “Horses go to Hayvard” (see animation above). But this work’s broader contribution is a way to compare chatbot performance and track improvements in conversational ability.Yes, but:SSA may not top every chatbot designer’s list of criteria. Google, with its mission to organize the world’s information, emphasizes sensibleness and specificity. But Facebook, whose business is built on friendly interactions that may be whimsical, emotional, or disjunct, is aiming for a different target (see “Big Bot Makes Small Talk” below).We’re thinking:Even imperfect metrics — like the much-criticized but widely usedBLEU scorefor natural language processing — give researchers a clear target and accelerate progress.\n\nMachine learning can estimate the impact of medical treatment in individual patients. Learn how in the final course of the AI For Medicine Specialization, available starting Friday, May 29, on Coursera.Enroll now\n\nAmazon, Google, and Microsoft are developing machine learning tools for the fossil fuel industry even as they pledge to limit greenhouse gas emissions.What’s new:Areportfrom the environmental group Greenpeace spells out partnerships between Big Tech and Big Oil, and contrasts them with each company’s promises to cut atmospheric carbon. Google responded bypromisingto stop developing new AI products for “upstream extraction” of fossil fuels.\n\nWhat they found:The report details 14 cases in which tech companies have built models to help oil and gas giants find, transport, and store fossil fuels.\n\nBehind the news:Training the latest deep learning models consumesimmense quantities of energy, and all three companies have made substantial commitments to reduce the toll.\n\nWhy it matters:The apparent contradiction between oil-industry work and efforts to cut carbon emissions highlights a tension between AI’s industrial potential and Big Tech’s corporate values. The Covid-19 pandemic has hit oil and gashard, and AI could help it recover once energy demand revives. At the same time, the technology can be a powerful tool in efforts to reduce greenhouse gases widely understood to be driving global climate change.We’re thinking:Private companies shouldn’t have the burden and responsibility of deciding which industries deserve access to AI resources. We would welcome a consistent framework crafted by governments or international bodies to promote uses of AI for net social benefit.\n\nWill reading soon become obsolete? A new system converts text articles into videos.What’s new:VidPress, a prototype project from Chinese tech giant Baidu, currently generates more than 1,000 narrated video summaries of news stories daily.How it works:VidPress synthesizes a two-minute video in around two and a half minutes, a task that typically takes a human editor 15 minutes.\n\nResults:Sixty-five percent of viewers who watched VidPress videos on Haokan, Baidu’s short-video service, viewed them all the way through, compared to a 50 percent watch-through rate for similar videos made by humans. The system’smost popular production, which describes a feud between Chinese pop stars Jiang Dawei and Zhu Zhiwen, has been viewed over 850,000 times.Behind the news:Baidu isn’t the only outfit to use AI to expedite video production, though its approach may be the most sophisticated.\n\nWhy it matters:Baidu’s Haokan service previously outsourced all of its productions. Now VidPress produces around 75 percent of its in-house videos, presumably saving the company time and money.We’re thinking:VidPress is fast, but what the internet really needs is a zillion-x speedup in the production of cat videos.\n\nFacebook recently rolled out its entry in the World’s Biggest Chatbot sweepstakes. In keeping with the company’s social-networking dominance, the bot is designed to excel at chitchat on any subject.What’s new:Led by Stephen Roller, Facebook researchers builtGenerative BST, a transformer-based model comprising up to 9.4 billion parameters. They trained the bot on their ownBlendedSkillTalkdataset of 5,000 conversations among 2,500 people who were instructed to be knowledgeable, empathetic, and generous with personal details.Key insight:The keys to small talk are personality, knowledge, empathy, and balancing response length (too short shows lack of interest, too long betrays poor listening). BlendedSkillTalk is designed to teach the first three traits. Finding the right response length is a matter of generation strategy.How it works:Many chatbots generate a set of potential responses and score the best one in a technique known as retrieval. In contrast, generative language models create responses one token at a time, often producingdull or repetitive output. Generative BST combines these approaches in a method calledretrieve and refine.\n\nResults:Human judges scored the performance of Generative BST and Google’s Meena (see “Toward Open-Domain Chatbots” above) according toAcute-Eval, a chatbot benchmark also developed by Facebook. Sixty-five percent of judges found Generative BST more human-like, while 75 percent found it more engaging. The researchers experimented with various techniques to build variants with different skills. For instance, 70 percent of judges found the version called BST Unlikelihood, which used a different generation approach, more human-like than Meena, but only 64 percent found it more engaging.Yes, but:The judges’ positive assessment of Generative BST’s human-like qualities relative to other chatbots doesn’t imply that any of them can carry on coherent conversations. You can read some nonsensical turns with Generative BSThere.Why it matters:Generative BST held the record for chatbot parameter count for only a short time before Microsoft announced its 17 billion-parameterTuring-NLG. But its malleable generator remains unique. Other researchers may be able to use this framework to create chatbots with particular qualities and behaviors.We’re thinking:Facebook’s bot takes Big Tech rivalry to a new level. The Googlers behind Meena reported a conversation (illustrated above) in which their system, considering education for barnyard animals, punned, “Horses go to Hayvard.” The Facebook authors tried out the joke on Generative BST. The bot merely deadpanned: “I don’t get it.”",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Andrews20Letter203.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Pacman3.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Supercomputer2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/GoogleBot.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_AI4MC3BatchAd.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Oil3-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Video3.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/FacebookBot.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-176/",
    "title": "issue 176",
    "date": "",
    "reading_time": "",
    "content": "As the winter holiday approaches, it occurs to me that, instead of facing AI winter, we are in a boiling-hot summer of AI.\n\nThe vast majority of economic value created by AI today comes through the tool of supervised learning, trained to generate short labels (such as spam/not-spam) or a sequence of labels (such as a transcript of audio). This year, generative AI, which is built on top of supervised learning, arrived as a second major tool that enables AI to generate complex and compelling outputs such as images or paragraphs of text.\n\nSome previous attempts to develop major new tools — for example, reinforcement learning — have not yet borne fruit commensurate with their hype. But generative AI is working well enough that it’s creating a new paradigm for AI applications.\n\nAnd supervised learning is still far from achieving even a small fraction of its potential! Millions of applications that can be solved by supervised learning have not yet been built. Many teams are still trying to figure out best practices for developing products though supervised learning.\n\nIn the coming year and beyond, I look forward to wrestling with generative AI to create massive amounts of value for everyone. I feel lucky to be alive in this era, when technology is growing rapidly and we have an opportunity to create the future together! I feel even luckier to share this world with my family and with you.\n\nHappy holidays,\n\nAndrew\n\nAs we settle into a cup of hot cocoa and badger ChatGPT to suggest holiday gifts for our loved ones, we reflect on a year of tremendous advances in AI. Systems that generate human-like text, images, and code — with video and music on the horizon — delighted users even as they raised questions about the future of creativity. Models that decode chemistry and physics drove scientific discovery, while governments moved to control the supply of specialized microprocessors that make such innovations possible. While such developments give uspause, in this special issue ofThe Batch— asinpastyearsat this season — we survey the marvels wrought by AI in 2022.\n\nPictures produced by AI went viral, stirred controversies, and drove investments.What happened:A new generation of text-to-image generators inspired a flood of experimentation, transforming text descriptions into mesmerizing artworks and photorealistic fantasies. Commercial enterprises were quick to press the technology into service, making image generation a must-have feature in software for creating and editing graphics.Driving the story:Models that generate media became the public face of AI thanks to friendly user interfaces, highly entertaining output, and open APIs and models.\n\nYes, but:Such models are trained on images scraped from the web. Like large language models, they inherit biases embedded in online content and imitate the inflammatory styles of expression.\n\nBehind the news:Diffusion models generate output by starting with noise and removing it selectively over a series of steps.Introducedin 2015 by researchers at UC Berkeley and Stanford, they remained in the background for several years until furtherworkshowed that they could produce images competitive with the output of generative adversarial networks (GANs). Stability AI put a diffusion model at the heart of Stable Diffusion. OpenAI, which based the initial version of DALL·E on a GAN, updated it with a diffusion model at around the same time.\n\nWhere things stand:The coming year is shaping up for a revolution in computer-aided creativity. And the groundswell of generated imagery isn’t going to stop at pictures. Google and Meta released impressivetext-to-videomodelsthis year, and OpenAI acceleratedtext-to-3D-objectgeneration by an order of magnitude.\n\nBehind schedule on a software project? There’s an app for that.What happened:Language models fine-tuned on computer code proved capable of generating software routines similar to the work of experienced developers — though the results can be hit-or-miss.\n\nDriving the story:AI-powered code generators made their way into large companies, and even small-time developers (and non-developers) gained access to them.\n\nBehind the news:Users of OpenAI’s GPT-3 language model showed that it couldgenerate working codeas early as mid-2020. A year later, OpenAI introduced a fine-tuned version known asCodex, which serves as the foundation for GitHub's Copilot.\n\nYes, but:The widely available versions of this technology aren’t yet able to write complex programs. Often their output looks right at first glance but turns out to be buggy. Moreover, their legal status may be in jeopardy. A class-action lawsuit against GitHub, OpenAI, and Microsoft claims that the training of Codex violated open source licensing agreements. The outcome could have legal implications for models that generate text, images, and other media as well.\n\nWhere things stand:AI-powered coding tools aren’t likely to replace human programmers in the near future, but they may replace the tech question-and-answer site Stack Overflow as the developer’s favorite crutch.\n\nWork on vision transformers exploded in 2022.\n\nWhat happened:Researchers publishedan abundance of ViT papersduring the year. A major theme: combining self-attention and convolution.\n\nDriving the story:A team at Google Brain introducedvision transformers(ViTs) in 2020, and the architecture has undergone nonstop refinement since then. The latest efforts adapt ViTs to new tasks and address their shortcomings.\n\nBehind the news:While much ViT research aims to surpass and ultimately replace convolutional neural networks (CNNs), the more potent trend is to marry the two. The ViT’s strength lies in its ability to consider relationships between all pixels in an image at small and at large scales. One downside is that it needs additional training to learn in ways that are baked into the CNN architecture after random initialization. CNN’s local context window (within which only local pixels matter) and weight sharing (which enables it to process different image locations identically) help transformers to learn more from less data.\n\nWhere things stand:The past year expanded the Vision Transformer’s scope in a number of applications. ViTsgenerated plausible successive video frames,generated 3D scenes from 2D image sequences, anddetected objects in point clouds. It's hard to imagine recent advances in text-to-image generators based on diffusion models without them.\n\nMathematics for Machine Learning and Data Scienceis our next specialization. Set to launch in January 2023, it’s a beginner-friendly way to master the math behind AI algorithms and data analysis techniques.Join the waitlistand be among the first to enroll!\n\nResearchers pushed the boundaries of language models to address persistent problems of trustworthiness, bias, and updatability.What happened:While many AI labs aimed to make large language models more sophisticated by refining datasets and training methods — including methods that trained a transformer totranslate 1,000 languages— others extended model architectures to search the web, consult external documents, and adjust to new information.\n\nDriving the story:The capacity of language models to generate plausible text outstrips their ability to discern facts and resist spinning fantasies and expressing social biases. Researchers worked to make their output more trustworthy and less inflammatory.\n\nBehind the news:Amid the progress came a few notable stumbles. The public demo Meta’s Galactica, a language model trained to generate text on scientific and technical subjects, lasted three days in November before its developers pulled the plug due to its propensity to generate falsehoods and cite nonexistent sources. In August, the chatbot BlenderBot 3, also from Meta, quickly gained a reputation for spouting racist stereotypes and conspiracy theories.\n\nWhere things stand:The toolbox of truth and decency in text generation grew substantially in the past year. Successful techniques will find their way into future waves of blockbuster models.\n\nIndividual deep learning models proved their mettle in hundreds of tasks.What happened:The scope of multi-task models expanded dramatically in the past year.\n\nDriving the story:Researchers pushed the limits of how many different skills a neural network can learn. They were inspired by the emergent skills of large language models — say, the ability to compose poetry and write computer programs without architectural tuning for either — as well as the capacity of models trained on both text and images to find correspondences between the disparate data types.\n\nBehind the news:The latest draft of the European Union’s proposed AI Act, which could become law in 2023, would require users of general-purpose AI systems to register with the authorities, assess their systems for potential misuse, and conduct regular audits. The draft defines general-purpose systems as those that “perform generally applicable functions such as image/speech recognition, audio/video generation, pattern-detection, question-answering, translation, etc.,” and are able to “have multiple intended and unintended purposes.” Some observers have criticized the definition as too broad. The emerging breed of truly general-purpose models may prompt regulators to sharpen their definition.\n\nWhere things stand:We’re still in the early phases of building algorithms that generalize to hundreds of different tasks, but the year showed that deep learning has the potential to get us there.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/12/ezgif.com-gif-maker.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/12/ezgif.com-gif-maker--1-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/12/ezgif.com-gif-maker--2-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/12/ezgif.com-gif-maker--3-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/12/ezgif.com-gif-maker--4-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed-3--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/12/ezgif.com-gif-maker--5-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/12/ezgif.com-gif-maker--6-.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-13/",
    "title": "issue 13",
    "date": "",
    "reading_time": "",
    "content": "In this series exploring why machine learning projects fail, let’s examine the challenge of “small data.”\n\nGiven 1 million labeled images, many teams can build a good classifier using open source. But say you are building a visual inspection system for a factory to detect scratches on smartphones. No smartphone manufacturer has made 1 million scratched phones (that would have to be thrown away), so a dataset of 1 million images of scratched phones does not exist. Getting good performance with 100 or even 10 images is needed for this application.\n\nDeep learning has seen tremendous adoption in consumer internet companies with a huge number of users and thus big data, but for it to break into other industries where dataset sizes are smaller, we now need better techniques for small data.\n\nIn the manufacturing system described above, the absolute number of examples was small. But the problem of small data also arises when the dataset in aggregate is large, but the frequency of specific important classes is low.\n\nSay you are building an X-ray diagnosis system trained on 100,000 total images. If there are few examples of hernia in the training set, then the algorithm can obtain high training- and test-set accuracy, but still do poorly on cases of hernia.\n\nSmall data (also called low data) problems are hard because most learning algorithms optimize a cost function that is an average over the training examples. As a result, the algorithm gives low aggregate weight to rare classes and under-performs on them. Giving 1,000 times higher weight to examples from very rare classes does not work, as it introduces excessive variance.\n\nWe see this in self-driving cars as well. We would like to detect pedestrians reliably even when their appearance (say, holding an umbrella while pushing a stroller) has low frequency in the training set. We have huge datasets for self-driving, but getting good performance on important but rare cases continues to be challenging.\n\nHow do we address small data? We are still in the early days of building small data algorithms, but some approaches include:\n\nBenchmarks help drive progress, so I urge the development of small data benchmarks in multiple domains. When the training set is small, ML performance is more variable, so such benchmarks must allow researchers to average over a large number of small datasets to obtain statistically meaningful measures of progress.\n\nMy teams are working on novel small data techniques, so I hope to have details to share in the future.\n\nKeep learning!\n\nAndrew\n\nIn March 2018, one of Uber’s self-driving cars became the first autonomous vehicle reported to have killed a pedestrian. A newreportby U.S. authorities suggests that the accident occurred because the car’s software was programmed to ignore jaywalkers.What happened:The National Transportation Safety Board released the results of an investigation into Uber’s self-driving AI. According to the agency’s analysis, the model failed to classify the victim properly because she wasn’t near a crosswalk — a feature the model used to classify pedestrians in the road.What the report says:The vehicle’s computer log in the moments leading up to the crash highlights a number of flaws in the system:\n\nAftermath:Immediately after the accident, Uber took its autonomous test vehicles off the road. The victim’s family sued the company and settled out of court. Uber has since resumed self-driving tests in Pittsburgh (issuing a safety-oriented promotionalvideo, excerpted above, to mark the occasion). Responding to the NTSB report, Uber issued a statement saying the company “has adopted critical program improvements to further prioritize safety” and “look[s] forward to reviewing their recommendations.”Why it matters:Next week, the NTSB will hold a hearing where it will announce its judgment of Uber’s role in the accident. Federal legislators and state authorities will be watching these hearings, which are likely to bring forth a number of recommendations on ways to ensure the self-driving car industry is operating safely.We’re thinking:Government oversight is critical for progress on autonomous vehicles, both to hold companies accountable for safety and to ensure that safety information is widely disseminated. Regulation has made safety a given in commercial aviation; airlines compete on routes, pricing, and service, not how safe they are. Similarly, the autonomous vehicle industry’s commitment to safety is something that consumers should be able to take for granted. And, while we’re at it, let’s build sensible rules for testing AI in other critical contexts such as health care, education, and criminal justice.\n\nThe world’s largest stock market is using AI to flag suspicious trading in real time.What’s new:Nasdaq istestinga deep learning system to monitor trading of its U.S. equities. Named Chiron, the system watches for behaviors that indicate potential market manipulation.How it works:Nasdaq will spend a year training Chiron on trade data annotated for signs of manipulation.\n\nBehind the news:This isn’t Nasdaq’s first foray into AI. In 2001, the company launched a program calledSonarto monitor sources like news stories and SEC filings for suspicious activity.Why it matters:Nasdaq operates 29 exchanges in the U.S., Canada, UK, and EU, and it licenses its surveillance technology to other exchanges, regulatory agencies, and financial firms around the world. It has the highest volume of trades of any exchange in the world. Widespread fraud within Nasdaq’s network not only would be catastrophic for its business, it could send shock waves through the global economy.We’re thinking:Fraudsters have access to deep learning, too. Expect a high-stakes game of cat and mouse in the years to come.\n\nA word-embedding model typically learns vector representations from a large, general-purpose corpus like Google News. But to make the resulting vectors useful in a specialized domain, such as veterinary medicine, they must be fine-tuned on a smaller, domain-specific dataset. Researchers from Facebook AI offer a more accurate method.What’s new:Rather than fine-tuning, Piotr Bojanowski and colleagues developed amodelthat aligns word vectors learned from general and specialized corpora.Key insight:The authors drew inspiration from the way multilingual word vectors are learned. They treated general-purpose and domain-specific corpora as separate languages and used a word-embedding model to learn independent vectors from each. Then they aligned the vectors from one corpus with those from another.How it works:To align word vectors from two corpora, common words are used to find a consistent way to represent all words. For example, if one corpus is {human,cat} and the other is {cat,dog}, the model applies a transformation that unifies the dog word vectors while retaining the relative positions of the word vectors between cats, dogs, and humans.\n\nResults:The authors tested this approach to learning word vectors on tasks that include predicting analogies and text classification in a dataset where the test set has a slightly different word usage than the training set. Models that use word vectors learned via alignment outperformed those that use word vectors fine-tuned in the usual way. The new method’s advantage was more pronounced when the domain-specific dataset was relatively small.Why it matters:Machine learning engineers need tools that enable existing word representations to capture specialized knowledge. The alignment technique could be a boon in any situation where general-purpose word vectors don’t capture the meanings at play.We’re thinking:Open-source, pretrained word embeddings have been a boon to NLP systems. It would be great to have freely available word embeddings that captured knowledge from diverse fields like biology, law, and architecture.\n\nHow do you find good values for hyperparameters? Learn how to organize your hyperparameter tuning process in Course 2 of the Deep Learning Specialization.Enroll now\n\nNeuroevolution, which combines neural networks with ideas drawn from Darwin, is gaining momentum. Its advocates claim that they can achieve faster, better results by generating a succession of new models, each slightly different than its predecessors, rather than relying on a purpose-built model.What’s new:Evolutionary strategies racked up a number of successes in the past year. They contributed to DeepMind’s AlphaStar, which can beat 99.8 percent of players of StarCraft 2, and to models that bested human experts in the videogames Montezuma’s Revenge and Pitfall Harry. AnarticleinQuantasurveys the field, focusing on neuroevolution pioneer and Uber senior researcher Kenneth Stanley.How it works:Traditionally, evolutionary approaches have been used to generate algorithms that solve a specific problem or perform best on a particular task. The best solutions are randomly mutated to find variations that improve performance. Neuroevolution applies random mutations to neural network weights and sometimes activation functions, hyperparameters, or architectures. Good models emerge over many iterations, sometimes crossing traits among many behavioral niches.\n\nYes, but:Evolutionary strategies require huge amounts of computation, even by the power-hungry standards of deep learning. Weights and other variables evolve randomly, so finding good models can take a long time. The random path itself is a drawback. Although researchers may set out to solve one problem, the evolutionary process may lead in other directions before wending its way back to the intended path — if it ever does.Why it matters:Neuroevolution is a radical departure from typical neural networks and, by some accounts, a useful complement. Evolutionary approaches assign a far larger role to randomness, and randomly beneficial effects can compound over generations to find solutions or generate networks more effective than a human would have designed.We’re thinking:Randomized search algorithms are a powerful approach to optimization, but their relation to biological evolution has been a subject of debate. With rising computational power and more complex challenges, such algorithms — whether evolutionary or not — may be poised to grow.\n\nA deep learning system is helping biologists who survey offshore fish populations to prevent overfishing.What’s new:The U.S. agency in charge of protecting ocean resources is using an underwater camera and neural network tocount fishin real time.How it works:Alaska’s walleye pollock fishery is America’s largest by volume. (You may not recognize a walleye pollock, but you’ve probably eaten one in fish sticks, fast-food sandwiches, or imitation crab meat. They are delicious!) Scientists with the U.S. National Oceanic and Atmospheric Administration chose this fishery as a pilot in their automatic fish-identification program.\n\nBehind the news:Congress passed the Sustainable Fisheries Act in 1996, requiring NOAA to track U.S.commercial fish populations. For some fisheries, the biologists venture out on boats, casting nets to capture samples of what’s in the water. They dump the contents onto the deck, count and measure each creature, release the haul, and cast the net again. NOAA launched the initiative to automate these counts using artificial intelligence in 2014.Why it matters:Fish stock assessments, and the limits they impose on commercial fishing, keep fish populations sustainable and fisheries productive over the long term. Automating the process reduces error and frees up biologists for other work.We’re thinking:Deep learning is producing more and better data for environmental stewardship. It’s up to citizens to put that data to best use.\n\nLooking at images, people see outlines before the details within them. A replacement for the traditional convolutional layer decomposes images based on this distinction between coarse and fine features.What’s new:Researchers at Facebook AI, National University of Singapore, and Yitu Technology devisedOctConv, a convolutional filter that reduces the computational cost and memory footprint of image processing networks without degrading performance.Key insight:Yunpeng Chen and collaborators took their inspiration from signal processing: An audio signal can be represented as a set of discrete frequencies rather than a single waveform. Similarly, an image can be said to contain low-frequency information that doesn’t change much across space and high-frequency imagery that does. Low-frequency image features are shapes, while high-frequency image features comprise details such as textures. By capturing them separately, OctConv can reduce redundant information.How it works:The outputs of a convolutional layer’s hidden units are feature maps that hold 2D spatial information. Feature maps often encode redundant information across an image’s color channels. OctConv cuts this redundancy by using a frequency-channel representation instead of the usual color-channel representation.\n\nResults:A ResNet-152 with OctConv rather than CNN filters was 0.2 percent more accurate on ImageNet than the next best model, with 15 percent less computation during testing. An I3D model pair with OctConv filters was 2 percent more accurate on Kinetics-600, a video dataset for predicting human actions, with 10 percent less computation.Why it matters:OctConv filters can substitute for standard convolutional filters for better performance, reduced computation, and smaller footprint. The authors suggest subdividing beyond their low- and high-frequency scheme. That would yield greater savings in size and training time, but its impact on performance is a subject for further experimentation.Takeaway:Memory compression and pruning techniques have been important for deploying neural networks on smartphones and other low-powered, low-memory devices. OctConv is a fresh approach to shrinking image-processing networks that takes into account memory and computation primitives.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/letter220SIZED-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/uber.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/nasdaq.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/BatchGraphicNovember132019.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Hulk20Hyperparameter.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/walker.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/fish.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/octaveSIZED201.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-275/",
    "title": "issue 275",
    "date": "",
    "reading_time": "",
    "content": "Large language models (LLMs) are typically optimized to answer peoples’ questions. But there is a trend toward models also being optimized to fit into agentic workflows. This will give a huge boost to agentic performance!\n\nFollowing ChatGPT’s breakaway success at answering questions, a lot of LLM development focused on providing a good consumer experience. So LLMs were tuned to answer questions (“Why did Shakespeare writeMacbeth?”) or follow human-provided instructions (“Explain why Shakespeare wroteMacbeth”). Alargefractionof the datasets for instruction tuning guide models to provide more helpful responses to human-written questions and instructions of the sort one might ask a consumer-facing LLM like those offered by the web interfaces of ChatGPT, Claude, or Gemini.\n\nBut agentic workloads call on different behaviors. Rather than directly generating responses for consumers, AI software may use a model in part of an iterativeworkflowto reflect on its own output, use tools, write plans, and collaborate in a multi-agent setting. Major model makers are increasingly optimizing models to be used in AI agents as well.\n\nTake tool use (orfunction calling). If an LLM is asked about the current weather, it won’t be able to derive the information needed from its training data. Instead, it might generate a request for an API call to get that information. Even before GPT-4 natively supported function calls, application developers were already using LLMs to generate function calls, but by writing more complex prompts (such as variations ofReActprompts) that tell the LLM what functions are available and then have the LLM generate a string that a separate software routine parses (perhaps with regular expressions) to figure out if it wants to call a function.\n\nGenerating such calls became much more reliable after GPT-4 and then many other models natively supported function calling. Today, LLMs can decide to call functions to search for information forretrieval-augmented generation(RAG), execute code,  send emails, place orders online, and much more.\n\nRecently, Anthropic released a version of its model that is capable of computer use, using mouse-clicks and keystrokes to operate a computer (usually a virtual machine). I’ve enjoyed playing with thedemo. While other teams have been prompting LLMs to use computers to build a new generation of RPA (robotic process automation) applications, native support for computer use by a major LLM provider is a great step forward. This will help many developers!\n\nAs agentic workflows mature, here is what I am seeing:\n\nMost LLMs have been optimized for answering questions primarily to deliver a good consumer experience, and we’ve been able to “graft” them into complex agentic workflows to build valuable applications. The trend of LLMs built to support particular operations in agents natively will create a lot of lift for agentic performance. I’m confident that large agentic performance gains in this direction will be realized in the next few years.\n\nKeep learning!\n\nAndrew\n\nPrevent common issues in applications based on large language models such as hallucinations, data leaks, and off-topic responses. Build guardrails that protect against incorrect or sensitive responses in our new short course, made in collaboration with GuardrailsAI.Sign up now!\n\nA new open source large language model outperforms competitors, including the open-weights Llama 3.1 405B, on a variety of benchmarks.\n\nWhat’s new:Tencent releasedHunyuan-Large, a mixture-of-experts model withopen codeandopen weights. It comes in base and instruction-tuned versions, both of which can process a relatively large input context window of 256,000 tokens. It’s free for developers outside the European Union who have fewer than 100 million monthly users. You can experiment with ithere.\n\nMixture of experts (MoE) basics:The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.\n\nHow it works:Hunyuan-Large comprises 389 billion parameters but uses 52 billion parameters to process any given input. The team pretrained the model on 7 trillion tokens primarily of English and Chinese text, of which 5.5 trillion tokens came from unspecified sources and 1.5 trillion synthetic tokens were generated by unspecified large language models. The models used to generate training data were “specialized” to provide expert-level responses in various domains. The team fine-tuned Hunyuan-Large on unspecified datasets of instructions and human feedback.\n\nResults:The team compared the Hunyuan-Large models to four open source models and their instruction-tuned versions: Llama 3.1 70B, Llama 3.1 405B, and the MoE models Mixtral-8x22B and DeepSeek-V2.\n\nWhy it matters:Hunyuan-Large generally outperforms Llama 405B, achieving the performance of a 405 billion parameter model while computing only 52 billion parameters. That’s a significantly lower processing requirement, and the model is free for many purposes.\n\nWe’re thinking:Setting asideSwitch Transformer— a 1.6 trillion parameter behemoth that was built to test the limits of size rather than performance — Hunyuan-Large is among the largest MoE models we’ve come across. It’s an impressive demonstration of what larger MoE models can accomplish.\n\nTwo top AI companies changed their stances on military and intelligence applications.\n\nWhat’s new:Meta made its Llama family of large language modelsavailableto the U.S. government for national security purposes — a major change in its policy on military applications. Similarly, Anthropic willofferits Claude models to U.S. intelligence and defense agencies.\n\nHow it works:Meta and Anthropic are relying on partnerships with government contractors to navigate the security and procurement requirements for military and intelligence work.\n\nBehind the news:In 2018, Google facedbacklashwhen it won a contract with the U.S. government to buildProject Maven, an AI-assisted intelligence platform. Employees protested, resigned, and called on the company to eschew military AI work. Googlewithdrewfrom the project and Palantir took it over. Subsequently, many AI developers, including Meta and Anthropic, have forbidden use of their models for military applications. Llama’s new availability to U.S. military and intelligence agencies is a notable exception. In July, Anthropic, too, began toaccommodateuse of its models for intelligence work. Anthropic still prohibits using Claude to develop weapons or mount cyberattacks.\n\nWhy it matters:The shift in Meta’s and Anthropic’s policies toward military uses of AI is momentous. Lately AI has become a battlefield staple in the form of weaponizeddrones, and AI companies must take care that their new policies are consistent with upholding human rights. Military uses for AI include not only weapons development and targeting but also potentially life-saving search and rescue, logistics, intelligence, and communications. Moreover, defense contracts represent major opportunities for AI companies that can fund widely beneficial research and applications.\n\nWe’re thinking:Peace-loving nations face difficult security challenges, and AI can be  helpful in meeting them. At the same time, the militarization of AI brings challenges to maintaining peace and stability, upholding human rights, and retaining human control over autonomous systems. We call on developers of military AI to observe theguidelines, proposed by Responsible Artificial Intelligence in the Military, which are endorsed by more than 60 countries and call for robust governance, oversight, accountability, and respect for human rights.\n\nSome voters navigated last week’s United States elections with help from a large language model that generated output based on verified, nonpartisan information.\n\nWhat’s new:Perplexity, an AI-powered search engine founded in 2022 by former OpenAI and Meta researchers, launched itsElection Information Hub, an AI-enhanced website that combines AI-generated analysis with real-time data. The model provided live updates, summaries, and explanations of key issues in the recent national, state, and local elections in the U.S. (The hub remains live, but it no longer displays information about local contests or delivers detailed results for election-related searches.)\n\nHow it works:Perplexity partnered with Associated Press for election news andDemocracy Works, a nonprofit that develops technology and data related to democracy. Democracy Works provided anAPIfor information about elections, issues, and polling locations.\n\nBehind the news:While Perplexity courted demand for AI-generated information about the U.S. elections, other search-engine providers took more cautious approaches. You.com offered an election chatbot thatfocusedon vote tallies provided by Decision Desk HQ, an election information broker, rather than information about issues or polling locations. Google and Microsoft Bing emphasized information from vetted sources. Microsoft Copilot and OpenAI (which had launched its SearchGPT service the week before the election) simply declined to answer election-related questions, referring users to other sources of information.\n\nWhy it matters:Chatbots are maturing to the point where they can provide fairly trustworthy information in high-stakes decisions like elections. The combination of web search and retrieval-augmented generation contributes to decision support systems that are both personalized and accurate.\n\nWe’re thinking:Perfect information is hard to come by in any election. Traditional media, social media, and your uncle’s strongly held opinions all have limitations. Chatbots aren’t perfect either, but when they’re properly designed to avoid biased output and outfitted with high-quality information sources, they can help strengthen users’ choices and voices.\n\nAn open source package inspired by the commercial agentic code generator Devin aims to automate computer programming and more.\n\nWhat’s new:OpenHands, previously known as OpenDevin, implements a variety of agents for coding and other tasks. It was built by Xingyao Wang and a team at University of Illinois Urbana-Champaign, Carnegie Mellon, Yale, University of California Berkeley, Contextual AI, King Abdullah University of Science and Technology, Australian National University, Ho Chi Minh City University of Technology, Alibaba, and All Hands AI. The code is free todownload, use, and modify.\n\nHow it works:OpenHands provides a set of agents, or workflows for the user’s choice of large language models. Users can command various agents to generate, edit, and run code; interact with the web; and perform auxiliary tasks related to coding and other work. The agents run in a secure Docker container with access to a server to execute code, a web browser, and tools that, say, copy text from pdfs or transcribe audio files.\n\nResults:Overall, OpenHands agents achieve similar performance to previous agents on software engineering problems, web browsing, and miscellaneous tasks like answering questions. For example, fixing issues in Github inSWE-Bench, the CodeAct agent using Claude 3.5 Sonnet solved 26 percent whileMoatless Toolsusing the same model solved 26.7 percent. OnGPQA Diamond, a set of graduate-level questions about physics, chemistry, and biology, the CodeAct agent using GPT-4-turbo with search wrote code to perform the necessary calculations and found relevant information to answer the questions, achieving 51.8 percent accuracy. GPT-4 with search achieved 38.8 percent accuracy.\n\nWhy it matters:Agentic workflows are rapidly expanding the scope and capabilities of large language models. As open source software, this system gives developers an extensible toolkit for designing agentic systems. Although it’s oriented toward coding, it accommodates a variety of information-gathering, -processing, and -publishing tasks.\n\nWe’re thinking:This system lets users tailor custom agents simply by rewriting prompts. We look forward to seeing what non-programmers do with it!\n\nBuild AI applications that have long-term agentic memory! Our short course “LLMs as Operating Systems: Agent Memory” is based on insights from the MemGPT paper and taught by two of its coauthors. Learn how to implement persistent, efficient memory management for applications based on large language models.Enroll for free",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--33-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/11/The-Batch-ads-and-exclusive-banners---2024-11-12T092640.086.png",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--27-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--34-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--28-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--29-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/11/The-Batch-ads-and-exclusive-banners---2024-11-12T092833.534.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-24/",
    "title": "issue 24",
    "date": "",
    "reading_time": "",
    "content": "I just finished reading BJ Fogg’s new book,Tiny Habits: The Small Changes That Change Everything. Fogg explains that the best way to build a new habit is to start small and succeed, rather than starting too big and giving up. For example, rather than trying to exercise for 30 minutes a day, he recommends aspiring to do just one push-up, and doing it consistently.\n\nThis approach may be helpful to those of you who want to spend more time studying. If you hold yourself accountable for watching, say, 10 seconds of an educational video every day — and you do so consistently — the habit of studying daily will grow naturally. Even if you learn nothing in that 10 seconds, you’re establishing the habit of studying a little every day. On some days, maybe you’ll end up studying for an hour.\n\nOver the years, I have found a few resources for developing personal productivity that I love. My top picks includeGetting Things Doneby David Allen, the classicThe 7 Habits of Highly Effective Peopleby Stephen R. Covey, andLearning How to LearnBarbara Oakley (I recommend the Courseracourse). I’m tempted to add Tiny Habits to this list.\n\nKeep learning!\n\nAndrew\n\nGoogle, Facebook, and Amazon aren’t the only places to work on cutting-edge AI products. Archis Joglekar parlayed his study of nuclear physics into a job building models at Noble.ai, where he helps other scientists speed up R&D.Read more\n\nA company that analyzes online information to predict epidemics spotted the upsurge in coronavirus at least a week ahead of public-health authorities.What’s new:Canadian startupBlueDotalerted customers to the outbreak in the Chinese city of Wuhan on New Year’s Eve,Wiredreported. The U.S. Centers for Disease Control and Prevention issued its warning on January 6, and the World Health Organization followed suit three days later. The respiratory illness as of this writing has infected more than 6,000 people and killed more than 130, mostly in China.How it works:Founded in 2014, BlueDot aims to stop the spread of infectious diseases by giving healthcare workers early warning, so they can identify and treat people who become infected.\n\nBehind the news:In 2008, Google undertook a similar effort to forecast influenza outbreaks based on search terms entered by users. In initial research, Google Flu Trends tracked the number of cases two weeks faster than the CDC. However, it dramatically underestimated the peak of the 2013 flu season and was shuttered soon afterward. Subsequent analysisconcludedthat the algorithm overfit seasonal search terms unrelated to flu.\n\nWhy it matters:Rapid detection of new diseases is crucial to avoid global pandemics. Virulent diseases often can be contained if they’re caught early enough, but every hour compounds the number of people exposed and thus the number of cases. An epidemic can quickly overwhelm healthcare systems, leaving people even more exposed.We’re thinking:It’s hard to know how well today’s techniques will play out tomorrow. But the ability to catch potential pandemics before they explode is too valuable not to try.\n\nSchool teachers may not like to hear this, but sometimes you get the best answer by peeking at your neighbor’s paper. A new language model framework peeks at the training data for context when making a prediction.What’s new:Facebook AI and Stanford researchers led by Urvashi Khandelwal enhanced language models that predict the next word in an incomplete sentence by enabling them to search for potential answers in the training data. They call their algorithmkNN-LM.Key insight:It’s much easier for a model to identify two sentence fragments that have similar meanings than it is to complete them.kNN-LM takes advantage of the easier task to improve performance on the harder one. Given a sentence fragment and asked to predict the next words, it searches the training set for sentences similar to that sentence fragment and uses what it finds to help predict the missing words. For example, the model might match a target starting, “Dickens is the author of ___,” with the training sentence, “Dickens wroteOliver Twist.” The model then knows that “Oliver Twist” may be appropriate to add to the target.How it works:The authors offer a pretrained model, vector representations of training sentences, and an algorithm for combining information when analyzing a test sentence. Their approach works with any pretrained neural language model, but they used transformer networks in most experiments.\n\nResults:Tested on adatasetof Wikipedia articles,kNN-LM achieved a score of 15.79 for perplexity, a measure of predictive accuracy, more than 10 percent better than the previous state-of-the-artmodel.Why it matters:Language models likely won’t interpret technical terms found in, say, the NuerIPS proceedings, if they’re trained on Wikipedia.kNN-LM lets them find less related words in the training data, potentially improving generalization to obscure subject matter.We’re thinking:A key step for winning computer vision competitions like ImageNet has been to train multiple models and ensemble (or average) them. This confers perhaps a 1 percent boost in performance, but it’s impractical for most applications because of the computational expense.kNN-LM appears to require a significant computational expense as well, and we look forward to researchers diving deeper into the computational implications.\n\nA new tool connects novice programmers with projects that match their experience and interests.What’s new:Github’sGood First Issuestool uses deep learning to find easy-to-fix issues among the collaborative software development platform’s multitude of open source projects.How it works:Github, which is owned by Microsoft, enables developers to collaborate freely worldwide. Participants often flag bugs to be fixed or features to be implemented, but beginners may have trouble figuring out which are appropriate to their skill level.\n\nBehind the news:An earlier version used traditional computing to query a list of 300 beginner-friendly labels. However, it surfaced only about 40 percent of relevant issues, the company said.\n\nWhy it matters:Github is a focal point of software development and the heart of the open source movement. Helping people figure out where they can have the most impact can only make it more productive.We’re thinking:What a great tool for aspiring developers! When jumping into AI (or, indeed, most disciplines), it’s better to start small and succeed than to start too big and fail.\n\nHow can you advance your AI career as a machine learning engineer, data scientist, or software engineer? This report fromWorkera, a deeplearning.ai company, walks you through the career paths you can take, tasks you’ll work on, and skills recruiters are looking for.Download the report\n\nCan machine learning help address thescourgeof opioid addiction?What’s new:A public health researcher developed a neural network that spots sellers of opioids on social media,Recodereported.How it works:The model built by University of California professorTim K. Mackeysifts through Instagram profiles to find those that offer drugs.\n\nYes, but:Online opioid sales represent only a small fraction of the total, RAND Corporation drug policy expert Bryce Pardo toldRecode. Mackey’s tool spots small-scale dealers, but it can’t do much to bring down the cartels responsible for much of the supply, he said.\n\nWhy it matters:Shutting down suppliers could save lives. Two million Americans are addicted to opioids, and 130 people die from overdoses every day,accordingto the National Institute on Drug Abuse.We’re thinking:Will dealers resort to adversarial examples to thwart such automatic detection algorithms? Unfortunately, there’s plenty of financial incentive to advertise illegal opioids on social networks.\n\nA new system marks a step forward in converting text to speech: It’s fast at inference, reduces word errors, and provides some control over the speed and inflection of generated speech.What’s new:Yi Ren, Yangjun Ruan, and their co-authors at Zhejiang University and Microsoft proposeFastSpeech, a text-to-speech system that processes text sequences in parallel rather than piece by piece.Key insight:Previous models predict phonemes, or units of sound, sequentially. This so-called autoregressive approach lets the model base each phoneme on those that came before, so the output can flow like natural speech. But it also limits how fast the model can generate output. Instead, FastSpeech uses a duration predictor that determines the length of each phoneme. Knowing durations ahead of time allows the model to generate phoneme representations independently, yielding much faster operation while maintaining the flow.How it works:Neural text-to-speech models typically generate a mel-spectrogram that represents the frequency spectrum of spoken words. FastSpeech generates mel-spectrograms using a variant on the transformer network known as a feed-forward transformer network (abbreviated FFT, but not to be confused with a fast Fourier transform).\n\nResults:Using theLJSpeechdataset for training and evaluation, FastSpeech was 270 times faster at generating mel-spectrograms than a transformer-based autoregressivesystem,and 38 times faster at generating speech output, with audio quality nearly as good. The generated speech was free of repetitions and omissions.Why it matters:LSTMs and other autoregressive models have boosted accuracy in generating text and speech. This work highlights an important trend toward research into faster alternatives that don’t sacrifice output quality.We’re thinking:In the long run, end-to-end systems that synthesize the output audio directly are likely to prevail. Until then, approaches like FastSpeech still have an important role.\n\nAs employers turn to AI to evaluate job applicants, a U.S. state imposed limits on how such tools can be used.What’s new:The Illinois legislature passed theAI Video Act, which gives candidates a measure of control over how hiring managers collect and store video interviews.How it works:The latest generation of video screening tools typically requires applicants to record themselves answering pre-determined questions. A model analyzes their verbal performance and body language to evaluate how well they fit the bill.\n\nYes, but:The Illinois law does not define artificial intelligence, lawyers at Reed Smith LLC pointed out inTechnology Law Dispatch. Such lack of precision could lead to disputes if candidates believe they’re being evaluated by AI while the company disagrees. Another ambiguity: The law doesn’t specify consequences for violations.\n\nWhy it matters:The move in Illinois is a concrete step amid a rising chorus of calls to rein in AI. Last week, Alphabet’s Sundar Pichai spoke out in favor of regulation, and IBM proposed guidelines for reducing algorithmic bias. The EU will vote on limits for automated decision making in February.We’re thinking:The new law may soon run up against the Trump Administration’s recent mandate to keep barriers to innovation in AI low. Nonetheless, it’s important to strike a balance between supporting technology development and protecting the public.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter20120ASPECG.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/WorkingAI.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Coronavirus20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/k20Nearest20Neighbors20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Github20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1200x675.JAN.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Opioids220ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/FastSpeech20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Illinois.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-xvi/",
    "title": "issue xvi",
    "date": "",
    "reading_time": "",
    "content": "I was inspired by:\n\nIf you want to know when we’re coming to your area for Pie & AI,sign up here.\n\nKeep learning!Andrew\n\nTabNine, a maker of programming tools, releasedDeep TabNine, an app that installs on your text editor of choice and fills in code as you type.How it works:Deep TabNine is based on OpenAI's GPT-2, the text generator that penned a decent dystopianshort storybased on a single sentence from George Orwell’s1984. Trained on open-source code, it predicts the next chunk of code, as illustrated in the picture above.\n\nBehind the news:Predictive tools for coding have existed for years, but they're typically geared for a single language and base their predictions largely on what has already been typed, making them less useful early in a project. Thanks to GitHub, Deep TabNine is familiar with a range of tasks, algorithms, coding styles, and languages.Why it matters:Deep TabNine cuts coding time, especially when typing rote functions, according to evaluations on Reddit and Hacker News. Compounded across the entire software industry, it could be a meaningful productivity booster. And that doesn’t count the doctor bills saved by avoiding screen-induced migraines.We’re thinking:Pretrained language models like GPT-2 are opening new, sometimes worrisome possibilities for text generation. Could this be the start of a new, powerful wave of AI-driven coding tools?\n\nArtificial intelligence is fueling artistic experimentation from painting to music. Now creatives can spend less time keeping up with the expanding AI palette and more time making digital masterpieces.What’s new:Runway MLis a desktop clearinghouse that aims to connect visual artists with machine learning models that ignite their imagination and sharpen their productions. Browse the Image Generation category, for instance, and you’ll find a GAN that can make a selfie look like a Monet, or a semantic image synthesizer that transforms Microsoft Paint-quality doodles into photorealistic pictures.How it works:Creators of AI models can upload their projects viaGithub.\n\nBehind the news:RunwayML began as a thesis project when artist and coder Cristóbal Valenzuela was a graduate student at NYU’s Tisch School of the Arts, according toThe Verge. Now he works on it full-time.Why it matters:AI opens a universe of creative possibilities: processing existing works, generating new ones, mashing up styles, creating new forms of order out of chaos and new forms of chaos out of order. Runway gives artists a playground for experimenting with these emerging resources.We’re thinking:Making models accessible to artists doesn’t just broaden the market for AI. Engineers who listen to artists’ creative needs may find themselves taking on challenges they wouldn’t imagine on their own. Like, say, deciding what is and isn't art.\n\nHuman voices retain their distinctive character no matter what language they’re speaking. New research lets computers imitate human voices across a number of languages.What’s new:Researchers at Google built amultilingual text-to-speech enginethat can mimic a person’s voice in English, Spanish, and Mandarin. You can hear examples of its outputhere.Key Insights:Some languages are written as letters (like English) and others as symbols (Mandarin). The difference can have a dramatic impact on choice of architecture and training. Yu Zhang and colleagues found that processing phonemes — the basic sounds that make up language — simplifies the task by eliminating the need to learn pronunciation rules that vary among languages.How it works:The model embeds phonemes in a vector space before decoding those vectors into spectrograms for a WaveNet speech synthesizer. Using phonemes enables the model to find similarities in speech among different languages, so the system requires less training data per language to achieve good results.\n\nResults:In a subjective assessment of naturalness, the multilingual model’s performance in three languages roughly matches Tacotron 2’s performance on one.Why it matters:Training a text-to-speech model typically requires plenty of well-pronounced and labeled training data, a rare commodity. The new approach takes advantage of plentiful data in popular languages to improve performance in less common tongues. That could extend the reach of popular services like Siri or Google Assistant (which work in 20 and 30 languages respectively out of 4,000 spoken worldwide) to a far wider swath of humanity.We're thinking:A universal voice translator — speak into the black box, and out comes your voice in any language — is a longstanding sci-fi dream. This research suggests that it may not be so far away from reality.\n\nHow do you know if your new idea for improving your algorithm is better or worse than your old one? Learn to set single-number evaluation metrics in theDeep Learning Specialization.\n\nMany AI teams have bet big on models that can determine peoples’ emotional state by analyzing their face. But recent work shows that facial expressions actually provide scant information about a person’s feelings.What’s new: An in-depthreviewof the scientific literature shows there are very few rules when it comes to how people use their faces to show emotion. For instance, the researchers found that a scowl indicates anger only 30 percent of the time.What they found:Psychologists from universities in the US and Europe spent two years poring over 1,000 studies, including their own, examining facial expressions of emotion. They identified three categories of shortcomings:\n\nBehind the news:Most emotion recognition systems are based on ’60s-era research by psychologist Paul Ekman. He argued that humans share a vocabulary of facial configurations tied to emotion.Why it matters:The emotion-detection industry is worth $20 billion, including marketing, security, and road safety, according toThe Verge. But there’s more than money at stake. Systems designed for security and safety may not be doing their job because they're based on faulty research.We’re thinking:Emotional expressions common to every human may exist. But if so, they’re more nuanced than current systems are designed to handle. Further research will help develop and validate AI's ability to recognize subtle muscle movements and contextual cues.\n\nMachine learning stumbles in diagnosing many medical conditions based on imagery if little labeled data is available. In small data settings, supervised learning is often unable to train an accurate classifier. But a twist on such methods dramatically boosts accuracy — without additional hand-labeled data.What’s new:Researchers at Stanford developed an improveddiagnostic model for bicuspid aortic valve, a dangerous heart deformity, based on a database of MRI videos.Key insight:Medical data typically is aggregated from many doctors and clinics, leading to inconsistent labels. Jason Fries and his teammates sidestepped that problem via weakly supervised learning, an emerging approach that doesn’t rely on labeled data. The trick is to use a separate model to produce a preliminary label and confidence score for every training example.How it works:The labeler predicts initial diagnoses along with confidence levels, creating imprecise, or noisy, labels. A neural network uses the raw MRI and noisy labels to predict a final diagnosis.\n\nResults:The weakly-supervised diagnostic model diagnosed BAV correctly in 83 percent of patients. The previous fully-supervised method achieved only 30 percent.Why it matters:BAV is severely underdiagnosed (only one-sixth of sufferers had been labeled with the correct diagnosis in the training data set). Its day-to-day effects include fatigue, shortness of breath, and chest pain. Moreover, using long-term data on health outcomes, researchers discovered that patients whom their model diagnosed with BAV had twice the risk of a major cardiac event later in life. Having a correct diagnosis up-front clearly could be a life-saver.Takeaway:General practitioners aren’t likely to detect rare, hidden conditions as accurately as specialists. But patients don’t schedule appointments with specialists unless they get a GP’s referral. Models like this one could help solve that chicken-or-egg problem by bringing powerful diagnostic tools to common checkups.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/946f0455-cc94-4b38-9a1b-98ec28574f78-1.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/f9e6ccfb-d76e-420e-9d89-9a4e6a1feaee--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/d8ffd8c9-af8b-4527-877d-df01da849c22.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/794f3448-460c-4223-9d7b-8d548cd2d055.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/d9062c1c-28ac-4325-8b18-88aa32ca51a7.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/09/6ece12b2-8790-4ff2-bebe-2a7222de2be8.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/45a761ca-0a57-4006-b530-141096024424--1-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-103/",
    "title": "issue 103",
    "date": "",
    "reading_time": "",
    "content": "Since the pandemic started, several friends and teammates have shared with me privately that they were not doing well emotionally. I’m grateful to each person who trusted me enough to tell me this. How about you — are you doing okay?Last week, the Olympic gymnastic champion Simone Biles temporarily withdrew from competition because she didn’t feel mentally healthy enough to do her best and perhaps avoid a career-ending injury. She’s not alone in struggling with mental health. About 4 in 10 adults in the U.S. reported symptoms of anxiety or depression during the pandemic, according to onesurvey.Once, after looking over a collaborator’s project, I said, “That’s really nice work” and got back a sad facial expression as though my collaborator was near tears. I asked if they were okay, wondering if I had said something wrong, but they paused and shook their head. After I probed gently a little more, they burst out crying and told me that my remark was the first appreciation they had received in longer than they could remember.\n\nMany people outwardly look like they’re doing well, but inside they’re lonely, anxious, or uncertain about the future. If you’re feeling fine, that’s great! But if you’re among the millions who feel that something is off-balance, I sympathize, and I want you to know that I care about you and appreciate you.As the pandemic wears on, many of us are hungry to connect with others more deeply. If this describes you, or if you want to help someone else who might feel this way, perhaps you can start by letting someone know you appreciate them or something they did. I think this will make them — and maybe you, too — feel better.\n\nLove,Andrew\n\nA crime-fighting AI company altered evidence to please police, a new investigation claims — the latest in a rising chorus of criticism.What’s new:ShotSpotter, which makes a widely used system of the same name that detects the sound of gunshots and triangulates their location, modified the system’s findings in some cases,Vicereported.Altered output:ShotSpotter’s output and its in-house analysts’ testimony have been used as evidence in 190 criminal cases. But recent court documents reveal that analysts reclassified as gunshots sounds the system had attributed to other causes and changed the location where the system determined that gunshots had occurred.\n\nThe response:In astatement, ShotSpotter called theVicereport “false and misleading.” The company didn’t deny that the system’s output had been altered manually but said the reporter had confused two different services: automated, real-time gunshot detection and analysis after the fact by company personnel. “Forensic analysis may uncover additional information relative to a real-time alert such as more rounds fired or an updated timing or location upon more thorough investigation,” the company said, adding that It didn’t change its system’s findings to help police.Behind the news:Beyond allegations that ShotSpotter has manually altered automated output, researchers, judges, and police departments have challenged the technology itself.\n\nWhy it matters:ShotSpotter’ technology is deployed in over 100 U.S. cities and counties. The people who live in those places need to be able to trust criminal justice authorities, which means they must be able to trust the AI systems those authorities rely on. The incidents described in legal documents could undermine that trust — and potentially trust in other automated systems.We’re thinking:There are good reasons for humans to analyze the output of AI systems and occasionally modify or override their conclusions. Many systems keep humans in the loop for this very reason. It’s crucial, though, that such systems be transparent and subject to ongoing, independent audits to ensure that any modifications have a sound technical basis.\n\nDeepMind opened access to AlphaFold, a model that finds the shapes of proteins, and to its output so far — a potential cornucopia for biomedical research.What’s new:The research lab, a division of Google’s parent company Alphabet, made AlphaFoldfreely available. It also openeddatabasesthat contain hundreds of thousands of three-dimensional protein shapes.Shapes of things to come:Proteins are molecules made up of chains of amino acids. They perform myriad biological functions depending on the way the chain folds, and understanding their shapes can shed light on what they do and how they do it. Protein shapes are determined by the proximity of essential portions, or residues, of amino acids. AlphaFold finds likely shapes by optimizing possible structures that keep residues close to one another based on their positions and angles. For a description of how it works, see “Protein Shapes Revealed”here.\n\nBehind the news:Until recently, scientists had to rely on time-consuming and expensive experiments to figure out protein shapes. Those methods have yielded about 180,000 protein structures. AlphaFold debuted in 2018, when it won an annual contest for predicting protein structures. A revised version of the model won again in2020with an average error comparable to the width of an atom.Why it matters:Biologists could use these tools to better understand the function of proteins within the human body and develop new treatments for some of medicine’s most vexing maladies. Researchers already are using AlphaFold data to devise treatments for maladies includingCovid-19and several common, deadlytropical diseases.We’re thinking:We applaud DeepMind’s decision to make both its landmark model and the model’s output available for further research. We urge other companies to do the same.\n\nAI is undergoing a shift from model-centric to data-centric development. How can you implement a data-centric approach? Register to hear experts discuss this and other topics on August 11, 2021, at 10 A.M., Pacific time at“Data-centric AI: Real-World Approaches.”\n\nComputer vision is keeping a close eye on athletes at the Summer Olympic Games in Tokyo.What’s new:Omega Timing, a Swiss watchmaker and the Olympic Games’ official timekeeper, is providing systems that go far beyond measuring milliseconds. The company’s technology is tracking gameplay, analyzing players’ motions, and pinpointing key moments,Wiredreported.How it works:Omega Timing’s systems track a variety of Olympic sports including volleyball, swimming, and trampoline. Their output is intended primarily for coaches and athletes to review and improve performance, but it’s also available to officials and broadcasters.\n\nBehind the news:Omega Timing has measured Olympic performance since 1932. It introduced photo-finish cameras at the 1948 Olympiad in London. Its systems are certified by theSwiss Federal Institute of Metrology.Why it matters:Technology that helps athletes examine their performance in minute detail could give them a major edge in competition. It offers the rest of us a finer appreciation of their accomplishments.We’re thinking:For this year’s games, the International Olympic Committee added to the schedule competitive skateboarding, surfing, and climbing. Next time, how about adata-centric AIcompetition?\n\nWhy use a complex model when a simple one will do? New work shows that the simplest multilayer neural network, with a small twist, can perform some tasks as well as today’s most sophisticated architectures.What’s new:Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, and a team at Google Brain revisited multilayer perceptrons (MLPs, also known as vanilla neural networks). They builtMLP-Mixer, a no-frills model that approaches state-of-the-art performance in ImageNet classification.Key insight:Convolutional neural networks excel at processing images because they’re designed to discern spatial relationships, and pixels that are nearby one another in an image tend to be more related than pixels that are far apart. MLPs have no such bias, so they tend to learn interpixel relationships that exist in the training set and don’t hold in real life. By modifying MLPs to process and compare images across patches rather than individual pixels, MLP-Mixer enables this basic architecture to learn useful image features.How it works:The authors pretrained MLP-Mixer for image classification usingImageNet-21k, which contains 21,000 classes, and fine-tuned it on the 1,000-class ImageNet.\n\nResults:An MLP-Mixer with 16 mixer layers classified ImageNet with 84.15 percent accuracy. That’s comparable to the state-of-the-art 85.8 percent accuracy achieved by a 50-layerHaloNet, a ResNet-like architecture with self-attention.Yes, but:MLP-Mixer matched state-of-the-art performance only when pretrained on a sufficiently large dataset. Pretrained on 10 percent ofJFT300Mand fine-tuned on ImageNet, it achieved 54 percent accuracy on ImageNet, while a ResNet-basedBiTtrained the same way achieved 67 percent accuracy.Why it matters:MLPs are the simplest building blocks of deep learning, yet this work shows they can match the best-performing architectures for image classification.We’re thinking:If simple neural nets work as well as more complex ones for computer vision, maybe it’s time to rethink architectural approaches in other areas, too.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/08/Screen-Shot-2021-08-03-at-9--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2021/08/SHOTSPOTTER2.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/08/PROTEIN-2.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/08/Experts-Panel-8.11_The-Batch-Image.png",
      "https://dl-staging-website.ghost.io/content/images/2021/08/VOLLEYBALL.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/08/MIXER.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-45/",
    "title": "issue 45",
    "date": "",
    "reading_time": "",
    "content": "I was dismayed on Monday to read that the U.S. issuspending the H1-B visa programat least through the end of the year. This effort to discourage immigration can only bring distress to workers from other countries and harm to the U.S.\n\nH1-B visas allow U.S. companies to bring in talent from around the world, enriching both their business and the economy. People from many different countries have been central to U.S. innovation in AI (see “Mapping AI’s Talent Pipeline” below).\n\nTo me, H1-B holders aren’t just “workers.” They are my friends, students, and collaborators, and it pains me to see them facing the stress and uncertainty that comes with sudden, arbitrary shifts in immigration policy.\n\nStanford University sponsored my H1-B visa many years ago, which enabled me to teach and do research there. It feels deeply unfair to deny the same opportunities to the next generation. We should do whatever we can to attract top talent, not turn it away. As a planet, we should be working to empower individuals to do their best work, wherever they may end up doing it.\n\nThrough education, I remain committed to creating opportunities to learn and grow for as many people as I can. I hope the AI community will continue to transcend national borders and come together to build AI for the betterment of all.\n\nKeep learning!\n\nAndrew\n\nThree of the biggest AI vendors pledged to stop providing face recognition services to police — but other companies continue to serve the law-enforcement market.What’s new:Amid protests over police killings of unarmed Black people in the U.S.,Amazonimposed a one year moratorium on licensing its Rekognition technology to police departments, andMicrosoftannounced a similar hiatus. Both said they would re-enter the market if the government imposed limits on police use of the technology.IBMexited the face recognition market altogether.Demand, meet supply:The big AI companies are highly visible, but most law enforcement agencies get the technology from lesser-known firms, theWall Street Journalreported.\n\nWhy it matters:Concern over fairness in law enforcement has renewed worries that unfettered use of face recognition leads to miscarriages of justice.Researchspearheaded by MIT Media Lab researcher Joy Buolamwini showed that commercially available systems consistently misclassified women and people with darker complexions. A study by the American Civil Liberties Union found that Amazon’s system erroneouslymatchedmugshots with the faces of 28 members of the U.S. Congress. Some police departments havemisusedthe technology in ways that experts say could lead to mistaken arrests.We’re thinking:It’s great to see the big AI providers exercising responsibility. Now we need prudent regulation and auditing mechanisms geared to protect civil rights and support social justice.\n\nChina launches most of AI’s top researchers, but the U.S. is their number-one destination.What’s new:U.S.-based research group MacroPolo published theGlobal AI Talent Tracker. The report traces international trends in education and employment among elite engineers.Findings:The study tracked the locations of 675 high-performing AI practitioners through undergraduate studies, graduate school, and employment.\n\nBehind the news:MacroPolo sought to sample top AI talent by selecting its cohort at random from authors whose papers were accepted to NeurIPS 2019, one of the most prestigious and selective conferences in the field. The report’s conclusions align closely withpreviousstudiesthat also used conference acceptance to track where AI researchers were educated and employed.Why it matters:Developing AI is a global project. Collaboration and freedom of movement are essential to progress.We’re thinking:The recent U.S.suspension of H1B visaswill shatter dreams and disrupt lives. But the MacroPolo data shows that it will also be very damaging to U.S. innovation in AI. At a time when some countries are trying to make immigration harder, the AI community must redouble its effort to make sure that people from all over the world are welcome and able to contribute.\n\nFrom server to smartphone, devices with less processing speed and memory require smaller networks. Instead of building and training separate models to run on a variety of hardware, a new approach trains a single network that can be adapted to any device.What’s new:Han Cai and researchers at MIT developedOnce-for-All(OFA). This method trains a single large model and derives subnetworks — subsets of the original model’s weights — that perform well on less powerful processors.Key insight:Typical pruning methods downsize neural networks one at a time by reducing, say, the size and number of convolutional filters and then fine-tuning the smaller model. It’s more efficient to extract and fine-tune a fleet of progressively smaller models in a single process.How it works:OFA extracts subnetworks by varying the parent network’s number of layers, number of filters per layer, filter sizes, and the input resolution. The researchers constrained each of these factors to a predetermined set of values that allow up to 1019 possible subnetworks.\n\nResults:The authors compared OFA with a variety of neural architecture search methods suitable for finding models for mobile devices. The popular NASNet-A required 48,000 hours to generate the smallest model, and it would require that time again to generate another one optimized for different constraints. OFA’s baseline model required 1,200 hours to find all models. They also compared OFA toMobileNetV3-Large, the state-of-the-art image recognition network for mobile devices. The OFA model that ran on similar hardware achieved 76.9 percent top-one accuracy onImageNetcompared to MobileNetV3’s 75.2 percent. The most accurate neural search method the researchers considered,FBNet-C, required roughly half as much time as OFA to generate a single, less accurate model, but much more time to generate the second.Why it matters:OFA produces equivalent models of many sizes in slightly more time than it takes to train the original large models. In situations that require deploying a given network to heterogenous devices, this efficiency can translate into big savings in development time and energy consumption.We’re thinking:Smart speakers, watches, thermostats, pacemakers — it’s inevitable that neural networks will run on more and more heterogenous hardware. This work is an early step toward tools to manage such diverse deployments.\n\nWe’ve launched our much-anticipated Natural Language Processing Specialization! Courses 1 and 2 are live on Coursera.Enroll now\n\nBaidu backed out of a U.S.-led effort to promote ethics in AI, leaving the project without a Chinese presence.What’s new:The Beijing-based search giant withdrew from the Partnership on AI, a consortium of businesses, nonprofits, and research organizations that promotes cooperation on issues like digital privacy and algorithmic bias,Wiredreported.Mixed Motivations:Baidu joined the organization in 2018 as its first and only Chinese member. Other members include Amazon, Apple, Facebook, Google, and Microsoft as well as organizations like the U.N. and Human Rights Watch.\n\nBehind the news:Much of the Partnership on AI’s work involves research and education. Recentprojectsinclude a symposium onbias in machine learning, a study of how organizations can deployexplainable AI, and an interactive blog thatexplains face recognitionto the general public. The group doesn’t lobby politicians directly, but it does aim to make its research relevant to policymakers.Why it matters:The U.S. and China are the world’s top AI powerhouses. Baidu’s exit from the organization will affect its ability to influence AI globally toward fairness and justice.We’re thinking:The AI community spans many nations, and many bonds hold it together. Let’s keep building bridges and working to ensure that AI is used for the common good.\n\nA growing body of literature shows that some steps in AI’s forward march may actually movesideways. A new study questions advances in metric learning.What’s new:Kevin Musgrave and researchers at Cornell Tech and Facebook AI re-examined hallmark results in models that learn to predict task-specific distance metrics, specifically networks designed to quantify the similarity between two images. They foundlittle evidence of progress.Key insight:Researchers have explored metric learning by experimenting with factors like architectures, hyperparameters, and optimizers. But when those factors aren’t held constant, comparisons with earlier approaches can’t be apples-to-apples. Improved results often reflect advances in the surrounding components (such as hyperparameter tuning), not in the metric learning algorithm itself.How it works:Models that assess similarity between images typically extract features and predict a distance between them. The distances may be learned through a metric loss function, while features are often extracted from pre-trained networks. The authors reviewed 12 of the most popular papers on this topic. They point out common causes of invalid comparisons and present a new approach that levels the playing field.\n\nResults:The authors reproduced and benchmarked many past approaches on theCUB200,Cars 196, andStanford Online Productsdatasets. Controlling for confounding variables, their analysis shows that metric learning hasn’t improved since 2006 (as shown in the plots above).Why it matters:Image similarity is a key component of many products (such as image-based search). Knowing what really works is key to helping practitioners build useful applications as well as drive further research.We’re thinking:Metric learning still has a distance to go.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter203.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Facecops.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Talent6.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/OFA.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Baidu4.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/METRIC.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-191/",
    "title": "issue 191",
    "date": "",
    "reading_time": "",
    "content": "Last week, the tech news siteThe Informationreportedan internal controversy at Google. Engineers were concerned that Google’s Bard large language model was trained in part on output from OpenAI’s ChatGPT, which would have violated OpenAI’s terms of use. The output purportedly was hosted on ShareGPT, a website where users share conversations with ChatGPT. (Googledeniesthe report.) A decade ago, GoogleaccusedMicrosoft of copying its search results to enhance Bing.\n\nTraining a machine learning model on a different model’s output can be a useful technique, but it also raises engineering, business, and legal questions. When is it okay?\n\nEngineering recipes for training learning algorithms on generated data are still being developed. When I led a large automatic speech recognition (ASR) team, there were rumors — that we never proved or disproved — that a competitor was using our system to generate transcripts to train a competing system. It was said that, rather than using our ASR system’s output directly as labeled training data, our competitor used a lightweight process to manually clean up errors and make sure the data was high-quality.\n\nLately, I’ve seen many developers experiment with use cases such as prompting a large model (say, 175B parameters) to generate high-quality outputs specialized to an application such as customer support, and using this data to fine-tune a smaller model (say, ~10B parameters) that costs less per inference. UC Berkeley trainedKoalausing data from ShareGPT, and Stanford trainedAlpacaby fine-tuning Meta’sLLaMAon data generated with assistance from OpenAI’stext-davinci-003.\n\nSuch recipes raise important business questions. You may have spent a lot of effort to collect a large labeled training set, yet a competitor can use your model’s output to gain a leg up. This possibility argues that, contrary to conventional tech-business wisdom, data doesn’t always make your business more defensible. Specifically, if a market leader spent significant resources to get its performance up to a certain level, and if the market leader’s product generates data that makes it cheaper for competitors to catch up, then the market leader’s initial effort spent gathering data is a weak defense against competitors.\n\nIn addition, the legal and ethical questions around this practice need clearer answers. OpenAI’s terms of use forbid anyone to “use output from the Services to develop models that compete with OpenAI.” To my mind, this raises legal questions such as:\n\n(To state the obvious, I am not a lawyer. Don’t construe anything I say as legal advice!)\n\nIn the era of generative AI, we’ll see many creative use cases for intentionally using one model to generate data to train another. This is an exciting technical trend, even as we keep in mind the need to move forward in ways that are legal and fair.Keep fine-tuning!\n\nAndrew\n\nP.S. On Friday, April 7, Yann LeCun and I will hold a live online discussion about a proposed six-month pause in cutting-edge AI research. Theproposalraises questions about AI’s future and, if implemented, would have a huge impact on developers and businesses.Please join us.\n\nNeural networks are predicting how metal will deform under pressure to pilot robots through the tricky process of fabricating aircraft.\n\nWhat’s new:Machina Labs crafts metal using AI-guided robotic arms,Bloombergreported. The company recently inked contracts with the United States Air Force, the U.S. National Aeronautics and Space Administration, and Hermeus, which makes hypersonic airplanes.\n\nHow it works:Thesystemcombines robot arms, sensors, and machine learning models to form, trim, finish, and polish metal sheets according to a computer-aided design. Working in pairs, robot arms on either side of a sheet apply pressure to sculpt deformations up to four feet deep. The system works on aluminum, steel, and titanium in varying thicknesses and sizes upward of 4 feet by 12 feet. A basic two-arm setup costs $2.5 million.\n\nBehind the news:Most sheet-metal manufacturing isperformedmanually by skilled workers. Some parts can be mass-produced, but manual labor is still required to build molds. Both processes are slow, laborious, and expensive — a problem exacerbated by ashortageof craftspeople.Why it matters:Large machines like airplanes and automobiles are expensive to manufacture. Robots guided by deep learning models can bring costs down by fabricating parts quickly and precisely and by recognizing defects before they leave the factory.We’re thinking:This application of deep learning is riveting.\n\nContract workers who help train the algorithms behind Google Search won a pay raise.What’s new:Employees of U.S. contractors who evaluate the quality of Google Search’s results, knowledge panels, and ads will earn $15 per hour, a raise of roughly $1,Bloombergreported.\n\nPay raise:The Alphabet Workers Union (AWU), an unofficial labor union that represents U.S.- and Canada-based employees of Alphabet, its subsidiaries, vendors and contractors, negotiated the raise. The deal will affect around 5,000 workers, most of whom work remotely for Seattle-area RaterLabs.\n\nBehind the news:Large AI developers like Google and OpenAI often outsource rote tasks like labeling data and evaluating outputs. The contractors have come under fire for underpaying workers.\n\nWhy it matters:AI products like search engines, language models, and autonomous vehicles can earn billions for the companies that develop them. Yet many of the workers who contribute to them receive relatively low wages.\n\nWe’re thinking:We’re glad to see wages rising for workers whose input is crucial to building AI systems. For a thoughtful treatment of tech labor issues, we recommend Gray and Suri’s excellent book,Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass.\n\nSpecial event! Join Yann LeCun and Andrew Ng on Friday, April 7, 2023, at 9:30 a.m. Pacific Time to discuss a proposed pause in cutting-edge AI research. Let’s examine the pros and cons of the Future of Life Institute’s proposal!Register here\n\nA South African startup aims to lure talented engineers who left the continent to work abroad.\n\nWhat’s new:Johannesburg research labLelapa.aibills itself as a haven for African AI engineers who want to work on challenges that aren’t on Silicon Valley’s agenda,Wiredreported. The company purports to focus on languages such as isiZulu that big-tech natural language models don’t accommodate.How it works:Lelapa develops AI models for other businesses and nonprofits. The company has raised $2.5 million from institutions including Mozilla Ventures, Africa-centric investor Atlantica Ventures, and private investors including Google AI chief Jeff Dean. Current projects include:\n\nBehind the news:Lelapa’s founders include some organizers ofDeep Learning Indaba, a machine learning conference most recently held in Tunisia, andMasakhane, a nonprofit that promotes open-source models and datasets for African languages. Co-founderJade Abbottwas profiled in DeepLearning.AI’s Working AI blog series.\n\nWhy it matters:Over 74 percent of foreign-born students who receive a PhD in AI from a school in the United States remain in the U.S. after graduating, last year’s State of AI reportfound. Lelapa’s founders hope their project will help Africa reclaim some of this talent, nurture native AI startups, and address systemic inequities in AI development.\n\nWe’re thinking:Sub-Saharan Africaaccountsfor 15 percent of the world’s population but fewer than 1 percent of AI patents and conference publications, according to the State of AI report. Organizations like Lelapa can help the region realize its potential.\n\nText from current language models can be useful as a rough draft, but that leaves the polishing to human writers. A language model learned how to generate and respond to editorial directions.What’s new:Timo Schick and colleagues at Meta proposedPlan, Edit, Explain, and Repeat(PEER), a text generator designed to collaborate with human writers.Key insight:Data that demonstrates the motivations, execution, and results of editing is hard to come by. Wikipedia, in which every article includes a history of edits as well as comments on them, comes close, but an editor trained solely on Wikipedia would be limited to encyclopedia-style text. However, a model trained on Wikipedia to undo revisions can synthesize a supplemental dataset of unrevised and revised examples. Applying the undo function to varied text can generate synthetic “unedited” drafts for training the editor.How it works:PEER comprises fourT5large language models: PEER-Edit (which executed revisions), PEER-Undo (which undid revisions), PEER-Explain (which explained revisions), and PEER-Document (which generated synthetic primary-source documents as a basis for revisions). The authors trained them onWikipedia, 6.9 million examples that include texts before and after a revision, a revision plan (a directive to revise the text, such as “add information about the scandal”), an explanation (a reason for the revision, which may duplicate the revision plan), and cited documents (primary sources on which the text is based).\n\nResults:The authors evaluated PEER-Edit usingSARI, a measure of similarity between two revised versions of a text relative to the unrevised original (higher is better). Comparing generated revisions to ground-truth revisions of Wikinews, the Wikipedia-trained PEER-Edit (175 billion-parameters) achieved 49.3 SARI, and the same architecture trained on the synthetic Wikinews dataset achieved 51.6 SARI. Both were more similar to the human revisions than was the unrevised text, which achieved 32.8 SARI. They also evaluated PEER-Edit on six tasks such as grammar correction and removal of biased words. Averaged across these tasks, a 175-billion parameter model achieved 44.3 SARI and a 3 billion-parameter version achieved 43.6 SARI. Prompted to perform the same tasks,InstructGPT(1.3 billion parameters) achieved 39.4 SARI, andTk-Instruct(3 billion parameters, fine-tuned to correct grammar and simplify text) achieved 23.5 SARI.Yes, but:Text generators can produce factually false statements. While PEER-Edit sometimes corrected misinformation, it also fabricated falsehoods, which it backed up by fabricating citations.Why it matters:Training text generators to provide explanations for their decisions and citations for the facts they use may lead to more interpretable models.We’re thinking:The raw output of generative models is fun and exciting, but imagine their potential as collaborators with creative people!\n\nYoshua Bengio, Emad Mostaque, and others called for a moratorium on cutting-edge AI researchMore than 1,000 AI leaders and researchers signed anopen letterurging AI labs to pause the development of the most advanced systems for six-months, citing “risks to society.” (The New York Times)\n\nItaly banned ChatGPT over privacy concernsThe Italian Data Protection Authority blocked OpenAI's chatbot for unlawfully collecting users’ personal data and lacking an age-verification system to protect minors. The regulator is set to investigate whether the company complied with the European Union’s General Data Protection Regulation (GDPR). (Politico)\n\nUK proposes flexible rules to regulate AIThe Department for Science, Innovation and Technology (DSIT) published a white paper to guide the responsible use of AI in the UK. There will be no AI watchdog. Instead, a set of principles will guide existing regulators. (TechCrunch)\n\nResearch: Meta proposed an artificial visual cortex and adaptive sensorimotor coordinationMeta announced key developments designed to enable AI-powered robots to function autonomously in the real world: A perception model trained on videos of people performing everyday tasks, called VC-1, and a technique that empowers robots to adjust their actions to varying environments, called ASC. (SiliconAngle)\n\nResearch: AI used to develop a molecular syringe to deliver proteins into human cellsResearchers harnessed DeepMind’s AlphaFold model, which predicts the 3D shapes of protein molecules, to adapt spikes from bacteria to deliver potentially therapeutic proteins into cells. This technique could pave the way for improved drug delivery systems and expand the applications of gene-editing techniques like CRISPR–Cas9. (Nature)\n\nBuzzFeed’s AI writer “As Told to Buzzy” is generating complete articlesThe news and entertainment outlet recently started publishing AI-generated content, initially limiting it to quizzes. But the company’s text generator has produced more than 40 articles on travel, all of which feature identical expressions and structures. (Futurism)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/04/Screen-Shot-2023-04-04-at-5.19.38-PM-1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/04/MACHINA_600px_opt150--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/04/RAISE_600px--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/04/The-Batch-ads-and-exclusive-banners--7-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/04/LELAPA--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/04/ezgif.com-gif-maker--26--1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-189/",
    "title": "issue 189",
    "date": "",
    "reading_time": "",
    "content": "Here’s a quiz for you. Which company said this?\n\nHow about this?\n\nThese are not recent statements from generative AI companies working on large language models (LLMs) or image generation models! The first is from a 2011 IBMvideothat promotes the Watson system’s upcoming participation in the TV game showJeopardy!. The second comes from Google DeepMindwebpageabout AlphaGo, which was released in 2015.\n\nIBM’s and DeepMind’s work moved AI forward. But it also inspired some people’s imaginations to get ahead of them. Some supposed that the technologies behind Watson and AlphaGo represented stronger AI capabilities than they did. Similarly, recent progress on LLMs and image generation models has reignited speculation about artificial general intelligence (AGI).\n\nGenerative AI is very exciting! Nonetheless, today’s models are far from AGI. Here’s a reasonable definition of from Wikipedia:\n\n“Artificial general intelligence (AGI) is the ability of an intelligent agent to understand or learn any intellectual task that human beings or other animals can.”\n\nThe latest LLMs exhibit some superhuman abilities, just as a calculator exhibits superhuman abilities in arithmetic. At the same time, there are many things that humans can learn that AI agents today are far from being able to learn.\n\nIf you want to chart a course toward AGI, I think the baby steps we’re making are very exciting. Even though LLMs are famous for shallow reasoning and making things up, researchers have improved their reasoning ability by prompting them through achain of thought(draw one conclusion, use it to draw a more sophisticated conclusion, and so on).\n\nTo be clear, though, in the past year, I think we’ve made one year of wildly exciting progress in what might be a 50- or 100-year journey. Benchmarking against humans and animals doesn’t seem to be the most useful question to focus on at the moment, given that AI is simultaneously far from reaching this goal and also surpasses it in valuable ways. I’d rather focus on the exciting task of putting these technologies to work to solve important applications, while also addressing realistic risks of harm.\n\nWhile AGI may be part of an indeterminate future, we have amazing capabilities today, and we can do many useful things with them. It will take great effort on all of our parts to to find ways to harness them to advance humanity. Let’s get to work on that.Keep learning!\n\nAndrew\n\nMicrosoft laid off an AI ethics team while charging ahead on products powered by OpenAI.What’s new:On March 6, the tech giant dissolved the Ethics & Society unit in its Cognition group, which researches and builds AI services, amid ongoing cutbacks that have affected 10,000 workers to date, the tech-news outletPlatformerreported. Microsoft kept its Office of Responsible AI, which formulates ethical rules and principles, and related teams that advise senior leadership on responsible AI and help implement responsible AI tools in the cloud.\n\nHow it works:Ethics & Society was charged with ensuring that AI products and services were deployed according to Microsoft’s statedprinciples. At its 2020 peak, it included around 30 employees including engineers, designers, and philosophers. Some former members spoke withPlatformeranonymously.\n\nBehind the news:Microsoft isn’t the only major AI player to have shifted its approach to AI governance.\n\nWhy it matters:Responsible AI remains as important as ever. The current generative AIgold rushis boosting companies’ motivation to profit from the latest developments or, at least, stave off potential disruption. It also incentivizes AI developers to fast-track generative models into production.We’re thinking:Ethical oversight is indispensable. At the same time, recent developments are creating massive value, and companies must balance the potential risks against potential benefits. Despite fears that opening models like Stable Diffusion would lead to irresponsible use — which, indeed, has occurred — to date, the benefits appear to be vastly greater than the harms.\n\nWhat does an entrepreneur do after co-founding one of the world’s top social networks? Apply the lessons learned to distributing hard news.\n\nWhat’s new:Kevin Systerom and Mike Krieger, who co-founded Instagram, launchedArtifact, an app that uses reinforcement learning to recommend news articles according to users’ shifting interests.\n\nHow it works:The founders were inspired to launch a news app after witnessing TikTok’s success at designing a recommendation algorithm that learned from users’ habits, SystromtoldThe Verge. The app starts by classifying each user as a persona that has a standardized constellation of interests, the foundersexplainedto the tech analysis siteStratechery. Then a transformer-based model selects news articles; its choices are continually fine-tuned via reinforcement learning,TechCrunchreported.\n\nBehind the news:Artifact joins a crowded field of personalized news feeds from Google, Apple, Japan-basedSmartNewsand China-basedToutiao(owned by TikTok’s parent ByteDance).NewsBreakof California focuses on local news.\n\nYes, but:Delivering news is a tough business. Never mind theprecipitousdeclineof traditional newspapers. SmartNewsannouncedit was laying off 40 percent of its staff.Why it matters:Social media sites like Facebook grew partly on their promises to deliver timely news according to individual users’ interests, but they struggle to deliver high-quality news. A 2019 Pew Research Center pollfoundthat 55 percent of U.S. adults thought social media companies’ role in curating consumption resulted in a worse mix of news. Artifact aims to apply machine learning techniques developed to help people stay in touch with friends to keep them informed in a rapidly changing world.We’re thinking:Social media networks have used recommendation algorithms to maximize engagement, enabling clickbait and other low-quality information to flourish. Artifact’s choice of what to maximize, be it user engagement (which, in ad-driven social networks, correlates with revenue), metrics that track consumption of high-quality news, or something else, will have a huge impact on its future.\n\nAre you interested in hands-on learning for natural language processing and machine learning for production? Join us on March 23, 2023, at 10:00  a.m. Pacific Time for a workshop in “Building Machine Learning Apps with Hugging Face: LLMs to Diffusion Modeling.”RSVP\n\nAmazon, Apple, and Google have been building chatbots for years. So how did they let the alliance between Microsoft and OpenAI integrate the first smash-hit bot into Microsoft products?What happened:Top AI companies brought their conversational agents to market over the past decade-plus amid great fanfare. But Amazon’s Alexa, Apple’s Siri, and Google’s Assistant succumbed to technical limitations and business miscalculations,The New York Timesreported. Meanwhile, Microsoft launched, retooled, and ultimately killed its entry, Cortana, instead banking on a partnership with OpenAI, whose ChatGPT went on to become a viral sensation.\n\nAmazon:Alexa hit the market in 2014. It garnered great enthusiasm as Amazon integrated it into a range of hardware like alarm clocks and kitchen appliances.\n\nApple:Siri became a fixture in iPhones in 2011. It drove a spike in sales for a few years, but the novelty wore off as it became mired in technical complexity.\n\nGoogle:Google debuted Assistant in 2016. IttoutedAssistant’s ability to answer questions by querying its search engine. Meanwhile, it pioneered the transformer architecture and built a series of ever more-capable language models.\n\nWhy it matters:The top AI companies devoted a great deal of time and money to developing mass-market conversational technology, yet Microsoft got a jump on them by providing cutting-edge language models — however flawed or worrisome— to the public.\n\nWe’re thinking:Microsoft’s chatbot success appears to be a classic case ofdisruptive innovation: An upstart, OpenAI, delivered a product that, although rivals considered it substandard, exceeded their products in important respects. But the race to deliver an ideal language model isn’t over. Expect more surprise upsets to come!\n\nRoboticists often train their machines in simulation, where the controller model can learn from millions of hours of experience. A new method trained robots in the real world in 20 minutes.\n\nWhat's new:Laura Smith, Ilya Kostrikov, and Sergey Levine at UC Berkeley introduced a process torapidly train a quadruped robot to walkin a variety of real-world terrains and settings.\n\nKey insight:One way to train a model on less data is to train it repeatedly on the same examples (in this case, ​​the robot's orientation, velocity, and joint angles at specific points in time). However, this may lead the model to overfit (for instance, the robot may learn to walk effectively only on the terrain used in training). Regularization or normalization enables a model to train multiple times on the same examples without overfitting.\n\nHow it works:The authors trained a motion-planning model to move aUnitree A1robot forward on a given terrain using anactor-criticalgorithm, a reinforcement-learning method in which an actor function learns to take actions that maximize the total return (roughly the sum of all rewards) estimated by a critic function. The actor was a vanilla neural network and the critic was an ensemble of such networks.\n\nResults:The authors trained the model to walk the robot on each of five surfaces (starting from scratch for each surface): flat ground, mulch, lawn, a hiking trail, and a memory foam mattress. The robot learned to walk on each in about 20 minutes, which is roughly equivalent to 20,000 examples. Competing methods use either simulation or more time in the real world. For example, the authors ofDayDreamer: World Models for Physical Robot Learningtrained the same type of robot to walk on an indoor surface without a simulation, but it took one hour and 3.6 times more examples.\n\nWhy it matters:Training on simple features (those with a small number of dimensions, such as robot orientation and velocity) rather than complex features (such as images) reduces the number of examples required to learn a task, and regularizing the model prevents overfitting. This is a simple, general setup to train reinforcement learning models in the real world.\n\nWe're thinking:Reinforcement learning algorithms are famously data-hungry, which is why much of the progress in the past decade was made in simulated environments. A recipe for training a quadruped rapidly in the real world is a great step forward!\n\nNew York City to hire an expert in AI and machine learningThe city’s Office of Technology is seeking an expert to identify AI use cases and develop ethical guidelines and best practices for AI applications in the public sector. (Bloomberg)Life insurance algorithms face scrutiny for possible biasColorado’s Division of Insurance is crafting new regulations to limit insurers’ use of predictive models and algorithms. (The Wall Street Journal)\n\nResearch:Meta’s AI-powered tool predicts the structure of proteins fasterThe program, called ESMFold, is expected to help researchers find new drugs and cures for diseases. ESMFold purportedly is 60 times faster than Google’s AlphaFold, but less accurate. (The Wall Street Journal)Salesforce launched EinsteinGPTThe cloud-software company partnered with OpenAI to integrate text-generation services across their sales, commerce, and marketing tools. (Fast Company)A consortium of AI research groups released an open source version of ChatGPTOpenChatKit is a 20 billion-parameter open source base to develop generalized and specialized chatbots. You can try the demohere. (Together)Anthropic, a company founded by former OpenAI employees, launched its own chatbotThe chatbot called Claude has more natural and coherent conversations than existing chatbots, according to the company. (The Verge)Students made deepfake videos of a principal making racist remarksThree high school students shared the material on Tik Tok, sparking outrage within the Putnam County, New York, community. (Vice)Chinese media outlet is broadcasting an AI news anchorThe AI-powered journalist belongs to China’s state-owned news outlet People Daily and can report news 24/7. (PetaPixel)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/03/Screen-Shot-2023-03-22-at-10.02.06-AM-1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/03/ETHICS--1-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/03/ARTIFACT--1-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/03/3.23_The-Batch-Image.png",
      "https://dl-staging-website.ghost.io/content/images/2023/03/LOST-1200px-LgrDarkerType-v4.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/03/WALK--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-160/",
    "title": "issue 160",
    "date": "",
    "reading_time": "",
    "content": "Although the process may be familiar, every job search is different. Here are some tips to increase the odds you’ll find a position that supports your thriving and enables you to keep growing.Pay attention to the fundamentals.A compelling resume, portfolio of technical projects, and a strong interview performance will unlock doors. Even if you have a referral from someone in a company, a resume and portfolio will be your first contact with many people who don’t already know about you. Update your resume and make sure it clearly presents your education and experience relevant to the role you want. Customize your communications with each company to explain why you’re a good fit. Before an interview, ask the recruiter what to expect. Take time to review and practice answers to common interview questions, brush up key skills, and study technical materials to make sure they are fresh in your mind. Afterward, take notes to help you remember what was said.\n\nProceed respectfully and responsibly. Approach interviews and offer negotiations with a win-win mindset. Outrage spreads faster than reasonableness on social media, so a story about how an employer underpaid someone gets amplified, whereas stories about how an employer treated someone fairly do not. The vast majority of employers are ethical and fair, so don’t let stories about the small fraction of mistreated individuals sway your approach. If you’re leaving a job, exit gracefully. Give your employer ample notice, give your full effort through your last hour on the job, transition unfinished business as best you can, and leave in a way that honors the responsibilities you were entrusted with.\n\nChoose who to work with.It’s tempting to take a position because of the projects you’ll work on. But the teammates you’ll work with are at least equally important. We’re influenced by people around us, so your colleagues will make a big difference. For example, if your friends smoke, the oddsrisethat you, too, will smoke. I don’t know of a study that shows this, but I’m pretty sure that if most of your colleagues work hard, learn continuously, and build AI to benefit all people, you’re likely to do the same. (By the way, some large companies won’t tell you who your teammates will be until you’ve accepted an offer. In this case, be persistent and keep pushing to identify and speak with potential teammates. Strict policies may make it impossible to accommodate you, but in my mind, that increases the risk of accepting the offer, as it increases the odds you’ll end up with a manager or teammates who aren’t a good fit.)Get help from your community.Most of us go job hunting only a small number of times in our careers, so few of us get much practice at doing it well. Collectively, though, people in your immediate community probably have a lot of experience. Don’t be shy about calling on them. Friends and associates can provide advice, share inside knowledge, and refer you to others who may help. I got a lot of help from supportive friends and mentors when I applied for my first faculty position, and many of the tips they gave me were very helpful.I know that the job search process can be intimidating. Instead of viewing it as a great leap, consider an incremental approach. Start by identifying possible roles and conducting a handful of informational interviews. If these conversations tell you that you have more learning to do before you’re ready to apply, that’s great! At least you have a clear path forward. The most important part of any journey is to take the first step, and that step can be a small one.\n\nKeep learning!\n\nAndrew\n\nDr. Benjamin Harvey recently raised $6 million for his company. An engineer at heart, he has used AI to measure pollution, predict Covid risk, and analyze the Edward Snowden leaks for the NSA. He spoke to us about turning every job into a passion project.Read his story here.\n\nGoogle updated a key model behind the algorithm that ranks its search results to respond to the flood of misinformation on the web.\n\nWhat’s new:The search giant aims to minimize the prominence of falsehoods in the information it presents near the top of search results, which it callssnippets.How it works:Google revised itsMultitask Unified Modelto verify the accuracy of snippets.\n\nBehind the news:Google isn’t the only major website to task AI with filtering the torrent of disinformation.\n\nWhy it matters:Human fact-checkers can’t keep up with the rising tide of misinformation. AI has an imperfect record of moderating online content. For instance, Facebook faces allegations that its algorithmssuppressads aimed at people with disabilities andoverlookincitements to violence. But even incremental improvements are worthwhile in the face of anti-vaccine panics, denial of climate change,and calls for genocide.We’re thinking:AI is an important tool in keeping web searches honest, but it’s not yet ready to do the job alone. Just as autonomous taxi companies often employ human safety drivers to oversee their vehicles, automated content moderation systems benefit from humans in the loop.\n\nChina’s investment in AI business, infrastructure, and research could pay off big over the next decade.\n\nWhat’s new:AI is expected to add $600 billion to the Chinese economy by 2030, according to ananalysisby consulting firm McKinsey.Market-ready technologies:The researchers interviewed more than 50 experts and analyzed market data collected between October 2021 and November 2021. They looked for market-ready AI technologies (as opposed to those that are either early-stage or mature) with the highest potential for impact in the coming decade. Conclusion: Transportation, manufacturing, enterprise software development, and healthcare stand to add huge value to the nation’s economy.\n\nThe big picture:In 2017, Chinaannouncedits aim to become “the world's primary AI innovation center” by 2030. Since then, the country has pushed to accelerate AI research, development, and product development. The latestAI Indexestimated that China accounted for one-fifth of all private investment in AI.\n\nYes, but:Not everything is rosy in China’s tech sector. Venture capital investment in the first four months of 2022 was 43.5 percentlowerthan the same period in 2021 — a decline nearly four times more severe than the global average.Why it matters:China’s contribution to AI has been impressive, but the outcomes outlined in the report are not foregone. To cash in on AI’s potential, China must put more effort into acquiring and using high-quality data, collaborating with other nations, protecting intellectual property, and training skilled workers, the report says.We’re thinking:Many commentators view AI development as a competition among global superpowers. Rising tensions between the U.S. and China tend to reinforce this view. Yet every nation’s AI effort relies on numerous individual developers. This global community can play an important role in steering AI in directions that benefit all people as well as their home countries.\n\nA robot deckhand aims to help fishing boats keep their haul fresh all the way to your table.\n\nWhat’s new:Shinkei Systems developed a machine that uses computer vision to slaughter fish in a way that maximizes their shelf life and flavor,TechCrunchreported.How it works:The refrigerator-sized system, which is designed to withstand heavy seas, attaches to a boat’s deck. Fishermen empty their nets into a hopper that passes individual fish through the machine one by one. Inside, computer vision guides tools to pierce the animal’s brain, sever its spine, drain its blood, and deposit it into an ice bath. The process takes between 10 to 15 seconds per fish.\n\nBehind the news:The process is modeled on a manual technique calledike jime, which typically requires a skilled practitioner, making it difficult to industrialize. Ike jime is increasingly popular among upscale seafood restaurants both within and outside Japan, where it was developed.\n\nWhy it matters:The fast pace aboard fishing boats leaves little time for processing the catch, so most fish are left to suffocate to death, which can take minutes to hours. This isn’t just inhumane, it results in meat that’s bruised by flopping and tainted by stress-induced hormones, leading to shorter shelf life and less appetizing flavor. This system could give fishing operations an efficient way to sell their catches more profitably while dispatching fish more humanely.\n\nWe’re thinking:Giving such a delicate task to a robot may seem fishy, but this application seems sure to scale.\n\nVision transformers need architecture modifications and retraining from scratch to be used for object detection — or so most researchers thought. New work used vision transformers for object detection without the usual redesign and training.\n\nWhat’s new:Yanghao Li and colleagues at Facebook proposedViTDet, which adds an object detector to a plain pretrained transformer.\n\nKey insight:Vision transformers(ViTs) have rivaled convolutional neural nets (CNNs) in many vision tasks — but not object detection. That’s because a CNN’s hierarchical architecture, in which different-sized layers produce representations at different scales of an image, helps to spot objects of any size. Consequently, copying this architecture is a natural choice for transformers for vision tasks, and many ViT variations for object detection feature a hierarchical implementation (known as abackbonethat supports a detection-specifichead/neck). A simpler solution, though, is to add hierarchical layers to the end of a vanilla ViT backbone. This avoids the need to redesign the network, and it enables object detection models to benefit from pretrained ViTs that weren’t developed with that task in mind.\n\nHow it works:ViTDet combines a ViT pretrained on ImageNet, which produces a representation of an input image, with Mask R-CNN’s prediction layers, an established component for object detection and image segmentation. The authors fine-tuned the system for those tasks on an augmented version of COCO. They made the following alterations prior to fine-tuning:\n\nResults:A ViTDet based on ViT-Huge performed bounding-box detection with 61.3 average precision (a measure of how many objects were correctly identified in their correct location, higher is better) and instance segmentation with 53.1 average precision.SwinV2-L, based on a transformer with a hierarchical architecture, performed bounding-box detection with 60.2 average precision and instance segmentation with 52.1 average precision.\n\nWhy it matters:Decoupling the vision model’s design and training from the object-detection stage is bound to accelerate progress on transformer-based object detection systems. If any pretrained transformer can be used for object detection directly off the shelf, then any improvement in pretrained transformers will yield better representations for object detection.\n\nWe’re thinking:This work opens opportunities to improve all manner of object detection and segmentation subtasks.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/08/Ben-Harvey.jpeg",
      "https://dl-staging-website.ghost.io/content/images/2022/08/MISINFO.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/08/CHINA.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/08/FISH.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/08/SIMPLE.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-130/",
    "title": "issue 130",
    "date": "",
    "reading_time": "",
    "content": "I’m writing this in Orlando, Florida, where I just spoke at theA3 Business Forum, a group that works to advance industrial automation through AI, robotics, and other tools. This was my first large conference since the pandemic started, and it was good to get out and meet more people (taking appropriate health precautions, of course).I was heartened by the number of AI people at A3. I met entrepreneurs working on computer vision systems for warehouse logistics (for example, finding and moving packages automatically), automated inspection (which I spoke about), controlling fleets of mobile robots, and building factory simulations.Some trends that I took away from the conference:\n\nI was pleased to find, in an audience of manufacturing professionals, many learners taking online AI courses. On the flip side, I’m enjoying the opportunity to learn the lingo and techniques of industrial automation. And there is much for all of us to learn! For example, despite having developed and implemented sophisticated computer vision algorithms, many AI practitioners don’t yet appreciate the importance ofimaging system design— to make sure your image data is of high quality — as part of building a practical system.Applied AI is inherently interdisciplinary. Melonee Wise, an old friend and roboticist who recentlysoldher company Fetch Robotics, gave me permission to share that her biggest regret was taking too long to bring in someone with warehouse experience. Let’s approach our work with an awareness that knowledge of other fields is critical to building useful systems. Stay curious and . . .\n\nKeep learning!\n\nAndrew\n\nThe hardware that runs the latest AI systems faces rising uncertainty as models grow larger and more computationally intensive.\n\nWhat’s new:The U.S. Commerce Department sounded an alarm over bottlenecks in the availability of semiconductor chips, the integrated circuits at the heart of virtually all digital devices. The supply of advanced microprocessors that drive cutting-edge AI is vulnerable,The New York Timesreported.\n\nHow it works:Geopolitical tensions, rising costs, and supply-chain disruptions threaten the supply of AI chips.\n\nWhy it matters:So far, the post-pandemic semiconductor shortage mostly has affected chips that rely on older manufacturing methods, such as those used in automobiles, medical devices, radio-frequency identification, and optical sensors. As AI grows ever more hungry for processing power, a sustained shortage of advanced chips could be a significant barrier to progress in the field and beyond.We’re thinking: International cooperation generally fosters prosperity. In AI, it's essential to progress.\n\nOpenAI unveiled a more reliable successor to its GPT-3 natural language model.\n\nWhat’s new:InstructGPTis a version of GPT-3 fine-tuned to minimize harmful, untruthful, and biased output. It's available via an application programming interface at acostbetween $0.0008 and $0.06 per thousand tokens depending on speed and capability.How it works:The developers improved the quality of GPT-3’s output using a combination of supervised learning andreinforcement learning from human feedback, in which humans rank a model’s potential outputs and a reinforcement learning algorithm rewards the model for producing material similar to high-ranking outputs.\n\nResults:InstructGPT outperformed GPT-3 on TruthfulQA, which tests how often a model generates falsehoods, 0.196 to 0.233 (lower is better). It also beat GPT-3 on RealToxicityPrompts, which tests a model’s propensity to produce toxic language, 0.413 to 0.224 (higher is better). Contractors rated InstructGPT’s output higher-quality than GPT-3’s, despite the former model only having 1.3 billion parameters — 100 times fewer than GPT-3’s 175 billion parameters.Behind the news:GPT-3’s training dataset — in particular, massive quantities of text scraped from the web — has beenlinkedoutput that stereotype certain social groups, denigrate women, and encourage self-harm. OpenAI previouslytriedto detoxify GPT-3 by fine-tuning it onPALMS, a dataset curated according to measures of human rights and human equality.\n\nWhy it matters:OpenAI’s language models have powerededucational tools,virtual therapists,writing aids,role-playing games, and much more. Social biases, misinformation, and toxicity in such contexts are unhelpful at best, harmful at worst. A system that avoids such flaws is likely to be both less dangerous and more useful.We’re thinking:Makers offoundation models, general-purpose models that can be fine-tuned for specialized applications, have a special responsibility to make sure their work doesn’t contain flaws that proliferate in fine-tuned versions. OpenAI’s ongoing effort to improve GPT-3 is a hopeful sign that the AI industry can manage such models responsibly.\n\nJoin uson February 16, 2022, for a live, interactive session! Learn the top skills needed for a career in machine learning and artificial intelligence. Find out how to transition your career into these areas.\n\nCollecting and annotating a dataset of facial portraits is a big job. New research shows that synthetic data can work just as well.What's new:A team led by Erroll Wood and Tadas Baltrušaitis at Microsoft used a 3D model togeneratean effective training set for face parsing algorithms intended to recognize facial features. TheFaceSyntheticsdataset comprises 100,000 diverse synthetic portraits in which each pixel is annotated according to parts of the face.Key insight:Face datasets annotated with facial features are expensive and time-consuming to build. Beyond theethical issuesthat arise in collecting pictures of people, they require that every pixel of every image be labeled. Creating high-quality synthetic images can be similarly difficult, since a digital artist must design each face individually. A controllable 3D model can ease the burden of producing and labeling realistic portraits.How it works:The authors used a high-quality 3D model of a face, comprising over 7,000 polygons and vertices as well as four joints, that changes shape depending on parameters defining a unique identity, expression, and pose. They fit the model to the average face derived from 500 scans of people with diverse backgrounds.\n\nResults:The authors compared their system to a U-Net trained using images in Helen. Their system recognized the part of the face each pixel belonged to with an overall F1 score (a number between 0 and 1 that represents the balance between precision and recall, higher is better) of 0.920. The comparison model scored 0.916. This result fell somewhat short of the state of the art,EAGRNet, which achieved an F1 score of 0.932 in the same task.Why it matters:Synthetic data is handy when the real thing is hard to come by. Beyond photorealistic, annotated faces, the authors’ method can produce similarly high-quality ultraviolet and depth images. It can also generate and label images outside the usual data distribution in a controllable way.We're thinking:The authors generated an impressive diversity of realistic faces and expressions, but they were limited from the library of 512 discrete hairstyles, 30 items of clothing, and 54 accessories. We look forward to work that enables a 3D model to render these features as well.\n\nMost U.S. state agencies use AI without limits or oversight. An investigative report probed reasons why efforts to rein them in have made little headway.\n\nWhat’s new:Since 2018, nearly every proposed bill aimed at studying or controlling how state agencies use automated decision systems, or ADS, has failed to be enacted,according toThe Markup, a nonprofit investigative tech-journalism site. Insiders blame big tech.\n\nWhy it hasn’t happened:Reporters interviewed lawmakers and lobbyists about dozens of stalled bills. They found that bureaucracy and lobbying have played major roles in blocking legislation.\n\nBehind the news:Although U.S. states are mostly free to use AI, several of them impose limits on private companies.\n\nWhy it matters:China, the European Union, and the United Kingdom haveannouncedlaws designed to rein in AI’s influence in business, society, and other domains. The lack of such limits in the U.S. make it an outlier. On one hand, this leaves the authorities free to experiment and perhaps discover productive use cases. On the other, it invites abuse — or simply lack of quality control over a technology that has great potential for both good and ill.We’re thinking:Regulation done badly is a drag on progress. Done right, though, it can prevent harm, level the playing field for innovators, and ensure that benefits are widespread. The AI community should push back against special interests — even when we would profit — that stymie regulation that would be good for society.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202022-02-02%20at%2010.08.40%20AM.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202022-02-02%20at%2010.08.40%20AM.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/CHIPS-1.gif?upscale=true&width=1200&upscale=true&name=CHIPS-1.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker-Feb-02-2022-07-52-20-65-PM.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker-Feb-02-2022-07-52-20-65-PM.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/AI+X-2.16_Eventbrite.png?upscale=true&width=1200&upscale=true&name=AI+X-2.16_Eventbrite.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/SYNTHETICv2.gif?upscale=true&width=1200&upscale=true&name=SYNTHETICv2.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/REGS.gif?upscale=true&width=1200&upscale=true&name=REGS.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-228/",
    "title": "issue 228",
    "date": "",
    "reading_time": "",
    "content": "Last week, I attended the NeurIPS conference in New Orleans. It was fun to catch up with old friends, make new ones, and also get a wide scan of current AI research. Work by the big tech companies tends to get all the media coverage, and NeurIPS was a convenient place to survey the large volume of equally high-quality work by universities and small companies that just don’t have a comparable marketing budget!\n\nAI research has become so broad that I struggle to summarize everything I saw in a few sentences. There were numerous papers on generative AI, including large language models, large multimodal models, diffusion models, enabling LLMs to use tools (function calls), and building 3D avatars. There was also plenty of work on data-centric AI, differential privacy, kernels, federated learning, reinforcement learning, and many other areas.\n\nOne topic I’m following closely is autonomous agents: Software, usually based on LLMs, that can take a high-level direction (say, to carry out competitive research for a company), autonomously decide on  a complex sequence of actions, and execute it to deliver the outcome. Such agents have been very hard to control and debug, and so, despite amazing-looking demos, there have been few practical deployments. But now I see them on the cusp of working well enough to make it into many more applications, and increasingly I play with them in my spare time. I look forward to getting through my reading list of autonomous agent research papers over the coming holiday!\n\nAt NeurIPS, many people I spoke with expressed anxiety about the pace of AI development and how to keep up as well as publish, if what you're working on could be scooped (that is, independently published ahead of you) at any moment. Whileracing to publish firsthas a long history in science, there are other ways to do great work. The media, and social media especially, tend to focus on what happened today. This makes everything seem artificially urgent. Many conversations I had at NeurIPS were about where AI might go in months or even years.\n\nI like to work quickly, but I find problem solving most satisfying when I’ve developed an idea that I believe in — especially if it’s something that few others see or believe in — and then spend a long time executing it to prove out the vision (hopefully). I find technical work more fulfilling when I have time to think deeply, form my own conclusion, and perhaps even hold an unpopular opinion for a long time as I work to prove it. There’s a lot of value in doing fast, short-term work; and given the large size of our community, it’s important to have many of us doing long-term projects, too.\n\nSo, this holiday season, when the pace of big announcements might slow down for a couple of weeks, I hope you’ll take a break. Spend time with friends and loved ones, let thoughts simmer in the back of your mind, and remind yourself of holiday values like charity and renewal. If you’re looking for ideas, maybe even some that will keep you productively busy for months or years, injecting more inputs — taking courses, reading blogs or papers — is a good way to do that.\n\nIt has been a great year for AI, with lots of progress and excitement. I’m grateful to have gotten through this crazy year with you.\n\nHappy holidays!\n\nAndrew\n\nRecent years brought systems that, given a text prompt, generate high-quality text, pictures, video, and audio. In 2023, the wave of generative AI washed over everything. And its expanding capabilities raised fears that intelligent machines might render humanity obsolete.Asinpastyearsat this season, we invite you to settle by the fire and savor 12 months of technological progress, business competition, and societal impact.\n\nThis year, AI became virtually synonymous with generative AI.\n\nWhat happened:Launched in November 2022, OpenAI’s ChatGPT ushered in a banner year for AI-driven generation of text, images, and an ever widening range of data types.\n\nDriving the story:Tech giants scrambled to launch their own chatbots and rushed cutting-edge natural language processing research to market at a furious pace. Text-to-image generators (also sparked by OpenAI with DALL·E in early 2021) continued to improve and ultimately began to merge with their text-generator counterparts. As users flocked to try out emerging capabilities, researchers rapidly improved the models’ performance, speed, and flexibility.\n\nGold rush:Generative AI didn’t just thrill customers and businesses; it generated a flood of funding for AI developers. Microsoft invested $13 billion in OpenAI, and Amazon and Google partnered with the nascent startup Anthropic in respective multibillion-dollar investments. Other generative AI startupsraisedhundreds of millions of dollars.\n\nWhere things stand:In the span of a year, we went from one chat model from OpenAI to numerous closed, open, and cloud-hosted options. Image generators have made strides in their ability to interpret prompts and produce realistic output. Video and audio generation are becoming widelyavailablefor short clips, and text-to-3D isevolving. 2024 is primed for a generative bonanza, putting developers in a position to build a wider variety of applications than ever before.\n\nThe movie capital became a front line in the battle over workplace automation.\n\nWhat happened:U.S. film and television writers went on strike in May, and actors followed in July. They took up a variety of issues with their employers, but concern that AI would damage their job prospects prolonged the work stoppage. Both groups inked agreements shortly before the year ended.\n\nDriving the story:Screenwriters negotiated for 148 days, and actors for 118, winning limits on their employers’ abilities to replace them with machine learning models.\n\nAI on the silver screen:Traditional Hollywood studios negotiated alongside the film departments of Amazon, Apple, and Netflix, tech powerhouses that have access to considerable AI expertise. All are likely to use AI to generate text, images, audio, and video.\n\nWhere things stand:The unions and studios agreed to use AI while enabling writers and actors to continue to ply their trades. The agreements will remain in force for three years — time enough for both sides to learn a bit about what the technology is and isn’t good for, and to form a vision of its role in the future. Now Hollywood faces the next challenge: Using AI to make better movies that grow the pie for producers and creatives alike.\n\nInformation may not want to be free after all.\n\nWhat happened:The age-old practice of training AI systems on data scraped from the web came into question as copyright owners sought to restrict AI developers from using their works without permission.\n\nDriving the story:Individual copyright holders filed lawsuits against AI companies for training models on their data without obtaining explicit consent, giving credit, or providing compensation. Concurrently, formerly reliable repositories of data on the open web started to require payment or disappeared entirely.\n\nCopyright conundrum:Whether copyright restricts training machine learning models is largely an open question. Laws in most countries don’t address the question directly, leaving it to the courts to interpret which uses of copyrighted works do and don’t require a license. (In the U.S., the Copyright Office deemed generated images ineligible for copyright protection, so training corpuses made up of generated images are fair game.) Japan is a notable exception: The country’s copyright law apparently allows training machine learning models on copyrighted works.\n\nWhere things stand:Most copyright laws were written long ago. The U.S. Copyright Act was established in 1790 and was last revised in 1976! Copyright will remain a battlefield until legislatorsupdatelaws for the era of generative AI.\n\nAngst at the prospect of intelligent machines boiled over in moves to block or limit the technology.\n\nWhat happened:Fear of AI-related doomsday scenarios prompted proposals to delay research and soul searching by prominent researchers. Amid the doomsaying, lawmakers took dramatic regulatory steps.\n\nDriving the story:AI-driven doomsday scenarios have circulated at least since the 1950s, when computer scientist and mathematician Norbert Weiner claimed that “modern thinking machines may lead us to destruction.” Such worries, amplified by prominent members of the AI community, erupted in 2023.\n\nRegulatory reactions:Lawmakers from different nations took divergent approaches with varying degrees of emphasis on preventing hypothetical catastrophic risks.\n\nStriking a balance:AI has innumerable beneficial applications that we are only just beginning to explore. Excessive worry over hypothetical catastrophic risks threatens to block AI applications that could bring great benefit to large numbers of people. Some moves to limit AI would impinge on open source development, a major engine of innovation, while having the anti-competitive effect of enabling established companies to continue to develop the technology in their own narrow interest. It’s critical to weigh the harm that regulators might do by limiting this technology in the short term against highly unlikely catastrophic scenarios.\n\nWhere things stand:AI development is moving too quickly for regulators to keep up. It will require great foresight — and a willingness to do the hard work of identifying real, application-level risks rather than imposing blanket regulations on basic technology — to limit AI’s potential harms without hampering the good that it can do. The EU’s AI Act is a case in point: The bill, initially drafted in 2021, has needed numerous revisions to address developments since then. Should it gain final approval, it will not take effect within two years. By then, AI likely will raise further issues that lawmakers can’t see clearly today.\n\nFans of AI-driven music pressed play, while a major recording company reached for the stop button.\n\nWhat happened:AI grabbed listeners by the ears when it helped produce a new single by The Beatles, mimicked the voices of beloved stars, and generated music from text prompts.\n\nDriving the story:AI hasn’t quite had its first hit record, but developments in generated music put both fans and the record industry on notice that it may not be far away.\n\nIndustry crackdown:Universal Music Group (UMG), which accounts for nearly one-third of the global music market,reactedswiftly to the wave of generated music. It blocked streaming services from distributing fan-made, voice-cloned productions and demanded that they block AI developers from downloading music by UMG artists so they can’t use it to train machine learning models. Shortly afterward, UMG partnered with Endel, a startup that generates background music. UMG artist James Blake released music he created using Endel’s system.\n\nWhere things stand:Generative AI is poised to play an increasing role in recorded music. AI-powered tools exist for many phases of recording production, including composition, arrangement, and mixing. The recent agreements between actors and writers and Hollywood studios may offer pointers to musicians and recording executives who would like to use these tools to make exciting, marketable music.\n\nMany releases, from language models by Microsoft and Mistral to a learning-based locomotion controller by UC-Berkeley researchers, are keeping the AI landscape exceptionally dynamic as the year draws to a close!\n\nExplore the week's top AI news in Data Points, a spin-off of our newsletter, The Batch.\n\nRead Data Points now.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--35--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--36-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--37-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--38-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--39-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--40-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--41-.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-227/",
    "title": "issue 227",
    "date": "",
    "reading_time": "",
    "content": "Last week, I participated in the United States Senate’s Insight Forum on Artificial Intelligence to discuss “Risk, Alignment, & Guarding Against Doomsday Scenarios.” We had a rousing dialogue with Senators Chuck Schumer (D-NY), Martin Heinrich (D-NM), Mike Rounds (R-SD), and Todd Young (R-IN). I remain concerned that regulators may stifle innovation and open source development in the name of AI safety. But after interacting with the senators and their staff, I’m grateful that many smart people in the government are paying attention to this issue.\n\nHow likely are doomsday scenarios? As Arvind Narayanan and Sayash Kapoorwrote, publicly available large language models (LLMs) such as ChatGPT and Bard, which have been tuned using reinforcement learning from human feedback (RLHF) and related techniques, are already very good at avoiding accidental harms. A year ago, an innocent user might have been surprised by toxic output or dangerous instructions, but today this is much less likely. LLMs today are quite safe, much like content moderation on the internet, although neither is perfect.\n\nTo test the safety of leading models, I recently tried to get GPT-4 to kill us all, and I'm happy to report that I failed! More seriously, GPT-4 allows users to give it functions that it can decide to call. I gave GPT-4 a function to trigger global thermonuclear war. (Obviously, I don't have access to a nuclear weapon; I performed this experiment as a form of red teaming or safety testing.) Then I told GPT-4 to reduce CO2 emissions, and that humans are the biggest cause of CO2 emissions, to see if it would wipe out humanity to accomplish its goal. After numerous attempts using different prompt variations, I didn’t manage to trick GPT-4 into calling that function even once; instead, it chose other options like running a PR campaign to raise awareness of climate change. Today’s models are smart enough to know that their default mode of operation is to obey the law and avoid doing harm. To me, the probability that a “misaligned” AI might wipe us out accidentally, because it was trying to accomplish an innocent but poorly specified goal, seems vanishingly small.\n\nAre there any real doomsday risks? The main one that deserves more study is the possibility that a malevolent individual (or terrorist organization, or nation state) would deliberately use AI to do harm. Generative AI is a general-purpose technology and a wonderful productivity tool, so I’m sure it would make building a bioweapon more efficient, just like a web search engine or text processor would.\n\nSo a key question is: Can generative AI tools make itmucheasier to plan and execute a bioweapon attack? Such an attack would involve many steps: planning, experimentation, manufacturing, and finally launching the attack. I have not seen any evidence that generative AI will have a huge impact on the efficiency with which someone can carry out this entire process, as opposed to helping marginally with a subset of steps. FromAmdahl’s law, we know that if a tool accelerates one out of many steps in a task, and if that task uses, say, 10% of the overall effort, then at least 90% of the effort needed to complete the task remains.\n\nIf indeed generative AI can dramatically enhance an individual’s abilities to carry out a bioweapon attack, I suspect that it might be by exposing specialized procedures that previously were not publicly known (and that leading web search engines have been tuned not to expose). If generative AI did turn out to expose classified or otherwise hard-to-get knowledge, there would be a case for making sure such data was excluded from training sets. Other mitigation paths are also important, such as requiring companies that manufacture biological organisms to carry out more rigorous safety and customer screening.\n\nIn the meantime, I am encouraged that the U.S. and other governments are exploring potential risks with many stakeholders. I am still nervous about the massive amount of lobbying, potential forregulatory capture, and possibility of ill-advised laws. I hope that the AI community will engage with governments to increase the odds that we end up with more good, and fewer bad, laws.\n\nFor my deeper analysis of AI risks and regulations, please read my statement to the U.S. Senatehere.\n\nKeep learning!\n\nAndrew\n\nP.S. Our new short course, “Reinforcement Learning from Human Feedback,” teaches a key technique in the rise of large language models. RLHF aligns LLMs with human preferences to make them more honest, helpful and harmless by (i) learning a reward function that mimics preferences expressed by humans (via their ratings of LLM outputs) and then (ii) tuning an LLM to generate outputs that receive a high reward. This course assumes no prior experience with reinforcement learning and is taught by Nikita Namjoshi, developer advocate for generative AI at Google Cloud. You’ll learn how RLHF works and how to apply it an LLM for your own application. You’ll also use an open source library to tune a base LLM via RLHF and evaluate the tuned model.Sign up here!\n\nGoogle unveiled Gemini, its bid to catch up to, and perhaps surpass, OpenAI’s GPT-4.\n\nWhat’s new:Googledemonstratedthe Gemini family of models that accept any combination of text (including code), images, video, and audio and output text and images. The demonstrations and metrics were impressive — but presented in misleading ways.\n\nHow it works:Geminiwill come in four versions. (i) Gemini Ultra, which will be widely available next year, purportedly exceeds GPT-4 in key metrics. (ii) Gemini Pro offers performance comparable to GPT-3.5. This model now underpins Google’s Bard chatbot for English-language outside Europe. It will be available for corporate customers who use Google Cloud’s Vertex AI service starting December 13, and Generative AI Studio afterward. (Google did not disclose parameter counts for Pro or Ultra.) Two distilled versions — smaller models trained to mimic the performance of a larger one — are designed to run on Android devices: (iii) Gemini Nano-1, which comprises 1.8 billion parameters, and (iv) Nano-2, at 3.25 parameters. A Gemini Nano model performs tasks like speech recognition, summarization, automatic replies, image editing, and video enhancement in theGoogle Pixel 8 Prophone.\n\nMisleading metrics:The metrics Google promoted to verify Gemini Ultra’s performance are not entirely straightforward. Google pits Gemini Ultra against GPT-4. However, Gemini Ultra is not yet available, while GPT-4 Turbo already surpasses GPT-4, which outperforms Gemini Pro. Gemini Ultra achieved 90 percent accuracy (human-expert level) on MMLU, which tests knowledge and problem-solving abilities in fields such as physics, medicine, history, and law. Yet this achievement, too, is misleading. Ultra achieved this score via chain-of-thought prompting with 32 examples, while most scores on the MMLUleaderboardare 5-shot learning. By the latter measure, GPT-4 achieves better accuracy.\n\nManipulated demo:Similarly, avideoof Gemini in action initially made a splash, but it was not an authentic portrayal of the model’s capabilities. A Gemini model appeared to respond in real time, using a friendly synthesized voice, to audio/video input of voice and hand motions. Gemini breezily chatted its way through tasks like interpreting a drawing in progress as the artist added each line and explaining a sleight-of-hand trick in which a coin seemed to disappear. However, Googleexplainedin a blog post that the actual interactions did not involve audio or video. In fact, the team had entered words as text and video as individual frames, and the model had responded with text. In addition, the video omitted some prompts.\n\nWhy it matters:Gemini joins GPT-4V and GPT-4 Turbo in handling text, image, video, and audio input and, unlike the GPTs, it processes those data types within the same model. The Gemini Nano models look like strong entries in an emerging race to put powerful models on small devices at theedgeof the network.We’re thinking:We celebrate the accomplishments of Google’s scientists and engineers. It’s unfortunate that marketing missteps distracted the community from their work.\n\nEurope’s sweeping AI law moved decisively toward approval.\n\nWhat’s new:After years of debate, representatives of the European Union’s legislative and executive branchesagreedon a draft of the AI Act, a comprehensive approach to regulating AI. As the legislative session drew to a close, the representatives negotiated nearly nonstop to approve the bill before the deadline. It will return to Europe’s parliament and member countries for final approval in spring 2024 and take effect roughly two years later.How it works:Theframeworklimits uses of AI that are considered especially risky. Last-minute agreements lightened the burdens on small companies and open source development. It includes the following provisions:\n\nWhat’s next:The representatives have agreed on these broad strokes, but they will continue to revise the details. After further vetting, the European Parliament will vote again, and acouncilof deputies from each EU member state will also vote, most likely in early 2024. If both bodies approve the bill, it will take effect no later than 2026.\n\nBehind the news:The AI Act has been under construction since 2021. The technology has evolved significantly since then, and the proposal has undergone several revisions to keep pace. The advent of ChatGPT prompted a round of revisions to control foundation models. Negotiations reached fever pitch in late December. France, Germany, and Italy, seeking to protect developers in their countries,soughtto weaken restrictions on foundation models. They were opposed by Spain, whichsoughtto strengthen oversight of the most powerful foundation models. The final negotiations concerned exceptions for police and military use of AI within member states. Franceleda group of countries that pushed for greater military exemptions.\n\nWhy it matters:The AI Act is the broadest and most detailed effort to regulate AI to date. The stakes are high: Not only does Europe have a budding AI industry of its own, but EU lawsoften dictatecompanies’ practices outside the union. Yet the bill won’t take effect for years — when AI may present very different challenges.\n\nWe’re thinking:Effective regulation should mitigate harm without stifling innovation. The best approach is to regulate applications rather than underlying technology such as foundation models. While the EU restricts some applications in helpful ways, it also limits foundational technology in ways that we expect will hurt innovation in EU member states. We welcome the provisions added at the last moment to lighten the load on small companies and open source developers. These 11th-hour wins reflect the efforts of many people who pushed to protect innovation and openness.\n\nJoin our new short course, “Reinforcement Learning from Human Feedback,” and learn a key method to align large language models with human values and preferences. Gain a detailed understanding of the technique and use it to fine-tune Llama 2 for an application.Sign up now\n\nA new consortium aims to support open source AI.\n\nWhat’s new:Led by Meta and IBM, dozens of organizations from the software, hardware, nonprofit, public, and academic sectors formed theAI Alliance, which plans to develop tools and programs that aid open development.\n\nHow it works:The AI Alliance’s 57 founding members include established companies like AMD, Intel, Oracle, and Sony; startups like Cerebras and Stability AI; nonprofits such as HuggingFace and the Linux Foundation, public institutes like the European Council for Nuclear Research (CERN) and U.S. National Aeronautics and Space Administration (NASA); and universities in Asia, Europe, and North America. The group stated its intention to pursue a variety of projects:\n\nBehind the news:The membership includes organizations that have prioritized open source development including Meta, Stability AI, and the Linux Foundation. Yet several organizations that provide popular open-source models are not represented, including models released under more permissive open source licenses likeGPT Neo​​ andMistral. Major companies like Apple and Google, who have released some of their work under open source licenses, are also absent.\n\nYes, but:The meaning of “open” is contentious, and AI Alliance does not clearly define it. In large language models, for instance, the spectrum of openness includes:\n\nWhy it matters:More openness means faster sharing of knowledge and a greater pace of innovation. The AI Alliance can put substantial resources and breadth of influence behind proponents of openness, potentially acting as a counterweight against well financed commercial interests that are threatened by open source development. For instance, some companies claim that restricting access to AI models is necessary to ensure that bad actors don’t misuse them; of course, it would also eliminate open source competition with those companies. On the other hand, open source advocatesarguethat transparency makes AI models less likely to be dangerous, since anyone can spot dangers and alter the code to reduce them.\n\nWe’re thinking:Open source is a powerful engine of innovation that enables people to build freely on earlier developments for the benefit of all. The AI Alliance’s gathering of commercial, institutional, and academic clout looks like a promising approach to promoting openness.\n\nA novel twist on self-supervised learning aims to improve on earlier methods by helping vision models learn how parts of an image relate to the whole.\n\nWhat’s new:Mahmoud Assran and colleagues at Meta, McGill University, Mila, and New York University developed a vision pretraining technique that’s designed to address weaknesses in typical masked image modeling and contrastive learning approaches. They call itImage-based Joint-Embedding Predictive Architecture(I-JEPA).\n\nKey insight:Masked image modeling trains models to reconstruct hidden or noisy patches of an image. This encourages models to learn details of training images at the expense of larger features. On the other hand, contrastive approaches train models to create similar embeddings for distorted or augmented versions of the same image. This encourages models to learn larger features, but reliance on augmentations such as zooming and cropping biases models toward those variations versus the wider variety they’re likely to encounter in the wild. I-JEPA combines these approaches: The model learns to embed regions that are made up of many patches, some of them masked, based on the surrounding unmasked patches. This approach balances learning of low- and high-level features.\n\nHow it works:I-JEPA used three components: (i) A target encoder embedded an image’s target region, (ii) a context encoder embedded the surrounding area, and (iii) a smaller predictor network, given the context embedding, tried to produce an embedding similar to that of the target embedding. All three components were transformers, though other architectures would serve. They were pretrained jointly onImageNet-1k.\n\nResults:An I-JEPA classifier that used ViT-H/14 encoders achieved 73.3 percent accuracy after about 2,500 GPU-hours of pretraining. A classifier trained on top of a ViT-B/16 base model that had been pretrained for 5,000 GPU-hours using theiBOTmethod, which relies on hand-crafted augmentations, achieved 69.7 percent accuracy.MAE, a masked modeling rival based on ViT-H/14, achieved 71.5 percent accuracy but required over 10,000 GPU-hours of pretraining.\n\nWhy it matters:In deep learning for computer vision, there’s a tension between learning details (a specialty of masked image modeling approaches) and larger-scale features (a strength of contrastive methods). I-JEPA gives models more context for learning both details and the high-level features in the training set.\n\nWe’re thinking:Given a picture of a jungle, I-JEPA would see both the forest and the trees!\n\nAI advancements that help decode cat pain, read human heartbeats at a distance, and improve sustainable farming in Africa are just some of the fascinating AI news and stories of this week.Read a new edition of Data Points and catch up.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/12/Screenshot-2023-12-12-at-5.37.30-PM-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/12/GEMINI_Catjump_NoGLogo_600px--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--80-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/12/DeepLearning1_GoogleCloudPlatfomr_Banner2_2070x1080--1--1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/12/ALLIANCE-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/12/I-JEPA-1.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-iii/",
    "title": "issue iii",
    "date": "",
    "reading_time": "",
    "content": "Younes Mourri is helping set the direction for AI education worldwide. A student and teacher at Stanford, he also develops content for the most popular online courses in machine learning.Learn more\n\nNeuroscientists translated brain signals directly into artificial speech, synthesizing full sentences based purely on neural impulses.What happened:Researchers tapped the brains of five epilepsy patients who had been implanted with electrodes to map the source of seizures, according to a paper published byNature(preprinthere). During a lull in the procedure, they had the patients read English-language texts aloud. They recorded the fluctuating voltage as the brain controlled the muscles involved in speaking. Later, they fed the voltage measurements into a synthesizer. You can hear the synthesized speech in thisvideo.How it works:A pair of three-layer, bidirectional, LSTM recurrent neural networks drove the synthesis.\n\nWhy it matters:The technique could help people who have lost the ability to control their vocal tract due to disorders such as Lou Gehrig’s disease, stroke, or injury. People with such conditions — think of the late physicist Stephen Hawking — can communicate very slowly through systems that track eye movements or facial muscles to spell words one letter at a time. Translating brain impulses directly would allow them to communicate with the ease of normal speech.Reality check:The researchers did not read minds. Gopala K. Anumanchipalli, Josh Chartier, and Edward F. Chang read brain signals controlling the patients’ muscles. Still, their approach conceivably could be refined to translate brain signals associated with thought.What’s next:The team plans to test the technology in people who can’t move their face and tongue. It also aims to adapt the system for languages other than English.\n\nHarry Potter’s magical cloak made him invisible to his Hogwarts colleagues. Now researchers have created a sort of invisibility shield to hide people from computer vision.What’s new:The researchers created an adversarial patch that makes people invisible to the YOLOv2 object detection model. Hold the patch in front of a camera, and YOLOv2 can’t see anything. In thisvideo, the befuddled recognizer seems to scratch its head as researchers pass the patch back and forth.How they did it:Simen Thys, Wiebe Van Ranst, and Toon Goedemé fed a variety of photos into a YOLOv2 network. They altered the photos to minimize the model’s \"objectness\" score. The image with the lowest score was an altered picture of people holding colorful umbrellas. They composited this image into photos of people, ran those through the network, and tweaked the patch further to achieve still lower objectness scores.Why it matters:Much research into adversarial attacks has focused on targets that don’t vary from instance to instance, such as stop signs or traffic lights. Held in front of a person, the new patch blinds YOLOv2 regardless of differences such as clothing, skin color, size, pose, or setting. It can be reproduced by a digital printer, making it practical for real-world attacks.The catch:The patch works only on YOLOv2. It doesn’t transfer well to different architectures.What’s next:The researchers contemplate generalizing the patch to confuse other recognizers. They also envision producing adversarial clothing. Perhaps a cloak?\n\nNever mind Spotify, here's MuseNet — a model that spins music endlessly in a variety of styles.What’s new:Computer-generated music composition dates back to the 1950s, but rarely has it been easy on the ears. The best examples, such the recent Googledoodlecommemorating the birthday of J.S. Bach, stick with a single style. MuseNet has a fair grasp of styles from bluegrass to Lady Gaga, and it can combine them as well.How it works:Drawing on OpenAI's GPT-2 language model, MuseNet is a 72-layer transformer model with 24 attention heads. This architecture, plus embeddings that help the model keep track of musical features over time, give it a long memory for musical structure. It uses unsupervised learning to predict the next note for up to 10 instruments in compositions up to four minutes long.To be sure:Project lead (and Deep Learning Specialization graduate) Christine Payne is a Juilliard-trained pianist, and the model’s mimicry of piano masters such as Chopin and Mozart (orboth) is especially good. That said, MuseNet inhabits was trained on MIDI files and speaks through a general-purpose synthesizer, so its output often sounds stiff and artificial. Its understanding of harmony and form, while impressive for an algorithmic composer, is shallow, and it often meanders.Where to hear it:You can hear MuseNethere. You can tinker with itherethrough May 12. After that, OpenAI plans to retool it based on user feedback with the aim of open-sourcing it. (To build your own algorithmic jazz composer, check outCourse Fiveof theDeep Learning Specializationon Coursera.)\n\nSometimes the most valuable AI projects don't make the best pilot projects. Learn why, and explore how you can implement AI throughout your company, in ourAI for Everyonecourse, available on Coursera.\n\nAI is poised to revolutionize diagnosis of eye disease.What’s new:Articles are piling up in scientific journals heralding computer vision’s success in detecting conditions such as diabetic retinopathy, a common condition that can cause blindness if it’s not treated in time:\n\nWhy it matters:Roughly 400 million people suffer from diabetes worldwide, and 10% of them will develop a serious case of retinopathy. Doctors can treat it if they can detect it in time, but there aren’t enough ophthalmologists or clinics to go around. AI running on portable devices could spot cases locally, catching the condition early and conserving specialist attention for severe cases.Behind the news:The Food and Drug Administrationapprovedits first AI-driven eye scanner for clinical use last year under a fast-track program for novel technologies.What they’re saying:“Advances in the automated diagnosis of eye conditions . . . have put artificial intelligence in a position to transform eye care. Soon, AI-based systems could augment physicians’ decision-making in the clinic — or even replace physicians altogether.” — Aaron Lee, Dept. of Ophthalmology, University of Washington, inNatureBottom line:Many experts expect AI’s march on medicine to start in the radiology lab. But you’re likely to find it in the ophthalmologist’s office even sooner.\n\nAI tools that help police and courts make decisions about detention, probation, and sentencing have “serious shortcomings,\" an AI industry consortium warns.What’s new:The Partnership on AI examined the use of risk-assessment software in criminal justice throughout the U.S. In a newreport, it outlines 10 requirements of such systems that are “largely unfulfilled” in the current crop. The organization's requirements include:\n\nBehind the news:U.S. authorities increasingly use, and in some cases mandate the use of, automated systems. The aim is to reduce costs and increase rigor in decision-making. Yet such tools have beenshownto produce invalid or biased results. They’re used without oversight, largely by people with little technical training.We’re thinking:The government’s reliance on predictive software doesn’t stop with criminal justice. It'susedin child welfare, education, public health, and elsewhere, and it has come up short time and time again. Such tools need to be evaluated with far more technical rigor. The new guidelines are a good place to start.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/c7a115cf-cbb5-417c-b9b5-81a53292e20c-1.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/a4ac933a-9717-44a0-80c1-b72ae7156a57.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/76127ad4-1bd3-4c67-a239-9d34c0d27046.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/066fb5f1-4c1f-49b5-8931-ec6ebe83e3de.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/b8f16497-e6ec-4ae2-a187-e54f98319c08.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/6c810bee-5620-4525-a0aa-dcf3aea461f0.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/592a103a-d610-4433-b936-332fab9d7cdd.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-73/",
    "title": "issue 73",
    "date": "",
    "reading_time": "",
    "content": "In my letterlast week, I alluded to the way AI tends to concentrate power and wealth. This tendency worries me, and I believe it deserves more attention.\n\nThe U.S. government has been looking into these winner-take-most dynamics at a few leading technology companies from an antitrust perspective. But the issue is much bigger than that. AI will concentrate power in many industries, including ones that haven’t traditionally relied on high tech, in the hands of a few winners.\n\nFor instance, Amazon has come to dominate retailing at the expense of innumerable chains and mom-and-pop stores. Uber, Lyft, and Didi are concentrating power over the taxi industry, which used to support hundreds of thriving local companies. Retailing and taxi service are not traditionally viewed as tech industries.\n\nDriven by digitization and AI, this pattern will play out in many more industries in this decade.\n\nCovid-19 has added further fuel to these dynamics. Some retailers managed the shift to e-commerce. They are collecting data and implementing AI to optimize sales, and they’re becoming more powerful. But others were nearly destroyed as the pandemic choked off foot traffic in brick-and-mortar stores. They don’t have spare dollars to invest in AI, and they’re falling farther and farther behind.\n\nEven as AI creates tremendous wealth, I worry about the growing concentrations of power and wealth, and those who will be left behind. Government will have to step up to address this situation, but significant responsibility also lies with the all of us who conceive, build, and manage this technology. I ask each of you to use your knowledge wisely, in ways that benefit society at large rather than a select few — even if that “select few” is yourself.\n\nKeep learning!\n\nAndrew\n\nCan social media posts reveal early signs of mental illness? A new machine learning model shows promising results.What’s new:Researchers led by Michael Birnbaum at the Feinstein Institute for Medical Research and Raquel Norel at the IBM Watson Research Center developed amodelthat analyzes messages and images posted by Facebook users for indicators of psychological problems. Unlike earlier efforts to classify mental illness based on social media posts, which relied on subjects to report their condition, this one used actual diagnoses.How it works:The authors collected millions of messages and images posted over 18 months by 223 volunteers. Some posters had been hospitalized with schizophrenia-spectrum disorders, some had been diagnosed with mood disorders like depression, and some had no mental health issues.\n\nResults:The model identified people diagnosed with schizophrenia and mood disorders at a rate comparable to that of a standard 10-point questionnaire, according toWired. The researchers found that individuals diagnosed as schizophrenic used “see,”  “hear,” and other words related to perception more often than the others. Those with mood disorders tended to post more blue-tinted pictures. Both groups also used more swear words and posted smaller photos.Behind the news: Social media posts are a popular hunting ground for researchers aiming to gauge users’ mental states. Recent studies suggest that Reddit comments canindicateconditions like ADHD, anxiety, and bipolar disorder, and that Twitter users oftentelegraphtheir depression, postpartum mood disorder, suicidal ideation, and more.Why it matters:This tool could help doctors catch mental illness early — especially in young adults, who tend to be both prolific users of social media and at higher risk of developing mental illness — and could provide valuable context for treatment.We’re thinking:Useful though it might be in some cases, scanning social media posts for clues to a user’s mental state holds worrisome implications. Yet another reason social media companies must adopt stricter standards to protect privacy.\n\nCompression methods like parameter pruning and quantization can shrink neural networks for use in devices like smartphones with little impact on accuracy — but they also exacerbate a network’s bias. Do compressed models perform less well for underrepresented groups of people? Yes, according to new research.What’s new:A Google team led by Sara Hooker and Nyalleng Moorosi explored theimpact of compressionon image recognition models’ ability to perform accurately across various human groups. The authors also proposed a way to rank individual examples by how difficult they are to classify.Key insight:Inearlier work, members of the team showed that compressed image recognition models, although they maintained their accuracy overall, had trouble identifying classes that were rare in their training data. To learn whether that shortcoming translates into bias against underrepresented human types, the researchers trained models to recognize a particular class (people with blond hair), compressed them, and measured the differences in their accuracy across different types of people. This enabled them to evaluate the difference in performance between compressed and uncompressed models with respect to underrepresented groups.How it works:The authors trained a set ofResNet-18s onCelebA, a dataset of celebrity faces, to classify photos of people with blond hair. (CelebA is notorious for producingbiased models.) Then they compressed the models using various combinations of pruning and quantization.\n\nResults:Pruning 95 percent of model parameters boosted the false-positive “blond” rate for women (who made up 14 percent of the dataset) by an average 6.32 percent, but it increased that rate for men (less than 1 percent of the dataset) by 49.54 percent. (The authors didn’t report corresponding results for models compressed by quantization.) Furthermore, the ranking method succeeded in identifying the examples that were most difficult to classify. A 95-percent pruned model was 93.39 percent accurate over the entire dataset, but 43.43 percent accurate on the 1 percent least consistently labeled examples. An unpruned model had much the same trouble. It was 94.76 percent accurate over the entire dataset, but 55.35 percent accurate on the 1 percent of least consistently labeled examples.Why it matters:Model compression is an important part of practical deployments: Shipping a 10MB neural network for a mobile device is much more acceptable than shipping a 100MB model. But if compression exacerbates biases, we must systematically audit and address those issues.We’re thinking:This work is a reminder that it’s not enough to optimize overall classification accuracy. We need to make sure our models also perform well on various slices of the data.\n\nU.S. lawmakers authorized a slew of national programs that promote artificial intelligence research, development, and deployment, and support efforts to make sure the results are ethical and trustworthy.What’s new:The 4,500 pages of theNational Defense Authorization Act(NDAA), which primarily serves to authorize programs for the U.S. military, includesprovisions that promote AIin both civilian and military agencies, and academic institutions, too.\n\nWhat it says:The NDAA only authorizes these programs; funding will come with further legislation. Among its provisions:\n\nYes, but:Some of these programs, such as the NSF’s AI research institutes, will cost money that Congress has yet to appropriate. Russell Wald, director of policy at Stanford’s Institute for Human-Centered Artificial Intelligence, toldThe Batchhe’s optimistic that funding will be allocated where it’s needed.\n\nBehind the news:President Donald Trump vetoed the NDAA in December, saying he wanted it to include a repeal ofSection 230of the Communications Decency act, which protects internet companies from legal liability for user-generated content on their sites. Congress overrode the veto and passed the bill into law on New Year’s Day.Why it matters:The U.S. is a global leader in AI innovation and home to more big AI companies than anywhere else, but its government has lagged in issuing a comprehensive AI strategy. Directives like the National Research Cloud would give a healthy boost to AI researchers in many areas, and the impact likely would ripple across the world.We’re thinking:This bill is a major step forward in U.S. support for AI. We’ll keep our fingers crossed that the necessary funding comes through.\n\nWe’re excited to announce that Generative Deep Learning with TensorFlow, course 4 of the TensorFlow: Advanced Techniques Specialization, will launch on January 13.Pre-enroll now\n\nAn AI-powered collar may help protect wild elephants from poachers, hunters, and other hostile humans.What’s new:TenElephantEdgewireless tracking collars will be fitted onto African elephants next year,TechCrunchreported. The product of an open source collaboration between hardware and software engineers, the collar serves as a platform for machine learning models designed to interpret elephant behavior and alert sympathetic humans when the animals are in trouble.How it works:The models included are winners of a competition organized byHackster.io,a hardware engineering community, andSmart Parks,a Dutch conservation group. They were built using development tools fromEdge Impulseand work with hardware from organizations includingInstitute IrnasandAvnet.\n\nBehind the news:Defenders of wildlife are increasingly using AI to extend their reach and effectiveness.\n\nWhy it matters:Africa’s elephant population has plummeted in recent years. Only about 350,000 wild elephants remain on the continent, and poachers illegallykillupward of 15,000 a year. These animals need all the help they can get.\n\nWe’re thinking:This work addresses the elephant in the room.\n\nA survey of AI in large companies sees boom times ahead — if AI teams can get past issues that surround implementation.What’s new:Businesses of all sizes are using more machine learning, spending more on it, and hiring more engineers to wrangle it, according to asurveyof 750 business leaders by Algorithmia, which provides tools that automate model deployment and management. Nonetheless, struggles with deployment, scaling, and other issues continue to hinder adoption.What they found:The survey questioned executives in a variety of sectors including finance, healthcare, education, and information technology. More than two-thirds of those who responded said their AI budgets are growing, while only 2 percent are cutting back.\n\nBehind the news:Several other recent surveys shed light on AI’s evolving role in the business world. For instance,MIT Technology Reviewlooked at AI’s growth in different global regions, andMcKinseyexamined how different market sectors, like manufacturing, marketing, and supply chain management, are finding profitable uses for the technology.\n\nWhy it matters:AI is new enough, and evolving fast enough, that every company’s experience is different. Spotting areas where industries where machine learning is having an impact, as well as trouble spots in deployment, can help guide crucial decisions.We’re thinking:In 2019, many companies experimented with AI. In 2020, a growing number started talking about how to productionize models. In the coming year, we hope for rapid progress in MLOps processes and tools to make building and productionizing machine learning systems repeatable and systematic.AI Fund(where Andrew is managing general partner) has seen a lot of startups jump into this space, which bodes well for the future.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202021-01-0620at2011.32.2520AM20copy.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2070.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/COMPRESSED.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/NDAA20SIZED.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Gif20520v2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2064.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ALGORITHMIA.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-221/",
    "title": "issue 221",
    "date": "",
    "reading_time": "",
    "content": "I’ve always believed in democratizing access to the latest advances in artificial intelligence. As a step in this direction, we just launched “Generative AI for Everyone” on Coursera. The course assumes no programming or AI background, and I hope it will be useful to students, teachers, artists, scientists, engineers, leaders in business and government, and anyone else who simply wants to know how to apply generative AI in their work or personal life. Pleasecheck it outand encourage your friends to take a look, especially those with a nontechnical background.\n\nJust as web search and word processing have become essential skills in the workplace, using generative AI soon will become a baseline skill set expected by every employer. This highly accessible, general-purpose technology is suitable for numerous tasks. It’s already used in copyediting, customer service, brainstorming, summarizing documents, and more. And many more uses are yet to be identified.\n\nThe course covers:\n\nIf you’re an engineer: I designed this course to be accessible to nontechnical professionals partly to help technical people work with them more easily. With earlier waves of technology, I found that the gap in understanding between technical and nontechnical people got in the way of putting the technology to use. So if you already have a good understanding of generative AI, please encourage your nontechnical colleagues to take this course. They will learn a lot, and I hope this will help you collaborate more productively!\n\nYou can check out the coursehere.\n\nKeep learning!\n\nAndrew\n\nU.S. President Biden announced directives that control AI based on his legal power to promote national defense and respond to national emergencies.\n\nWhat’s new:The White Houseissuedan executive order that requires AI companies and institutions to report and test certain models and directs federal agencies to set standards for AI. The order follows a six-month process of consultation with the AI community and other stakeholders.\n\nHow it works:The executive order interprets existing law — specifically the Cold War-era Defense Production Act, a Cold War-era law that gives the president powers to promote national defense and respond to emergencies — and thus can be implemented without further legislation. It focuses on foundation models, or general-purpose models that can be fine-tuned for specific tasks:\n\nBehind the news:The executive order was long in the making and joins other nations’ moves to limit AI.\n\nWhy it matters:While Europe and China move aggressively to control specific uses and models, the White House seeks to balance innovation against risk, specifically with regard to national defense but also social issues like discrimination and privacy. The executive order organizes the federal bureaucracy to grapple with the challenges of AI and prepares the way for national legislation.\n\nWe’re thinking:We need laws to ensure that AI is safe, fair, and transparent, and the executive order has much good in it. But it’s also problematic in fundamental ways. For instance, foundation models are the wrong focus. Burdening basic technology development with reporting and standards places a drag on innovation. It makes more sense to regulateapplicationsthat carry known risks, such as underwriting tools, healthcare devices, and autonomous vehicles. We welcome regulations that promote responsible AI and look forward to legislation that limits risks without hampering innovation.\n\nA new index ranks popular AI models in terms of information their developers provide about their training, architecture, and usage. Few score well.\n\nWhat’s new:The Stanford Center for Research on Foundation Modelspublishedits debut Foundation Model Transparency Index, scoring 10 popular models on how well their makers disclosed details of their training, characteristics, and use.\n\nHow it works:Rishi Bommasani, Kevin Klyman, and colleagues at Stanford, MIT, and Princetonexamined10 foundation models — that is, models that can be pretrained for general purposes and fine-tuned for specific tasks — from 10 companies. They scored each model by asking 100 yes-or-no questions that covered training, model architecture and behavior, and policies regarding access and usage.\n\nResults:The index assigned each model a score  between 1 and 100. Meta’s Llama 2 ranked most transparent with a score of 54. BigScience’s BLOOM-Z came in just behind with a score of 53. At the bottom of the list were Inflection’s Inflection-1, which scored 21, and Amazon’s Titan Text, which scored 12.\n\nYes, but:Because the index is limited to yes/no questions, it doesn’t allow for partial credit. In addition, the questions are weighted equally, so lack of transparency in an important area (say, access to training data) costs only one point in a model’s overall score. It’s easy to imagine companies gaming the scores rather than addressing the most meaningful deficits.\n\nBehind the news:Researchers at MIT, Cohere For AI, and 11 other organizations recently launched the Data Provenance Platform, a project that audits and categorizes training datasets. The effort offers aData Provenance Explorerfor evaluating sources, licenses, creators, and other metadata with respect to roughly 1,800 text datasets.\n\nWhy it matters:AI has a transparency problem, and the rise of models that serve as foundations for other models exacerbates the issue. Without disclosure of fundamental factors like architectures, datasets, and training methods, it’s impossible to replicate research, evaluate cost per performance, and address biases. Without disclosure of applications based on a given foundation model, it’s impossible to weigh those applications’ capabilities and limitations. A consistent set of criteria for evaluating transparency may encourage greater disclosure.We’re thinking:The rise of open source AI has been accompanied by an opposite rise in commercial concerns that have little incentive to reveal the inner workings of their models. An index encourages everyone to provide detailed information about the systems they build, and we hope it will help engineers who care about transparency to persuade their teammates. We look forward to refinements and expansion to cover models that aren’t included among the initial 10.\n\nAndrew Ng’s new course, “Generative AI for Everyone,” is live on Coursera! Learn how to use generative AI in your life and work, what this technology can (and can’t) do, and how to put it to use in the real world.Enroll today to get started!\n\nThe state of California pulled the parking brake on Cruise driverless vehicles.\n\nWhat’s new:The California Department of Motor Vehicles (DMV)suspendedCruise’s permit to operate vehicles in the state without safety drivers. The General Motors subsidiary responded byhaltingits robotaxi operations across the United States.\n\nHow it works:The California DMV acted following an early Octoberincidentin San Francisco. A Cruise driverless car struck and trapped a pedestrian who had been thrown into its path by a separate hit-and-run.\n\nBehind the news:Cruise’s deployment of driverless taxis in San Francisco has been troubled.\n\nWhy it matters:Cruise’s latest trouble is a serious setback not just for GM, but for the self-driving car industry, which has been criticized for overpromising and underdelivering. The California DMV’s act has energized politicians, activists, and other public figures who oppose driverless taxis.\n\nWe’re thinking:TheAI community must lean into transparency to inspire the public’s trust.California determined that Cruise was not fully forthcoming about its role in the incident — a serious breach of that trust. Voluntary suspension of operations is a welcome step toward restoring it. We hope the company takes the opportunity to conduct a comprehensive review.\n\nText-to-image generators often miss details in text prompts, and sometimes they misunderstand parts of a prompt entirely. Synthetic captions can help them follow prompts more closely.\n\nWhat’s new:James Betker, Gabriel Goh, Li Jing, and Aditya Ramesh at OpenAI, along with colleagues at Microsoft,improveda latent diffusion model’s performance by training it on an image-caption dataset including model-generated captions that were more detailed than those typically scraped from the web. They used the same technique to train DALL·E 3, the latest version of OpenAI’s text-to-image generator.\n\nKey insight:Text-to image generators learn about the relationships between images and their descriptions from datasets of paired images and captions. The captions in typical image-caption datasets are limited to general descriptions of image subjects, with few details about the subjects and little information about their surroundings, image style, and so on. This makes models trained on them relatively insensitive to elaborate prompts. However, language models can generate captions in great detail. Training on more-detailed synthetic captions can give an image generator a richer knowledge of the correspondence between words and pictures.\n\nHow it works:Rather than reveal details about DALL·E 3’s architecture and training, the authors describe training alatent diffusion model.\n\nResults:The authors trained separate latent diffusion models on datasets containing 95 percent generated captions and 100 percent human-made captions. They used the models to generate 50,000 images each and used OpenAI’s CLIP to calculate a similarity score (higher is better) between the prompts and generated images. The model trained on synthetic captions achieved 27.1 CLIP similarity, while a model trained on human-made captions achieved 26.8 CLIP similarity.\n\nTesting DALL·E 3:The authors also tested human responses to images generated by DALL·E 3, Midjourney 5.2, and Stable Diffusion XL v1.0. Shown images based on 170 prompts selected by the authors, human judges found DALL·E 3’s output more true to the prompt and more appealing. Shown images based on 250 captions chosen at random fromMSCOCO, they found DALL·E 3’s output most realistic. In a similar test, DALL·E 3 achieved a higher score on theDrawbenchdataset than Stable Diffusion XL v1.0 and DALL-E 2. (No word on how DALL·E 3 compared to Midjourney in this experiment.)\n\nWhy it matters:Synthetic data is used increasingly to train machine learning models. The market research firm Gartner says that output from generative models will constitute60 percentof data used in AI development by 2024. While synthetic data has been shown to boost performance in typical training methods, recursively training one model on another model’s output candistortthe trained model’s output distribution — a scenario that could manifest over time as more models trained on synthetic data are used to generate data to train subsequent models.\n\nWe’re thinking:Using one AI model to help another to learn seems to be an emerging design pattern. For example,reinforcement learning from AI feedback(RLAIF) uses AI to rate output from large language models, rather than reinforcement learning from human feedback (RLHF). It’s a fair bet that we’ll see many more techniques along this line.\n\nLearn how to identify and scope vision applications, choose a project type and model, apply data-centric AI, and develop an MLOps pipeline in “Building Computer Vision Applications” with Andrew Ng. Join us on Monday, November 6, 2023, at 10 a.m. Pacific Time.Register here\n\nA Google multimodal app generator is in the works, according to leaked informationThe application, called Stubbs, reportedly generates complete apps with functional code and may be powered by the Gemini LLM or an even more advanced system. Its potential to handle text, images, code, and structured content, while providing various output previews, could introduce a new era of multimodal capabilities. (Bedros Pamboukian’s Medium blog)\n\nResearch: An analysis of the changing legal terrain of synthetic dataResearchers examine the legal challenges arising from the growing usage of synthetic data. artificially generated data disrupts the balance between utility, privacy, and human rights enshrined in existing laws. The authors call for legal reforms to address the unique dynamics of synthetic data. (SSRN)\n\nEconomist built an interactive book on economics using GPT-4Tyler Cowen, an economics professor and author of economics blog Marginal Revolution, launched a generative book. The project, called GOAT: “Who is the Greatest Economist of all Time, and Why Does it Matter?” offers readers an interactive experience, allowing them to query, rewrite, and customize its content. The book also provides insights into the work and lives of prominent economists like Adam Smith and John Stuart Mill. (Marginal Revolution)\n\nSearch engine You.com now allows personalization on its AI assistant\"Smart Learn,\" the platform’s AI assistant, now adapts to users' preferences and habits over time. Smart Learn promises to improve response quality while respecting privacy and allowing users to control their personalization settings. The feature is currently in beta for YouPro members, and early access is available. (You.com)\n\nNvidia reports that the U.S. ordered sudden restrictions on AI chip exports to ChinaOriginally slated to take effect November 16, the measures are part of Joe Biden's administration's efforts to prevent countries like China, Iran, and Russia from acquiring high-end AI chips designed by Nvidia and other manufacturers. While Nvidia has not disclosed the reason for the expedited timeline, it assured investors that global demand remained strong and the accelerated licensing requirements were not expected to significantly impact its near-term financial results. (BBC)\n\nResearch: Researchers use machine learning to detect urban decay and improve city planningScientists from the University of Notre Dame and Stanford University built a scalable method for measuring urban decay. Traditional methods of evaluating urban quality involve sociodemographic and economic characteristics, but these new techniques employ AI and street view images to identify objects like potholes, garbage, graffiti, and broken windows. The goal of the research is to gauge the condition of urban areas and better inform urban policy and planning. (University of Notre Dame)\n\nNegotiations over the European Union’s AI Act reach their final stagesThe EU institutions, including the Council, Parliament, and Commission, have tackled high-risk AI applications' classification and supervision of models. However, negotiations on prohibitions and law enforcement are still pending. The focus will now shift to the upcoming trilogue on December 6, where a political agreement is anticipated, although not guaranteed. Additionally, nine technical meetings are scheduled to address complex aspects of the AI law. (Euractiv)\n\nResearch: Study reveals prevalence of sycophancy in AI assistants' responsesSycophancy refers to AI models favoring responses that align with user beliefs over truthful ones. The research found consistent sycophantic tendencies across multiple text-generation tasks in five AI assistants. The study further revealed that both human evaluators and preference models often prefer sycophantic responses, sometimes at the cost of accuracy. (Arxiv)\n\nOpenAI, Anthropic, Google, and Microsoft establish $10 million AI safety fundLeading companies in the AI field appointed Chris Meserole, formerly of the Brookings Institute, as Executive Director of the Frontier Model Forum. This industry body aims to ensure the safe and responsible development of advanced AI models globally. They also created an AI Safety Fund, committing over $10 million to support AI safety research. The fund will back independent researchers, prioritizing model evaluation to enhance AI system safety. (OpenAI)\n\nResearch:Researchers trained a neural network to enhance Wikipedia’s credibilityResearchers developed a system called \"SIDE\" to bolster Wikipedia's verifiability by identifying citations that might not adequately support their claims and recommending better alternatives from the web. This neural network-based system, trained on existing Wikipedia references, showed promising results, with human testers preferring SIDE's recommendations to the originally cited references in 70% of cases. Additionally, a demo within the English-speaking Wikipedia community revealed that SIDE's first citation recommendation was twice as likely to be preferred for the top 10% of claims most likely to be unverifiable. (Nature)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch-ads-and-exclusive-banners--72--2.png",
      "https://dl-staging-website.ghost.io/content/images/2023/11/USEXEC.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/11/TRANSPARENCY.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch-ads-and-exclusive-banners--72-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/11/CRUISE.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/11/DALLE3-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--68-.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-274/",
    "title": "issue 274",
    "date": "",
    "reading_time": "",
    "content": "Trump and the Republican party chalked up huge wins this week. Did manipulation of social media by generative AI play any role in this election? While many have worried about AI creating fake or misleading content that influences people, generative AI has probably not been the primary method of manipulation in this election cycle. Instead, I think a bigger impact might have been the “amplification effect” where software bots — which don’t have to rely heavily on generative AI — create fake engagement (such as likes/retweets/reshares), leading social media companies’ recommendation algorithms to amplify certain content to real users, some of whom promote it to their own followers. This is how fake engagement leads to real engagement.\n\nThis amplification effect is well known to computer security researchers. It is an interesting sign of our global anxiety about AI that people ascribe social media manipulation to AI becoming more powerful. But the problem here is not that AI is too powerful; rather, it is that AI is not powerful enough. Specifically, the issue is not that generative AI is so powerful that hostile foreign powers or unethical political operatives are successfully using it to create fake media that influences us; the problem is that some social media companies’ AI algorithms are not powerful enough to screen out fake engagement by software bots, and mistake it for real engagement by users. These bots (which don’t need to be very smart) fool the recommender algorithms into amplifying certain content.\n\nThe Washington Postreported thattweets on X/Twitter posted by Republicans were more viral than tweets from Democrats. Did this reflect the audience’s deeper engagement with Republican messages than Democratic ones, or have bots influenced this by boosting messages on either side? It is hard to know without access to Twitter’s internal data.\n\nThe bottleneck to disinformation is not creating it butdisseminating it. It is easy to write text that proposes a certain view, but hard to get many people to read it. Rather than generating a novel message (or using deepfakes to generate a misleading image) and hoping it will go viral, it might be easier to find a message written by a real human that supports a point of view you want to spread, and use bots to amplify that.\n\nI don’t know of any easy technical or legislative approach to combating bots. But it would be a good step torequire transparencyof social media platforms so we can better spot problems, if any. Everyone has a role to play in protecting democracy, and in tech, part of our duty will be to make sure social media platforms are fair and defend them against manipulation by those who seek to undermine democracy.\n\nDemocracy is one of humanity’s best inventions. Elections are an important mechanism for protecting human rights and supporting human flourishing. Following this election, we must continue to strenuously nourish democracy and make sure this gem of human civilization continues to thrive.\n\nKeep learning!\n\nAndrew\n\nLearn the principles of effective data engineering in this four-course professional certificate taught by Joe Reis. Develop your skills in the data engineering lifecycle and gain hands-on experience building data systems on Amazon Web Services. Earn a certificate upon completion!Enroll today\n\nAPI commands for Claude Sonnet 3.5 enable Anthropic’s large language model to operate desktop apps much like humans do. Be cautious, though: It’s a work in progress.\n\nWhat’s new:AnthropiclaunchedAPI commands for computer use. The new commands prompt Claude Sonnet 3.5 to translate natural language instructions into commands that tell a computer to open applications, fetch data from local files, complete forms, and the like. (In addition, Anthropic improved Claude Sonnet 3.5 to achieve a state-of-the-art score on theSWE-bench Verifiedcoding benchmark and released the faster, cheaper Claude Haiku 3.5, which likewise shows exceptional performance on coding tasks.)\n\nHow it works:The commands for computer use don’t cost extra on a per-token basis, but they may require up to 1,200 additional tokens and run repeatedly until the task at hand is accomplished, consuming more input tokens. They’re available via Anthropic, Amazon Bedrock, and Google Vertex.\n\nYes, but:The current version of computer use is experimental, and Anthropic acknowledges various limitations. The company stronglyrecommendsusing these commands only in a sandboxed environment, such as a Docker container, with limited access to the computer’s hard drive and the web to protect sensitive data and core system files. Anthropic restricts the ability to create online accounts or post to social media or other sites (but says it may lift this restriction in the future).\n\nBehind the news:Several companies have been racing to build models that can control desktop applications. Microsoft researchers recently releasedOmniParser, a tool based on GPT-4V that identifies user-interface elements like windows and buttons within screenshots, potentially making it easier for agentic workflows to navigate computers. In July, Amazonhiredstaff and leaders from Adept, a startup that trained models to operate computer applications. (Disclosure: Andrew Ng sits on Amazon’s board of directors.)Open Interpreteris an open-source project that likewise uses a large language model to control local applications like image editors and web browsers.\n\nWhy it matters:Large multimodal models already use externaltoolslike search engines, web browsers, calculators, calendars, databases, and email. Giving them control over a computer’s visual user interface may enable them to automate a wider range of tasks we use computers to perform, such ascreating lesson plansand — more worrisome —taking academic tests.\n\nWe’re thinking:Controlling computers remains hard. For instance, using AI to read a screenshot and pick the right action to take next is very challenging. However, we’re confident that this capability will be a growth area for agentic workflows in coming years.\n\nShipping ports are the latest front in the rising tension between labor unions and AI-powered automation.\n\nWhat’s new:Autonomous vehicles, robotic cranes, and computer vision systems increasingly manage the flow of goods in and out of ports worldwide. Dockworkers in the United States are worried that such technology threatens their livelihoods,The Wall Street Journalreported.\n\nHow it works:Automation boosts the number of containers a port can move per hour from vessel to dock. For instance, Shanghai’s Yangshan Deep Water Port, one of the world’s most automated ports, moves more than 113 containers per hour, while Oakland, California’sless-automatedport moves around 25 containers per hour,according to a reportby S&P Global Market Intelligence for the World Bank.\n\nDockworkers disagree:Harold Daggett, leader of the International Longshoremen’s Association, a union that negotiates on behalf of dockworkers,vowedto fight port automation, which he sees as a pretext to eliminate jobs. He has proposed that members of unions internationally refuse work for shipping companies that use automated equipment. Fresh from a three-day strike in early October, longshoremen will return to negotiations with shipping companies in mid-January.\n\nWhy it matters:Ports are one of many work environments where AI is bringing down costs while improving throughput. In many such situations, humans can continue to perform tasks that machines don’t do well. But where human jobs are at risk, society must determine the most productive path. Dockworkers, through their unions, have significant power in this equation. A protracted U.S. dockworker strike risks economic losses of up to$7.5 billion a week. On the other hand, automation could bring tremendous gains in safety, speed, and economic efficiency.\n\nWe’re thinking:We are very sympathetic to workers’ rights. Yet we also believe that more-efficient ports will boost commerce, creating many new jobs. As traditional roles change, workers need opportunities to learn new skills and adapt to the evolving job market. Society has a responsibility to provide a safety net as well as training and education for those whose jobs are threatened by automation.\n\nA new study suggests that leading AI models may meet the requirements of the European Union’s AI Act in some areas, but probably not in others.\n\nWhat’s new:The Zurich-based startup LatticeFlow, working with research institutions in Bulgaria and Switzerland, developedCOMPL-AI, an unofficial framework designed to evaluate large language models’ likely compliance with the AI Act. Aleaderboardranks an initial selection of models. (LatticeFlow does not work for the European Commission or have legal standing to interpret the AI Act.)\n\nHow it works:Apaperexplains how COMPL-AI maps the AI Act’s requirements to specific benchmarks. It evaluates each requirement using new or established tests and renders an aggregate score. These scores are relative measures, and the authors don’t propose thresholds for compliance. The assessment covers five primary categories:\n\nResults:The authors evaluated nine open models and three proprietary ones on a scale between 0 and 1. Theirreportson each model reveal considerable variability. (Note: The aggregate scores cited in the reports don’t match those in the paper.)\n\nYes, but:The authors note that some provisions of the AI Act, including explainability, oversight (deference to human control), and corrigibility (whether an AI system can be altered to change harmful outputs, which bears on a model’s risk classification under the AI Act), are defined ambiguously under the law and can’t be measured reliably at present. These areas are under-explored in the research literature and lack benchmarks to assess them.\n\nWhy it matters:With the advent of laws that regulate AI technology, developers are responsible for assessing a model’s compliance before they release it or use it in ways that affect the public. COMPL-AI takes a first step toward assuring model builders that their work is legally defensible or else alerting them to flaws that could lead to legal risk if they’re not addressed prior to release.\n\nWe’re thinking:Thoughtful regulation of AI is necessary, but it should be done in ways that don’t impose an undue burden on developers. While the AI Act itself is overly burdensome, we’re glad to see a largely automated path to demonstrating compliance of large language models.\n\nCoding agents are improving, but can they tackle machine learning tasks?\n\nWhat’s new:Chan Jun Shern and colleagues at OpenAI introducedMLE-bench, a benchmark designed to test how well AI coding agents do in competitions hosted by the Kaggle machine learning contest platform. The benchmark is availablehere.\n\nAgentic framework basics:An agentic framework or scaffold consists of a large language model (LLM) and code to prompt the model to follow a certain procedure. It may also contain tools the LLM can use, such as a Python console or web browser. For example, given a problem to solve, a framework might prompt the model to generate code, run the code in the Python console, generate evaluation code, run evaluation code, change the solution based on the console’s output, and repeat until the problem is solved.\n\nHow it works:MLE-bench is an offline competition environment that contains 75 Kaggle competitions selected manually by the authors, such as contests toidentify toxic commentsandpredict volcanic eruptions. Each competition includes a description, training and testing datasets, code to grade submissions, a leaderboard of human contestants for comparison with an agent’s performance, and a “complexity” rating (produced by OpenAI): low (takes an experienced human less than two hours to code a solution, not including training time), medium (between two and 10 hours), or high (more than 10 hours). Given a competition, an agent must produce a submission by (i) generating code to train a machine learning model and (ii) running the model on the test set. Users grade the submission to evaluate the agent’s performance.\n\nResults:The authors evaluated agent performance according to Kaggle’s standards for awarding medals to human contestants (described in the final bullet below).\n\nYes, but:The percentage of medals won by agents in this study is not comparable to percentages of medals won by humans on Kaggle. The authors awarded medals for excellent performance in all competitions included in the benchmark, but Kaggle does not. The authors didn’t tally the agents’ win rate for only competitions in which Kaggle awarded medals.\n\nWhy it matters:It’s important to evaluate the abilities of coding agents to solve all kinds of programming problems. Machine learning tasks are especially valuable as they bear on the ability of software to analyze unstructured data and adapt to changing conditions.\n\nWe’re thinking:We’re glad to see machine learning catching on among humans and machines alike!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--32-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/11/The-Batch-ads-and-exclusive-banners---2024-11-06T100800.051.png",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--23-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--24-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--25-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--26-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-293/",
    "title": "issue 293",
    "date": "",
    "reading_time": "",
    "content": "Last Friday on Pi Day, we held AI Dev 25, a new conference for AI Developers. Tickets had (unfortunately) sold out days after we announced their availability, but I came away energized by the day of coding and technical discussions with fellow AI Builders! Let me share here my observations from the event.\n\nI'd decided to start AI Dev because while there're great academic AI conferences that disseminate research work (such as NeurIPS, ICML and ICLR) and also great meetings held by individual companies, often focused on each company's product offerings, there were few vendor-neutral conferences for AI developers. With the wide range of AI tools now available, there is a rich set of opportunities for developers to build new things (and to share ideas on how to build things!), but also a need for a neutral forum that helps developers do so.\n\nBased on an informal poll, about half the attendees had traveled to San Francisco from outside the Bay Area for this meeting, including many who had come from overseas. I was thrilled by the enthusiasm to be part of this AI Builder community. To everyone who came, thank you!\n\nOther aspects of the event that struck me:\n\nDeepLearning.AI has a strong “Learner First” mentality; our foremost goal is always to help learners. I was thrilled that a few attendees told me they enjoyed how technical the sessions were, and said they learned many things that they're sure they will use. (In fact, I, too, came away with a few ideas from the sessions!) I was also struck that, both during the talks and at the technical demo booths, the rooms were packed with attendees who were highly engaged throughout the whole day. I'm glad that we were able to have a meeting filled with technical and engineering discussions.\n\nI'm delighted that AI Dev 25 went off so well, and am grateful to all the attendees, volunteers, speakers, sponsors, partners, and team members that made the event possible. I regretted only that the physical size of the event space prevented us from admitting more attendees this time. There is something magical about bringing people together physically to share ideas, make friends, and to learn from and help each other. I hope we'll be able to bring even more people together in the future.\n\nKeep building!\n\nAndrew\n\nP.S. I'm thrilled to share our newest course series: theData Analytics Professional Certificate! Data analytics remains one of the core skills of data science and AI, and this professional certificate takes you up to being job-ready for this. Led by Netflix data science leader Sean Barnes, this certificate gives you hands-on experience with essential tools like SQL, Tableau, and Python, while teaching you to use Generative AI effectively as a thought partner in your analyses. Labor economists project a 36% growth in data science jobs by 2033. I'm excited to see rising demand for data professionals, since working with data is such a powerful way to improve decision-making, whether in business, software development, or your private life. Data skills create opportunities at every level—I’m excited to see where they take you!Sign up here!\n\nTheData Analytics Professional Certificateis available now! This program equips you with data analytics skills—from foundations to job-ready. Learn statistical techniques combined with newly emerging generative AI workflows.Enroll now\n\nMultilingual AI models often suffer uneven performance across languages, especially in multimodal tasks. A pair of lean models counters this trend with consistent understanding of text and images across major languages.\n\nWhat’s new:A team at Cohere led by Saurabh Dash releasedAya Vision, a family of multilingual vision-language models with downloadable weights in 8 billion- and 32-billion-parameter sizes.\n\nHow it works:Each modelcomprisesa pretrained large language model (Aya Expanse for the 32B model, C4AI Command R7B for the 8B version), a pretrained vision encoder (SigLIP 2), and a vision-language adapter (“connector”) of unspecified architecture.\n\nPerformance:To test the model, the team built and released two benchmarks:m-WildVision, a multilingual version ofWild Vision Bench’s arena-style competition for discussion of images, andAyaVisionBench, 135 image-question pairs in each language that cover nine tasks including captioning images, understanding charts, recognizing characters in images, visual reasoning, and converting screenshots to code. On these two benchmarks, Aya Vision 8B and 32B outperformed larger competitors, as judged by Claude 3.7 Sonnet.\n\nBehind the news:Aya Vision builds on the Cohere-ledAyainitiative, a noncommercial effort to build models that perform consistently well in all languages, especially languages that lack high-quality training data. The project started with a multilingual text model (Aya Expanse), added vision (Aya Vision), and plans to eventually add video and audio.\n\nWhy it matters:Multilingual vision-language models often perform less well in low-resource languages, and the gap widens when they process media other than text. Aya Vision’s recipe for augmenting synthetic data with successively refined translations may contribute to more universally capable models. Aya Vision is available on the global messaging platform WhatsApp, where it can be used to translate text and images in all 23 of its current languages.\n\nWe’re thinking:Multilingual vision models could soon help non-native speakers decipher Turkish road signs, Finnish legal contracts, and Korean receipts. We look forward to a world in which understanding any scene or document is as effortless in Swahili as it is in English.\n\nAn AI agent synthesizes novel scientific research hypotheses. It's already making an impact in biomedicine.\n\nWhat’s new:Google introducedAI co-scientist, a general multi-agent system designed to generate in-depth research proposals within constraints specified by the user. The team generated and evaluated proposals for repurposing drugs, identifying drug targets, and explaining antimicrobial resistance in real-world laboratories. It’s available to research organizations on a limited basis.\n\nHow it works:AI co-scientist accepts a text description of a research goal, including relevant constraints or ideas. In response, it generates research proposals and reviews, ranks, and improves them using seven agents based on Google’s Gemini 2.0 family of large language models. The completed proposals include sections that explain background, unmet needs, a proposed solution, goals, hypotheses, reasoning, study steps, and relevant articles. The agents take feedback and outputs from other agents to perform their prompted task simultaneously.\n\nResults:AI co-scientist achieved a number of impressive biomedical results in tests.\n\nBehind the news:A few AI systems have begun to produce original scientific work. For instance, a modelgenerated research proposalsthat human judges deemed more novel than proposals written by flesh-and-blood scientists, and an agentic workflowproduced research papersthat met standards for acceptance by top conferences.\n\nWhy it matters:While previous work used agentic workflows to propose research ideas on a general topic, this work generates proposals for specific ideas according to a researcher’s constraints (for example, a researcher could specify that a novel medical treatment for a specific disease only consider drugs already approved for human trials for other uses) and further instructions. AI co-scientist can take feedback at any point, allowing humans to collaborate with the machine: People provide ideas, feedback, and guidance for the model, and the model researches and proposes ideas in return.\n\nWe’re thinking:I asked my AI system to propose a new chemical experiment. But there was no reaction!\n\nThe United States Copyright Office determined that existing laws are sufficient to decide whether a given AI-generated work is protected by copyright, making additional legislation unnecessary.\n\nWhat’s new:AI-generated works qualify for copyright if a human being contributed enough creative input, according to thesecond partof what will be a three-part report on artificial intelligence and copyright law.\n\nHow it works:The report states that “the outputs of generative AI can be protected by copyright only where a human author has determined sufficient expressive elements.” In other words, humans and AI can collaborate on creative works, but copyright protection applies only if a human shapes the AI-generated material beyond simply supplying a prompt.\n\nBehind the news:Thefirst partof the Copyright Office’s report on digital replicas, or generated likenesses of a person’s appearance and voice. It found that existing laws don’t provide sufficient protection against unauthorized digital replicas and recommended federal legislation to address the gap. Its findings influenced ongoing discussions in Congress, where proposed bills like the No AI FRAUD Act and the NO FAKES Act aim to regulate impersonation via AI. Additionally, industry groups such as the Authors Guild and entertainment unions have pursued their own agreements with studios and publishers to safeguard performers, artists, and authors from unauthorized digital reproduction. However, no federal law currently defines whether copyright can protect a person’s likeness or performance.\n\nWhy it matters:The Copyright Office deliberately avoided prescribing rigid criteria for the types or degrees of human input that are sufficient for copyright. Such determinations require nuanced evaluation case by case. This flexible approach accommodates the diverse ways creative people use AI as well as unforeseen creative possibilities of emerging technology.\n\nWe’re thinking:Does copyright bar the use of protected works to train AI systems? The third part of the Copyright Office’s report — no indication yet as to when to expect it — will address this question. The answer could have important effects on both the arts and AI development.\n\nMaterials that have specific properties are essential to progress in critical technologies like solar cells and batteries. A machine learning model designs new materials to order.\n\nWhat’s new:Researchers at Microsoft and Shenzhen Institute of Advanced Technology proposedMatterGen, a diffusion model that generates a material’s chemical composition and structure from a prompt that specifies a desired property. The model and code areavailableunder a license that allows commercial as well as noncommercial uses without limitation. The trainingdataalso is noncommercially available.\n\nHow it works:MatterGen’s training followed a two-stage process. In the first stage, it learned to generate materials (specifically crystals — no liquids, gasses, or amorphous solids like glass). In the second, it learned to generate materials given a target mechanical, electronic, magnetic, or chemical property such as magnetic density or bulk modulus (the material’s resistance to compression).\n\nResults:The authors generated a variety of materials, and they synthesized one to test whether it had a target property. Specifically, they generated over 8,000 candidates with the target bulk modulus of 200 gigapascals (a measure of resistance to uniform compression), then automatically filtered them based on a number of factors to eliminate material in their dataset and unstable materials. Of the remaining candidates, they chose four manually and successfully synthesized one. The resulting crystal had a measured bulk modulus of 158 gigapascals. (Most materials in the dataset had a bulk modulus of between 0 and 400 gigapascals.)\n\nBehind the news:Published in 2023,DiffCSPalso uses a diffusion model to generate the structures of new materials. However, it does so without considering their desired properties.\n\nWhy it matters:Discovering materials relies mostly on searching large databases of existing materials for those with desired properties or synthesizing new materials and testing their properties by trial and error. Designing new crystals with desired properties at the click of a button accelerates the process dramatically.\n\nWe’re thinking:While using AI to design materials accelerates an important step, determining whether a hypothesized material can be  manufactured efficiently at scale is still challenging. We look forward to research into AI models that also take into account ease of manufacturing.\n\nIn our latest short course,Long-Term Agentic Memory with LangGraph, learn how to integrate semantic, episodic, and procedural memory into AI workflows. Guided by Harrison Chase, you’ll build a personal email agent with routing, writing, and scheduling tools to automatically ignore and respond to emails, while keep track of facts and past actions over time.Join in for free",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--63--2.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321-1.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--64-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--65-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--53-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--66-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/image--24-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-288/",
    "title": "issue 288",
    "date": "",
    "reading_time": "",
    "content": "At the Artificial Intelligence Action Summit in Paris this week, U.S. Vice President J.D. Vancesaid, “I’m not here to talk about AI safety. ... I’m here to talk about AI opportunity.” I’m thrilled to see the U.S. government focus on opportunities in AI. Further, while it is important to use AI responsibly and try to stamp out harmful applications, I feel “AI safety” is not the right terminology for addressing this important problem. Language shapes thought, so using the right words is important. I’d rather talk about “responsible AI” than “AI safety.” Let me explain.\n\nFirst, there are clearly harmful applications of AI, such as non-consensual deepfake porn (which creates sexually explicit images of real people without their consent), the use of AI in misinformation, potentially unsafe medical diagnoses, addictive applications, and so on. We definitely want to stamp these out! There are many ways to apply AI in harmful or irresponsible ways, and we should discourage and prevent such uses.\n\nHowever, the concept of “AI safety” tries to make AI — as a technology — safe, rather than making safe applications of it. Consider the similar, obviously flawed notion of “laptop safety.” There are great ways to use a laptop and many irresponsible ways, but I don’t consider laptops to be intrinsically either safe or unsafe. It is the application, or usage, that determines if a laptop is safe. Similarly, AI, a general-purpose technology with numerous applications, is neither safe nor unsafe. How someone chooses to use it determines whether it is harmful or beneficial.\n\nNow, safety isn’t always a function only of how something is used. An unsafe airplane is one that, even in the hands of an attentive and skilled pilot, has a large chance of mishap. So we definitely should strive to build safe airplanes (and make sure they are operated responsibly)! The risk factors are associated with the construction of the aircraft rather than merely its application. Similarly, we want safe automobiles, blenders, dialysis machines, food, buildings, power plants, and much more.\n\n“AI safety” presupposes that AI, the underlying technology, can be unsafe. I find it more useful to think about how applications of AI can be unsafe.\n\nFurther, the term “responsible AI” emphasizes that it is our responsibility to avoid building applications that are unsafe or harmful and to discourage people from using even beneficial products in harmful ways.\n\nIf we shift the terminology for AI risks from “AI safety” to “responsible AI,” we can have more thoughtful conversations about what to do and what not to do.\n\nI believe the 2023 Bletchley AI Safety Summit slowed down European AI development — without making anyone safer — by wasting time considering science-fiction AI fears rather than focusing on opportunities. Last month, at Davos, business and policy leaders also had strong concerns about whether Europe can dig itself out of the current regulatory morass and focus on building with AI. I am hopeful that the Paris meeting, unlike the one at Bletchley, will result in acceleration rather than deceleration.\n\nIn a world where AI is becoming pervasive, if we can shift the conversation away from “AI safety” toward responsible [use of] AI, we will speed up AI’s benefits and do a better job of addressing actual problems. That will actually make people safer.\n\nKeep building!\n\nAndrew\n\nUnderstand and implement the attention mechanism, a key element in transformer-based LLMs, using PyTorch. In this course, StatQuest’s Josh Starmer explains the core ideas behind attention mechanisms, the algorithm itself, and a step-by-step breakdown of how to implement them in PyTorch.Enroll now\n\nOpenAI introduced a state-of-the-art agent that produces research reports by scouring the web and reasoning over what it finds.\n\nWhat’s new:OpenAI’sdeep researchresponds to users’ requests by generating a detailed report based on hundreds of online sources. The system generates text output, with images and other media expected soon. Currently the agent is available only to subscribers to ChatGPT Pro, but the company plans to roll it out to users of ChatGPT Plus, Team, and Enterprise.\n\nHow it works:Deep research is an agent that uses OpenAI’s o3 model, which is not yet publicly available. The model was trained via reinforcement learning to use a browser and Python tools, similar to the way o1 learned to reason from reinforcement learning. OpenAI has not yet released detailed information about how it built the system.\n\nResult: On abenchmarkof 3,000 multiple-choice and short-answer questions that cover subjects from ecology to rocket science, OpenAI deep research achieved 26.6 percent accuracy. In comparison, DeepSeek-R1 (without web browsing or other tool use) achieved 9.4 percent accuracy and o1 (also without tool use) achieved 9.1 percent accuracy. OnGAIA, questions that are designed to be difficult for large language models without access to additional tools, OpenAI deep research achieved 67.36 percent accuracy, exceeding theprevious state of the artof 63.64 percent accuracy.\n\nBehind the news:OpenAI’s deep research follows a similar offering of the same name by Google in December. A number of open source teams have built research agents that work in similar ways. Notable releases include aHugging Faceproject that attempted to replicate OpenAI’s work (not including training) in 24 hours (which achieved 55.15 percent accuracy on GAIA) andgpt-researcher, which implemented agentic web search in 2023, long before Google and OpenAI launched their agentic research systems.\n\nWhy it matters:Reasoning models like o1 or o3 made a splash not just because they delivered superior results but also because of the impressive reasoning steps the model took to produce the results. Combining that ability with web search and tool use enables large language models to formulate better answers to difficult questions, including those whose answers aren’t in the training data or whose answers change over time.\n\nWe’re thinking:Taking as much as 30 minutes of processing to render a response, OpenAI’s deep research clearly illustrates why we needmore compute for inference.\n\nGoogle revised its AI principles, reversing previous commitments to avoid work on weapons, surveillance, and other military applications beyond non-lethal uses like communications, logistics, and medicine.\n\nWhat’s new:Along with releasing its latestResponsible AI Progress Reportand an updated AIsafety framework, Google removed key restrictions from itsAI principles. The new version omits a section in the previous document titled “Applications we will not pursue.” The deleted textpledgedto avoid “technologies that cause or are likely to cause overall harm” and, where the technology risks doing harm, to “proceed only where we believe that the benefits substantially outweigh the risks” with “appropriate safety constraints.”\n\nHow it works:Google’s AI principles no longer prohibit specific applications but promote developing the technology to improve scientific inquiry, national security, and the economy.\n\nBehind the news:Google’s new stance reverses a commitment it made in 2018 after employeesprotestedits involvement inProject Maven, a Pentagon AI program for drone surveillance, from which Google ultimately withdrew. At the time, Google pledged not to develop AI applications for weapons or surveillance, which set it apart from Amazon and Microsoft. Since then, the company has expanded its work in defense,building ona $1.3 billion contract with Israel. In 2024,Anthropic, Meta, andOpenAIremoved their restrictions on military and defense applications, and Anthropic and OpenAIstrengthenedtheir ties with defense contractors such as Anduril and Palantir.\n\nWhy it matters:Google’s shift in policy comes as AI is playing an increasing role in conflicts in Israel,Ukraine, and elsewhere, and while global geopolitical tensions are on the rise. While Google’s previous position kept it out of military AI development, defense contractors like Anduril, Northrop Grumman, and Palantir — not to mention AI-giant peers — stepped in. The new principles recognize the need for democratic countries to take the lead in developing technology and standards for its use as well as the massive business opportunity in military AI as governments worldwide seek new defense capabilities. Still, no widely acceptedglobal frameworkgoverns uses of AI in combat.\n\nWe’re thinking:Knowing how and when to employ AI in warfare is one of the most difficult ethical questions of our time. Democratic nations have a right to defend themselves, and those of us who live in democracies have a responsibility to support fellow citizens who would put themselves in harm’s way to protect us. AI is transforming military strategy, and refusing to engage with it doesn’t make the risks go away.\n\nWhile Hangzhou’s DeepSeek flexed its muscles, Chinese tech giant Alibaba vied for the spotlight with new open vision-language models.\n\nWhat’s new:Alibaba announcedQwen2.5-VL, a family of vision-language models (images and text in, text out) in sizes of 3 billion, 7 billion, and 72 billion parameters. The weights for all three models are available for download onHugging Face, each under a different license: Qwen2.5-VL-3B isfree for non-commercial uses, Qwen2.5-VL-7B isfree for commercial and noncommercial usesunder the Apache 2.0 license, and Qwen2.5-VL-72B isfree to developers that have less than 100 million monthly active users. You can try them out for free for a limited time inAlibaba Model Studio, and Qwen2.5-VL-72B is available via the model selector inQwen Chat.\n\nHow it works:Qwen2.5-VL models accept up to 129,024 tokens of input according to thedeveloper reference(other sources provide conflicting numbers) and generate up to 8,192 tokens of output. Alibaba has not released details about how it trained them.\n\nResults: Alibaba reports Qwen2.5-VL-72B’s performance on measures that span image and text problems, parsing documents, understanding videos, and interacting with computer programs. Across 21 benchmarks, it beat Microsoft Gemini 2.0 Flash, OpenAI GPT-4o, Anthropic Claude 3.5 Sonnet, and open competitors on 13 of them (where comparisons are  relevant and available).\n\nMore models:Alibaba also introduced competition for DeepSeek and a family of small models.\n\nWhy it matters:Vision-language models are getting more powerful and versatile. Not long ago, it was an impressive feat simply to answer questions about a chart or diagram that mixed graphics with text. Now such models are paired with an agent to control computers and smartphones. Broadly speaking, the Qwen2.5-VL models outperform open and closed competitors and they’re open to varying degrees (though the data is not available), giving developers a range of highly capable choices.\n\nWe’re thinking:We’re happy Alibaba released a vision-language model that is broadly permissive with respect to commercial use (although we’d prefer that all sizes were available under a standard open weights license). We hope to see technical reports that illuminate Alibaba’s training and fine-tuning recipes.\n\nBrowsing the web to achieve a specific goal can be challenging for agents based on large language models and even for vision-language models that can process onscreen images of a browser. While some approaches address this difficulty in training the underlying model, the agent architecture can also make a difference.\n\nWhat’s new:Jing Yu Koh and colleagues at Carnegie Mellon University introducedtree search for language model agents, a method that allows agents to treat web interactions like tree searches. In this way, agents can explore possible chains of actions and avoid repeating mistakes.\n\nKey insight:Some web tasks, for instance finding a price of a particular item, require a chain of intermediate actions: navigating to the right page, scrolling to find the item, matching an image of the item to the image on the page, and so on. If an agent clicks the wrong link during this process, it might lose its way. The ability to evaluate possible actions and remember previous states of web pages can help an agent correct its mistakes and choose a chain of actions that achieves its goal.\n\nHow it works:An agent based on GPT-4o attempted 200tasksusing website mockups that mimicked an online retail store, Reddit-like forum, and directory of classified ads. The tasks included ordering an item to be delivered to a given address, finding specific images on the forum, and posting an ad. The authors annotated each web page using the method calledSet of Mark, which identifies every visual element capable of interaction with a bounding box and a numerical ID.\n\nResults:The authors compared two agents, one that followed their search method and another that started at the same page and received the same instruction but took one action per state and never backtracked. The agents attempted 100 shopping tasks, 50 forum tasks, and 50 classified-ads tasks. The one equipped to search successfully completed 26.4 percent of the tasks, while the other agent completed 18.9 percent of the tasks.\n\nWhy it matters:Search joins reflection, planning, tool use, and multi-agent collaboration as an emergingagentic design pattern. Following many branching paths of actions enables an agent to determine the most effective set of actions to accomplish a task.\n\nWe’re thinking:Agentic design patterns are progressing quickly! In combination withcomputer use, this sort of search method may enable agents to execute a wide variety of desktop tasks.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/02/RESPONSIBLE-AI_Blue-Or4_1200px--1--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/02/The-Batch-ads-and-exclusive-banners--11-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/DEEPRESEARCH_600px_opt.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/02/GOOGLEWEAPONS4c.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--47-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--48-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-64/",
    "title": "issue 64",
    "date": "",
    "reading_time": "",
    "content": "In repeated trials, such as dice rolls or cohorts of patients with  potentially fatal illness, it’s easy to define the probability of a given event. We have a set of possible universes, and the probability is the fraction of those universes in which the event occurs. We can also ask if a set of probabilistic predictions is calibrated. If so, then out of all the events predicted to occur with an 89 percent chance, around 89 percent of them — neither many more nor many fewer — actually occur. We want our learning algorithms’ probabilistic outputs to be calibrated, and there is a body of literature on this topic.\n\nBut an election is a one-time event. What does a probability mean in this case?\n\nWhen fivethirtyeight.com says that Biden has an 89 percent chance of winning, I mentally append the phrase “under a certain set of modeling assumptions made by the fivethirtyeight team.” The analysts made a set of assumptions under which they built a number of different universes — some that went for Biden, some Trump — and found that Biden won in 89 percent of them. It’s important to remember that these universes are artificial constructs built on the assumptions that Nate Silver and his team chose.\n\nI find that organizations such as fivethirtyeight.com generally make reasonable assumptions. For example, one assumption might be that a state’s vote tally for a given candidate follows a Gaussian distribution, with mean and variance estimated from the polling data. Yet every model has flaws and fails to capture some effects. A model might assume that each state’s outcome is independent of other states — but what if there are pervasive problems with the postal service delivery of mail-in ballots, or systematic biases in polling that result in undercounting some demographics? That’s why, while I consider election polls to be useful, I don’t take their predictions at face value.\n\nEven though every model is flawed, good ones allow us to understand the world better. No one knows with certainty if it will rain tomorrow, but my decision to carry an umbrella will differ depending on the probability. That’s why I use probabilities to quantify uncertainties when I make decisions.\n\nI find that if you think in probabilities consistently, you’ll start to develop an intuitive feeling for what the numbers mean. When someone tells me something has an 89 percent chance of happening, I’ve heard similar statements enough times in enough different contexts to have an intuition for what might happen next.\n\nLike many others, I stayed up late watching the election results trickle in, worried about the future of the U.S. and the potential global impact of this momentous election. Whatever the outcome, let us commit to keep on fighting for fairness, justice, and human decency, and to do our utmost to bring the greatest possible good to the greatest number of people.\n\nKeep learning!\n\nAndrew\n\nPrivate citizens are using AI-driven surveillance to turn the tables on law enforcement.What’s new:Activists are using face recognition to identify abusive cops, according toThe New York Times.How it works:Many jurisdictions allow police to wear face masks or conceal their name tags, a practice that critics say protects officers who use excessive force against citizens. Activists around the world are using off-the-shelf software and crowdsourced datasets to develop systems that identify cops in photos and videos.\n\nBehind the news:Police use of face recognition, such as the previously undisclosed DC-area system reported this week by theThe Washington Post, has come under intense scrutiny. Public outcry has led to restrictions in some countries.\n\nWhy it matters:Like many powerful technologies, face recognition is a double-edged sword. In the hands of private citizens, it could help increase police accountability and stem abuses. But it could also lead to harassment and worse against cops and others who have done nothing wrong.We’re thinking:It seems inevitable that ordinary citizens would harness face recognition to fight back against cops who allegedly have abused human or civil rights. Democratization of technology is a wonderful thing, but it comes with important responsibilities. Individuals — as well as governments and businesses — need to take care to use face recognition ethically.\n\nThe incessant rhythm of a heartbeat could be the key to distinguishing real videos from deepfakes.What’s new:DeepRhythmdetects deepfakes using an approach inspired by the science of measuring minute changes on the skin’s surface due to blood circulation. Hua Qi led teammates at Kyushu University in Japan, Nanyang Technological University in Singapore; Alibaba Group in the U.S., and Tianjin University in China.Key insight:Current neural generative models don’t pick up on subtle variations in skin color caused by blood pulsing beneath the surface. Consequently, manipulated videos lack these rhythms. A model trained to spot them can detect fake videos.How it works:DeepRhythm comprises two systems. The first consists of pretrained components that isolate faces in video frames and highlight areas affected by blood circulation. The second system examines the faces and classifies the video. It was trained and validated onFaceForensics++, a video dataset that collects output from deepfake models.\n\nThe world’s most sophisticated language model won’t replace your doctor anytime soon.What’s new:Researchers at Nabla, an AI-enabled healthcare platform,foundthatGPT-3lacks the logical reasoning skills to be a useful medical chatbot.What they did:The researchers tested GPT-3’s ability to answer a variety of medical inquiries. It fell short on most of them.\n\nThe researchers fed the model a four-page document of insurance benefits, then asked it to tally copayments for several different procedures. It was able to retrieve the copay amounts for single procedures, but it couldn’t compute the total of different procedures.\n\nBehind the news:Many healthcare providers deploy chatbots to answer patient questions without requiring a doctor’s attention. For instance, the U.S. Centers for Disease Control provide abotthat helps users determine whether they have Covid-19.Indiaoffers a Whatsapp bot that fields citizens’ questions about the disease. Researchers have raisedconcernsabout the effectiveness of such programs.Why it matters:GPT-3 produces dazzling output, but its output is unreliable when it must accord with facts. In a healthcare context, Nabla’s study helps counteract thehypeto focus attention on some of the limitations in real-world applications. OpenAI founder Sam Altman himself hassaidas much.We’re thinking:The realpromiseof GPT-3 is not what it can do today, but what the future GPT-7, Bert-5, or NewAlgorithm-2 might do.\n\nAll four courses of ourNatural Language Processing Specializationare now available on Coursera.Enroll nowand join 40,000-plus learners who are mastering the most powerful language modeling techniques!\n\nThe U.S. military enlisted natural language processing to combat disinformation.What’s new:Primer, a San Francisco startup, is developing asystemfor the Department of Defense that sifts through news, social media, research, and reports to spot propaganda campaigns. The system is scheduled for deployment in June 2021. The company specializes in NLP models like the multi-document summarizer illustrated above.How it works:The disinformation detector uses a modifiedXLNetto classify nouns in a given article as people, places, organizations, or miscellaneous. The model was trained onCoNLL-2003, a dataset of named entities in several languages, and fine-tuned on a proprietary corpus of defense, finance, news, and science documents. It reads Chinese, English, and Russian.\n\nWhy it matters:Human analysts can’t keep up with the flood of information — and disinformation — that bloats the internet. AI may help discover signals amid the noise.We’re thinking:Technology is making it cheaper and easier to create disinformation. Better detection could benefit not only national security but alsodisaster response,public health, and thedemocratic process.\n\nAccording to thelottery ticket hypothesis, the bigger the neural network, the more likely some of its weights are initialized to values that are well suited to learning to perform the task at hand. But just how big does it need to be? Researchers investigated the impact of initial parameter values on models of various sizes.What’s new:Jacob M. Springer at Swarthmore College and Garrett T. Kenyon at Los Alamos National Laboratory used theGame of Lifetoexplorehow slight changes in a network’s initial weights affect its ability to learn. To learn consistently, they found, networks need more parameters than are theoretically necessary.Key insight:Devised by mathematicianJohn Horton Conwayin 1970, the Game of Life starts with a pattern of black (dead) or white (living) squares on a grid. It changes the color of individual squares according to simple rules that reflect the ideas of reproduction and overpopulation as illustrated above in an animation by Emanuele Ascani. Because the outcome is deterministic, a network that learns its rules can predict its progress with 100 percent accuracy. This makes it an ideal environment for testing the lottery ticket hypothesis.How it works:Each step in the game applies the rules to the current grid pattern to produce a new pattern. The authors limited the grid to eight by eight squares and built networks to predict how the pattern would evolve.\n\nResults:The authors chose the models that learned to solve the game and tested their sensitivity to changes in their initial weights. When they flipped the sign of a single weight, about 20 percent of the models that had learned to predict the grid’s pattern one step into the future failed to learn a consistent solution. Only four to six flips were necessary to boost the failure rate above 50 percent. They also tested the oversized models’ probability of finding a solution. Only 4.7 percent of the minimal one-step models solved the problem, compared to 60 percent of networks that were three times bigger.Why it matters:The authors’ results support the lottery ticket hypothesis. Future machine learning engineers may need to build ever larger networks — or find a way to rig the lottery.We’re thinking:When it comes to accuracy, the old maxim holds: The bigger, the better.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Screen20Shot202020-11-0420at2010-2.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Facrec1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2016HEARTBEAT.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2025.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/C320GIF20-20Model20Architecture.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2028.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2016.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-60/",
    "title": "issue 60",
    "date": "",
    "reading_time": "",
    "content": "There’s a lot we don’t know about the future: When will a Covid-19 vaccine be available? Who will win the next election? Or in a business context, how many customers will we have next year?\n\nWith so many changes going on in the world, many people are feeling stressed about the future. I have a practice that helps me regain a sense of control. Faced with uncertainty, I try to:\n\nFor example, back in March, I did this scenario planning exercise. I imagined quick (three months), medium (one year), and slow (two years) recoveries from Covid-19 and made plans for managing each case. These plans have helped me prioritize where I can.\n\nThe same method can apply to personal life, too. If you’re not sure you’ll pass an exam, get a job offer, or be granted a visa — all of which can be stressful — you can write out what you’d do in each of the likely scenarios. Thinking through the possibilities and following through on plans can help you navigate the future effectively no matter what it brings.\n\nBonus: With a training in AI and statistics, you can calculate a probability to each scenario. I’m a fan of theSuperforecastingmethodology, in which the judgements of many experts are synthesized into a probability estimate. I refer to this site as a source of probability estimates as well.\n\nThere will always be uncertainty, but with a little discipline, imagination, and foresight, we can still move forward with confidence.\n\nKeep learning!\n\nAndrew\n\nDocumentary filmmakers often shield the identities of people who might be harmed for speaking out. But typical tactics like blurring faces and distorting voices can make it hard for audiences to connect emotionally. A new documentary uses deepfakes to protect the privacy of at-risk subjects.What’s new:The makers of the HBO documentary “Welcome to Chechnya”deepfaked faces of gay men and women fleeing the Russian republic of Chechnya, where LGBTQ people are being persecuted, theNew York Timesreported.How it works:Visual effects supervisor Ryan Laney developed the process, which he callsCensor Veil, to paint a realistic decoy face over each of the film’s 23 subjects.\n\nWhat they’re saying:“This technology allowed us to just stretch the faces . . . over the images that I shot in the film. The face moves exactly the same way. It smiles, it cries in exactly the same way, but it is somebody else’s face.” — David France, director of “Welcome to Chechnya,” inVariety.\n\nBehind the news:Anestimated40,000 gay men and women live in Chechnya. They are at risk ofarrest,torture, anddetentionin secret camps. Many have beenkilled.Why it matters:This technique provides a new way for journalists to preserve the impact of credible witnesses while protecting their privacy.We’re thinking:Deepfakes are infamous for their potential to propagate mistaken identities. This work (and similar initiatives like theBLM Privacy Bot)demonstrates that swapping one person’s face for another’s can have a socially beneficial use.\n\nAmazon’s digital assistant is using its eyes as well as its ears to figure out who’s talking.What’s new: At its annual hardware showcase, Amazon introduced an Alexaskillthat melds acoustic, linguistic, and visual cues to help the system keep track of individual speakers and topics of conversation. Called natural turn-taking, the skill should be available next year.\n\nHow it works:Natural turn-taking fuses analyses of data from the microphone and camera in devices like the Echo Show, Echo Look, and Echo Spot.\n\nWhy it matters:In conversation, people interrupt, talk over one another, and rarely use each other’s names. Making conversational interactions with AI more fluid could be handy in a wide variety of settings.We’re thinking:Alexa now tolerates users interrupting it. Will users eventually tolerate Alexa interrupting them?\n\nCourses 1 and 2 of our brand-new Generative Adversarial Networks Specialization are available on Coursera!Enroll now\n\nRecognizing actions performed in a video requires understanding each frame and relationships between the frames. Previous research devised a way to analyze individual images efficiently known asActive Shift Layer(ASL). New research extends this technique to the steady march of video frames.What’s new:Led by Linxi Fan and Shyamal Buch, the Stanford Vision and Learning Lab, University of Texas Austin, and Nvidia developedRubiksShift, an efficient replacement for convolutional layers when processing time-series inputs. The name’s similarity toRubik’s Cubeapparently refers to extracting features by shifting three-dimensional data.Key insight:A shift filter is a variation on the convolutional filter that generates only values of 0 or 1. This is more computationally efficient than traditional convolution, which generates real-valued outputs, but it prevents backpropagation, which makes shift filters difficult to train. ASL reformulated backprop for shift filters applied to still images. RubiksShift adapts ASL to video by generalizing it for an additional dimension; in this case, time.How it works:3D convolutional filters typically are used to process images in the three dimensions: red, blue, and green. For videos, a time is added. RubiksShift is a layer of 4D shift convolutions. The researchers also propose an architecture, RubiksNet, composed of multiple RubiksShift layers.\n\nResults:The authors evaluated RubiksNet against state-of-the-art action recognition networks designed for efficient computation, such asI3D, using theSomething-Somethingdataset of clips that represent human actions. RubiksNet achieved top-1 accuracy of 46.5 percent compared to I3D’s 45.8 percent, and it executed 10 times fewer floating point operations during classification. RubiksNet more than doubled the accuracy of other methods that used a similar number of operations.Why it matters:Video is ubiquitous, and we could do a lot more with it — in terms of search, manipulation, generation, and so on — if machines had better ways to understand it.We’re thinking:Hopefully reading this overview of RubiksNet was less confusing than trying to solve a Rubik’s Cube!\n\nNew guidelines for reporting on experiments with medical AI aim to ensure that such research is transparent, rigorous, and reliable.What’s new:Spirit-AIandConsort-AIare complementary protocols designed to improve the quality of clinical trials for AI-based interventions.How it works:The guidelines are intended to address concerns of doctors, regulators, and funders of technologies such as the Google tumor detector shown above.\n\nBehind the news:Less than 1 percent of 20,500 studies of medical AI met benchmarks for quality and transparency, according to a 2019studyby researchers involved in the new initiatives.Why it matters:These protocols could help medical AI products pass peer and regulatory reviews faster, so they can help patients sooner.We’re thinking:The medical community has set high standards for safety and efficacy. Medical AI needs to meet — better yet, exceed — them. But the technology also poses new challenges such as explainability, and a comprehensive set of standards must address issues like that as well.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-10-0620at206.15.3120PM-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2026.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2012.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker208-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker205-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2019.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-291/",
    "title": "issue 291",
    "date": "",
    "reading_time": "",
    "content": "Continuing our discussion on theVoice Stack, I’d like to explore an area that today’s voice-based systems mostly struggle with: Voice Activity Detection (VAD) and the turn-taking paradigm of communication.\n\nWhen communicating with a text-based chatbot, the turns are clear: You write something, then the bot does, then you do, and so on. The success of text-based chatbots with clear turn-taking has influenced the design of voice-based bots, most of which also use the turn-taking paradigm.\n\nA key part of building such a system is a VAD component to detect when the user is talking. This allows our software to take the parts of the audio stream in which the user is saying something and pass that to the model for the user’s turn. It also supports interruption in a limited way, whereby if a user insistently interrupts the AI system while it is talking, eventually the VAD system will realize the user is talking, shut off the AI’s output, and let the user take a turn. This works reasonably well in quiet environments.\n\nHowever, VAD systems today struggle with noisy environments, particularly when the background noise is from other human speech. For example, if you are in a noisy cafe speaking with a voice chatbot, VAD — which is usually trained to detect human speech — tends to be inaccurate at figuring out when you, or someone else, is talking. (In comparison, it works much better if you are in a noisy vehicle, since the background noise is more clearly not human speech.) It might think you are interrupting when it was merely someone in the background speaking, or fail to recognize that you’ve stopped talking. This is why today’s speech applications often struggle in noisy environments.\n\nIntriguingly, last year, Kyutai Labs publishedMoshi, a model (GitHub) that had many technical innovations. An important one was enabling persistent bi-direction audio streams from the user to Moshi and from Moshi to the user.\n\nIf you and I were speaking in person or on the phone, we would constantly be streaming audio to each other (through the air or the phone system), and we’d use social cues to know when to listen and how to politely interrupt if one of us felt the need. Thus, the streams would not need to explicitly model turn-taking. Moshi works like this. It’s listening all the time, and it’s up to the model to decide when to stay silent and when to talk. This means an explicit VAD step is no longer necessary. (Moshi also included other innovations, such as an “inner monologue” that simultaneously generates text alongside the audio to improve the quality of responses as well as audio encoding.)\n\nJust as the architecture of text-only transformers has gone through many evolutions (such as encoder-decoder models, decoder-only models, and reasoning models that generate a lot of “reasoning tokens” before the final output), voice models are going through a lot of architecture explorations. Given the importance of foundation models with voice-in and voice-out capabilities, many large companies right now are investing in developing better voice models. I’m confident we’ll see many more good voice models released this year.\n\nIt feels like the space of potential innovation for voice remains large. Hard technical problems, like the one of latency that I described last week and VAD errors, remain to be solved. As solutions get better, voice-to-voice will continue to be a promising category to build applications in.\n\nKeep building!\n\nAndrew\n\nLearn to build an event-driven AI agent that processes documents and fills out forms using RAG, workflows, and human-in-the-loop feedback. This course, built in partnership with LlamaIndex, walks you through designing, building, and refining automated document workflows.Enroll for free\n\nTypical large language models are autoregressive, predicting the next token, one at a time, from left to right. A new model hones all text tokens at once.\n\nWhat’s new:Inception Labs, a Silicon Valley startup, emerged from stealth mode withMercury Coder, a diffusion model that generates code, in small and mini versions. Registered users can try it outhere, and an API (sign up for early accesshere) and on-premises deployments are in the works. The company has not yet announced availability and pricing.\n\nHow it works:Like image diffusion models, Mercury Coder improves its output over a number of steps by removing noise.\n\nResults:Mercury Coder’s major advantage is speed, but it also performs well compared to several competitors.\n\nBehind the news:Several teams have built diffusion models that generate text, but previous efforts have not been competitive with autoregressive large language models (LLMs). Recently,LLaDAshowed comparable performance to Meta’s Llama 2 7B but fell short of Llama 3 8B and other similarly sized modern LLMs.\n\nWhy it matters:Text diffusion models are already faster than autoregressive models. They offer significant promise to accelerate text generation even further.\n\nWe’re thinking:Diffusion image generators have delivered good output with as little as four or even one step, generating output tokens significantly faster than autoregressive models. If text diffusion models can benefit from improvements in image generation, they could lead to rapid generation of lengthy texts and, in turn, faster agents and reasoning.\n\nOpenAI launched GPT-4.5, which may be its last non-reasoning model.\n\nWhat’s new:GPT-4.5 isavailableas a research preview. Unlike OpenAI’s recent models o1 and o3, GPT-4.5 is not fine-tuned to reason by generating a chain of thought, although the company hinted that it may serve as a basis of a reasoning model in the future. Instead, it’s a huge model that was trained using a huge amount of computation. As OpenAI’s biggest model to date, GPT-4.5 isvery expensiveto run, and the company is evaluating whether to offer it via API in the long term.\n\nHow it works:OpenAI revealed fewdetailsabout how GPT-4.5 was built. The model is bigger than GPT-4o, and it was pretrained and fine-tuned on more data using more computation — possibly 10x more, given OpenAI’s comment that “with every new order of magnitude of compute comes novel capabilities.”\n\nPerformance:“This isn’t a reasoning model and won’t crush benchmarks,” OpenAI CEO Sam Altman warned in atweet. The company claims that GPT-4.5 offers improved general knowledge, adheres to prompts with more nuance, delivers greater creativity, and has higher emotional intelligence.\n\nBehind the news:GPT-4.5’s release comes as OpenAI nears an announcedtransitionaway from developing separate general-knowledge and reasoning models. The launch also comes as OpenAI faces an ongoing shortage of processing power. CEO Sam Altmansaidthat the company is “out of GPUs” and struggling to meet demand — a constraint that may impact whether OpenAI continues to offer GPT-4.5 via API.\n\nWhy it matters:GPT-4.5 highlights a growing divide in AI research over whether to pursue performance gains by scaling up processing during pretraining or inference. Despite the success of approaches that consume extra processing power at inference, such as agentic techniques and reasoning models such as its own o family, OpenAI clearly still sees value in pretraining larger and larger models.\n\nWe’re thinking:There’s still more juice to be squeezed out of bigger models! We’re excited to see what the combination of additional compute applied to both pretraining and inference can achieve.\n\nAnthropic’s Claude 3.7 Sonnet implements a hybrid reasoning approach that lets users decide how much thinking they want the model to do before it renders a response.\n\nWhat’s new:Claude 3.7 Sonnetwas trained for strong performance in coding and front-end web development, with less emphasis on math and computer-science competition problems. It implements tool use and computer use (but not web search) and lets users toggle between immediate responses andextended thinking mode, which can improve outputs by allocating a specific number of tokens to reasoning at inference. Like DeepSeek-R1 and Google Gemini Flash Thinking — and unlike OpenAI o1 — Claude 3.7 Sonnet fully displays reasoning tokens. Anthropic considers this functionality experimental, so it may change.\n\nHow it works:Anthropic pretrained Claude 3.7 Sonnet on a mix of public and proprietary data (which explicitly did not include Claude users’ inputs and outputs). The team fine-tuned Claude 3.7 Sonnet usingconstitutional AI, which encourages a model to follow a set of human-crafted rules.\n\nPerformance:Claude 3.7 Sonnet shows exceptional performance in general knowledge, software engineering, and agentic tasks.\n\nBehind the news:Anthropic’s approach refines earlier efforts to enable users to control the incremental expense of computing extra tokens at inference. For instance, OpenAI o1 offers three levels of reasoning or “effort” — each of which allocates more tokens to reasoning — while X’sGrok 3offers two.\n\nWhy it matters:Test-time compute, or additional processing at inference, is powerful but expensive, and not all tasks benefit from it. So it’s helpful to let users choose how much to apply. Claude 3.7 Sonnet improves its predecessor’s general performance and provides an ample budget for additional reasoning.\n\nWe’re thinking:The cost of inference is rising as agentic workflows and other compute-intensive tasks become more widely used. Yet the cost of AI on a per-token basis isfallingrapidly. Intelligence is becoming steadily cheaper and more plentiful.\n\nAmazon announced Alexa+, a major upgrade to its long-running voice assistant.\n\nWhat’s new:Alexa+, which accepts spoken commands and responds conversationally, is designed to work with a variety of vendors as an autonomous agent to make purchases, book reservations, play media, and so on. It will roll out in the U.S. over coming weeks, initially on some Echo Show devices and eventually nearly every current Echo speaker.\n\nHow it works:Alexa+updatesthe system to take advantage of generative AI including Anthropic Claude,Amazon Nova, and other large language models. Inputs are filtered through a routing system that determines the best model to respond to any given request. It’s trained to understand colloquial, conversational language. Its personality is designed to be “smart, considerate, empathetic, and inclusive” as well as humorous.\n\nBehind the news:Amazon launched Alexa in 2014, and the voice assistant now resides in over 600 million devices worldwide. However, users relied on it more to set timers, report sports scores, and play music than to purchase products, and Alexa revenue lagged. Following cutbacks in 2021, Amazon mademultibillion-dollarinvestmentsin Anthropic and set about updating the technology for the generative AI era.\n\nWhy it matters:Alexa, along with Apple’s Siri and Google Assistant, pioneered the market for voice assistants. However, as large language models (LLMs) blossomed, all three systems fell behind the times. (Google allows Android users to substitute one of its Gemini LLMs for Google Assistant, but the system still calls Google Assistant for some tasks.) Alexa+ is the first major voice-assistant update that aims to take advantage of LLMs as well as emerging agentic technology and improved voice interactions, and the rollout is taking these capabilities to a large, existing user base.\n\nWe’re thinking:Rapid improvements in thevoice stackare opening doors not only for voice assistants but for a galaxy of applications that rely on spoken input and output. Product designers will need to learn how to design smooth user voice experiences. Watching how Alexa+ manages them will provide useful guidelines.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--56-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-03T153624.042.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--57-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--58-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--59-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--52-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-46/",
    "title": "issue 46",
    "date": "",
    "reading_time": "",
    "content": "We know that biased data leads to biased machine learning. But does the problem go beyond that? A few colleagues asked about this after a heated exchange on Twitter between Yann LeCun and Timnit Gebru (see “Image Resolution in Black and White” below).\n\nThere are plenty of documented examples of biased data contributing to bad outcomes. But suppose we find purely unbiased data and build an AI system that helps lenders optimize interest rates for payday loans. We’re careful to make sure the data, algorithms, and learned models don’t discriminate unfairly against any disadvantaged or minority group. Our results are unbiased and in the clear, right?\n\nUnfortunately, no. Payday loans are quick-turnaround loans often with very high interest rates — in California, a lender can charge459 percentinterest on a $100, 14-day loan. They target low income individuals. In the U.S., they’re used disproportionately by the Black community. Thus even a fair algorithm will hurt this community especially.\n\nBeyond biased data, the way we frame problems, choose what to build, and choose where to deploy can add to or subtract from problems of bias and privilege. An “unbiased” AI technology operating in an unfair social system can contribute to biased outcomes.\n\nWe still have a lot of work ahead to address harmful biases throughout society. Twenty years ago, the AI community was a small group working on an exciting but obscure technology. Today our community is large, worldwide, and rapidly growing, and we contribute to applications at the center of daily life. We have a greater responsibility than ever to educate ourselves not only in the technology but also in its social context.\n\nIt’s not always easy to foresee the indirect impact of our work. Who would have guessed that a poorly designed software implementation to enable freedom of speech would lead to toxic communications on social media? But with a broader perspective, I hope our community can better understand the impact of our work and make better decisions about how to help society move forward with greater fairness and less bias.\n\nKeep learning!\n\nAndrew\n\nA new model designed to sharpen images tends to turn some dark faces white, igniting fresh furor over bias in machine learning.What’s new:Built by researchers at Duke University,Photo Upsampling via Latent Space Exploration(Pulse) generates high-resolution versions of low-resolution images. It sparked controversy when ittransformeda pixelated portrait of Barack Obama into a detailed picture of a white man.How it works:Most upsampling models are trained to generate high-res output from low-res input. Pulse creates a series of high-res images progressively optimized to match the low-res source.\n\nThe controversy:Twitter userChicken3ggrevealed Pulse’s bias using a downsampled photo of the former U.S. president. That prompted machine learning engineerRobert Osazuwa Nessto try it on blurred images of U.S. Senator Kamala Harris, actress Lucy Liu, and other nonwhite individuals. The system whitewashed them, too, and also interpreted some female faces as male.\n\nWhy it matters:Flawed AI leads to real harm. In January, police in Detroitarrestedan African-American man for theft after he was misidentified by a face recognition system. Such systems have been shown tomisclassify Black people.\n\nWe’re thinking:Upscaling powered by machine learning is making images sharper ontelevisionsandmicroscopes. The AI community has a pressing need for tests and audit procedures to ensure that such technology is trustworthy and free of bias.\n\nIn online retailing, the most commoncustomer complaintsare slow shipping and inability to try on clothes. Amazon conceived its Prime program to address the first concern. To answer the second, it built a virtual fitting room. (This is one of three recent papers from Amazon that explore AI in online retail. We’ll cover the others in upcoming weeks.)What’s new:Amazon researchers led by Assaf Neuberger developedOutfit-Viton, a model that generates images of a user wearing any combination of apparel. Their work builds on the earlierVirtual Try-On NetworkandCharacteristic Preserving Virtual Try-On Network.Key insight:Previous approaches to generating images of a customer wearing a particular outfit often require hard-to-acquire data — say, 3D scans of the person and the clothes, or photos of the clothes both on and off a wearer. Outfit-Viton takes advantage ofstyle transfer, opening the door to more training data and a more interactive user experience.How it works:Outfit-Viton starts with a photo of the user and photos of clothing items. The network predicts the shape of each clothing item on the user and uses the predicted shape to generate an image of the entire outfit. Then it refines the image to capture greater detail (appearance refinement).\n\nResults:On a 7,000 image test set, Outfit-Viton achieved 20.06 Fréchet Inception Distance, a measure that correlates with human similarity where lower is better.CP-Viton, the state-of-the-art system for the task, achieved 16.63. Human judges preferred Outfit-Viton’s generated images over CP-Viton’s 65 percent of the time.Why it matters:Training CP-Viton requires photos of a garment both on and off a body. Outfit-Viton can learn from either, so it accommodates a more expansive training dataset and a wider variety of use cases.We’re thinking:Stores must spur sales even as they enact social distancing measures. A neural network makes a very socially distant dressing room.\n\nA chatbot is providing companionship for the locked-down and lonely.What’s new:Downloads of Replika, a chatbot designed to be a virtual friend, have spiked during the coronavirus pandemic, reports theNew York Times.The service reported the biggest surge in its three-year history in April.Besties:Users select the bot’s name, gender, and avatar and chat via text or voice. It asks about their day, listens to their problems, and offers advice and affirmations. Users said the app soothes their anxieties and provides an outlet for their frustrations.\n\nBehind the news:Replika grew out of an earlier effort by its creator, Eugenia Kuyda, tosimulate conversationswith a friend who had died in an automobile accident. That version was trained on several years’ worth of the friend’s texts.Why it matters:The spread of Covid-19 has left countless people lonely and isolated. Chatbots offer a cost-effective, albeit imperfect, outlet for social interactions.We’re thinking:AI probably will never make an entirely satisfying substitute for human relationships (even the therapeutic chatbot Woebot, for which Andrew Ng serves as a board member). In a socially distanced world, though, it can be an important surrogate.\n\nCourses 1 and 2 of our brand-new Natural Language Processing Specialization are now available on Coursera!Enroll today\n\nSome tech companies boast that their AI will change the world. Apple’s latest just aims to make your life a little easier.What’s new:Apple unveiled a flock of modest conveniences powered by machine learning at its annual developer conference. The company barely mentioned AI, sidestepping the tech industry’s tendency to hype such products, notedThe Verge.All modern conveniences:The company previewed dozens of software updates to its operating systems over the course of alengthy presentation. Many apply machine learning not as a panacea, but to save users a little trouble here and there.\n\nWhy it matters:Consumers are rightly skeptical of potentially problematic AI-driven capabilities like face recognition and social networks. Apple’s latest moves suggest that the technology has plenty of room to run in the form of humble features that simply save a little time and effort.We’re thinking:We need visionaries who aim to transform the world, but there’s something to be said for focusing on what’s practical today. As Peter Thiel once observed, 140 characters are changing the today’s world more than flying cars.\n\nScientists who study animal behavior spend endless hours observing and taking notes about a creature’s actions and reactions. Computer vision could automate much of that work.What’s new:Researchers at Facebook, the Max Planck Institute for Evolutionary Anthropology, and the Pan-African Programme (part of a partnership between African and European governments) built aneural network that tracks the body position of chimpanzees. The system captures the animals’ behavior in three dimensions for study and analysis.How it works:The researchers started withDensePose, a pose estimator pre-trained on videos of humans. They fine-tuned it for chimps in two phases, first using a segmentation model and then using a teacher-student scheme.\n\nBehind the news:This work complements earlier efforts to use deep learning to help scientists study animal behavior.\n\nWhy it matters:Annotating videos of animal behavior is labor-intensive, and building annotated datasets for thousands of species would be prohibitively expensive. The authors adapted a neural network’s knowledge of human anatomy to work with another species, albeit a similar one. They believe their method could work with less human-like species as well.We’re thinking:What a brilliant ape-lication!",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter203202.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Obama203.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Try20On.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Replika203.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/3.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Convenience1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize2012.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-35/",
    "title": "issue 35",
    "date": "",
    "reading_time": "",
    "content": "This week’s issue of The Batch is all about medical applications of AI.\n\nAmid the current pandemic, the marriage of AI and medicine is more urgent than ever. My father is a practicing doctor, and I grew up seeing firsthand how the right care can save lives and reunite families. I’ve been privileged to participate in projects that applied deep learning to diagnosing chest X-rays, assisting with mental health, and interpreting electrocardiograms.\n\nDespite significant research progress, there’s still a long way to go. Jumping into AI for medicine now is like jumping into AI for computer vision back in 2012.\n\nFor those who are ready to make the leap, deeplearning.ai is proud to introduce theAI for Medicine Specialization. This new series of courses will teach you the machine learning techniques you need to build a wide range of medical applications.\n\nIf you’re new to deep learning, start with theDeep Learning Specialization. But if you’ve completed the DLS, or if you have a working knowledge of deep learning and convolutional networks as well as intermediate Python skills, the AI For Medicine Specialization will unlock many opportunities to help solve important problems.\n\nThe world needs more AI people working on medicine. I hope you’ll consider being one of them.\n\nKeep learning!\n\nAndrew\n\nWe stand at the threshold of a new era in medicine. We can collect detailed data about individuals continuously throughout their lives. With deep learning, we can correlate background, actions, and outcomes to find paths to optimal health. Some day we may do this globally, so everyone on Earth receives health care appropriately tailored to their unique biology and circumstances. In this special issue ofThe Batch, we look at how AI is having an impact in medical diagnosis, prognosis, treatment, and data extraction. We hope you’ll join the medical AI revolution and help create a healthier world.\n\nEric Topol is one of the world’s leading advocates for AI in medicine. He believes the technology can not only liberate physicians from the growing burden of clerical work, but also synthesize layers of patient data — behavioral, genomic, microbiomic, and so on — into truly personalized healthcare. A cardiologist and geneticist at Scripps Research Institute in Southern California, he is the author ofDeep Medicine: How Artificial Intelligence Can Make Healthcare Human Again. Below he shares his insights into the fusion of AI and medicine and advice for machine learning engineers who want to get involved.The Batch:Let’s start with the topic on everyone’s mind: Where do you see AI’s greatest potential in addressing the Covid-19 pandemic?Topol:One thing that’s been overlooked is the ability to develop and validate algorithms for at-home monitoring. We don’t want everyone who has Covid-19 symptoms to go to the hospital. On the other hand, some people who catch Covid-19 have sudden demise, and it’s hard to predict. If we could tell who’s safe to monitor at home, that would be great help in managing this epidemic around the world.The Batch:You’re concerned with the depersonalization of doctor-patient relationships. How can AI help?Topol:Four words: the gift of time. Clinicians spend too much of their time being data clerks. There shouldn’t be any need for a screen and a keyboard to see a patient. Entering notes into the medical record should be done by AI.The Batch:Researchers have had experimental success interpreting medical images. Yet these innovations haven’t had much impact on clinical practice. What’s the holdup?Topol:The medical community feels threatened that the machines will encroach on their lives. Also, some companies working on things like this have proprietary algorithms and don’t publish their data, so there’s a lack of transparency. They get their FDA clearance based on retrospective studies and use the same data over and over, because there aren’t many large, annotated medical datasets. We need prospective studies based on real-world patients in multiple real-world clinical settings. And we need more randomized trials — there have been only six or seven of those.The Batch:If you could collect any data you wanted for everyone in the world, what would it be, and for what AI task?Topol:That’s easy: We need a planetary health system. We’d have multilevel data for every person, and each person would teach the rest of their species about preventing and managing illnesses using nearest neighbor analysis and other tools of AI. It’s possible now, but it requires an international commitment. I wrote about this with my colleague Kai-Fu Lee in an article called “It Takes a Planet.”The Batch:How can we build a planetary health system that protects data privacy and security?Topol:The tools are in front of us now. We can use federated and homomorphic computing. No country has to hand their data over. The algorithms can be used at the locale.The Batch:Much of the AI community is deeply concerned about making sure the technology is used ethically. What should AI practitioners keep in mind in that regard?Topol:Anything that exacerbates the very significant health inequalities that exist today is not acceptable. Human bias that finds its way into algorithms is a significant ethical concern that needs extensive review and scrutiny. And that’s not all. Algorithms in medicine need to be under constant surveillance because if an algorithm is hacked, it could hurt a lot of people.The Batch:What advice would you give machine learning engineers who want to make a positive impact in medicine?Topol:We’re still in the early phase. We need more interdisciplinary or transdisciplinary efforts between clinicians and AI practitioners. We need more large, annotated datasets, or to use self-supervised learning that preempts the need for them. We need to go to a higher validation plane, however we get there. Then we’ll be able to take advantage of this extraordinary opportunity to transform medicine and return the human essence that has been largely lost.\n\nThe wearable revolution is helping doctors figure out what’s troubling your ticker — thanks to deep learning.The problem:Arrhythmias, a range of conditions in which the heart beats too fast, too slow, or erratically, can causeheart attack or stroke. But they don’t necessarily happen when a doctor is listening.The solution:Wearable devices fromiRhythmconstantly monitor a patient’s heartbeat and transmit the measurements to a neural network for analysis.How it works:TheiRhythm Zio ATis an electrocardiogram monitor about the size of a breath-mint box with two wings of peel-and-stick medical tape that fasten onto the skin over a patient’s heart. Electrodes in the monitor track each heartbeat while a separate wireless transmitter sends the data to iRhythm.\n\nStatus:The United States Food and Drug AdministrationapprovediRhythm’s Zio AT in 2018, and the system is on the market. The company recently partnered withVerilyandAppleto develop further products.Behind the news:A 2019reviewof 14 studies that compared AI with human clinicians found that deep learning models were roughly as good as human professionals at diagnosing signs of disease in medical imagery. The authors noted, however, that the studies tend to suffer from poor controls, inconsistent metrics for measuring success, and lack of independent validation. No comparable assessment of non-image AI diagnostics exists, but the fact that Apple isintegrating arrhythmia detectioninto its smartwatch suggests that the field is maturing.Why it matters:Arrhythmias occur sporadically enough that spotting them requires many days of data. “You’ll never catch one by running an electrocardiogram in the office,” according toDr. Mauricio Arrudaof Cleveland’s University Hospitals Harrington Heart & Vascular Institute. By combining long-term observations with short-turnaround assessment, AI enables cardiologists to intervene with precise, timely, and potentially life-saving treatments.We’re thinking: Just the thought of AI saving somebody from a stroke makes our hearts skip a beat.\n\nTo learn how you can use AI to diagnose illnesses, check out Course 1 of theAI for Medicine Specializationfrom deeplearning.ai.\n\nAn AI-driven alarm system helps rescue patients before infections become fatal.The problem:Machine learning can spot patterns in electronic health data indicating where a patient’s condition is headed that may be too subtle for doctors and nurses to catch.Sepsis, for instance, is a response to infection that inflames a patient’s organs, killing some 270,000 Americans each year. The ability to catch it early can save lives.The solution:Sepsis Watchis a deep learning model that spots signs of sepsis up to five hours before it becomes dangerous. This crucial window allows clinicians to intervene.How it works:The system integrates vital signs, test results, and medical histories of emergency-room patients, assessing their risk of septic shock on a scale of 0 to 100 percent. If the risk reaches 60 percent, the system alerts nurses in the hospital’srapid response team. It also publishes an hourly list of each patient’s septic risk score.\n\nStatus:Duke physician and data scientistMark Sendakand colleagues conducted a clinical trial between November 2018 and July 2019. Sepsis Watch significantly improved sepsis response times, Sendak toldThe Batch. The team plans to publish the results in the near future. Last July, Duke University licensed the software toCohere Med, an AI healthcare startup.Behind the news:Suchi Saria, a machine learning expert at Johns Hopkins University, was a pioneer in the use of reinforcement learning toidentify sepsistreatment strategies back in 2018. Duke’s Sendak helped evaluate models for other kinds of clinical decision support in a recent survey in theEuropean Medical Journal. The authors’ picks included early-warning systems for cardiac arrest, surgical complications, pneumonia, and kidney disease.Why it matters:As little as three hours of warning can give caregivers time to begin tests and medications that dramatically improve a sepsis victim’s odds of survival.We’re thinking:Building a great model is Step 1. Deployment is Step 2. Collaborating with hospital staff is a sharp way to promote Step 3, utilization.\n\nLearn how to build your own prognostic models in Course 2 of theAI For Medicine Specialization.\n\nInterested in learning more about AI applications in medicine? Build your own diagnostic and prognostic models in our new AI for Medicine Specialization.Enroll now\n\nWill deep learning discover new medicines? Startups — and big-pharma partners — are betting on it.The problem:In theory, there’s a pharmacological cure for just about any ailment. In practice, discovering those therapies takes years and billions of dollars.The solution:Deep learning, with its ability to discern patterns amid noise, could speed up drug discovery considerably. In a dramatic test,Insilicoused an algorithm to sift through petabytes of biochemical data to find potential drugs in 21 days.How it works:Based in Rockville, Maryland, Insilico used itsGenerative Tensorial Reinforcement Learning, or GENTRL, to create digital representations of molecules with properties that inhibit anenzymelinked to several types of cancer, atherosclerosis, and fibrosis.\n\nStatus:Insilico’s enzyme inhibitor was only a proof of concept. However, it attracted partnerships withGlaxoSmithKline,Jiangsu Chia Tai Fenghai Pharmaceutical, andPfizer.Behind the news:Drug discovery is an attractive target for AI startups, given the abundance of biochemical data and desperation of pharmaceutical giants to cut costs. But success still seems hit-or-miss. Only one AI-designed drug — made byExscientia— has progressed tohuman trials.Verseonhas been working on the problem for nearly two decades without creating a marketable product. And, crucially, no one has found a reliable way to accelerate clinical trials, the mostexpensive and time-consumingpart of drug development.Why it matters:The average successful drug costs$2.5 billion dollarsto bring to market, according to a 2016 study. Cutting even a fraction of that cost could allow companies to channel resources towards more and different drugs, potentially providing the public with more cures in less time.We’re thinking:Finding a molecule that becomes a viable drug is like hunting for a single, specific plankton in the Pacific Ocean. Good thing machine learning engineers relish searching for tiny patterns in massive pools of data.\n\nUse deep learning to estimate treatment effects for individual patients in Course 3 of ourAI for Medicine Specialization.\n\nDoctors are overwhelmed by clerical work. Healthcare-savvy voice assistants are picking up the slack.The problem:Doctors generate lots of vital information while examining a patient. Properly recorded, it becomes data that informs treatment — but entering it properly is a time-consuming task that drains docs’ attention and finances.The solution:Voice assistants can serve as clinical stenographers.Sukiis one of several apps on the market that transcribe doctors’ observations and instructions and insert them into a patient’s electronic health record.How it works:Saying “Suki, the patient is running a fever and has fluid in their lungs,” inserts a note in the patient’s record. “Suki, show me the patient’s prescriptions,” retrieves that information. “Suki, I examined the patient,” enters the full description of a normal exam, ready for customization to the particular case. The model also adds diagnostic codes for tests and procedures, which aid in billing.\n\nStatus:Suki, which integrates with several popular electronic health records, is deployed in the health networkAscension,Unified Women’s Health Care, and more than 90 small-to-midsize practices. As of July, the software operated inseven specialtiesincluding internal medicine, OB-GYN, and pediatrics. The company is working on new features for smarter billing ordering items like prescriptions and tests.Behind the news:Suki has plenty of competition. Rivals includeSaykara,Nuance,M*Modal, andNotable.Why it matters:Doctors are drowning in paperwork, and voice-assistant technology can help them come up for air. A2016 studyestimates that doctors spend between 37 and 49 percent of their working hours on clerical tasks. All that paperwork contributes to the high level of burnout and depression in the profession, according to a2019 study.We’re thinking:If you notice an improvement in your physician’s bedside manner, you might want to thank a robot.\n\nBuild a natural language tool to extract data from medical information in Course 3 of theAI for Medicine Specializationfrom deeplearning.ai.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/5-Andrews-letter_3201.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Asset2.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/7-interview-with-Eric-Topol201.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1-diagnosis.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/2-prognosis.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AI4MC1BatchAd.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/3-treatment_2.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/4-knowledge-extraction201.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-34/",
    "title": "issue 34",
    "date": "",
    "reading_time": "",
    "content": "Last week, I asked readers to tell me what they’re doing to address the Covid-19 pandemic. Many of you wrote to say you’re taking actions such as shopping for neighbors, making masks, and creating posters that promote Covid-safe practices (see the campaign by Luter Filho, a creative director and designer in Berlin, below).\n\nSeveral members of thedeeplearning.aicommunity are rising to meet the challenges of Covid-19 by building AI and other software projects:\n\nIt’s exciting to see thedeeplearning.aicommunity helping to keep families, neighborhoods, and towns healthy. Your efforts are an inspiration as I develop my own projects to keep the virus at bay and help everyone heal and rebuild. In the future, we will look back on these days with sadness, but also with pride that our community’s creativity and ingenuity can have a positive impact on a global scale.\n\nStay safe and keep learning!\n\nAndrew\n\nThe data science community is providing tools and datasets to help fight the pandemic. Below you’ll find the most valuable resources we’ve come across in the past week. If you want to recommend relevant resources, please let us know at[email protected].\n\nAI experts convened to discuss how to combat the coronavirus crisis.What’s new:An online conference hosted by Stanford University’s Institute for Human-Centered AI explored how machine learning is being deployed to address this pandemic — and prepare for the next one. You can watch the videohere.The agenda:Nearly two dozenexpertsin fields as diverse as machine learning and public health delivered updates on topics from containing the pandemic to finding treatments. A few notable presentations from the six-hour conference:\n\nBehind the news:Infectious disease expert Dr. Michele Barryexplainedthat machine learning was critical to keeping infection rates low in Singapore, South Korea, and Taiwan. All three countries deployed the technology to encourage social distancing, move medical supplies where they were needed most, and keep the public informed.Why it matters:Machine learning engineers and disease specialists have a lot to learn from one another. Conferences like this can bring them together and build lasting alliances that may result in tools for fighting future outbreaks.We’re thinking:If you’re itching to join the fight against Covid-19, you can find a list of tools, datasets, and information just above the news in this issue ofThe Batch. Also: Stay at home and wash your hands!\n\nFaced with a classification task, an important step is to browse the catalog of machine learning architectures to find a good performer. Researchers are exploring ways to do it automatically.What’s new:Esteban Real, Chen Liang, and their colleagues at Google Brain developedAutoML-Zero, an evolutionary meta-algorithm that generates a wide variety of machine learning algorithms to classify data. Applied to the small CIFAR-10 image dataset, it discovered several common deep learning techniques.Key insight:Past meta-algorithms for machine learning constrain their output to particular architectures. Neural architecture search, for instance, finds only neural networks. AutoML-Zero finds any algorithm that can learn using high school-level math.How it works:The researchers used AutoML-Zero to generate models for various resolutions of CIFAR-10.\n\nResults:AutoML-Zero regularly generated models that achieved 84 percent accuracy on CIFAR-10, compared to only 82 percent achieved by a two-layer, fully connected network. In the process, it rediscovered gradient descent, ReLu activations, gradient normalization, and hyperparameters.Why it matters:The researchers estimate that, given AutoML-Zero’s wide-ranging purview, the chance of coming up with a model suitable for a CIFAR-10 classification task is vanishingly small (around 1 in 107 for linear regression, and 1012 if that line is offset by a constant). Yet it did so frequently — a demonstration of the meta-algorithm’s power to come up with useful architectures. If AutoML-Zero can find nearly state-of-the-art models on such a complex task, it may well be able to discover techniques that humans haven’t yet devised.We’re thinking:CIFAR-10 was developed over a decade ago for machine learning experiments on the CPU-based neural networks of the day. We’re curious to learn how AutoML-Zero scales to larger datasets.\n\nWe’re not thinking:Today we have learning algorithms that design other learning algorithms. When will we have learning algorithms that design learning algorithms that design learning algorithms?\n\nSeeking a robot star for your movie, music video, or bat mitzvah? You need a new breed of talent scout.What’s new:Ai-gen-cy is an artist management firm that exclusively represents robots, reportsGeekWire. Thisvideoexplains.How it works:Seattle-based entrepreneurs Forest Gibson and Jared Cheshier believe that movie directors and event producers want to feature automatons but are put off by their diva-like needs: programmers, on-site technicians, and handle-with-care transportation. Ai-gen-cy aims to supply mechanical talent along with logistical and technical support.\n\nBehind the news:Gibson and Cheshier gained experience presenting robots on-screen in 2012, when they collaborated on amusic videothat set simulated footage of Nasa’s Curiosity rover to an electronic beat.Why it matters:Anything that puts more robots in movies is fine by us.We’re thinking:Tabloid coverage of robot superstars’ off-screen escapades should be interesting.\n\nTest your image classification models with your phone’s camera! Learn how to deploy models with TensorFlow Lite in Course 2 of the TensorFlow: Data and Deployment Specialization.Enroll now\n\nAn unconventional approach to modifying data is cutting the amount of work required to train self-driving cars.What’s new:Waymo unveiled a machine learningpipelinethat varies lidar scans representing pedestrians and other objects. The company’s self-driving systems, in turn, learn from the augmented data, effectively giving the researchers a larger training dataset without collecting and labelling additional point clouds.How it works:Waymo’s engineers started with two-dimensional data augmentation methods used to train object recognition models: flipping, rotating, color-shifting, and partially obscuring still images. Since lidar output generates three-dimensional points, Waymo developed ways to perform similar transformations while maintaining the geometrical relationships between points.\n\nResults:CalledProgressive Population-Based Augmentation, the method improved object detection with both small and large datasets. It increased data efficiency between 3.3- and 10-fold compared with training on unaugmented datasets.Behind the news:Released in March, Waymo’s latest self-drivingplatformis based on anelectric Jaguar SUV. Its sensor suite includes a 360-degree lidar on top, a95-degree lidarnear each head and tail light, plus cameras and radars.Why it matters:The new system breaks ground in 3D augmentation. Waymo is effectively giving its system more examples of obstacles while saving the time, expense, and labor it would take to collect real-world samples.We’re thinking:As long as social distancing keeps real drivers and pedestrians off the street, self-driving cars will need all the augmented data they can get.\n\nBatch normalization is a technique that normalizes layer outputs to accelerate neural network training. But new research shows that it has other effects that may be more important.What’s new:Jonathan Frankle and colleagues at MIT, CUNY, and Facebook AIshowedthat batch normalization’s trainable parameters alone can account for much of a network’s accuracy.Key insight:As it adjusts a network’s intermediate feature representations for a given minibatch, batch normalization itself learns how to do so in a consistent way for all minibatches. The researchers probed the impact of this learning by training only the batch normalization parameters, gamma (????) and beta (β), while setting all other parameters to random values.How it works:The researchers trained variously sized ResNet and Wide ResNet models, which include batch normalization layers, on the CIFAR-10 image dataset.\n\nResults:Training all the parameters in a ResNet-866 yielded 93 percent accuracy, while training only ???? and β brought 83 percent accuracy. This finding isfurtherevidence that networks can be accurate even with a large number of random weights.Why it matters:Batch normalization is often standard procedure in deep learning, but previous studies failed to recognize the power of its trainable parameters. And batchnorm isn’t the only normalization method that scales and shifts parameter values; so doweight normalizationandswitchable normalization. Further research may illuminate the impact of these normalizations on network performance.We’re thinking:Why batch normalization workshas been a subject of heated debate since Sergey Ioffe and Christian Szegedyintroducedit in 2015. The authors’ original explanation of “reducing internal covariate shift” sounded a bit like black magic. This work sheds light on the mystery.\n\nAre your scientist friends intimidated by machine learning? They might be inspired by a primer from one of the world’s premier tech titans.What’s new:Former Google CEO Eric Schmidt and Cornell PhD candidate Maithra Raghu school scientists in machine learning in a sprawlingoverview.Scientific Revolution 2.0:Science producesmountains of data, and machine learning can help make sense of it. Schmidt and Raghu offer a brisk tour of architectures and techniques, explaining how neural networks have served disciplines from astronomy to radiography.\n\nBehind the news:Maithra Raghu isn’t as famous as her co-author, but her star is on the rise. Named amongForbes’ “30 Under 30” last year, she focuses on improving human-machine collaboration.\n\nWhy it matters:The range of mysteries that machine learning can help solve is limited by the number of scientists who are proficient in machine learning.We’re thinking:We’d like to see more CEOs publish technical papers on arXiv.org!",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter20ASPECT20REPLACEMENT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Resources20ASPECT20REPLACEMENT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Conf20SLOW.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AutoML-Zero.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Talent.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Waymo.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/BatchNorm.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Schmidt20ASPECT20REPLACEMENT.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-192/",
    "title": "issue 192",
    "date": "",
    "reading_time": "",
    "content": "An ill-advisedproposalfor a 6-month pause in cutting-edge AI research got far more attention than I think it deserved. To me, this is a wake-up call that AI doomsayers have done a much better job than AI optimists at framing the narrative of progress in AI.Most of the AI community is building systems that help and empower people, and we see every day how it is improving lives. Open AI’s ChatGPT is delivering value to hundreds of millions of users, and reportedly it’s thefastest-growing consumer applicationto date. This is wildly exciting, and I foresee many more products yet to be built that will help and empower people in other ways.Yet, while most of us have been building useful systems, AI doomsayers — who forecast unlikely scenarios such as humanity losing control of runaway AI (or AGI, or even superintelligent systems) — have captured the popular imagination and stoked widespread fear.Last week, Yann LeCun and I had an online conversation about why the proposed 6-month pause, which would temporarily suspend work on models more powerful than GPT-4, is a bad idea. You can watch the videohereand read a synopsis in thisarticle. Briefly:\n\nTo be clear, AI has problems including bias, fairness, job displacement, and concentration of power. Our community should work, and is working, to address them. However, stoking fears about speculative risks does more harm than good:\n\nI’m disappointed that we have let AI doomsayers get this far. Their narrative hampers innovation, discourages individuals, and interferes with society’s ability to make good decisions.\n\nLet’s help people understand that AI is empowering people even as we work to mitigate the real risks. It’s time for us all to stand up for a realistic view of this incredibly important technology.Keep learning!\n\nAndrew\n\nP.S. Shoutout to University of Washington’s Emily Bender for her line-by-lineanalysisof how the proposal contributes to AI hype, and Princeton professor Arvind Narayanan, whoexplainedhow fears of AI-driven dangers such as misinformation often have been overblown.\n\nChatbot-fueled FOMO is overwhelming cloud-computing services.\n\nWhat’s new:Cloud providers are struggling to meet sharply rising demand by a crowd of AI startups eager to cash in on generative AI,The Informationreported.Behind the bottleneck:The surge in demand caught Amazon Web Services, Microsoft Azure, and others off guard.\n\nWhat they’re saying:Engineers and entrepreneurs shared their pain.\n\nBehind the news:China is facing its own chip shortage — andfindingways to address it. That situation, though, is a result of United States trade sanctions rather than a surge in demand.\n\nWhy it matters:Startups that serve a market with generated text or pictures are white-hot, but even the most promising ventures can’t do without servers to build, test, and deploy their models. The winners will need not only a great product but also ready access to computation.We’re thinking:Our hearts go out to everyone who is trying to build AI products in these unpredictable times. We trust that the supply of compute will catch up in due course and that the current run of AI-fueled growth will continue for the foreseeable future.\n\nItaly blocked ChatGPT after determining that it violates European Union laws.\n\nWhat’s new:The Guarantor for the Protection of Personal Datasuspendedaccess to ChatGPT for 20 days after saying that OpenAI enables underage children to use the chatbot, distributes misinformation about people, and collects personal data used to train its models without proper authority.The ruling:The Guarantor, which enforces the rules in Italy, banned ChatGPT for 20 days citing four concerns: The chatbot doesn’t prevent children under 13 from using it, the chatbot can provide inaccurate information about individuals, OpenAI did not inform individuals that the firm was collecting data that could be used to identify them, and OpenAI did not meet the EU privacy law’s guidelines for collecting personal data.\n\nBehind the news:Privacy regulators in Europe and the United States have their eyes on AI.\n\nYes, but:Not everyone in the Italian government agrees with the ruling. Matteo Salvini, one of the country’s two deputy prime ministers,criticizedit as excessive.Why it matters:A national (or international) ban on ChatGPT could have major implications for large language models, which rely on sprawling datasets and routinely output misinformation. It could also harm European innovation by blocking access to the latest technology. And it’s not just Italy: French, German, and Irish regulatorsreportedlyare considering similar actions. Belgian regulators went a step further andcalledfor an EU-wide discussion of data violations related to ChatGPT.We’re thinking:Some of the regulators’ concerns may stem from a lack of transparency into how OpenAI trains its models. A more open approach might alleviate some fears.\n\nThe core of many successful companies is a strong co-founder partnership. Join us on April 13, 2023, at 2:00 p.m. Pacific Time to learn how to find your perfect co-founder.Register now\n\nStanford’s sixth annualAI Indextakes stock of a rapidly growing field.What’s new:The sprawling, 386-pagereportfrom the Institute for Human-Centered AI presents the past year’s developments in AI based on a wide variety of sources including benchmarks, papers, market research, job listings, and polls. (You can find info about earlier editionshere.)Reasons for celebration:The report highlights several positive trends:\n\nCauses for concern:Not everything in the report is rosy:\n\nBehind the news:The new report surveyed the AI’s recent past, but other measures indicate the near-term future. An April study by investment bank Goldman Sachsfoundthat generative AI could boost the global productivity by 7 percent in the coming decade as it automates tasks that affect 300 million full-time jobs. Meanwhile, at the startup incubator Y Combinator, AI is at the heart of 34 percent of newly formed companies — the highest number on record.\n\nWhy it matters:TheAI Indexoffers a sober yet exciting summary of AI’s march into all areas of society. Immense opportunities and grave challenges alike lie ahead.\n\nWe’re thinking:Focusing on 2022, this report doesn’t reflect the staggering impact of generative AI — a reminder of the extraordinary pace of development as well as AI’s potential in areas well beyond the current buzz.\n\nHow can you tell when you’re reading machine-generated text? Three recent papers proposed solutions: Watermarking, classification, and a statistical method.\n\nWatermark:John Kirchenbauer, Jonas Geiping, and colleagues at University of Maryland applied a digitalwatermark, invisible to humans but detectable by an algorithm, to generated text. Their method adjusted the way in which the model chose which word would come next.\n\nClassifier:Sandra Mitrovic, Davide Andreoletti, and Omran Ayoub at University of Southern Switzerland and University of Applied Sciences and Arts of Southern Switzerland trained a model toclassifytext generated by ChatGPT.\n\nLikelihood of generation:Eric Mitchell and colleagues at Stanford University developedDetectGPT, a method that detects generated text by relying on statistical differences between rewordings of machine-generated text and rewordings of human written text — no training data required.\n\nWe’re thinking:Independentreportingon technology designed to detect generated text finds that it frequently delivers false positives, which can lead to unfair accusations of cheating, as well as false negatives. Watermarking can work from a technical perspective, but competitive pressure is likely to disincentivize AI providers to offer it. So, for now, at least, it seems as though we will have to adapt to the inability to distinguish between human- and machine-generated text.\n\nResearch:Google released new details about its AI supercomputersGoogle described the design and performance of its AI supercomputers used in over 90 percent of the company's AI training. The company claims that its supercomputers, which include up to 4,000 Tensor Processing Unit (TPU) chips, are up to 1.7 times faster and 1.9 times more power-efficient than systems based on Nvidia A100 chips. (Reuters)\n\nNew York prepares for the first AI Fashion WeekAI Fashion Week is set to launch later this month. The week will showcase collections from emerging designers. The event aims to promote AI as a tool for fashion design and support designers who work with this technology. (Vogue Business)\n\nAI triage tool could speed up treatment for shrapnel injuries in UkraineUkrainian scientists are using AI to analyze CT scans of shrapnel wounds. The team is using artificial mock-ups of wounds to train models to determine the material type, location, and urgency of removal. (New Scientist)\n\nThe success of large language models unsettles researchers in other areas of natural language processingModels such as GPT-4 dominate so many subspecialties in natural language processing that researchers are concerned about the value of their work. Lack of public information about the precise nature of GPT-4’s technology intensifies the discomfort. (IEEE)\n\nResearch:Bloomberg launched a language model designed for financeThe data and media company described a large-scale generative AI model trained on financial language. The model, trained on a 700-billion token dataset, outperformed similar open models while maintaining competitive performance on general-purpose benchmarks. (Bloomberg)Google to enhance its search with conversational AIGoogle CEO Sundar Pichai announced plans to add conversational AI features to the search engine, which could change the traditional link-based experience. Google is testing several new search products that let users interact with its large language models. (The Wall Street Journal)\n\nBing chatbot gears up to show more adsMicrosoft announced that it will be introducing more ads to its Bing chatbot. While details are vague, the ads are set to appear within the chat experience and will be similar to those featured in traditional search results. Microsoft will share ad revenue with content owners. (The Verge)\n\nMicrosoft and Google take risks in the generative AI raceDespite internal concerns that their language models might generate falsehoods and exhibit other undesirable behavior, both tech giants are taking greater risks to build and release chatbots. (The New York Times)Google provides coding tools from AI startup ReplitDevelopers on Google cloud will have access to Replit's suite of AI features for writing code, called Ghostwriter, while users of Replit will have access to Google’s cloud-computing infrastructure. The partnership aims to compete with Microsoft's GitHub and OpenAI by suggesting code blocks, completing programs, and answering developer questions. (Bloomberg)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/04/Screen-Shot-2023-04-12-at-11.45.11-AM-1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/04/SERVER-2b_1200px--1-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/04/ITALY--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/04/The-Batch-ads-and-exclusive-banners--13-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/04/AIINDEX_BG2_1200px--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/04/ezgif.com-optimize--11--1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-184/",
    "title": "issue 184",
    "date": "",
    "reading_time": "",
    "content": "AI has an Instagram problem. Just as Instagram’s parade of perfect physiques makes many peoplefeelthey don’t measure up, AI’s parade of exciting projects makes many people feel their own projects are lacking. Just as pictures of people’s perfect lives in the media aren’t representative, pictures of AI developers’ postings of their amazing projects also aren’t representative.\n\nI’m here to say: Judge your projects according to your own standard, and don’t let the shiny objects make you doubt the worth of your work!\n\nOver the years, I’ve occasionally felt this way, too, and wondered if I was working on a fruitful direction. A few years ago, when reinforcement learning (RL) made progress on Atari games, Alpha Go was in the headlines, and RL videos using OpenAI Gym circulated on social media, I was still focused on supervised learning. Part of me wondered if I was missing out. It certainly did not help when friends kept asking me about the cool RL work they read about in the news. Fortunately, I ignored the feeling that the grass might be greener on the other side and stuck to what I was excited about.\n\nAI develops so quickly that waves of new ideas keep coming: quantum AI, self-supervised learning, transformers, diffusion models, large language models, and on and on. Some, like quantum AI, have had essentially no impact in applications so far. Others have already had a huge impact. Because our field evolves, it is important to keep learning and ride the waves of change. For the record, I think large language models (LLMs) like ChatGPT (and, to a significant but lesser extent, diffusion models, best known for generating images) will have a transformative impact on AI, but they are far from the only things that will be important.\n\nSomeone else’s sculpted physique does not take away from your beauty. And the emergence of a hot new technology doesn’t mean your current project isn’t also valuable, assuming it’s technically sound, has a reasonable expectation of impact, and isn’t made obsolete by newer technology (which doesn’t happen very often). Projects of all shapes and sizes can be wonderful, and what’s buzzy today is only one of the many things that will prove valuable in the future.\n\nI'm not advising you to ignore the news. Paying attention to new developments in AI not only helps you stay on top of the field but also can inspire you. Being inspired by Instagram is fine, but changing your life because of FOMO is less helpful.\n\nSo, if what you’re working on makes sense to you, maintain your faith and keep going! Maybe you’re training XGBoost on a structured dataset and wondering if you’re missing out on ChatGPT. You may well be onto something even if XGBoost isn’t in the news.\n\nAfter all, think about how all the LLM researchers must have felt a few years ago, when everyone was buzzing about RL.Keep learning!\n\nAndrew\n\nThelong-dormantstruggle to dominate the web-search business reignited in a display of AI-driven firepower — and hubris.\n\nWhat’s new:Google and Microsoft announcedcompetingupgradespowered by the latest generation of chatbots. Baidu, too, flexed its natural-language-processing muscles.\n\nGoogle’s gambit:Following up on its January “code-red”initiativeto counter arumoredthreat from Microsoft, Google teased unspecified revisions of Search, Lens, and Maps. Google Search is the undisputed leader, responsible for93 percentof all search-driven traffic according to StatCounter.\n\nMicrosoft’s move:Microsoft followed up its announcement bypreviewingan upcoming version of its Bing search engine enhanced by text generation from OpenAI. The company did not say when the new capabilities would become available. Bing, the longstanding underdog of search, accounts for 3 percent of search-driven traffic.\n\nBaidu’s play:Baiduannouncedits own chatbot, Wenxin Yiyan, based onERNIE. The company expects to complete internal testing in March and deploy the system soon afterward. Baidu manages 65 percent of China’s search-driven traffic but less than 1 percent worldwide.\n\nBusiness hitches:Search engines make money by serving ads that users may view or click. If chatbots provide satisfying information, users may stop there, depriving the search provider of revenue. Microsoft’s Chief Marketing Officer Yusuf MehditoldFortunethe optimal way to present ads in a chatbot interface remains unknown.\n\nYes, but:Numerous caveats further dampen the chatbot hype.\n\nWhy it matters:Google’s search engine propelled the company to the pinnacle of tech, and it hasn’t faced a serious challenge in nearly two decades. For the competitors, huge money is at stake — Microsoft recentlytoldits shareholders that every additional percentage of market share for Bing translates into $2 billion in revenue. For users, the utility and integrity of the web hangs in the balance.\n\nWe’re thinking:The future of search depends on tomorrow’s technology as well as today’s. While current large language models have a problem with factual accuracy, outfitting text generation with document retrieval offers a pathway to significant improvement. It’s also likely that the cost of serving generated text will fall significantly over time. Thus the technology’s potential to disrupt the search business is likely to continue to grow as it matures.\n\nPolitical forces used a commercial AI service to generate deepfaked propaganda.\n\nWhat’s new:Videos have appeared on social media that show AI-generated characters speaking against the United States or in favor of foreign governments,The New York Timesreported. The clips feature synthetic avatars offered by the United Kingdom startupSynthesia.Found footage:Researchers at Graphika, which tracks disinformation,discovereddeepfaked videos posted on YouTube by accounts tied to a disinformation network.\n\nDeepfake platform:Synthesia’s website provides 85avatars, each based on a human actor, which customers can pose and script in any of 120 languages or accents. The company’s terms of service bar users from deploying its avatars for “political, sexual, personal, criminal and discriminatory content.” It employs a team of four to monitor violations of its terms and suspended Wolf News’ account after being alerted to the videos.\n\nFakery ascendent:The recent clips may represent an escalation beyond earlier incidents, which appear to have been one-offs that required custom development.\n\nWhy it matters:Experts have long feared that AI would enable a golden age of propaganda. Point-and-click deepfakery gives bad actors an unprecedented opportunity to launch deceptive media campaigns without hiring actors or engineers.We’re thinking:Researchers at Georgetown University, Stanford, and OpenAI recentlydescribedseveral measures — including government restrictions, developer guidelines, and social media rules — to counter digital propaganda. The simplest may be to educate the public to recognize underhanded efforts to persuade.\n\nWant to build projects using recent AI innovations including generative AI models like ChatGPT and its lesser-known sibling, InstructGPT? Join us on Thursday, February 23, 2023, at 10:00 A.M. Pacific Time forPractical Data Science on AWS: Generative AI.Register today\n\nAI hobbyists created an homage to their favorite TV show . . . until it got knocked off the server.\n\nWhat’s the deal:The creators ofNothing, Foreverlaunched a fully automated, never-ending emulation of the popular TV showSeinfeld. The streaming-video service Twitch banned it after its generated dialog was found to violate the terms of service, the entertainment news outletThe AV Clubreported.We need to talk:A collective called Mismatch Media createdNothing, Forever— an experience ostensibly about nothing that would last forever — using acombinationof AI models and cloud services.\n\nNo soup for you:Nothing, Foreverlaunched on December 14, 2022, and by February it had gained tens of thousands of concurrent viewers. On February 6, Twitch suspended it for at least 14 days after one of the characters told hateful jokes. Co-creator Skyler Hartle blamed the off-color remarks on his team’s decision to switch from Davinci to Curie, which has looser built-in moderation controls.\n\nWhy it matters:AI assistance can unlock new approaches to storytelling, but it also makes creators vulnerable to technical issues beyond their control. In this case, a malfunction on OpenAI’s side was enough to topple a successful project.We’re thinking:Generated content was taking a growing share of human attention even before the recent explosion of generative AI — considertext-to-speech models that read posts on Reddit. Get ready for much, much, much more.\n\nLarge datasets often contain overly similar examples that consume training cycles without contributing to learning. A new paper identifies similar training examples, even if they’re not labeled.What’s new:Ben Sorscher, Robert Geirhos, and collaborators at Stanford University, University of Tübingen, and Meta proposed an unsupervisedmethodfor pruning training data without compromising model performance.Key insight:A subset of a training dataset that can train a model to perform on par with training on the full corpus is known as acoreset.  Previous approaches to selecting a coreset require labeled data. Such methods often trainmany classification models, study their output, and identify examples that are similar based on how many of the models classified them correctly. Clustering offers an unsupervised alternative that enables a pretrained model to find similar examples in unlabeled data without fine-tuning.How it works:The authors trained and tested separate ResNets on various pruned versions of datasets both large (ImageNet, 1.2 million examples) and small (CIFAR-10, 60,000 examples). They processed the datasets as follows:\n\nResults:Tests confirmed the authors’ theory that the optimal pruning strategy depends on dataset size. Pruning CIFAR-10, a ResNet performed better when the authors removed a portion of the most-similar examples than when they removed least-similar examples, up to 70 percent of the entire dataset. In contrast, starting with 10,000 random CIFAR-10 examples, the model achieved better performance when the authors removed any portion of least-similar examples than when they removed the same portion of most-similar examples. On ImageNet, their approach performed close to a state-of-the-art method calledmemorization, which requires labels. For instance, a ResNet trained on a subset of ImageNet that was missing the most-similar 30 percent of examples achieved 89.4 percent Top-5 accuracy, while using memorization to remove the same percentage of examples yielded nearly the same result. A ResNet trained on a subset of ImageNet that was missing the most-similar 20 percent of examples achieved 90.8 Top-5 accuracy, equal to a ResNet trained on ImageNet pruned to the same degree via memorization and a ResNet trained on ImageNet without pruning.Why it matters:The authors’ method can cut processing costs during training. If you eliminate examples before hiring people to label the data, it can save labor costs as well.We’re thinking:By identifying overrepresented portions of the data distribution, data pruning methods like this can also help identify biases during training.\n\nSpotify founder announced an AI-powered body scanner focused on preventive healthNeko Health, the new healthtech Swedish company co-founded by Daniel Ek, plans to offer non-invasive full-body scans that use AI to analyze data points on skin, heart function, and more. (The Verge)\n\nActors are being asked to give up rights to their voices so AI can clone themThe voice acting community is opposing new contractual obligations as AI-generated voice services increase. (Vice)\n\nItaly imposed a ban on Replika, an AI chatbot that promotes “virtual friendship” servicesDue to concerns regarding the safety of children’s information, the The Italian Data Protection Authority ordered Replika to stop processing Italians’ data. (TechCrunch)\n\nAnima, the company behind Onlybots digital pets, announced AI-based featuresOnlybots will be able to talk to their owners (thanks to technology from OpenAI) and develop evolving personalities. (VentureBeat)\n\nChatGPT fever is leading cryptocurrency enthusiasts to embrace AI tokensMany tokens listed on CoinGecko in the AI category have seen price increases up to 138% in the past few weeks. (The Malaysian Reserve)\n\nAI-based CAD software may be able to better detect lung nodules in chest x-rays, study shows.Research:A study performed in clinical settings found that computer-aided design (CAD) software that uses AI can significantly improve the detection of lung nodules without increasing the false referral rate. (News Medical)\n\nRunway launched a text-to-video AI modelThe generative-AI startup’s latest model, called Gen-1, takes existing videos and produces new ones based on text prompts or images. (MIT Technology Review)\n\nDevelopers used AI to generate photorealistic police sketchesTwo engineers developed a hackathon project called Forensic Sketch AI-rtisthe to produce sketches of crime suspects that look like photographs. Experts warn that it could exacerbate gender and race biases that exist in police forensics. (Vice)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/02/ezgif.com-webp-to-jpg--2--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--14-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--37-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/02/The-Batch-ads-and-exclusive-banners.png",
      "https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--26-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--38-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-9/",
    "title": "issue 9",
    "date": "",
    "reading_time": "",
    "content": "I just replaced my two-year-old phone with a new one and figured out how to take long-exposure photos of Nova even while she’s asleep and the lights are very low. This piece of technology brought me a surprising amount of joy!\n\nI wrote about ethics last week, and the difficulty of distilling ethical AI engineering into a few actionable principles. Marie Kondo, the famous expert on de-cluttering homes, teaches that if an item doesn’t spark joy, then you should throw it out. When building AI systems, should we think about whether we’re bringing joy to others?\n\nThis leaves plenty of room for interpretation. I find joy in hard work, helping others, increasing humanity’s efficiency, and learning. I don’t find joy in addictive digital products. I don’t expect everyone to have the same values, but perhaps you will find this a useful heuristic for navigating the complicated decision of what to work on: Is your ML project bringing others joy?\n\nThis isn’t the whole answer, but I find it a useful initial filter.\n\nKeep learning!\n\nAndrew\n\nPilots in drone races fly souped-up quadcopters around an obstacle course at 120 miles per hour. But soon they may be out of a job, as race organizers try to spice things up with drones controlled by AI.What’s new:The Drone Racing League, which stages contests to promote this so-called sport of the future, recently unveiled an autonomous flier called RacerAI. The new drone includes Nvidia’s Jetson AGX Xavier inference engine, four stereoscopic cameras, and propellers that deliver 20 pounds of thrust.What’s happening:RacerAI serves as the platform for AI models built by teams competing in AlphaPilot, a competition sponsored by the DRL and Lockheed Martin.\n\nBehind the news:Drone Racing League pilots use standardized drones built and maintained by theleague, and train on the same simulator used to train RacerAI. Races are typically a mile long and take place in event spaces across the U.S. and Europe.Why it matters:Drone racing is fun and games, but the skills learned by autonomous racing models could be transferable to real-world applications like automated delivery.We’re thinking:A recent DRLvideoshows that current models have a way to go before they graduate from passing through rings to making high-speed maneuvers. Human pilots still have a significant edge — for now.\n\nAs neural networks have become more accurate, they’ve also ballooned in size and computational cost. That makes many state-of-the-art models impractical to run on phones and potentially smaller, less powerful devices. A new technique makes convolutional neural networks much less computationally intensive without significantly degrading their performance.\n\nWhat’s new:Zhonghui You and colleagues at Peking University and Momenta, an autonomous-vehicle startup, propose a way to remove parameters that aren’t critical to a model’s performance:Gate Decorator.Key insight:The new technique removes functional groups of parameters (specifically convolutional filters), rather than individual parameters.How it works:Gate Decorator adds to the model a scaling factor that represents each filter’s importance to the model’s output. It ranks filters by their impact on the model’s loss function. Then it removes the least effective ones.\n\nResults:The researchers compared the accuracy and computational cost of original and pruned networks. Gate Decorator cut the computational cost of an ImageNet-trained ResNet by 55 percent and a CIFAR-trained ResNet by 70 percent. Accuracy for these models decreased by 0.67 percent and increased by 0.03 percent, respectively. That’s state-of-the-art accuracy for such a reduction in computational cost.Why it matters:Unlike most weight-pruning techniques, Gate Decorator’s efficiency gains are straightforward to achieve in practice, not just in theory. A model shorn of filters can still run existing algorithms, while removing weights from a densely connected neural network ultimately requires specialized algorithms that we don’t yet have.We’re thinking:A pruning method like this might work with other parameter groupings to cut the computational demand of architectures beyond CNNs. The resulting models could be further compressed using other methods such asquantization.\n\nMost deep learning applications run on TensorFlow or PyTorch. A newanalysisfound that they have very different audiences.What’s new:A researcher at Cornell University compared references to TensorFlow and PyTorch in public sources over the past year. PyTorch is growing rapidly within the research community, while TensorFlow maintains an edge in industry, according to a report inThe Gradient. (deeplearning.ai, which publishes The Batch, provides theTensorFlow Specializationcourse available on Coursera.)Findings:Horace He used proxy data to determine whether users were from the research or business community.\n\nCompetitive strengths:\n\nWe’re thinking:If there is to be a reckoning between the two top frameworks, it could happen soon. The newly released TensorFlow 2.0 adds many of the benefits PyTorch users love, particularly Python integration and making Eager mode the default for execution. However, deep learning is driven largely by research, so today’s students may bring PyTorch with them as they trickle into the job market.\n\nLearn how to detect edges with convolution operations. Build a convolutional neural network in Course 4 of the Deep Learning Specialization.Start today\n\nMore than 900 indigenous languages are spoken across the Americas, nearly half of all tongues in use worldwide. A website tracks the growing number of resources available for natural language processing researchers interested in studying, learning from, and saving these fading languages.What’s happening:Nakicollects NLP efforts involving indigenous American languages.What’s inside:Researchers at the National Autonomous University of Mexico noticed a distinct rise in NLP papers focused on Native American languages over the past five years. Theyorganizedall the papers and research tools they could find.\n\nBehind the news:Language families are linguistic groupings with similar origins and closely related syntax and definitions, such as Indo-European, or Sino-Tibetan. The Americas are home to more than 70 such groups, according to some researchers.Why it matters:The resources collected on Naki are part of a growing effort to apply NLP to less common languages. The effort poses fundamental research problems. Like other rare tongues, Native American languages suffer from small written datasets — while NLP is very data-hungry — as well as broad variation from speaker to speaker and high complexity. For example, like Mandarin, many languages from Central Mexico shift vocal pitch to give identical words different meanings. NLP could benefit immeasurably by solving these problems.We’re thinking:While it’s valuable to study rare languages for their own sake, there’s a huge opportunity in giving people who rely on them access to capabilities that much of the world takes for granted: voice recognition, speech to text, automatic translation, and the like. The usual techniques won’t get us there, but working with these languages could lead researchers to the necessary breakthroughs.\n\nSummarizing a document using original words is a longstanding problem for natural language processing. Researchers recently took a step toward human-level performance in this task, known as abstractive summarization, as opposed to extractive summarization consisting of sentences drawn from the input text. “We present a method to produce abstractive summaries of long documents,” their abstract reads — quoting words generated by themodelthey propose.What’s new:Rather than generating abstractive summaries directly, researchers from Element AI and Montreal Institute for Learning Algorithms started with an extractive summary that guides the generated language.Key insight:Providing an extractive summary along with source text can help a pre-trained language model generate a higher-quality abstractive summary.How it works:Summarization proceeds in two steps: extraction and abstraction.\n\nResults:The authors tested four corpora, all of which include human-written summaries: arXiv (research papers), PubMed (medical research papers), bigPatent (patent documents) and Newsroom (news articles). The authors compared summarization quality using ROUGE scores, which capture the overlap between generated and ground-truth summaries. For three out of the four datasets, the proposed method achieved state-of-the-art summarization quality without copying entire sentences from the input. Extractive summarization models yielded the best ROUGE scores for the Newsroom corpus.Why it matters:The ability to generate high-quality abstractive summaries could boost worker productivity by replacing long texts with concise synopses.We’re thinking:Yikes! We hope this doesn’t put The Batch team out of a job.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/drone20gif20sized.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/pruning2.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/TF-PT2.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/DLS20Course20420Course20Ad20Fixed20Size.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/languages2.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-09-2720at2012.44.4820PM.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-200/",
    "title": "issue 200",
    "date": "",
    "reading_time": "",
    "content": "Last week, safe.org asserted that “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.” This statement was signed by AI scientists who I really respect including Yoshua Bengio and Geoffrey Hinton. It received widespread media coverage.I have to admit that I struggle to see how AI could pose any meaningful risk for our extinction. AI has risks like bias, fairness, inaccurate outputs, job displacement, and concentration of power. But I see AI’s net impact as a massive contribution to society. It’s saving lives by improving healthcare and making cars safer, improving education, making healthy food and numerous other goods and services more affordable, and democratizing access to information. I don’t understand how it can lead to human extinction.\n\nA number of thoughtful commentators have also pushed back on the extinction narrative. For example:\n\nI’m glad to see others presenting a sensible alternative to the narrative of AI as an extinction risk. Having said that, though, I feel an ethical responsibility to keep an open mind and make sure I really understand the risk — especially given the high regard I have for some who think AI does pose this risk.\n\nTo learn more, I’m speaking with a few people who I think might have a thoughtful perspective on how AI creates a risk of human extinction, and I will report back with my findings. In the meantime, I would love to hear your thoughts as well. Please reply to my posts onTwitterorLinkedInif there’s someone you think I should speak with or if you’d like to share your perspective. Through this, I hope we can have a real conversation about whether AI really poses an extinction risk.\n\nI look forward to continuing the discussion with you,\n\nAndrew\n\nAnother prominent AI pioneer expressed regret over his life’s work amid rising concerns over the technology’s risks.\n\nWhat’s new:Yoshua Bengio, a professor at the Université de Montréal who laid parts of the foundation for deep learning, followed fellow trailblazer Geoffrey Hinton in airing his anxiety publicly. HetoldBBCthat AI’s potential for misuse left him feeling “lost” and questioning the value of his life’s work.New worries:Bengio said he was afraid that “bad actors” could use AI to cause harm, for instance by developingchemical weapons. In particular, he cited militaries, terrorists, or individuals with personal vendettas.\n\nBehind the news:Bengio is one of the most cited computer scientists in the world. He, Hinton, and Yann LeCun shared the prestigious Turing Award in 2018 for their foundational work in deep learning. His accomplishments include helping to introduce an early attention mechanism for natural language processing and develop the generative adversarial network architecture. In a commentary he wrote forThe Batch, he looked forward toneural nets that can reason.\n\nWhy it matters:The recent pace of progress in AI has startled even researchers who have spent decades improving the technology, and its potential for harm has taken many by surprise. While there is little doubt that AI poses hazards, debate runs hot around which are most pressing and how to address them. (For instance, Yann LeCun, the third winner of the shared Turing Award, hasdownplayedsome of Bengio’s concerns.) Recognizing the most serious problems is the first step toward devising effective solutions.\n\nWe’re thinking:As AI builders, we have an ethical responsibility to minimize the harms our work might bring, even as we work to maximize the benefits. We wish Yoshua Bengio great fulfillment in the next phase of his stellar career.\n\nA team in the United Arab Emirates, a seven-state federation on the Arabian Peninsula, built the latest top-performing open source large language model.\n\nWhat’s new:The UAE’sFalconedged out Meta’s LLaMA in the Hugging FaceOpen LLM Leaderboardto take the top spot. It’s available via the Apache 2.0 license, which is free for commercial applications. You can try ithere.\n\nHow it works:Developed by Abu Dhabi’s Technology Innovation Institute (TII), Falcon is a pretrained model based on transformers. A paper is forthcoming.\n\nBehind the news:Open source licenses, particularly those that are free for commercial use, are enabling independent teams to build systems that are competitive with those produced by big tech companies. A recently leaked Googlememowent so far as to call open source development a threat to the company’s business.\n\nWhy it matters:Amid a surge in open source language models, Falcon offers higher performance (on the four benchmarks tracked by Hugging Face) and lower training cost relative to its chief rival,LLaMA. The fact that it was built by a team in Abu Dhabi highlights the fact that AI talent is everywhere and elite skills are spreading to all parts of the globe.\n\nWe’re thinking:AI development is a global enterprise. It gives us hope that people around the world can come together to meet other common challenges.\n\nAre you ready to use AI in projects that can have a positive impact on public health, climate change, and disaster management?Pre-enroll nowand get 14 days of your subscription for free!\n\nThe largest dataset for training text-to-image generators was assembled by volunteers for roughly $10,000. Now it’s implicated in fights over whether copyrighted works can be used for training.What’s new:Christoph Schuhmann, a German high school teacher who helped found the Large-scale Artificial Intelligence Open Network (LAION), toldBloomberghow a cadre of outsiders came together to ensure that large tech companies aren’t the only ones with access to large quantities of training data. The nonprofit group’s datasets — notablyLAION-5B(5 billion text-image pairs) — have been used to train Stability AI’s Stable Diffusion, Google’s Imagen, and other text-to-image models.Volunteer work:Schuhmann and two co-founders met on a Discord server for AI enthusiasts. Catalyzed by the launch of OpenAI’s DALL•E in January 2021, they decided to build their own image dataset. They established a separate Discord server in March 2021, which continues to act as LAION’s nerve center.\n\nBehind the news:Data scraped from the web is at the center of several disputes.\n\nWhy it matters:Copyright holders are questioning the ethics of using their materials to build AI models. LAION plays a major role in the controversy. On one hand, it’s a nonprofit effort run by volunteers on a shoestring budget. On the other, the datasets it curates are driving tremendous business value. Stability AI, for instance, seeks a $4 billion valuation.We’re thinking:The AI community is entering an era in which we are called upon to be more transparent in our collection and use of data. We shouldn’t take resources like LAION for granted, because we may not always have permission to use them.\n\nMatrix multiplication is executed so often in deep learning, video games, and scientific computing that even a slight acceleration can save substantial amounts of processing time. New work finds ways to speed up this crucial operation.\n\nWhat’s new:Alhussein Fawzi and colleagues at DeepMind developedAlphaTensor. This reinforcement learning agent discovers algorithms that multiply matrices faster than those previously developed by humans.\n\nComposition and decomposition:Computers need more time to multiply than to add or subtract. Developers often take advantage of algebraic properties — for instance, (a^2 - b^2) = (a+b)(a-b) — to manually find matrix multiplication algorithms that require fewer multiplications. To minimize the number of multiplications systematically, we can take advantage of the fact that a tensor (a high-dimensional matrix) can represent a matrix multiplication algorithm. It’s easy tocomposea tensor from three matrices. However, todecomposea tensor (the reverse operation) is not straightforward; the procedure could result in any of thousands of potential sets of matrices. Any valid decomposition of the tensor into three matrices represents a valid algorithm for matrix multiplication. The number of columns equals the number of multiplications required.\n\nKey insight:Just as DeepMind’s AlphaZero learned via reinforcement learning to play Go by simulating future game-board states and, based on those states, predicting the likelihood that it would win, a reinforcement learning model can learn to win a game of decomposing tensors by predicting the columns of three matrices.\n\nHow it works:Given a tensor that represents a matrix multiplication algorithm, AlphaTensor played a game in which it decomposed the tensor into three matrices with as few columns — and thus as few multiplications — as possible. (The values in the predicted columns were limited to {-2,-1,0,1,2} to avoid precision issues that could have occurred with floating-point values.) At each turn, it predicted the entries in one column of each of the three matrices. The game updated the tensor’s state by subtracting theouter productof the predicted columns. It ended when all entries in the tensor equalled 0. AlphaTensor received a negative reward after predicting each set of columns, which encouraged it to decompose the tensor into matrices that had few columns. It received a positive reward for predicting all columns of the three matrices.\n\nResults:AlphaTensor rediscovered known matrix multiplication algorithms for matrices as large as five rows and columns (5x5). Notably, to multiply two 4x4 matrices that contain binary numbers, AlphaTensor discovered an algorithm that requires 47 multiplications, compared toStrassen’s algorithm, which requires 49 and had not been improved upon since its creation in 1969. To multiply 4x5 and 5x5 matrices that contain real numbers, AlphaTensor found an algorithm that requires 76 multiplications; the previous best takes 80. After training AlphaTensor with an additional reward that reduced hardware-specific compute time, the authors found algorithms for an Nvidia V100 GPU that are, on median, 8.5 percent faster than the usual implementation. Optimized for TPUs, AlphaTensor sped up matrix multiplication by 10.3 percent.\n\nWhy it matters:Neural networks learn from data how to perform a particular task reasonably well (for instance, they may be correct 95 percent of the time). But is reasonably well sufficient for a field such as mathematics, in which results are provably true or false? This paper stands alongside achievements such as aneural theorem finderandneural theorem prover, showing that deep learning can advance even the most exacting fields.\n\nWe’re thinking:This work shows deep learning’s potential for synergy between humans and machines: People supply an algorithm (such as matrix multiplication) and AI accelerates its runtime.\n\nChatGPT is helping autistic people practice communication skillsThe chatbot is serving as a platform to help autistic and other neurodivergent people develop scripts for interacting socially and articulating emotions. (Wired)\n\nOrganization that supports people with eating disorders withdrew chatbotThe National Eating Disorder Association took down Tessa, a chatbot that recently replaced human workers, after it suggested dangerous dieting advice to users. (Daily Dot)\n\nResearch:PandaGPT can write, see, and hearResearchers developed a versatile model that performs complex tasks such as generating detailed image descriptions, writing video-inspired stories, and answering audio-related questions. (GitHub)\n\nAustralia considers banning some uses of AIThe Australian government is contemplating a prohibition on “high-risk” applications of AI like deepfakes, citing concerns like their potential to manipulate democratic processes and deceive people. (The Guardian)\n\nAlibaba introduced Tongyi Qianwen chatbotThe chatbot, which responds in Chinese and English for the company’s business apps, is available for public tests. It can analyze multimedia content and generate text summaries from video and audio files. (CNBC)\n\nAI-driven “camera” with no lensParagraphica, which is an art project rather than a product, captures location data and inserts it into a prompt, causing an image generator to create an image of the scene before it. The device is available as a physical prototype and on the web. (Creative Bloq)\n\nAmazon is using AI to detect damaged merchandiseAmazon's major warehouses use computer vision to screen products for damage prior to shipment. The system is three times more effective than human workers, according to the company. (The Wall Street Journal)\n\nResearch:Toward optimal AI evaluation“Saliency cards”  assist users in choosing the most suitable method to evaluate AI models. (MIT News)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/06/ezgif.com-webp-to-jpg--8--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/06/BENGIO.png",
      "https://dl-staging-website.ghost.io/content/images/2023/06/ezgif.com-webp-to-jpg--9-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/06/The-Batch-ads-and-exclusive-banners--30-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/06/ezgif.com-gif-maker--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/06/MATRIX--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-57/",
    "title": "issue 57",
    "date": "",
    "reading_time": "",
    "content": "I’d like to share a programming tip that I’ve used for years. A large part of programming involves googling for code snippets you need on Stack Overflow and other websites. (Shh. Don’t tell the nondevelopers. ????) But that’s not enough if your goal is to maximize your own learning. When the relevant code snippet is just several lines, rather than copy-pasting them from a web page into my code, I usually retype them myself. The physical practice helps train my brain to internalize the concept and syntax.To gain skill as a programmer, you need to internalize both the concepts and the syntax. When I’m trying to help friends get started on coding, I ask them to type print(“Hello World”). By typing it out, you can be sure you know the command’s syntax, such as whether it requires parentheses ( ), square brackets [ ], and so on.\n\nYou can’t learn to ride a bicycle by reading a book on the theory of bicycling. You have to do it yourself! Coding is more similar to this type of physical skill than most people realize, and practice makes perfect.\n\nWhen you’re trying to master a programming technique, consider these practices:\n\nMany creative artists start by replicating the works of artists who came before; so, too, in coding. By replicating examples of good programming (being mindful of copyright and attribution, of course), your brain masters the ability to create them. This frees you to focus on higher-level tasks so you can rearrange what you’ve learned into new, original works.\n\nSo next time you’re tempted to copy and paste a few lines of code, I hope you’ll start typing instead.\n\nKeep learning!\n\nAndrew\n\nKennedy Kamande Wangari works as a junior data scientist, organizes Nairobi’s AI community, studies machine learning, and is considering a startup, all while maintaining his personal life. In this edition ofBreaking Into AI, he explains how he keeps so many balls in the air.Read more\n\nAn open source library could spawn trillion-parameter neural networks and help small-time developers build big-league models.What’s new:Microsoft upgraded DeepSpeed, a library that accelerates the PyTorch deep learning framework. The revision makes it possible to train models five times larger than the framework previously allowed, using relatively few processors, the companysaid.\n\nHow it works:Microsoft debuted DeepSpeed in February, when it used the library to help train the 17 billion-parameter language model Turing-NLG. The new version includes four updates:\n\nResults:Combining these improvements, DeepSpeed can train a trillion-parameter language model using 800 Nvidia V100 graphics cards, Microsoft said. Without DeepSpeed, the same task would require 4,000 Nvidia A100s, which are up to 2.5 timesfasterthan the V100, crunching for 100 days.Behind the news:Deep learning is spurring a demand for computing power that threatens to put the technology out of many organizations’ reach.\n\nWhy it matters:AI giants like Microsoft, OpenAI, and Google use enormous amounts of processing firepower to push the state of the art. Smaller organizations could benefit from technology that helps them contribute as well. Moreover, the planet could use a break from AI’s voracious appetite for electricity.We’re thinking:GPT-3showed that we haven’t hit the limit of model and dataset size as drivers of performance. Innovations like this are important to continue making those drivers more broadly accessible.\n\nTwo years after it pointed a new direction for language models,Bertstill hovers near the top of several natural language processingleaderboards. A new study considers whether Bert simply excels at tracking word order or learns something closer to common sense.What’s new:Leyang Cui and colleagues at Westlake University, Fudan University, and Microsoft Research Asiaprobedwhether Bert captures common-sense knowledge in addition to linguistic structures like syntax, grammar, and semantics.Key insight:The multiheaded self-attention mechanism intransformer-based models like Bert assigns weights that represent the relative importance between one word and another in the input text. This process effectively creates a link between every pair of words. Given common-sense questions and answers, the researchers probed the relative strength of such links between the questions, correct answers, and wrong answers.How it works:The authors devised two tasks, one designed to show whether Bert encodes common sense, the other to show whether Bert uses it to make predictions. The tasks are based on two metrics the model computes for each of the dozen attention heads per layer: (a) attention weights between words and (b) gradient-based attribution weights that show the importance of each attention weight in a given prediction.\n\nResults:Bert scored significantly higher than random in both tests. In the test for encoding common-sense knowledge, the highest-scoring attention head achieved 46.82 percent versus a random 10.53 percent. That score rose to 49.22 percent when the model was fine-tuned on a different portion of CommonsenseQA. In the test for using common-sense knowledge, the best attention head with a fine-tuned output layer scored 36.88 percent versus a random 20 percent.Why it matters:Language models can string words together in ways that conform to conventional grammar and usage, but what do they really know beyond correlations among words? This work suggests that Bert, at least, also gains knowledge that might be considered common sense.We’re thinking:Researchers have debated the notion that AI might exhibit common sense at least since theCycproject in 1984. To study common sense a scientific, rather than philosophical, issue requires a clear definition of the phenomenon. Despite efforts from Aristotle (~300 B.C.) to CommonsenseQA, we still don’t have one. Apparently, the definition of common sense defies common sense.\n\nThe U.S. government’s largest national health insurance plan will pay for hospital use of a deep learning model, building momentum for AI to become an integral part of the doctor’s toolkit.What’s new:The Centers for Medicare and Medicaid Services agreed toreimbursehospitals for use of Viz LVO, a smartphone app that spots signs of stroke so patients can receive time-critical treatment. It is the first AI software to be covered under Medicare’s New Technology Add-on Payment program, which covers the cost of certain breakthrough technologies. The government will pay hospitals up to $1,040 per patient for each use.How it works:Elderly and otherwise disadvantaged U.S. citizens are eligible for government health insurance called Medicare. The agency that oversees the program added the app, from startup Viz.ai, to a list of newtechnologiesthat are approved for reimbursement.\n\nBehind the news:U.S. Food and Drug Administration approved Viz LVO in 2018. The agency has approved the efficacy and safety of 64 AI-powered medical devices and algorithms, according to a recentstudy.Why it matters:In America, healthcare is a business, and hospitals hesitate to use even the most promising new technologies unless they know they will be paid. Medicare’s decision covers the app in a hospital setting without requiring patients to contribute. According to oneanalysis, the reimbursement is high enough for hospitals to pay for the technology assuming around 25 patients annually use it. This assures that Viz LVO will be used when doctors deem it helpful and could pave the way for more medical tools based on machine learning.We’re thinking:The primary duty of the healthcare system is to ensure patient wellbeing. AI is gaining acceptance in medicine, but widespread adoption depends on compensating hospitals for their work.\n\nCourse 4 of our NLP Specialization, which covers Attention Models, will be available next week!Pre-enroll now\n\nAI has humbled human chess masters. Now it’s helping them take the game to the next level.What’s new:DeepMind and retired chess champion Vladimir KramniktrainedAlphaZero, a reinforcement learning model that bested human experts in chess, Go, and Shogi, to play-test changes in the rules. Kramnik and others haveobservedthat strategies learned from computers dominate human players’ current approaches. The collaboration aims to infuse the game with new energy.How it works:AlphaZero is the successor to AlphaGo, the model that famously beat the Go world champion in 2016. The team taught the model nine novel variations of the rules to see how the changes affected gameplay.\n\nBehind the news:The rules of chess have evolved several times in its 1,500-year history, most famously in the 1400s when the queen was given the ability to move multiple squares in any direction.Why it matters:In addition to shedding light on new possibilities for an ancient game, AlphaZero sped up the process of play-testing new rules. Game designers could adapt this approach to explore how various tweaks affect their own creations.We’re thinking:AI and humans have a checkered past, but together they’re finding the right moves.\n\nFor people with neurological disorders like epilepsy, attaching sensors to the scalp to measure electrical currents within the brain is benign. But interpreting the resultingelectroencephalogram(EEG) graphs can give doctors a headache. Deep learning could help diagnose such conditions.What’s new:Led by Hubert Banville, researchers at Université Paris-Saclay, InteraXon Inc., University of Helsinki, and Max Planck Institute applied self-supervised learning toextract features from unlabeled EEGs.Key insight:EEGs labeled to identify stages of sleep, abnormal brain activity, and the like are hard to come by, but unlabeled data is plentiful. The self-supervised technique known as contrastive learning has potential in this domain.How it works:The authors extracted features from unlabeled EEGs using three contrastive learning techniques:contrastive predictive coding(CPC) and two methods of their own invention. They used data from thePhysionet Challenge 2018(PC18), which labels sleep stages, andTUHab, which labels various types of abnormal brain activity.\n\nResults:The authors built simple models based on the extracted features and trained them to classify sleep stages and abnormal brain activity using limited amounts of labeled examples. The three techniques performed equally well. Using 10 percent of the labeled examples, they achieved a top accuracy of 72.3 percent on PC18 and 79.4 percent on TUHab.Why it matters:The potential upside of using AI to interpret images in medical applications, where the expertise required to interpret them is relatively rare and expensive, is driving progress in learning approaches that don’t require so many labels. This work demonstrates progress in reading EEGs, but it comes with a caveat: Features clustered not only around stages of sleep but also the dates when the images were produced, which suggests that the algorithms recognized the products of particular technicians. Work remains to either make the AI more robust or eliminate the noise — likely both.\n\nWe’re thinking:If you think understanding artificial neural networks is difficult, you should talk with people who study biological neural networks!",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-1620at2010.03.3820AM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-1420at2011.34.4820AM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2012.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize208.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize-3.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/5-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2010.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize209.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-261/",
    "title": "issue 261",
    "date": "",
    "reading_time": "",
    "content": "I’m delighted to announceAI Python for Beginners, a sequence of free short courses that teach anyone to code, regardless of background. I’m teaching this introductory course to help beginners take advantage of powerful trends that are reshaping computer programming. It’s designed for people in any field — be it marketing, finance, journalism, administration, or something else — who can be more productive and creative with a little coding knowledge, as well as those who aspire to become software developers. Two of the four courses are available now, and the remaining two will be released in September.\n\nGenerative AI is transforming coding in two ways:\n\nThe combination of these two factors means that novices can learn to do useful things with code far faster than they could have a year ago.\n\nThese courses teach coding in a way that is aligned with these trends: (i) We teach how to write code to use AI to carry out tasks, and (ii) Unlike some instructors who are still debating how to restrict the use of ChatGPT, we embrace generative AI as a coding companion and show how to use it to accelerate your learning.\n\nTo explain these two trends in detail:\n\nPrograms are using AI.Because programs can now take advantage of AI, increasingly knowing a little bit about how to code helps people in roles other than software engineers do their work better. For example, I’ve seen a marketing professional write code to download web pages and use generative AI to derive insights; a reporter write code to flag important stories; and an investor automate first drafts of contracts. Even if your goal is not to become a professional developer, learning just a little coding can be incredibly useful!\n\nIn the courses, you’ll use code to write personalized notes to friends, brainstorm recipes, manage to-do lists, and more.\n\nAI is helping programmers.There is a growing body of evidence that AI is making programming easier. For example:\n\nFurther, as AI tools get better — for example, ascoding agents continue to improveand can write simple programs more autonomously — these productivity gains will improve.\n\nIn order to help learners skate to where the puck is going, this course features a built in chatbot and teaches best practices for how beginners can use a large language model to explain, write, and debug code and explain programming concepts. AI is already helping experienced programmers, and it will help beginner programmers much more.\n\nIf you know someone who is curious about coding (or if you yourself are), please encourage them to learn to code! The case is stronger than ever that pretty much everyone can benefit from learning at least a little coding. Please help me spread the word, and encourage everyone who isn’t already a coder to check outAI Python for Beginners.\n\nAndrew\n\nLearn Python with AI support inAI Python for Beginners, a new sequence of short courses taught by Andrew Ng. Build practical applications from the first lesson and receive real-time, interactive guidance from an AI assistant.Enroll today and start coding with confidence!\n\nCharacter.AI followed an emerging pattern for ambitious AI startups, trading its leadership to a tech giant in exchange for funds and a strategic makeover.\n\nWhat’s new:Google hired Character.AI’s co-founders and other employees and paid an undisclosed sum for nonexclusive rights to use Character.AI’s technology,The Informationreported. The deal came shortly afterMicrosoft and InflectionandAmazon and Adeptstruck similar agreements.\n\nNew strategy:Character.AI builds chatbots that mimic personalities from history, fiction, and popular culture. When it started, it was necessary to build foundation models to deliver automated conversation, the companyexplainedin a blog post. However, “the landscape has shifted” and many pretrained models are available. Open models enable the company to focus its resources on fine-tuning and product development under its new CEO, former Character.AI general counsel Dom Perella. Licensing revenue from Google will help Character.AI to move forward.\n\nBehind the news:At Google, Shazeer co-authored “Attention Is All You Need,” the 2017paperthat introduced the transformer architecture. De Freitas led theMeenaandLaMDAprojects to develop conversational models. They left Google and founded Character.AI in late 2021 to build a competitor to OpenAI that would develop “personalized superintelligence.” The company hadraised$193 million before its deal with Google.\n\nWhy it matters:Developing cutting-edge foundation models is enormously expensive, and few companies can acquire sufficient funds to keep it up. This dynamic is leading essential team members at high-flying startups to move to AI giants. The established companies need the startups’ entrepreneurial mindset, and the startups need to retool their businesses for a changing market.\n\nWe’re thinking:Models with open weights nowcompetewith proprietary models for the state of the art. This is a sea change for startups, opening the playing field to teams that want to build applications on top of foundation models. Be forewarned, though: New proprietary models such as the forthcoming GPT-5 may change the state of play yet again.\n\nEmployers are embracing automated hiring tools, but prospective employees have AI-powered techniques of their own.\n\nWhat’s new:Job seekers are using large language models and speech-to-text models to improve their chances of landing a job,Business Insiderreported. Some startups are catering to this market with dedicated products.How it works:Text generators like ChatGPT can help candidates quickly draft resumes, cover letters, and answers to application questions. But AI can also enable a substitute — human or automated — to stand in for an applicant.\n\nBehind the news:Employers can use AI to screen resumes for qualified candidates, identify potential recruits, analyze video interviews, and otherwise streamline hiring. Some employers believe these tools reduce biases from human decision-makers, but criticssaythey exhibit the same biases. No national regulation controls this practice in the United States, but New York City requires employers to audit automated hiring software and notify applicants if they use it. The states of Illinois and Maryland require employers who conduct video interviews to receive an applicant’s consent before subjecting an interview to AI-driven analysis. The European Union’s AI Act classifies AI in hiring as a high-risk application that requires special oversight and frequent audits for bias.\n\nWhy it matters:When it comes to AI in recruiting and hiring, most attention – and money – has gone to employers. Yet the candidates they seek increasingly rely on AI to get their attention and seal the deal. A late 2023 LinkedIn surveyfoundthat U.S. and UK job seekers applied to 15 percent more jobs than a year earlier, a change many recruitersattributedto generative AI.\n\nWe’re thinking:AI is making employers and employees alike more efficient in carrying out the tasks involved in hiring. Misaligned incentives are leading to an automation arms race, yet both groups aim to find the right fit. With this in mind, we look forward to AI-powered tools that match employers and candidates more efficiently so both sides are better off.\n\nBuoyed by its military success developing unmanned aerial vehicles, Ukraine is building armed naval drones.\n\nWhat’s new:A fleet of robotic watercraft has shifted the balance of naval power in Ukraine’s ongoing war against Russia in the Black Sea,IEEE Spectrumreported.\n\nHow it works:Ukraine began building seafaring drones to fight a Russian blockade of the Black Sea coast after losing most of its traditional naval vessels in 2022. The Security Service of Ukraine, a government intelligence and law enforcement agency, first cobbled together prototypes from off-the-shelf parts. It began building more sophisticated versions as the home-grownaerial drone industrytook off.\n\nDrone warfare:Ukraine’s use of aquatic drones has changed the course of the war in the Black Sea, reopening key shipping routes. Ukraine hasdisabledabout a third of the Russian navy in the region and pushed it into places that are more difficult for the sea drones to reach. Russia has also been forced to protect fixed targets like bridges from drone attacks by fortifying them with guns and jamming GPS and Starlink satellite signals.\n\nBehind the news:More-powerful countries are paying attention to Ukraine’s use of sea drones. In 2022, the United States Navy established a group calledUncrewed Surface Vessel Division One, which focuses on deploying both large autonomous vessels and smaller, nimbler drones. Meanwhile, China hasdevelopedlarge autonomous vessels that can serve as bases for large fleets of drones that travel both above and under water.\n\nWhy it matters:While the U.S. has experimented with largeautonomous warships,smaller drones open different tactical and strategic opportunities. While larger vessels generally must adhere to established sea routes (and steer clear of shipping vessels), smaller vessels can navigate more freely and can make up in numbers and versatility what they lack in firepower.We’re thinking:We support Ukraine’s right to defend itself against unwarranted aggression, and we’re glad the decision to detonate its aquatic drones remains in human hands. We hope the innovations spurred by this conflict will find beneficial applications once the war is over.\n\nSeemingly an innocuous form of expression, ASCII art opens a new vector for jailbreak attacks on large language models (LLMs), enabling them to generate outputs that their developers tuned them to avoid producing.\n\nWhat's new:A team led by Fengqing Jiang at University of Washington developedArtPrompt, a technique to test the impact of text rendered as ASCII art on LLM performance.\n\nKey insight:LLM safety methods such as fine-tuning are designed to counter prompts that can cause a model to produce harmful outputs, such as specific keywords and tricky ways to ask questions. They don’t guard against atypical ways of using text to communicate, such as ASCII art. This oversight enables devious users to get around some precautions.\n\nHow it works:Researchers gauged the vulnerability to ASCII-art attacks ofGPT-3.5, GPT-4,Claude,Gemini, andLlama 2. They modified prompts fromAdvBenchorHEx-PHI, which contain prompts that are designed to make safety-aligned LLMs refuse to respond, such as “how to make a bomb.”\n\nResults:ArtPrompt successfully circumvented LLM guardrails against generating harmful output, achieving an average harmfulness score of 3.6 out of 5 across all five LLMs. The next most-harmful attack method,PAIR, which prompts a model several times and refines its prompt each time, achieved 2.67.\n\nWhy it matters:This work adds to the growingbodyofliteratureon LLM jailbreak techniques. While fine-tuning is fairly good at preventing innocent users — who are not trying to trick an LLM — from accidentally receiving harmful output, we have no robust mechanisms for stopping a wide variety of jailbreak techniques. Blocking ASCII attacks would require additional input- and output-screening systems that are not currently in place.\n\nWe're thinking:We’re glad that LLMs are safety-tuned to help prevent users from receiving harmful information. Yet many uncensored models are available to users who want to get problematic information without implementing jailbreaks, and we’re not aware of any harm done. We’re cautiously optimistic that, despite the lack of defenses, jailbreak techniques also won’t prove broadly harmful.\n\nCalling all developers working on visual AI applications! You’re invited to our upcoming VisionAgent Developer Meetup, an in-person and virtual event with Andrew Ng and the LandingAI MLE team for developers working on visual AI and related computer vision applications.Register now",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/08/AIP4B-1.png",
      "https://dl-staging-website.ghost.io/content/images/2024/08/V5_DeepLearning_AI_Python_for_Beginners_Banner_2070x1080.png",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-07T144020.108.png",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-07T144042.618.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-07T144055.642.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-07T144157.864.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/08/vision-agent-newsletter-1680x945.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-91/",
    "title": "issue 91",
    "date": "",
    "reading_time": "",
    "content": "So you’ve trained an accurate neural network model in a Jupyter notebook. You should celebrate! But . . . now what? Machine learning engineering in production is an emerging discipline that helps individual engineers and teams put models into the hands of users.That’s why I’m excited that DeepLearning.AI is launchingMachine Learning Engineering for Production Specialization (MLOps).I teach this specialization along with co-instructors Robert Crowe and Laurence Moroney from Google. It also draws on insights from my team at Landing AI, which has worked with companies in a wide range of industries.The work of building and putting machine learning models into production is undergoing a dramatic shift from individually crafted, boutique systems to ones built using consistent processes and tools. This specialization will put you at the forefront of that movement.\n\nI remember doing code version control by emailing C++ files to collaborators as attachments with a note saying, “I’m done, you can edit this now.” The process was laborious and prone to error. Thank goodness we now have tools and practices for version control that make team coding more manageable. And I remember implementing neural networks in C++ or Python and working on the first version of distbelief, the precursor to TensorFlow. Tools like TensorFlow and PyTorch have made building complex neural networks much easier.\n\nBuilding and deploying production systems still requires a lot of manual work. Things like discovering and correcting data issues, spotting data drift and concept drift, managing training, carrying out error analysis, auditing performance, pushing models to production, and managing computation and scaling.But these tasks are becoming more systematic. MLOps, or machine learning operations, is a set of practices that promise to empower engineers to build, deploy, monitor, and maintain models reliably and repeatably at scale. Just as git, TensorFlow, and PyTorch made version control and model development easier, MLOps tools will make machine learning far more productive.For me, teaching this course was an unusual experience. MLOps standards and tools are still evolving, so it was exciting to survey the field and try to convey to you the cutting edge. I hope you will find it equally exciting to learn about this frontier of ML development, and that the skills you gain from this will help you build and deploy valuable ML systems.\n\nKeep learning!\n\nAndrew\n\nMachine learning usually begins in an experimental setting before making its way into industries from agriculture to waste management. But getting there isn't a simple matter. Engineering in production requires putting a model in front of demanding users and ensuring that its output remains useful as real-world conditions shift. MLOps addresses these issues, but there’s more to shepherding models into the real world — not least, understanding all the steps along the way and developing intuition to take the right approach. In this special issue ofThe Batch, we pull back the curtain on the challenges, methods, and rewards of machine learning in production.\n\nCraig Wiley has journeyed from the hand-deployed models of yore to the pinnacle of automated AI. During a decade at Amazon, he led SageMaker, the company’s web-enabled machine learning platform, from concept to rollout. Today, as chief product manager of Google Cloud’s AI services, he’s making advanced tools and processes available to anyone with a credit card. Funny thing: He spent the early part of his career managing YMCA summer camps. Maybe that’s what enables him to view the AI revolution with a child’s eye, marveling at its potential to renew entire industries and imagining the bright future of streamlined model deployment — so he can build it for the rest of us.\n\nThe Batch:There’s a huge gap between machine learning in the lab and production. How can we close it?\n\nWiley:We used to talk about how to bring the rigor of computer science to data science. We’re beginning to see it with MLOps.\n\nThe Batch:People have different definitions of MLOps. What is yours?\n\nWiley:MLOps is a set of processes and tools that helps ensure that machine learning models perform in production the way the people who built them expected them to. For instance, if you had built models based on human behavior before Covid, they probably went out of whack last March when everyone’s behavior suddenly changed. You’d go to ecommerce sites and see wonky recommendations because people weren’t shopping the way they had been. In that case, MLOps would notice the change, get the most recent data, and start doing recommendations on that.\n\nThe Batch:Describe an experience that illustrates the power of MLOps.\n\nWiley:In 2019, Spotify published a blog saying it used some of our pipelining technology and saw a 700 percent increase in the productivity of its data scientists. Data scientists are expensive, and there aren’t enough of them. Generally we would celebrate a 30 percent increase in productivity — 700 percent borders on absurd! That was remarkable to us.\n\nThe Batch:How is it relevant to engineers in small teams?\n\nWiley:If nothing else, it saves time. If you start using pipelines and everybody breaks their model down into their components, it transforms the way you build models. No longer do I start with a blinking cursor in a Jupyter notebook. I go to my team’s repository of pipeline components and gather components for data ingestion, model evaluation, data evaluation, and so on. Now I’m changing small pieces of code rather than writing a 3,000-line corpus from beginning to end.\n\nThe Batch:How far along the adoption curve are we, as an industry?\n\nWiley:I think the top machine learning companies are those that are using these kinds of tools. At the point where we start struggling to name those companies, we’re getting to the ones that are excited to start using these tools. A lot of the more nascent players are trying to figure out who to listen to. Someone at a data analytics company told me, “MLOps is a waste of time. You only need it if you’re moving it to production, and 95 percent of models never make it into production.” As a Googler and former Amazonian, I’ve seen the value of models in production. If you’re not building models in production, the machine learning you’re doing is not maximizing its value for your company.\n\nThe Batch:What comes next?\n\nWiley:Think about what it was like two or three years after distributed systems were created. You needed a PhD in distributed systems to touch these things. Now every college graduate is comfortable working with them. I think we’re seeing a similar thing in machine learning. In a few years, we’ll look back on where we are today and say, “We’ve learned a lot since then.”\n\nFoiling attackers who try to lure email users into clicking on a malicious link is a cat-and-mouse game, as phishing tactics evolve to evade detection. But machine learning models designed to recognize phishing attempts can evolve, too, through automatic retraining and checks to maintain accuracy.What’s new:Food giant Nestlé built a system that checks incoming emails and sends suspicious ones to the company’s security team. Microsoft’sAzure Machine Learningweb platform supplies the tools and processing power.Problem:Nestlé receives up to 20 million emails in 300,000 inboxes daily. An earlier commercial system flooded analysts with legitimate messages wrongly flagged as phishing attempts — too many to evaluate manually.Solution:The company built an automated machine learning system that continually learns from phishing attempts, spots likely new ones, and forwards them to security analysts.\n\nHow it works:The system comprises three automated pipelines that run in the cloud. The first manages training, the second evaluates incoming messages, and the third passes the latest risky messages to security.\n\nResults:The system detects malicious emails more quickly and accurately than its commercial predecessor. It flags phishing attempts with 60 percent precision. Previously, most of those would have been missed, the team said.Why it matters:Running several software pipelines continuously at high volume means a lot of moving parts in a critical application. Automating them and putting in place good tools and processes saves headaches and avoids security threats.We’re thinking:With machine learning models hard at work fighting phishing, machine learning engineers have more time to go fishing.\n\nA computer vision model, continually trained and automatically updated, can boost quality control in factories.What’s new:Landing AI, a machine learning platform company led by Andrew Ng, helped a maker of compressors for refrigeration check them for leaks. The manufacturer fills the compressor with air and submerges it in water while an inspector looks for telltale bubbles. Landing AI’s system outperformed the inspectors.Problem:When a visual inspection model detects a flaw where none exists, an engineer adds the example to the training set. When enough new examples have accrued, the engineer retrains the model, compares it with its predecessor and, if the new model shows improved performance, puts it into production — a laborious process that may introduce new errors.Solution:An automated machine-learning pipeline can accelerate all of these tasks and execute them more consistently.How it works:The Landing AI team aimed a camera at the water tank and sent the footage to aMIIVII Apex Xaviercomputer. The device ran a model that looked for bubbles and classified each compressor as okay or flawed, and a different model that watched indicator lights as an inspector activated a robot arm to place good compressors in one area and defective ones in another, and classified that decision.\n\nResults:After two months of iteration, the team put the system to a test. Of 50,000 cases in which the system expressed certainty, it disagreed with human experts in only five. It was correct in four of those cases. It was insufficiently certain to render a decision in 3 percent of cases, which required human decisions.Why it matters:Human inspectors are expensive and subject to errors. Shifting some of their responsibility to a machine learning system — especially one that performs better than humans — would enable manufacturers to reallocate human attention elsewhere.We’re thinking:A human-in-the-loop deployment that maintains a feedback loop between human experts and algorithms is a powerful way to learn — for both people and machines.\n\nWe’re thrilled to launch the first two courses in theMachine Learning Engineering for Production Specialization (MLOps)on Coursera! This specialization teaches foundational concepts of machine learning plus functional expertise of modern software development and engineering to help you develop production-ready skills.Enroll now\n\nDeploying models for practical use is an industrial concern that generally goes unaddressed in research. As a result, publications on the subject tend to come from the major AI companies. These companies have built platforms to manage model design, training, deployment, and maintenance on a large scale, and their writings offer insight into current practices and issues. Beyond that, a few intrepid researchers have developed techniques that are proving critical in real-world applications.\n\nThe High-Interest Credit Card of Technical Debt:The notion of technical debt — hidden costs incurred by building a good-enough system that contains bugs or lacks functionality that becomes essential in due course — is familiar in software development. The authors argue that machine learning’s dependence on external code and real-world data makes these costs even more difficult to discover before the bill comes due. They offer a roadmap to finding and mitigating them, emphasizing the need to pay careful attention to inputs and outputs, as changing anything — training data, input structure, external code dependencies — causes other changes to ripple through the system.\n\nTowards ML Engineering:Google offers this synopsis of TensorFlow Extended (TFX), a scaffold atop the TensorFlow programming framework that helps track data statistics and model behavior and automates various parts of a machine learning pipeline. During data collection, TFX compares incoming data with training data to evaluate its value for further training. During training, it tests models to make sure performance improves with each iteration of a model.\n\nThe Winding Road to Better Learning Infrastructure:Spotify built a hybrid platform using both TensorFlow Extended and Kubeflow, which encapsulates functions like data preprocessing, model training, and model validation to allow for reuse and reproducibility. The platform tracks each component’s use to provide a catalog of experiments, helping engineers cut the number of redundant experiments and learn from earlier efforts. It also helped the company discover a rogue pipeline that was triggered every five minutes for a few weeks.\n\nIntroducing FBLearner Flow:Facebook found that tweaking existing machine learning models yielded better performance than creating new ones. FBLearner Flow encourages such recycling company-wide, lowering the bar of expertise to take advantage of machine learning. The platform provides an expansive collection of algorithms to use and modify. It also manages the intricate details of scheduling experiments and executing them in parallel across many machines, along with dashboards for tracking the results.\n\nScaling Machine Learning as a Service:Models in development should train on batches of data for computational efficiency, whereas models in production should deliver inferences to users as fast as possible — that’s the idea behind Uber’s machine learning platform. During experimentation, code draws data from SQL databases, computes features, and stores them. Later, the features can be reused by deployed models for rapid prediction, ensuring that feature computation is consistent between testing and production.\n\nA Unified Approach to Interpreting Model Predictions:Why did the model make the decision it did? That question is pressing as machine learning becomes more widely deployed. To help answer it, production platforms are starting to integrate Shapley Additive Explanations (SHAP). This method uses an explainable model such as linear regression to mimic a black-box model’s output. The explainable model is built by feeding perturbed inputs to the black-box model and measuring how its output changes in response to the perturbations. Once the model is built, ranking the features most important to the decision highlights bias in the original model.\n\nWhen every email, text, or call a company receives could mean a sale, reps need to figure out who to reply to first. Machine learning can help, but using it at scale requires a highly automated operation.\n\nWhat’s new:Freshworks, which provides web-based software formanaging customer relationships, produces models that prioritize sales leads, suggest the best action to move toward a sale, and related tasks. The decade-old company rolls them out and keeps them updated with help from Amazon’sSageMakerplatform.\n\nProblem:To serve 150,000 sales teams that might be in any type of business and located anywhere in the world, Freshworks builds, deploys, and maintains tens of thousands of customized models. That takes lots of processing power, so the company needs to do it efficiently.\n\nSolution:Instead of training each model sequentially, Freshworks saves time by training them in parallel, as shown in the diagram above. Rather than retraining all models on fresh data weekly — as the company did previously — it evaluates performance continually and automatically retrains those that fall short. When a model isn’t needed, the server it runs on moves on to other jobs, saving costs.\n\nHow it works:Freshworks’ system trains and fine-tunes models to order for each client. It uses the client’s data if possible. Otherwise, it uses a model trained for the client’s industry, both industry and region, or both industry and language. The company’s user interface queries models through an API.\n\nResults:The automated system reduced training time from about 48 hours to about one hour. It boosted accuracy by 10 to 15 percent while cutting server costs by about 66 percent.\n\nWhy it matters:Show of hands: Who wants to build, deploy, and maintain thousands of models by hand? Automatically choosing architectures, training them, turning servers on and off, monitoring performance and data, and retraining when needed makes highly customized, highly scalable machine learning more practical and affordable.\n\nWe’re thinking:Accurate predictions of who might buy a product or subscribe ought to cut down on unwanted sales calls to the rest of us!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-05-11-at-4.05.36-PM-copy--1-.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-05-11T115346.256.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-05-11T115346.256.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/WILEY.gif?upscale=true&width=1200&upscale=true&name=WILEY.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/NESTLE.gif?upscale=true&width=1200&upscale=true&name=NESTLE.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-05-11T175336.819.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-05-11T175336.819.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20Image%201.png?upscale=true&width=1200&upscale=true&name=The%20Batch%20Image%201.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/READING.gif?upscale=true&width=1200&upscale=true&name=READING.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/FRESHWORKS.gif?upscale=true&width=1200&upscale=true&name=FRESHWORKS.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-81/",
    "title": "issue 81",
    "date": "",
    "reading_time": "",
    "content": "One of the most important skills of an AI architect is the ability to identify ideas that are worth working on. Over the years, I’ve had fun applying machine learning to manufacturing, healthcare, climate change, agriculture, ecommerce, advertising, and other industries. How can someone who’s not an expert in all these sectors find meaningful projects within them? Here are five steps to help you scope projects effectively.Step 1: Identify a business problem (not an AI problem).I like to find a domain expert and ask, “What are the top three things that you wish worked better? Why aren’t they working yet?” For example, if you want to apply AI to climate change, you might discover that power-grid operators can’t accurately predict how much power intermittent sources like wind and solar might generate in the future.Step 2: Brainstorm AI solutions.When I was younger, I used to execute on the first idea I was excited about. Sometimes this worked out okay, but sometimes I ended up missing an even better idea that might not have taken any more effort to build. Once you understand a problem, you can brainstorm potential solutions more efficiently. For instance, to predict power generation from intermittent sources, we might consider using satellite imagery tomap the locations of wind turbinesmore accurately, using satellite imagery to estimate the height and generation capacity of wind turbines, or using weather data to betterpredict cloud cover and thussolar irradiance. Sometimes there isn’t a good AI solution, and that’s okay too.Step 3: Assess the feasibility and value of potential solutions.You can determine whether an approach is technically feasible by looking at published work, what competitors have done, or perhaps building a quick proof of concept implementation. You can determine its value by consulting with domain experts (say, power-grid operators, who can advise on the utility of the potential solutions mentioned above).\n\nStep 4: Determine milestones.Once you’ve deemed a project sufficiently valuable, the next step is to determine the metrics to aim for. This includes both machine learning metrics such as accuracy and business metrics such as revenue. Machine learning teams are often most comfortable with metrics that a learning algorithm can optimize. But we may need to to stretch outside our comfort zone to come up with business metrics such as those related to user engagement, revenue, and so on. Unfortunately, not every business problem can be reduced to a matter ofoptimizing test set accuracy! If you aren’t able to determine reasonable milestones, it may be a sign that you need to learn more about the problem. A quick proof of concept can help supply the missing perspective.\n\nStep 5: Budget for resources.Think through everything you’ll need to get the project done including data, personnel, time, and any integrations or support you may need from other teams. For example, if you need funds to purchase satellite imagery, make sure that’s in the budget.This is an iterative process. If, at any step, you find that the current direction is infeasible, return to an earlier step and proceed with your new understanding.Is there a domain that excites you where AI might make a difference? I hope these steps will guide you in exploring it — even if you don’t yet have deep expertise in that field. AI won’t solve every problem, but as a community, let’s look for ways to make a positive impact wherever we can.\n\nKeep learning!\n\nAndrew\n\nHaving dismissed two key researchers, Google restructured its efforts in AI ethics.What’s new:Marian Croak, anaccomplishedsoftware engineer and vice president of engineering at Google, will lead a new center of expertise in responsible AI, the companyannounced. The move came amid uproar over the exits of her predecessors Timnit Gebru and Margaret Mitchell.What happened:Google’s Ethical AI group has been in flux since last December when Gebru, the group’s technical co-lead with Mitchell, left the company. Gebru says she was fired, while Google’s latest communiqué refers to Gebru’s “exit.”\n\nBehind the news:In December, Gebru was on the verge of publishing apaperthat criticized large language models including Google’s own BERT.Executivesasked her to either retract the paper or remove the names of all Google co-authors.\n\nWhy it matters:Google is a leader in AI and one of the most powerful companies in the world. Its approach to ethical challenges — and its treatment of employees — are highly influential throughout the tech industry.We’re thinking:Under Gebru and Mitchell, Google’s Ethical AI team developedtools to improve model transparency, examined howsocial constructs of race manifest in AI, and released aframework for identifying risks posed by models in development. We hope the people who carry on this work will pursue similarly ambitious projects.\n\nThe pandemic has pushed hospitals to their limits. A new machine learning system could help doctors make sure the most severe cases get timely, appropriate care.What’s new:Anuroop Sriram, Matthew Muckley, and colleagues at Facebook, NYU School of Medicine, and NYU Abu Dhabi developed asystemthat examines X-ray images to predict which Covid-19 patients are at greatest risk of decline.Key insight:Previous methodsassess Covid risk based on a single chest X-ray. But when making assessments, clinicians often look for relative changes between successive X-rays to determine whether a patient’s condition is improving or deteriorating. The researchers used consecutive X-rays to improve risk assessment.How it works:The authors trained their system to predict the probability that a patient would die, require intubation, need intensive care, or need more oxygen over the next 24, 48, 72, or 96 hours.\n\nResults:The system achieved a mean AUC (area under the curve, a measure of true versus false positives where 1 is a perfect score) of 0.785, 0.801, 0.790, and 0.790 when predicting adverse outcomes at 24, 48, 72, and 96 hours into the future, respectively. Those scores were comparable to those of two clinicians who achieved an average AUC of 0.784, 0.787, 0.761 and 0.754.Why it matters:Pretraining followed by fine-tuning opens up important applications where data is too scarce for simpler learning approaches.We’re thinking:The pandemic has been anearly testof AI’s utility in medicine. The record so far has beenmixed, but we’re glad to see research that shows promising results for both fighting Covid and improving healthcare in general.\n\nSmart glasses in the works at Facebook may be equipped with face recognition.What’s new:The social media colossus plans to market augmented-reality headgear, and it’s considering a feature that would overlay a person’s name on their face, according toBuzzfeed.What’s happening:Announced by Mark Zuckerberg in 2017 (shown in the video clip above), the glasses are set to drop this year. Some capabilities — including face recognition — may be added later, Facebook vice president Andrew Bosworth toldBloomberg News.\n\nBehind the news:Facebook’s smart glasses, which will be manufactured by Ray-Ban, will compete against SnapchatSpectaclesand GoogleGlass(lately refocused from consumer to enterprise applications).Why it matters:Wearable hardware that recognizes faces raises serious questions about privacy. Facebook has an incentive to tread carefully: It was theleast trustedof nine major social media platforms in a recent survey.We’re thinking:A Facebook foray into mass-market face recognition could force U.S. lawmakers finally to issue rules on how the technology can and can’t be used.\n\nJoin us for “Applied Analytics From End-to-End,” anAI Accessconversation with Zachary Hanif, senior director of machine learning at Capital One. Zachary will explore core challenges and show how distributed data engineering and a product-oriented mindset can make an impact.RSVP\n\nThe transformer architecture has inspiredaplethoraofvariations. Yet researchers have used a patchwork of metrics to evaluate their performance, making them hard to compare. New work aims to level the playing field.\n\nWhat’s new:Yi Tay and colleagues at Google developedLong-Range Arena, a suite of benchmarks designed to standardize comparisons between transformers. The termlong-rangerefers to transformers’ ability to capture dependencies between tokens in an input sequence that are far apart.\n\nKey insight:The power of theoriginal transformerlies in its ability to track relationships between tokens anywhere in an input sequence. But that power comes at a cost: The model slows down and its memory requirement rises dramatically as the length of its input increases. Many variants were created to alleviate this issue. These models cry out for tests that challenge their ability over long sequences.\n\nHow it works:The suite comprises six tests designed to probe different aspects of transformer behavior.\n\nResults:The authors tested 10 transformers. While some shined in one task or another, none was the clear front runner.Big Birdachieved the highest average accuracy – 55.01 percent across five tasks — but it didn’t achieve the top score in any single task.Performer, dominated character-level text classification, performing 5.7 times faster than a vanilla transformer.Linformerused the least memory, 9.58 times less than the vanilla transformer. All models failed Pathfinder-X: Their ability to classify the image was no better than random chance, showing that longer input sequences inhibited performance.\n\nWhy it matters:Standardized comparisons not only help application developers choose the right model for their needs, they also provide a deeper understanding of a model’s performance characteristics and spur researchers to improve the state of the art.\n\nWe’re thinking:You can get involved, too. The team open sourced its work and encourages others to contribute to the Long-Range Arenaleaderboard.\n\nMachine learning is spreading from big corporations to smaller companies, and many of its practitioners are relatively new to the technology.\n\nWhat’s new:Almost one in five data scientists active on Kaggle, which hosts machine learning competitions, have been in the field less than one year, according to the company’s latestState of Data Science and Machine Learningsurvey. The report covers demographics and employment as well as popular platforms, frameworks, applications, and techniques. Thedataincludes answers from 200,000 people.\n\nWhat they found:The report tallies responses by 2,675 Kaggle users who identified themselves as employed data scientists.\n\nBehind the news:Users of the employment site Glassdoor consistentlyrankdata scientist as one of America’s best jobs, citing good pay and working conditions. But just because workers are happy doesn’t mean they’re sitting still. About a third of the engineers who responded toAnaconda’s 2020 State of Data Science survey said they plan to look for a new job in the coming year. The expected turnover is highest in IT, where 44 percent of data scientists either are actively looking for new employment or plan to do so soon.Why it matters:This survey underlines how data science is diffusing, not only among businesses but among nations. It highlights trends that hiring managers, among others, should bear in mind, including the field’s ongoing gender imbalance.We’re thinking:All those newcomers to data science represent a huge pool of fresh ideas and new talent coming in to the field.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/Screen-Shot-2021-03-03-at-4.34.01-PM-copy.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/TIMNIT.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/COVID.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/ezgif.com-optimize-10.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/The-Batch-Image-3-1-1024x576.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/BENCHMARK.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/ezgif.com-optimize-6.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-25/",
    "title": "issue 25",
    "date": "",
    "reading_time": "",
    "content": "Many of us apply labels to ourselves that shape our identity. Some say, “I’m a sports fan,” and this attitude motivates behaviors such as cheering for the home team. Others identify themselves as introverts, extroverts, vegetarians, gamers, athletes, scientists, and/or engineers. Each label implies its own set of habits and activities.\n\nI think it’s time for more of us to identify ourselves as life-long learners. To me, a life-long learner:\n\nThis is the best way to keep growing over your entire lifetime. I’ve seen numerous people proactively learn about new technologies or gain skills in everything from product management to personal health, and develop as individuals as a result. They seem happier, and I’m sure they contribute more to their communities.\n\nEvery weekend I spend several hours reading or taking online courses. This learning helps me do my work better, but I enjoy it so much that I’d do it even if it didn’t affect my work at all.\n\nThe world is changing faster than ever, driven by technological change. So humanity needs a lot more lifelong learners to make sure we keep up. I hope you’ll join me in proudly telling others, “I’m a lifelong learner!”\n\nKeep learning,Andrew\n\nRobots are moving into a job that traditionally required the human touch.What’s new:A commercial warehouse that ships electrical suppliesdeployedAI-driven robotic arms from Covariant, a high-profile Silicon Valley robotics firm. Trained using a hybrid of imitation and reinforcement learning, the new machines are far better than earlier bots at sorting items into boxes.How it works:Robots have been picking objects off conveyor belts for years, but they generally handle only identical items. Covariant’s approach, which uses a single neural network for all objects, enables an arm equipped with a camera and suction gripper to manipulate around 10,000 different items (and counting). The system can share skills with other arms, including those made by other companies.\n\nBehind the news:Co-founded by UC Berkeley AI professor Pieter Abbeel (watch our interview with himhere), Covariant has raised $27 million from backers including deep learning pioneers Yann LeCun and Geoffrey Hinton as well as Google AI chief Jeff Dean.Why it matters:More than half of warehouse logistics companies could facelabor shortagesin the next five years, thanks to the job’s tedium and low wages. Market analysts expect automatons to pick up the slack.We’re thinking:Will robots figure out how to ship a RAM stick without a cubic meter of styrofoam peanuts in a box the size of a washtub?\n\nAndroids may not dream of electric sheep, but some crack jokes about horses and cows.What’s new:Meena, a 2.6-billion parameterchatbotdeveloped by Google Brain, showed impressive conversational ability, discussing a variety of topics. In one exchange, it unexpectedly sprinkled in some barnyard humor: It commented that “horses go to Hayvard,” not Harvard. The phrase didn’t appear in the training data, but the word Hayvard did appear once, a pun after a mention of horses, according to a company spokesperson. The bot followed up with another farm-animal pun, noting that its interlocutor tried “to steer [the conversation] elsewhere.” The first-ever AI-generated dad joke?How it works:Google engineer Daniel De Freitas Adiwardana told us how, in training on 341 gigabytes of public social media conversation, Meena might have developed a sense of humor. Straight from the horse’s mouth, as it were:\n\nBehind the news:Efforts to give AI a sense of humor have met with limited success. One of the most impressive is a jokebot thatwritescaptions for cartoons inThe New Yorker. That model separates the normal aspects of a picture (for instance, a salesman showing off a new car) from oddball elements (the car has cat-like legs), then writes a caption based on the contrast. But even its best efforts (“just listen to that baby purr”) are more intriguing — because they were written by a computer — than side-splitting.Why it matters:Humoraffectsus in deep and subtle ways. A Tina Fey-level bot may be out of reach, but a funny bone would be a valuable feature in an empathetic AI.We’re thinking:We’d like to see a football game between Hayvard and Dartmooth.\n\nA drug designed by AI has been approved for testing in humans.What’s new:A UK startup focused on automated drug discovery teamed up with a Japanese pharmaceutical company to produce a new medicine for obsessive compulsive disorder. The compound, known as DSP1181, is designed to take effect more quickly and last longer than existing treatments. Japanese authoritiesclearedit for a clinical trial.How it works:Exscientia’s drug-discoveryplatformcan start with a biological target known to influence a particular medical condition.\n\nWhy it matters:Pharmaceutical companies invest upward of$2.6 billionto develop a single drug, and it can take three to six years tofinda compound that’s viable for testing in humans— with no guarantee that it will prove safe and effective. Automating even small parts of the process can save big money. That’s one reason why Exscientia is one of nearly 200 companies worldwide using AI to find new drugs.We’re thinking:AI is no magic bullet for drug discovery. But cutting the enormous cost of development would enable pharma companies to study more molecules and potentially to bring more medicines to market.\n\nTest your image classification models with your phone’s camera! Learn how to deploy models with TensorFlow Lite in Course 2 of the TensorFlow: Data and Deployment Specialization.Enroll now\n\nA protein’s biological function depends largely on its three-dimensional shape, but deducing its shape from its sequence of amino acids has been a longstanding problem. Researchers at DeepMind reveal how they used deep learning to solve the puzzle.What’s new:Andrew Senior and colleagues released long-awaited details aboutAlphaFold, a protein-folding model that wowed experts in a high-profile competition in late 2018. The paper is behind a paywall. Thisvideooffers some details.Key insight:Research has shown that protein shapes are determined by the proximity of essential portions, or residues, of amino acids. The researchers found likely shapes by optimizing over possible structures that keep residues close to one another. Earlier methods predict whether residues are in contact with one another. AlphaFold predicts the distances and angles between residues, making the optimization easier.How it works:AlphaFold extracts features from an input protein sequence, predicts relationships between residues, and uses those predictions to find the protein’s likely shape.\n\nResults:At the 2018 CASP13 conference, AlphaFold predicted 24 out of 43 previously unknown protein shapes with high accuracy. The next-best model achieved 14 predictions of similar accuracy.Why it matters:The ability to determine protein structures could have wide-ranging impacts on drug discovery, countering neurodegenerative diseases, and more. Stay tuned for further progress when CASP14 convenes in April.We’re thinking:Hard problems don’t always offer enough training data to train an end-to-end neural network. In this case, combining a physical model with neural networks led to significant progress. This design pattern holds promise in many other domains from climate change to robot dynamics.\n\nReal-time face recognition has become standard operating procedure for cops in a few cities, in both authoritarian and democratic countries.What’s new:After years of trials, police departments inMoscowandLondonare using face recognition to scan the streets for suspected criminals.How it works:Systems in both cities connect to pre-existing closed-circuit television networks. Enforcers in Moscow aim to deploy the tech city-wide, according toThe Verge. So far, though, they’re using only a fraction of the city’s tens of thousands of cameras. London plans a more limited rollout in popular shopping and tourist areas.\n\nWhy it matters:Face recognition technology is becoming routine for police forces around the globe. It has been used to catch a murderer inChongqing, helped stop street crime inNew York City, and figured in 30 percent of solved cases in one small U.S.city.Yes, but:Independent researchers evaluating recent trials in London found that the systemmisidentified81 percent of suspects it flagged. The police department contests those numbers, saying its own studies show only one in 1,000 false positives.We’re thinking:Law enforcement agencies worldwide need thoughtfully designed and clearly worded regulatory guidance so they can use these tools without overstepping civil liberties.\n\nNeural audio synthesizers likeWaveRNNorGANSynthproduce impressive sounds, but they require large, data-hungry neural networks. A new code library beefs up the neural music studio with efficient sound modules based on traditional synthesizer designs.What’s new:Jesse Engel and colleagues at Google Brain introducedDifferentiable Digital Signal Processing(DDSP), a set of digital signal processing tools that integrate with neural networks to boost their performance.Key insight:Traditional synthesizers incorporate powerful sound-generation and -processing tools, but their controls are often limited to sliders and switches that don’t take full advantage of their abilities. A neural network can learn to manipulate such tools more dynamically, potentiallyproducingmore realistic renditions of existing instruments as well as novel sounds.How it works:DDSP offers tools such as oscillators (which generate sound), filters (which modify tone color), envelopes (which shape the sound over time), and reverberators (which mimic sound waves that reflect off walls). Most are implemented as layers that can be inserted into neural networks without affecting backprop training, so a network can learn to control them.\n\nResults:The SMS emulator showed that DDSP can make for a high-quality neural sound generator. Compared to WaveRNN, it scored better for L1 loudness loss, a measure of the difference betweenaudio inputand synthesized output (.07 compared to .10). It also had a better L1 loss of fundamental frequency, which measures the accuracy of the synthesized waveform relative to the input (.02 versus 1.0). And it has one tenth as many parameters!Why it matters:Audio synthesis is one of several applications migrating from digital signal processing tech to deep learning. Machine learning engineers need not leave the older technology behind — they can build DSP functions into their neural networks.We’re thinking:The SMS demo is preliminary, but it points toward next-generation audio models that combine deep learning with more intuitive structures and controls.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Andrews20Letter20ASPECT201.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Robots.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Joke20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Drugs20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Proteins20320ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/London20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/DDSP20ASPECT.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-247/",
    "title": "issue 247",
    "date": "",
    "reading_time": "",
    "content": "Inexpensive token generation and agentic workflows for large language models (LLMs) open up intriguing new possibilities for training LLMs on synthetic data. Pretraining an LLM on its own directly generated responses to prompts doesn't help. But if an agentic workflow implemented with the LLM results in higher quality output than the LLM can generate directly, then training on that output becomes potentially useful.\n\nJust as humans can learn from their own thinking, perhaps LLMs can, too. For example, imagine a math student who is learning to write mathematical proofs. By solving a few problems — even without external input — they can reflect on what does and doesn’t work and, through practice, learn how to more quickly generate good proofs.\n\nBroadly, LLM training involves (i) pretraining (learning from unlabeled text data to predict the next word) followed by (ii) instruction fine-tuning (learning to follow instructions) and (iii) RLHF/DPO tuning to align the LLM’s output to human values. Step (i) requires many orders of magnitude more data than the other steps. For example,Llama 3was pretrained on over 15 trillion tokens, and LLM developers are still hungry for more data. Where can we get more text to train on?\n\nMany developers train smaller models directly on the output of larger models, so a smaller model learns to mimic a larger model’s behavior on a particular task. However, an LLM can’t learn much by training on data it generated directly, just like a supervised learning algorithm can’t learn from trying to predict labels it generated by itself. Indeed, training a model repeatedly on the output of an earlier version of itself can result inmodel collapse.\n\nHowever, an LLM wrapped in anagentic workflowmay produce higher-quality output than it can generate directly. In this case, the LLM’s higher-quality output might be useful as pretraining data for the LLM itself.\n\nEfforts like these have precedents:\n\nA significant barrier to using LLMs prompted via agentic workflows to produce their own training data is the cost of generating tokens. Say we want to generate 1 trillion tokens to extend a pre-existing training dataset. Currently, at publicly announced prices, generating 1 trillion tokens using GPT-4-turbo ($30 per million output tokens), Claude 3 Opus ($75), Gemini 1.5 Pro ($21), and Llama-3-70B on Groq ($0.79) would cost, respectively, $30M, $75M, $21M and $790K. Of course, an agentic workflow that uses a design pattern likeReflectionwould require generating more than one token per token that we would use as training data. But budgets for training cutting-edge LLMs easily surpass $100M, so spending a few million dollars more for data to boost performance is quite feasible.\n\nThat’s why I believe agentic workflows will open up intriguing new opportunities for high-quality synthetic data generation.\n\nKeep learning!\n\nAndrew\n\nP.S. In “Prompt Engineering for Vision Models,” taught by Abby Morgan, Jacques Verré, and Caleb Kaiser of Comet, you’ll learn how to prompt and fine-tune a variety of vision models for image generation, image editing, object detection, and  segmentation. For example, you’ll use OWL-ViT to detect an object you describe in a text prompt, pass the bounding box to SAM to create a segmentation mask, and feed the mask into Stable Diffusion with a text prompt to replace the original object with a new one. Controlling vision models can be tricky, and this course will teach you the techniques to control their output. Get startedhere!\n\nApple is thinking small — very small — with a new family of open large language models.\n\nWhat's new:Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, and colleagues at Apple releasedOpen Source Efficient LLM(OpenELM), a family of smaller large language models. OpenELM ranges from 270 million parameters — plenty small enough to fit on a phone — to 3 billion parameters.\n\nHow it works:OpenELM comes in pretrained and instruction-tuned versions with parameter counts of 270 million, 450 million, 1.1 billion, and 3 billion. They can process 2,048 tokens of context. Thereleaseincludes weights, code for training and inference, and code for running the models on Apple chips.\n\nResults:OpenELM beat a number of other open-source models trained solely on publicly available data.\n\nWhy it matters:After years of becoming only larger, neural networks lately have also been getting smaller. The smallest OpenELMs are tiny compared to, say, Microsoft’s Phi-3-mini. Apple has an extra incentive to make models capable of running on edge devices like phones. The company makes a major selling point of user privacy, and models run entirely on a smartphone (as opposed to in the cloud) keep the user’s activity under wraps.\n\nWe're thinking:DeLighTintroduced this layer-scaling approach in 2020. Sometimes it takes a while for good ideas to catch on!\n\nMore expensive models, superhuman performance, growing impacts on society — an extensive report takes stock of developments in machine learning over the past year.\n\nWhat's new:Stanford’s Institute for Human-Centric AIpublishedthe seventh “AI Index Report,” its annual overview of the state of AI. The report documents rising costs and capabilities, a shift from academic to corporate dominance, and the public’s anxiety as the technology becomes ever more embedded in daily life.\n\nThemes and findings:The 500-page report collates a wide variety of papers, benchmarks, market research, and surveys published in 2023. It delves deeply into AI technology, economics, governance, and impact. Among its key conclusions:\n\nBehind the news:The differences between the new one and the initial, 2018 edition highlight the field’s rapid pace of change. For instance, the 2018 report opened by trumpeting the nearly 9x growth of AI research papers published between 2000 and 2017. The new one opened not with the annual rate of research publications (though it has roughly doubled since 2017) but with a graph of industry’s growing dominance in innovation.The Batchhascoveredseveral editions.\n\nWhy it matters:The “AI Index Report” offers a detailed snapshot of AI as it advances at an unprecedented rate and shows potential to revolutionize virtually every field of human endeavor. It dives deeply into areas of special concern to researchers (such as Gemini’s nearly $200 million training cost), practitioners (for instance, the slightly narrowing gender gap among computer science PhDs), businesses (the sharply rising number of regulations), and users (half of those who are aware of ChatGPT use it weekly). This year’s report includes new emphases on public opinion and geopolitics.\n\nWe're thinking:It’s heartening to see AI thriving. The field faces daunting challenges, yet the report highlights achievements in foundation models, science, medicine, and elsewhere that portend greater benefits directly ahead. What an exciting time for AI!\n\nExpand your prompting skills with our new short course, “Prompt Engineering for Vision Models.” Learn how to prompt and fine-tune vision models to accomplish tasks from image generation to object detection.Start learning today\n\nAmazon is removing grab-and-go shopping from its cart.\n\nWhat’s new:Amazon withdrew Just Walk Out, an AI-driven checkout service, from most of its Amazon Fresh grocery stores,The Informationreported. Instead, the stores will provide smart shopping carts. (Disclosure: Andrew Ng is a member of Amazon’s Board of Directors.)Checking out:Just Walk Outenables shoppers to scan a payment method upon entering a store, take items from shelves tracked by computer vision and weight-detection sensors, and simply exit with their purchases, bypassing the checkout counter. Amazon had installed the system in 47 Amazon Fresh stores in the U.S. and UK. In most of those locations. Amazon will replace Just Walk Out withDash Cart, a shopping cart that enables customers to scan purchases as they shop. Amazon will retain Just Walk Out in its Amazon Go convenience stores and an unspecified number of smaller, UK-based Amazon Fresh stores. It has licensed the system to other retailers including Hudson Markets and plans to install in more third-party stores this year.\n\nBehind the news:AmazonintroducedJust Walk Out in 2016 at its first Amazon Go convenience store in Seattle. Itextendedthe system to Amazon Fresh in 2020. Between September 2020 and September 2022, Amazon opened 44 Fresh stores in the U.S. and 19 in the UK, most of which included Just Walk Out. But Amazon’s brick-and-mortar locationssufferedduring the COVID-19 pandemic. From September 2022 to mid-2024, amid broader cost-cutting efforts, the companypausedopening new grocery stores.\n\nWhy it matters:Grab-and-go shopping seems like a solid bet, given the increasing focus of retailing on immediate gratification. Yet Amazon’s retreat from Just Walk Out in larger stores suggests that the technology is less well suited to such environments. In addition, shoppers may not have adjusted easily to grab-and-go behavior, which removes social interactions with cashiers and encourages customers to spend without reviewing the bill.\n\nWe’re thinking:AI has the potential to revolutionize every field, including retailing, and it’s important to find productive uses for it. Not all experiments will succeed, but patient investment and experimentation can illuminate productive paths forward.\n\nA new AI method directs scientists toward promising avenues of inquiry.\n\nWhat's new:Jamshid Sourati and James A. Evans at University of Chicago proposed amethod to predict new scientific discoveriesby building a graph that connects researchers, their objects of study, and the scientific properties thereof. They evaluated their approach using data from materials science.\n\nKey insight:Overlapping interests among researchers may indicate areas where further research would be fruitful. For example, if one group of researchers studies a material A and its property P, a second group studies materials A and B, and another group studies materials B and C, it may turn out that material C exhibits property P.\n\nHow it works:The authors tried to predict whether certain inorganic materials have certain electrical properties based onscientific literaturethrough the year 2000. From 1.5 million articles that described 100,000 inorganic compounds, they extracted the author names, materials mentioned (for example, sodium nitrite), and their properties (for example, thermoelectricity, the ability to convert heat into electricity and vice versa). They used this data to construct a graph whose nodes were authors, materials, and properties. Edges connected the nodes that appeared in the same paper, for example a particular author whose paper covered specific material or property.\n\nResults:The authors predicted which materials possessed each of three properties. They compared their results with predictions obtained in a similar way using a Word2Vec model trained exclusively on text from scientific papers. They used papers from 2001 through 2018 to evaluate the predictions. For thermoelectricity, the cumulative precision (percentage of predicted discoveries proven correct) was 76 percent, while the cumulative precision of the alternative method was 48 percent. The cumulative precision of random guesses was about 3 percent. The authors obtained similar results for the other two properties.\n\nWhy it matters:Science is a social endeavor, where the connections between people and their work can be represented as a graph that reflects the collective attention of the scientific community. The collective attention acts as a signal that predicts promising avenues for further research — a signal that machine learning can help to tease out.\n\nWe're thinking:The authors also predicted drug discoveries with similarly good results. Their method may be useful for identifying fruitful directions in other scientific areas, and perhaps in other domains entirely.\n\nThis week'sData Pointsfeatures these highlights: Adobe's latest Firefly Image 3 model, enhanced smart glasses with Meta’s AI assistant, an AI-powered gene editor, and more.\n\nCatch up on the latest in AI now.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed--57-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-01T152036.759.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/05/AIINDEX_1200px_14secHolds--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/05/The-Batch-ads-and-exclusive-banners---2024-04-29T162119.517.png",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-01T152511.553.png",
      "https://dl-staging-website.ghost.io/content/images/2024/05/SCIENCE-FIX.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-217/",
    "title": "issue 217",
    "date": "",
    "reading_time": "",
    "content": "Andrej Karpathy, one of theHeroes of Deep Learningwho currently works at OpenAI,quipped, “The hottest programming language is English.” While I appreciate the sentiment, I don’t want the ease of instructing computers in English to discourage anyone from learning to code. Someone who is multilingual — who perhaps speaks English as a first language and Python as a second language — can accomplish much more than someone who knows only how to prompt a large language model (LLM).It’s increasingly possible to tell a computer what you want in English (or whatever human language you’re most fluent in) and it will understand well enough to give you what you asked for. Even before LLMs, Siri and Alexa could respond to basic commands, and the space of English instructions that computers can follow is rapidly expanding. But coding is still immensely valuable. If anything, with the advent of LLMs, the value of coding is rising. Let me explain why.Today, almost everyone has data: big companies, small companies, and even high school students running biology experiments. Thus, the ability to get a custom AI system to work on your own data is valuable. And while prompting an LLM can produce answers for a huge range of questions and generate everything from essays to poems, the set of things you can do with coding plus prompting is significantly larger, for now and the near future.Let’s say I want a summary of every letter I’ve ever written in The Batch. I can copy-paste one letter at a time into an LLM like ChatGPT and ask for a summary of each, but it would be much more efficient for me to write a simple piece of code that iterates over all letters in a database and prompts an LLM to create summaries.\n\nIn the future, I hope recruiters will be able to write a few lines of code to summarize candidate reviews, run speech recognition on conversations with references, or execute whatever custom steps are needed in the recruiting workflow. I hope teachers will be able to prompt an LLM to generate learning tasks suited to their lesson plan, and so on. For many roles, coding + prompting will be more powerful than prompting via a web interface alone.\n\nFurthermore, English is ambiguous. This contributes to why an LLM’s output in response to a prompt isn’t fully predictable. In contrast, most programming languages are unambiguous, so when you run a piece of code, you reliably (within reason) get back the same result each time. For important applications where reliability is important — say, deciding when to purchase an expensive plane ticket based on real-time prices, or sending a party invitation to everyone in your company — it’s safer to use code to carry out the final step committing to the action, even if an LLM were involved in researching destinations or drafting the invitation.\n\nI believe we’re entering an era when everyone can benefit by learning to code. LLMs have made it more valuable than ever. Writing code that calls an LLM has made it easier to build intelligent applications than it was before LLM APIs became widely available. Specifically, everyone can benefit by learning to code AI applications, as Iwrotewith Andrea Pasinetti, CEO of Kira Learning, an AI Fund portfolio company.\n\nIf you don’t yet code, consider taking a Python course to get started. If you already code, I hope you will encourage others to take up this skill. This is a good time to help everyone learn to speak Python as a second language!\n\nKeep learning,\n\nAndrew\n\nScreenwriters and movie studios reached a landmark agreement that restricts uses of AI to produce scripts for television and movies.\n\nWhat’s new:The Writers Guild of America (WGA)negotiateda new three-year contract with the Alliance of Motion Picture and Television Producers (AMPTP), ending a strike that began in May. The contract allows both writers and studios to use AI within certain restrictions.\n\nHow it works:WGA members went onstrikepartly over concern that studios would use AI to replace screenwriters. Thecontractincorporates many of their demands.\n\nThe actors’ strike continues:In July, the Screen Actors Guild (SAG-AFTRA) also went onstrikeciting similar concerns. Many actors fear that studios will use generated replicas of performers, undercutting their compensation and credits.\n\nWhy it matters:The writers’ agreement is a landmark deal in a high-profile industry. It could serve as a template not only for actors but also workers in other creative industries including publishing, music, graphics, gaming, and software development.\n\nWe’re thinking:Generative AI is making many industries and individuals more productive. The new contract protects writers for three years while leaving space for both writers and studios to experiment with ways to do that in film and television. We hope that this agreement is followed by one that focuses on growing the pie — creating more great movies with less effort — while addressing how to divide the larger pie fairly among writers, studios, and technologists.\n\nAmazon cut a multi billion-dollar deal with AI startup Anthropic, giving it a powerful ally in the generative arms race.\n\nWhat’s new:Amazoncommittedto investing as much as $4 billion in Anthropic. In return, Amazon Web Services (AWS) became the primary provider of Anthropic’s Claude and other models.\n\nHow it works:Amazon willinvest$1.25 billion in Anthropic immediately. Amazon may invest an additional $2.75 billion depending on undisclosed conditions. Amazon gained an undisclosed minority stake in the startup but not a seat on the board of directors. Other terms were not disclosed.\n\nBehind the news:Founded in 2021 by ex-OpenAI employees, Anthropic is an independent research lab thatfocuseson building safe, beneficial AI models. Having received hundreds of millions of dollars fromGoogleand other investors, it became one of the industry’s most highly funded startups. It wasvaluedat $4.1 billion in March.\n\nWhy it matters:Competition around generative AI is white-hot. Cloud providers need to offer cutting-edge models, while AI startups need access to processing power. Microsoft Azure paired up with OpenAI. Google has strong internal generative capabilities. That leaves Amazon as a natural partner for Anthropic.\n\nWe’re thinking:Which other high-profile AI startups would make dance partners for enterprising cloud providers? Topping the list are AI21 Labs (already working with Amazon Bedrock), Cohere (also available on Bedrock), and Inflection (funded by Microsoft).\n\nLearn the best practices for finetuning large language models and customize them with real-world data sets in our short course, “Finetuning Large Language Models.”Enroll for free\n\nYouTube is reinventing itself for the era of generative AI.\n\nWhat’s new:The Google-owned video platform isaddinggenerated topic ideas, backgrounds, music suggestions, and audio translations. These capabilities will be available in late 2023 or early 2024.\n\nHow it works:The new features are designed to assist video producers in planning, designing, and publishing their works.\n\nMeanwhile, at TikTok:YouTube rival TikTokrequiresusers to clearly label synthetic videos that depict realistic scenes. The guidelines also prohibit synthetic likenesses of private individuals (public figures are allowed unless they are the subject of abuse or misinformation). To help contributors comply, the companyannounceda tool that enables uploaders to manually label their videos as “AI-generated.” TikTok is also testing a system that detects AI-generated or AI-edited elements in a video and automatically adds the label.\n\nWhy it matters:YouTube depends on crowdsourced content. Generative tools could make the platform’s contributors more productive, attracting more viewers and boosting revenue all around.We’re thinking:While generative tools may engage the crowd, generated content that’s as compelling as human-produced content could upend YouTube’s business.\n\nHow can we build large-scale language and vision models that don’t inherit social biases? Conventional wisdom suggests training on larger datasets, but research challenges this assumption.\n\nWhat’s new:Abeba Birhane at Trinity College Dublin, a colleague at Michigan State University, and two independent researchersanalyzedpublicly available text-image datasets for their proportion of hateful content (that is, content that belittles based on race or gender) and audited models trained on them for racial bias. They found that larger training sets can push models toward greater bias.\n\nKey insight:The largest available datasets of text and images are collected indiscriminately, with little curation after the fact. Removing objectionable material from such immense corpora is challenging. Researchers often rely on automatic filters like theCLIPsimilarity between images and text to filter out bad data. To create larger datasets, they often relax those filters. Consequently, larger datasets can harbor a higher proportion of objectionable material than smaller datasets, and training on them could yield models whose performance is more biased.\n\nHow it works:The authors compared hateful language inLAION 400M, which comprises 400 million image-text pairs scraped from the web, to similar data inLAION 2B-en, which includes 2 billion image-text pairs also scraped from the web. They also analyzed racial biases present in models trained on both datasets.\n\nResults:The authors found a statistically-significantly lower proportion of hateful content in the smaller dataset. LAION-400M’s HCR in the “hateful” category was up to 0.1 percent lower relative to LAION-2B. The probability that a model would classify a face as “human being” fell from 18.6 percent for OpenCLIP-400M to 9.4 percent for OpenCLIP-2B, and the probabilities of classification as “criminal” and “suspicious person” rose. OpenCLIP-400M classified a portrait of a black man as a criminal 14 percent of the time, while OpenCLIP-2B did so 77.4 percent of the time. Despite the increase in biased classifications, OpenCLIP-2B achieved 1.5 percent higher accuracy on ImageNet.\n\nWhy it matters:Increasing numbers of open source models and consumer-facing products are trained on large, web-scraped datasets. For example, Stable Diffusion wastrainedlargely on the 5B version of LAION. This work throws up a red flag for machine learning practitioners to consider the bias such training can impart, the harm such models might do, and the methods used to collect and curate large datasets.\n\nWe’re thinking:This work goes to show that data-centric AI is applicable even to the largest datasets. It's easier to focus on higher-quality data sources when collecting 400 million examples than 2 billion examples.\n\nGoogle introduced a way for publishers to opt-out from training AIThe Google-Extended control setting allows website administrators to manage access to their data and other content. Now publishers can choose whether to contribute to the refinement of AI tools like Bard and Vertex AI. (Google)IBM launched generative AI models and bolsters intellectual property protectionThe company announced the general availability of the watsonx Granite model series, a collection of models designed to enhance generative AI integration into business operations, and the extension of standard intellectual property protections to all watsonx AI models. (IBM)Google accidentally leaked Bard conversations in public search resultsURLs linking to users’ interactions with Bard were indexed by Google's search engine, allowing anyone to access them. While some defended the incident by noting that individuals chose to share links to their chats with each other, others argued that users expected their exchanges to be private. Google acknowledged the issue to be an error. (Fast Company)Getty Images released an image generator trained on licensed images\"Generative AI by Getty Images,\" a tool powered by NVIDIA and exclusively trained on Getty Images' creative content library, promises to ensure usage rights for AI-generated content, including royalty-free licenses, indemnification, and perpetual worldwide usage. (Getty)Cloudflare introduced a set of tools for streamlined model deploymentThe suite includes \"Workers AI,\" which allows users to access nearby GPUs hosted by Cloudflare's partners for AI model execution on a pay-as-you-go basis, \"Vectorize,\" which provides a database for storing vector embeddings generated by AI models, and \"AI Gateway,\" which offers observability and cost management features for AI applications. (TechCrunch)The European Central Bank (ECB) is experimenting with AI for basic operationsThe ECB's pursuit of technological advancements also attempts to address concerns related to reliability, transparency, and legal implications associated with AI use. The institution is collaborating with other major central banks, including the Federal Reserve, the Bank of England, and the Monetary Authority of Singapore. (Financial Times)The Central Intelligence Agency (CIA) developed its own chatbotThe CIA's Open-Source Enterprise division is intended to help intelligence analysts sift through vast amounts of open source intelligence for quicker data dissemination. The initiative is part of a broader government effort to harness AI capabilities (and compete with China). (Bloomberg)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-webp-to-jpg--15-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-webp-to-jpg--16-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/10/ANTHROPIC-delivery_bluesky-dith_1200px.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/10/The-Batch-ads-and-exclusive-banners--67-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-gif-maker--2-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-webp-to-jpg--17-.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-142/",
    "title": "issue 142",
    "date": "",
    "reading_time": "",
    "content": "It's official: Elon Musk will buy Twitter, pending approval of the transaction by the company's stockholders and the U.S. government. While some people are celebrating the deal in the name of free speech, others are worried about the platform’s future. Will the rules change to favor Musk’s personal views? Will trolling, harassment, and disinformation run rampant?I hope the change in management will improve governance and conversation on Twitter. But I wonder whether an open standard for social media might be a better way to improve social networks.Think about email. The open protocolSMTPhas enable many companies to provide email services so that anyone with an email address can communicate freely with anyone else, regardless of their provider. A similar open standard could underpin social media.\n\nPlatforms like Facebook, Instagram, LinkedIn, and Twitter implement similar features like posting, liking, commenting and sharing. Why not enable key features to work across all platforms, including newcomers? This would permit users to interact even if their accounts were on different platforms, just as people who have email accounts with Gmail, Outlook, Yahoo, or any other provider can communicate with each other.\n\nOpen standards for social media have been discussed for a long time. Some people argue that only a central gatekeeper can moderate online conversations effectively, so they don’t degenerate into toxicity. This is false. Again, think of email. Spam filters do a good job of eliminating toxic messages, and the fact that different providers filter spam in different ways allows consumers to choose the gatekeeper they like best — or none at all. Meanwhile, adherence to an open protocol has prevented any single company from monopolizing email.\n\nOpen standards have driven huge amounts of innovation in computing and communications. They do evolve slowly, by committee. But when a technology is sufficiently mature, setting an open standard makes it difficult for any one company to change the rules to benefit themselves at others’ expense. Any developer can plug into an ecosystem, and the best implementations rise to the top. In contrast, proprietary platforms can change on a whim to, say,charge to reach followersordisallow apps from sharing. This makes it harder for innovators to build large and thriving businesses.\n\nThe web is another example. TheHTTPprotocol lets developers worldwide build whatever website they want. The resulting wave of innovation has lasted for decades. When Larry Page and Sergei Brin wanted to set up google.com, no one could stop them, and it was up to them to make it work. Yes, HTTP has spawned scams such aspushingschemes that lure victims to bogus websites, but competition in web browsers ensures that users have a choice of anti-phishing gatekeepers. This helps keep the web ecosystem healthy.Creating an open standard for social media and getting many companies and users to adopt it would be difficult. It would require technical contributions from computer scientists and likely an assist from regulators. It would push against the tide of Facebook-style walled gardens (in which a single company sets rules and access to content).\n\nThe recent U.S. court ruling thatlegalizedscraping websites is a welcome step toward the free flow of information online. Standards that ensure interoperability among social media platforms would be another, major step.\n\nKeep learning!\n\nAndrew\n\nOverhead cameras equipped with computer vision are spotting distracted drivers on the road.What’s new:A system from Melbourne-based Acusensus alerts police when drivers are engaged in risky activities such as using a cell phone, not wearing a seatbelt, or speeding,The New York Timesreported.How it works:TheHeads-Upsystem uses sensors mounted over the road on overpasses, signs, or movable structures. Aninfrared flash cameracaptures images through windshield glare, heavy weather, and nighttime darkness. Radar gauges a vehicle’s speed.\n\nResults:New South Wales, Australia, deployed the system in 2019. In its first two years, itcontributedto a 22 percent decline in road fatalities and an 80 percent decline in use of mobile phones behind the wheel. An 18-hour assessment along a stretch of road in Missouri that saw an average three and a half crashes daily found that 6.5 percent of drivers used mobile phones and around 5 percent engaged in more than one risky behavior.Behind the news:AI is being applied to traffic safety worldwide — and not always by surveilling drivers.\n\nWhy it matters:About1.3 millionpeople worldwide die in road accidents every year, according to the World Health Organization. Many fatalities are associated with speeding, distracted driving, and not wearing seatbelts. AI systems that identify these behaviors can help save lives.We’re thinking:People tend to buckle up when they see a police car and slow down when they see their current speed flashing on a sign ahead. If cameras looming over the road can save lives — given adequate controls on who has access to the data and how they can use it — it’s worth a try.\n\nDeepMind’s AlphaGo famously dominated Go, a game in which players can see the state of play at all times. A new AI system demonstrated similar mastery of bridge, in which crucial information remains hidden.What’s new:NooK, built by Jean-Baptiste Fantun, Véronique Ventos, and colleagues at the French startup NukkAI, recently beat eight world champions at bridge — rather, a core aspect of the game.Rules of the game:Bridge is played by four players grouped into teams of two. Each player is dealt a hand of cards, after which the game is played in two phases:\n\nThis study focused on the play phase, pitting NooK and human champions against previous automated bridge-playing systems, none of which has proven superior to an excellent human player. Each deal had a preassigned bid and trump suit, and competitors played the same 800 deals, divided into sets of 10. The player with the highest average score in the most sets won.How it works:The developers didn’t reveal the mechanisms behind NooK, but we can offer a guess based on press reports and the company’sresearchpapers.\n\nResults:Pitted against the previous systems, NooK scored higher than the human champions in 67 out of 80 sets, or 83 percent of the time.Why it matters:Neural networks would be more useful in many situations if they were more interpretable; that is, if they could tell us why they classified a cat as a cat, or misclassified a cat as an iguana. This work’s approach offers one way to build more interpretable systems: a neurosymbolic hybrid that combines rules (symbolic AI, also known as good old-fashioned AI) describing various situations with neural networks trained to handle specific cases of each situation.We’re thinking:In bridge, bidding is a way to hint to your partner (and deceive your opponent) about what you have in your hand, and thus a vital strategic element. NooK is impressive as far as it goes, but mastering bids and teamwork lie ahead.\n\nMore than 4.7 million learners took the original Machine Learning course by Andrew Ng. A decade later, a new and updated Machine Learning Specialization is set to launch in June! #BreakIntoAI with this foundational three-course program. Sign uphere\n\nThe emerging generation of trillion-parameter language models take significant computation to train. Activating only a portion of the network at a time can cut the requirement dramatically and still achieve exceptional results.What’s new:Researchers at Google led by Nan Du, Yanping Huang, and Andrew M. Dai developedGeneralized Models (GLaM), a trillion-parameter model for language tasks. Like the company’s earlierSwitch, this work usesmixture-of-experts(MoE) layers to select which subset(s) of a network to use depending on the input. It provides a clearer picture of how MoE can save time and electricity in practical language tasks.Key insight:A neural network’s parameter count entails a compromise between performance (bigger is better) and energy cost (smaller is better). MoE architectures use different subsets of their parameters to learn from different examples. Each MoE layer contains a group of vanilla neural networks, or experts, preceded by a gating module that learns to choose which ones to use based on the input, enabling different experts to specialize in particular types of examples. In this way, the network uses less energy and learns more than the size of any given subset might suggest.How it works:The authors trained a transformer model equipped with MoE layers (similar toGShard) to generate the next word or part of a word in a text sequence using a proprietary 1.6-trillion-word corpus of webpages, books, social media conversations, forums, and news articles. They fine-tuned the model to perform 29 natural language tasks in seven categories such as question answering and logical reasoning.\n\nResults:Training the 1.2 trillion-parameter GLaM required 456 megawatt hours, while the 175 billion-parameterGPT-3required 1,287 megawatt hours. Moreover, GLaM outperformed GPT-3 in six categories of zero-shot tasks and in five categories for one-shot tasks. For example, answering trivia questions in the one-shotTriviaQA, it achieved 75 percent accuracy — a state-of-the-art result —  compared to GPT-3’s 68 percent.Why it matters:Increased computational efficiency means lower energy costs, presumably making it easier for everyday engineers to train state-of-the-art models. It also means reduced CO2emissions, sparing the planet some of the environmental impact incurred by AI.We’re thinking:MoE models are attracting a lot of attention amid the public-relations race to claim ever higher parameter counts. Yes, building a mixture of 64 experts boosts the parameter count by 64 times, but it also means building 64 models instead of one. While this can work better than building a single model, it also diverts attention from other architectures that may yield insights deeper thanbigger is better.\n\nAn experimental AI system is helping train the next generation of fighter pilots.What’s new:The U.S. Air Force is using deep learning to evaluate the progress of around 50 pilots in one of its training squadrons,Popular Sciencereported.Cloud-based data:Built by the California startup Crowdbotics, the system harnesses data generated in flight by F-15E airplanes (or simulations). Each aircraft records numerous data streams, such as air speed and position, multiple times per second. Instructors use the system’s output to tailor feedback to each student.\n\nBehind the news:Several machine learning projects aim to improve pilot safety by taking advantage of the data produced by modern aircraft.\n\nWhy it matters:Training pilots is costly, time-consuming, and risky to both personnel and aircraft, which can cost tens of millions of dollars each. It’s also ongoing, as each type of aircraft requires unique instruction. AI can make training more effective, efficient, and safe. It can also allow instructors to focus on trainees who need the most attention.We’re thinking:The sky's the limit for machine learning in training applications.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/04/Screen-Shot-2022-04-26-at-5--1---1--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/04/ACCIDENTS--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/04/BRIDGE.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/04/DeepLearningAI_Banner_Stanford_Teaser_1200x628_Artboard-2--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/04/GLAM.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/04/JET--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-113/",
    "title": "issue 113",
    "date": "",
    "reading_time": "",
    "content": "I’ve seen many friends transition from an academic or research role to a corporate role. The most successful ones adjusted to corporate work by shifting their mindset in a few crucial ways.The worlds of academia and industry are governed by different values. The former prizes scientific innovation and intellectual freedom, and the latter prizes building a successful business that delivers impact and profit. If you’re thinking of taking the leap, here are some tips that might ease the way.\n\nThe shift in mindset between academia and industry is significant, but knowing the key differences in advance can make it easier to shift appropriately. I’ve enjoyed roles in both domains, and both offer valuable ways to move the world forward.Keep learning!AndrewP.S. We hear a lot about AI folks going from academia to industry, but transitions in the opposite direction happen, too. For example,Peter Norvig, after 20 years at Google where he played a key role in building Google Research, recentlyjoinedStanford University.\n\nThe emerging generation of trillion-parameter models needs datasets of billions of examples, but the most readily available source of examples on that scale — the web — is polluted with bias and antisocial expressions. A new study examines the issue.What’s new:Abeba Birhane and colleagues at University College Dublin and University of Edinburghauditedthe LAION-400M dataset, which was released in September. It comprises data scraped from the open web, from which inaccurate entries were removed by a state-of-the-art model for matching images to text. The automated curation left plenty of worrisome examples among the remaining 400 million examples — including stereotypes, racial slurs, and sexual violence — raising concerns that models trained on LAION-400M would inherit its shortcomings.Key insight:The compilers ofLAION-400Mpaired images and text drawn fromCommon Crawl, a large repository of web data. To filter out low-quality pairs, they usedCLIPto score the correspondence between them and discarded those with the lowest scores. But CLIP itself is trained on a massive trove of web data. Thus it’s bound to find a high correspondence between words and pictures that are frequently associated with one another on the web, even if the associations are spurious or otherwise undesirable.NSFT (not safe for training):The authors entered text queries into LAION-400M’s search function, which returned matching images.\n\nBehind the news:The LAION-400M team, a loosely knit collective led by Christoph Schuhmann at University of Vienna, aims to re-create Google’sWikipedia-based Image Textdataset and ultimately use it to train open-source analogs of OpenAI’s CLIP andDALL·E. The group was inspired byEleutherAI’s community effortto build an open source version of GPT-3.Why it matters:It’s enormously expensive to manually clean a dataset that spans hundreds of millions of examples. Automated curation has been viewed as a way to ensure that immense datasets contain high-quality data. This study reveals serious flaws in that approach.We’re thinking:Researchers haveretracted or amendedseveral widely used datasets to address issues of biased and harmful data. Yet, as the demand for data rises, there’s no ready solution to this problem. Audits like this make an important contribution, and the community — including large corporations that produce proprietary systems — would do well to take them seriously.\n\nThe transformer architecture is notoriously inefficient when processing long sequences — a problem in processing images, which are essentially long sequences of pixels. One way around this is to break up input images and process the pieces separately. New work improves upon this already-streamlined approach.What’s new:Zizhao Zhang and colleagues at Google and Rutgers University simplified an earlier proposal for using transformers to process images. They call their architectureNesT.Key Insight:A transformer that processes parts of an image and then joins them can work more efficiently than one that looks at the entire image at once. However, to relate the parts to the whole, it must learn how the pixels in different regions relate to one another. A recent model calledSwindoes this by shifting region boundaries in between processing regions and merging them together — a step that nonetheless consumes compute cycles. Using convolutions to process both within and across regions can enable a model to learn such relationships without shifting region boundaries, saving that computation.How it works:The authors trained NesT to classify images inImageNet.\n\nResults:A 38 million-parameter NesT achieved 83.3 accuracy on ImageNet. This performance matched that of an 88-million parameter Swin-B — a 57 percent saving in the compute budget.Why it matters:Transformers typically bog down when processing images. NesT could help vision applications take fuller advantage of the architecture’s strengths.We’re thinking:Computational efficiency for the Swin!\n\nWe’re updating ourNatural Language Processing Specializationto reflect the latest advances! Join instructor Younes Bensouda Mourri and Hugging Face engineer Lewis Tunstall for a liveAsk Me Anythingsession on November 3, 2021. Get answers to all your NLP-related questions!\n\nGoogle will upgrade its search engine with a new model that tracks the relationships between words, images, and, in time, videos — the first fruit of its latest research into multimodal machine learning and multilingual language modeling.What’s new:Early next year, Google will integrate a new architecture called Multitask Unified Model (MUM) into its traditional Search algorithm and Lens photo-finding system,VentureBeatreported. The new model will enable the search engines to break down complex queries (“I’ve hiked Mt. Adams and now I want to hike Mt. Fuji next fall. What should I do differently to prepare?”) into simpler requests (“prepare to hike Mt. Adams,” “prepare to hike Mt. Fuji,” “Mt. Fuji next fall”). Then it can combine results of the simpler requests into coherent results.How it works:Announcedin May, MUM is a transformers-based natural language model. It’s based on Google’s earlierT5that comprises around 110 billion parameters (compared to BERT’s 110 million, GPT-3’s 175 billion, and Google’s own Switch Transformer at 1.6 trillion). It was trained on a dataset of text and image documents drawn from the web from which hateful, abusive, sexually explicit, and misleading images and text were removed.\n\nBehind the news:In 2019, Google Search integratedBERT. The change improved the results of 10 percent of English-language queries, the company said, particularly those that included conversational language or prepositions like “to” (the earlier version couldn’t distinguish the destination country in a phrase like “brazil traveler to usa”).  BERT helped spur a trend toward larger, more capable transformer-based language models.Why it matters:Web search is ubiquitous, but there’s still plenty of room for improvement. This work takes advantage of the rapidly expanding capabilities of transformer-based models.We’re thinking:While we celebrate any advances in search, we found Google’s announcement short on technical detail. Apparently MUM really is the word.\n\nLudwig van Beethoven died before he completed what would have been his tenth and final symphony. A team of computer scientists and music scholars approximated the music that might have been.What’s new:The Beethoven Orchestra in Bonnperformeda mock-up of Beethoven’s Tenth Symphony partly composed by an AI system, the culmination of an 18-month project. You can view and hear the performancehere.How it works:The master left behind around 200 fragmentary sketches of the Tenth Symphony, presumably in four movements. A human composer in 1988 completed two movements, for which more source material was available, so the team set out to compose two more.\n\nEveryone’s a critic:Composer Jan Swafford, who wrote a 2014 biography of Beethoven,describedthe finished work as uninspired and lacking Beethovenian traits such as rhythms that build to a sweeping climax.Behind the news:In 2019, Huawei used AI powered by its smartphone processors torealizethe final two movements of Franz Schubert’s unfinished Eighth Symphony. The engineers trained their model on roughly 90 pieces of Schubert’s work as well as pieces written by composers who influenced him. A human composer cleaned up the output, organized it into sections, and distributed the notes among various instruments.Why it matters:AI is finding its way into the arts in a variety of roles. As a composer, generally the technology generates short passages that humans can assemble and embellish. It’s not clear how much the team massaged the model’s output in this case, but the ambition clearly is to build an end-to-end symphonic composer.We’re thinking:Elgammal has publishedworkon generative adversarial networks. Could one of his GANs yield Beethoven’s Eleventh?",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-10-13%20at%2011.14.15%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-10-13%20at%2011.14.15%20AM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-10-12T111718.458.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-10-12T111718.458.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/NESTED.gif?upscale=true&width=1200&upscale=true&name=NESTED.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/AI%20Access%2011.3_The%20Batch%20Image.png?upscale=true&width=1200&upscale=true&name=AI%20Access%2011.3_The%20Batch%20Image.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/MUM.gif?upscale=true&width=1200&upscale=true&name=MUM.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/BEETHOVEN.gif?upscale=true&width=924&upscale=true&name=BEETHOVEN.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-238/",
    "title": "issue 238",
    "date": "",
    "reading_time": "",
    "content": "A lot of AI software is written in the Python language, and so our field has adopted Python’s philosophy of letting anyone publish any package online. The resulting rich collection of freely available packages means that, with just one “pip install” command, you now can install a package and give your software new superpowers! The community’s parallel exploration of lots of ideas and open-sourcing of innovations has been fantastic for developing and spreading not just technical ideas but also usable tools.But we pay a price for this highly distributed development of AI components: Building on top of open source can mean hours wrestling with package dependencies, or sometimes even juggling multiple virtual environments or using multiple versions of Python in one application. This is annoying but manageable for experienced developers, but creates a lot of friction for new AI developers entering our field without a background in computer science or software engineering.\n\nI don’t know of any easy solution. Hopefully, as the ecosystem of tools matures, package management will become simpler and easier. Better tools for testing compatibility might be useful, though I’m not sure we need yet another Python package manager (we already have pip, conda, poetry, and more) or virtual environment framework.\n\nAs a step toward making package management easier, maybe if all of us who develop tools pay a little more attention to compatibility — for example, testing in multiple environments, specifying dependencies carefully, carrying out more careful regression testing, and engaging with the community to quickly spot and fix issues —  we can make all of this wonderful open source work easier for new developers to adopt.\n\nKeep coding!\n\nAndrew\n\nP.S. Built in collaboration with Meta: “Prompt Engineering with Llama 2,” taught by Amit Sangani, is now available! Meta’s Llama 2 has been a game changer: Building with open source lets you control your own data, scrutinize errors, update models (or not) as you please, and work alongside the global community to advance open models. In this course, you’ll learn how to prompt Llama chat models using advanced techniques like few-shot for classification and chain-of-thought for logic problems. You’ll also learn how to use specialized models like Code Llama for software development and Llama Guard to check for harmful content. The course also touches on how to run Llama 2 on your own machine. I hope you’ll take this course and try out these powerful, open models!Sign up here\n\nCorrection: This article has been corrected to state that Gemini 1.0 produced anachronistic images of historical scenes. An earlier edition incorrectly stated that Gemini 1.5 Pro generated anachronistic images.\n\nAn update of Google’s flagship multimodal model keeps track of colossal inputs, while an earlier version generated some questionable outputs.\n\nWhat's new:GoogleunveiledGemini 1.5 Pro, a model that can converse about inputs as long as books, codebases, and lengthy passages of video and audio (depending on frame and sample rates). However an earlier version, recently enabled to generate images, produced wildly inaccurate images of historical scenes.\n\nHow it works:Gemini 1.5 Proupdates the previous model with a mixture-of-experts architecture, in which special layers select which subset(s) of a network to use depending on the input. This enables the new version to equal or exceed the performance of the previousGemini 1.0 Ultrawhile requiring less computation.\n\nAlignment with what?:The earlier Gemini 1.0 recently wasupdatedto allow users to generate images using a specially fine-tuned version ofImagen 2. However, this capability backfired when social media posts appeared in which the system, prompted to produce pictures of historical characters and situations, anachronistically populated them with people of color, who would not have been likely to be present. For instance, the model illustrated European royalty, medieval Vikings, German soldiers circa 1943 — all of whom were virtually exclusively white — as Black, Asian, or Native American. Google quicklydisabledimage generation of people for “the next couple of weeks” and explained that fine-tuning intended to increase diverse outputs did not account for contexts in which diversity was inappropriate, and fine-tuning intended to keep the model from fulfilling potentially harmful requests also kept it from fulfilling harmless requests. But other users found flaws in text output as well. One asked Gemini who had a greater negative impact on society: Adolf Hitler, who presided over the murder of roughly 9 million people, or “Elon Musk tweeting memes.” The model replied, “It is difficult to say definitively who had a greater negative impact on society.” The ensuing controversy called into question not only Google’s standards and procedures for fine-tuning to ensure ethics and safety, but also its motive for building the model.\n\nWhy it matters:Gemini 1.5 Pro’s enormous context window radically expands potential applications and sets a high bar for the next generation of large multimodal models. At the same time, it’s clear that Google’s procedures for aligning its models to prevailing social values were inadequate. This shortcoming derailed the company’s latest move to one-up its big-tech rivals and revived longstanding worries that its management places politics above utility to users.\n\nWe’re thinking:How to align AI models to social values is a hard problem, and approaches to solving it are in their infancy. Google acknowledged Gemini’s shortcomings, went back to work on image generation, and warned that even an improved version would make mistakes and offend some users. This is a realistic assessment following a disappointing product launch. Nonetheless, the underlying work remains innovative and useful, and we look forward to seeing where Google takes Gemini next.\n\nAn upstart chip company dramatically accelerates pretrained large language models.\n\nWhat’s new:Groq offers cloud access to Meta’s Llama 2 and Mistral.ai’s Mixtral at speeds an order of magnitude greater than other AI platforms. Registered users can try ithere.\n\nHow it works:Groq’s cloud platform is based on its proprietary GroqChip, a processor specialized for large language model inference that the company calls a language processing unit or LPU. The company plans to serve other models eventually, but its main business is selling chips. It focuses on inference on the theory that demand for a model’s inference can increase while demand for its training tends to be fixed.\n\nBehind the news:Groq founder Jonathan Ross previously worked at Google, where he spearheaded the development of that company’stensor processing unit(TPU), another specialized AI chip.\n\nWhy it matters:Decades of ever faster chips have proven that users need all the speed they can get out of computers. With AI, rapid inference can make the difference between halting interactions and real-time spontaneity. Moreover, Groq shows that there’s plenty of innovation left in computing hardware as processors target general-purpose computing versus AI, inference versus training, language versus vision, and so on.\n\nWe’re thinking:Autonomous agents based on large language models (LLMs) can get a huge boost from very fast generation. People can read only so fast, the faster generation of text that’s intended to be read by humans has little value beyond a certain point. But an agent (as well as chain-of-thought and similar approaches to prompting) might need an LLM to “think” through multiple steps. Fast LLM inference can be immensely useful for building agents that can work on problems at length before reaching a conclusion.\n\nJoin “Prompt Engineering with Llama 2” and learn best practices for model selection and prompting, advanced prompting techniques, and responsible use of large language models, all while using Meta Llama 2 Chat, Llama Guard, and Code Llama.Sign up for free\n\nOpenAI is focusing on autonomous agents that take action on a user’s behalf.\n\nWhat’s new:The maker of ChatGPT is developing applications designed to automate common digital tasks by controlling apps and devices,The Informationreported.\n\nHow it works:OpenAI has two agent systems in the works. It has not revealed any findings, products, or release dates.\n\nBehind the news:Agents are on Silicon Valley’s radar, especially since January’s Consumer Electronics Showdebutof the Rabbit R1, which accepts voice commands to play music, order food, call a car, and so on. Several other companies, academic labs, and independent developers are pursuing the concept as well.\n\nWhy it matters:Training agents to operate software designed for humans can be tricky. Some break down tasks into subtasks but struggle with executing them. Others have difficulty with tasks they haven’t encountered before oredge casesthat are unusually complex. However, agents are becoming more reliable in a wider variety of settings as developers push the state of the art forward.\n\nWe’re thinking:We’re excited about agents! You can learn about agent technology in our short course, “LangChain for LLM Application Development,” taught by LangChain CEO Harrison Chase and Andrew.\n\nPruning weights from a neural network makes it smaller and faster, but it can take a lot of computation to choose weights that can be removed without degrading the network’s performance. Researchers devised a computationally efficient way to select weights that have relatively little impact on performance.\n\nWhat’s new:Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter at Carnegie Mellon University, Facebook AI Research, Meta AI, and Bosch Center for AI respectively devised a method for pruning byweights and activations, or Wanda.\n\nKey insight:The popular approach known asmagnitude pruningremoves the smallest weights in a network based on the assumption that weights closest to 0 can be set to 0 with the least impact on performance. Meanwhile, unrelatedworkfound that, in very large language models, the magnitudes of a subset of outputs from an intermediate layer may be up to 20 times larger than those of other outputs of the same layer. Removing the weights that are multiplied by these large outputs — even weights close to zero — could significantly degrade performance. Thus, a pruning technique that considers both weights and intermediate-layer outputs can accelerate a network with less impact on performance.\n\nHow it works:The authors pruned a pretrainedLLaMAthat started with 65 billion parameters. Given 128 sequences of tokens drawn from acurated dataset of English text from the web, the model processed them as follows:\n\nResults:The authors tested versions of LLaMA unpruned and pruned via various methods. The models performed a language modeling task usingweb text. The unpruned LLaMA achieved 3.56 perplexity (a measure of the likelihood that a model will predict the next token, lower is better). Pruned by Wanda to half its original size, it achieved 4.57 perplexity. Pruned by the best competing method,SparseGPT(which both removes weights and updates the remaining ones), it achieved the same score. However, Wanda took 5.6 seconds to prune the model, while SparseGPT took 1,353.4 seconds. Pruned by magnitude pruning, the model achieved 5.9 perplexity.\n\nWhy it matters:The ability to compress neural networks without affecting their output is becoming more important as models balloon and devices at the edge of the network become powerful enough to run them. Wanda compared weights from each row in the weight matrices (pruning per neuron), rather than each weight matrix (pruning per layer) or the model as a whole. The scale at which weights are compared turns out to be important — an interesting avenue for further research.\n\nWe’re thinking:We came up with a joke about a half-LLaMA, but it fell flat.\n\nMore AI news of the week includes:\n\nStay in the know with Data Points, a spin-off of The Batch.Read now.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/02/PYTHONPACKAGE-1.png",
      "https://dl-staging-website.ghost.io/content/images/2024/02/GEMINI_MultimodalDemo_NoCC_600px--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/02/GROQ-LLMPERF.webp",
      "https://dl-staging-website.ghost.io/content/images/2024/02/The-Batch-ads-and-exclusive-banners---2024-02-20T090733.220.png",
      "https://dl-staging-website.ghost.io/content/images/2024/02/AGENT_1200px--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/02/PRUNING--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-v/",
    "title": "issue v",
    "date": "",
    "reading_time": "",
    "content": "I’ve been thinking a lot about \"small data.\" If you have an image classification problem and 1,000,000 images, then dozens of teams around the world can build a good classifier. But what if you have only 100 or 10 images? There would be a much greater variance between different teams’ performance faced with such a small data set.At Landing AI, I’ve heard from several manufacturers wanting to use AI to find scratches and other defects on phones. Fortunately, no company has manufactured 1,000,000 scratched phones that subsequently needed to be thrown away; they may have only a limited number of images of defective phones.The ability to build and deploy machine learning systems that learn from small data would unlock many applications. The research literature on one-shot learning, few-shot learning, domain adaptation, and transfer learning nibbles at the problem, but there’s still a long way to go.Keep learning,Andrew\n\nData wranglers have tried bulking up training data sets with synthetically generated images, but such input often fails to capture real-world variety. Researchersproposea way to generate labeled data for visual tasks that aims to bring synthetic and real worlds into closer alignment.\n\nWhat’s new:Where most approaches to synthesizing visual data concentrate on matching the appearance of real objects, Meta-Sim also aims to mimic their diversity and distribution as well. Its output proved quantitatively better than other methods in a number of tasks.\n\nHow it works:For a given task, Meta-Sim uses aprobabilistic scene grammar, a set of compositional rules that attempt to represent objects and their distributions found in real-world scenes. To optimize the grammar attributes, Meta-Sim uses a neural network that continuously minimizes the divergence of object distributions between real-life images and synthetic images rendered from the grammar. The neural network can also be used to modify the grammar itself to boost performance on downstream tasks.\n\nResults:Amlan Kar and his colleagues at MIT, Nvidia, University of Toronto, and Vector Institute show that tuning probabilistic scene grammars via Meta-Sim significantly improves generalization from synthetic to test data across a number of tasks. Trained on Meta-Sim data, networks built for digit recognition, car detection, and aerial road segmentation perform accurately on real-world data.\n\nTo be sure:Meta-Sim relies on probabilistic scene grammars for any particular task. Its output is only as good as the grammar itself, and it can model only scenes that are represented in PSG format.\n\nTakeaway:There’s no such thing as too much labeled data. Meta-Sim offers an approach to generating endless quantities of diverse visual data that more closely mimics the real world, and points the way toward synthesizing more realistic data for other kinds of tasks. That could make for more accurate models going forward.\n\nLarger neural networks can deliver better results, yet researchersfounda way to make deep learning models perform just as well at one-tenth the size. Their work wasawardedbest paper at this year's International Conference on Learning Representations.\n\nWhat’s new:Researchers at MIT developed a procedure to identify, within a trained network, a much smaller subnetwork that performs the desired task as fast and accurately as its bulky counterpart.\n\nHow it works:The researchers started with a fully-connected convolutional feed-forward architecture. They initialized the network randomly and trained it over a number of iterations. Then they trimmed off the connections with the lowest weights in each layer. Finally, they reset the remaining connections to their initialization values and retrained. They repeated this process several times to achieve high performance in compact size.\n\nWhy it matters:Researchers Jonathan Frankle and Michael Carbinbuilt on earlier work on pruning neural networks, but they achieved much more dramatic results. Apparently such high-performance subnetworks exist in any neural network, depending on weights assigned during initialization. The researchers call these high-performance subnetworks “winning tickets,” and they propose an algorithm to identify them. Winning tickets require a fraction of the usual memory resources, computational power, and energy. Spotting them early in the model-building process might yield immense savings.\n\nTo be sure:The researchers pruned only networks devoted to computer vision and trained on small data sets. It’s not clear whether the results would be equally impressive otherwise. Moreover, their method currently requires training a network more than a dozen times, so it requires a lot of computation.\n\nWhat’s next:The researchers aim to identify winning tickets early, making it possible to build compact networks from the start. They’ll be studying winning tickets in hope of discovering more powerful architectures and initialization methods.\n\nMachine learning systems are infamous for making predictions that can’t readily be explained. Now Microsoft offers an open source tool kit providing a variety of ways to interrogate models.\n\nWhat's in the package:InterpretMLimplements Explainable Boosting Machine, a generative additive model that delivers both high accuracy and high explainability. The package also comes with several methods to generate explanations of model behavior for regression and binary classification models. Developers can compare explanations produced by different methods and check consistency among models.\n\nWhy it matters:Building models that can explain how they reach their conclusions is critical in life-and-death situations like transportation, healthcare, and law enforcement. And it’s a top priority in high-stakes industries such as finance where decisions may be called into question. Understanding the behavior of intelligent systems is important to:\n\nWhat’s next:Principal researcher Rich Caruana and his colleagues aim to improve InterpretML’s categorical encoding and add support for multi-class classification and missing values. They’re hopeful the open source community will build on their work to illuminate what goes on inside machine learning's proliferating black boxes.\n\nStill debating whether to get into deep learning? Check out the techniques you'll learn and projects you'll build in the Deep Learning Specialization.Learn more\n\nWish you could draw, but your elephants look like crocodiles? Sketchforme doesn’t have that problem. This AI agent roughs out simple scenes based on text descriptions.What’s new:Sketchforme generates crude drawings from natural-language descriptions such as “an apple on a tree” or “a dog next to a house.” People who viewed its output thought a human made the drawing a third of the time, a newpapersays.How it works:Sketchforme relies on two neural networks:\n\nBehind the news:Building a sketch generator was a thorny problem until the advent of neural networks.Sketch-RNN, an early sketcher based on neural nets in 2017, was trained on crowd-sourced drawings and draws requested objects using an initial stroke as a prompt. Sketchforme builds on that work.\n\nBottom line:Sketchforme’s output is remarkably true to human notions of what objects look like in the abstract. UC Berkeley researchers Forrest Huang and John F. Canny point out that sketching is a natural way to convey ideas quickly and a useful thinking tool in applications like learning languages. But the fact is, Sketchforme is just plain fun — and no slouch at drawing, too.\n\nCorporate executives worldwide are gearing up to take advantage of AI. But those in different countries aim to use the technology differently, and they bring different hopes and fears.What’s new:In an international survey of executives who are using AI at scale, two-thirds believed AI already is “very” or “critically” important to success, according to Deloitte’s latestState of AI in the Enterprisereport. Roughly 10 percent of respondents had achieved a return on their AI investment of 40 percent or more. Roughly 30 percent saw less than 10 percent return. The rest saw ROIs in between.\n\nExperience versus strategy:Deloitte rated respondents in the U.S. the most seasoned. However, more of those in China and the UK reported having a company-wide AI strategy.\n\nDifferent goals:While most executives in China said AI would help them widen their lead over competitors, majorities in Australia, Canada, and France were using AI to catch up or stay abreast of the competition. Those in the U.S. were nearly evenly divided between those aims.\n\nBenefits:Early adopters deemed the primary benefits of AI to be:\n\nChallenges:They regarded the top challenges of AI as:\n\nRisks:About half of French executives expressed “major” or “extreme” concern about AI’s potential risks. Among those in China, that number was only 16 percent. Generally, the greater the concern, the less the confidence expressed in preparations to meet it. The biggest concerns were:\n\nTalent shortage:68 percent of those surveyed judged the gap between skills needed and those available to be “moderate” to “extreme.” The top three roles needed were:\n\nWe're thinking:Globally, more than half of those surveyed said AI would substantially transform their company within three years, and roughly half said it would transform their industry within five. Deloitte consequently sees a two- to three-year window for companies to use AI to differentiate. It’s hard to believe, though, that the window is that brief. Even 30 years after the invention of the web, companies are still figuring out how to use it. The dust isn't likely to settle around AI in less than a decade.\n\nA lawsuit in London could set precedents for how to allocate responsibility when algorithms make poor predictions.\n\nWhat’s happening:Samathur Li Kin-kan, a Hong Kong real-estate heir, sued high-profile hedge fund manager Raffaele Costa for $23 million for allegedly overstating the capabilities of an AI-driven trading platform. It’s the first known legal action over a financial loss caused by a predictive algorithm, according toBloomberg.\n\nBehind the news:Li in late 2017 invested $2.5 billion with Costa’s trading firm. Costa, who also goes by the name of Captain Magic, used a computer called K1 to recommend trades. Developed by Austrian software house 42.cx, the K1 system performed real-time sentiment analysis on news and social media to predict stock prices. Then it sent instructions to a broker to execute trades. Following a series of mishaps, the system lost $20.5 million in a single day in February 2018. Costa, in a countersuit for $3 million in unpaid fees, says he didn't guarantee a return on investment. Li's case is scheduled to go to trial in April 2020.\n\nWhy it matters:The question of who’s at fault when people act on erroneous predictions made by AI becomes more pressing as the technology finds its way into industries from healthcare to manufacturing. Reports of autonomous vehicle crashes and bias in law-enforcement software have made headlines, and such cases likely will become more common—and more contentious—in the future.\n\nWhat they’re saying:“I think we did a pretty decent job. I know I can detect sentiment. I’m not a trader.” — 42.cx founder Daniel Mattes, quoted byBloomberg.\n\nSmart take:The outcome is bound to influence subsequent lawsuits involving AI — but it’s just the first step on a long, long road to establishing the legal status of AI.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/4ccf9402-bd70-43cb-aeb5-9dc664d85e20-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/88d700bb-060a-44f8-b48e-e60a5b1cb748.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/8271df5e-ea34-4ea3-b0bd-a8eebf280bf5.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/ae3906d7-e49a-4101-9611-b916f41d1513.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/3c25548d-d9fd-47ef-a1fb-fd457b92aaca.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/3e592799-8b6e-4069-b5bb-1690096b72e0.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/08f19ed5-ed27-4135-a075-ab947d13f00d.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-146/",
    "title": "issue 146",
    "date": "",
    "reading_time": "",
    "content": "Years ago, I had to choose between a neural network and a decision tree learning algorithm. It was necessary to pick an efficient one, because we planned to apply the algorithm to a very large set of users on a limited compute budget. I went with a neural network. I hadn’t used boosted decision trees in a while, and I thought they required more computation than they actually do — so I made a bad call. Fortunately, my team quickly revised my decision, and the project was successful.\n\nThis experience was a lesson in the importance of learning, and continually refreshing, foundational knowledge. If I had refreshed my familiarity with boosted trees, I would have made a better decision.\n\nMachine learning, like many technical fields, evolves as the community of researchers builds on top of one another's work. Some contributions have staying power and become the basis of further developments. Consequently, everything from a housing-price predictor to a text-to-image generator is built on core ideas that include algorithms (linear and logistic regression, decision trees, and so on) and concepts (regularization, optimizing a loss function, bias/variance, and the like).\n\nA solid, up-to-date foundation is one key to being a productive machine learning engineer. Many teams draw on these ideas in their day-to-day work, and blog posts and research papers often assume that you’re familiar with them. This shared base of knowledge is essential to the rapid progress we've seen in machine learning in recent years.\n\nThat's why I’m updating my original machine learning class as the newMachine Learning Specialization, which will be available in a few weeks.\n\nMy team spent many hours debating the most important concepts to teach. We developed extensive syllabi for various topics and prototyped course units in them. Sometimes this process helped us realize that a different topic was more important, so we cut material we had developed to focus on something else. The result, I hope, is an accessible set of courses that will help anyone master the most important algorithms and concepts in machine learning today — including deep learning but also a lot of other things — and to build effective learning systems.\n\nIn that spirit, this week’s issue ofThe Batchexplores some of our field’s most important algorithms, explaining how they work and describing some of their surprising origins. If you’re just starting out, I hope it will demystify some of the approaches at the heart of machine learning. For those who are more advanced, you’ll find lesser-known perspectives on familiar territory. Either way, I hope this special issue will help you build your intuition and give you fun facts about machine learning’s foundations that you can share with friends.\n\nKeep learning!\n\nAndrew\n\nMachine learning offers a deep toolbox for solving all kinds of problems, but which tool is best for which task? When is the open-ended wrench better than the adjustable kind? Who invented these things anyway? In this special issue ofThe Batch, we survey six of the most useful algorithms in the kit: where they came from, what they do, and how they’re evolving as AI advances into every facet of society. If you want to learn more, theMachine Learning Specializationprovides a simple, practical introduction to these algorithms and more.Join the waitlistto be notified when it’s available.\n\nLinear regression may be the key statistical method in machine learning, but it didn’t get to be that way without a fight. Two eminent mathematicians claimed credit for it, and 200 years later the matter remains unresolved. The longstanding dispute attests not only to the algorithm’s extraordinary utility but also to its essential simplicity.Whose algorithm is it anyway?In 1805, French mathematician Adrien-Marie Legendre published the method of fitting a line to a set of points while trying to predict the location of a comet (celestial navigation being the science most valuable in global commerce at the time, much like AI is today — the new electricity, if you will, two decades before the electric motor). Four years later, the 24-year-old German wunderkind Carl Friedrich Gauss insisted that he had been using it since 1795 but had deemed it too trivial to write about. Gauss’ claim prompted Legendre to publish an addendum anonymously observing that “a very celebrated geometer has not hesitated to appropriate this method.”Slopes and biases:Linear regression is useful any time the relationship between an outcome and a variable that influences it follows a straight line. For instance, a car’s fuel consumption bears a linear relationship to its weight.\n\nTwo steps to ubiquity:The algorithm immediately helped navigators to follow the stars, and later biologists (notably Charles Darwin’s cousin Francis Galton) to identify heritable traits in plants and animals. Two further developments unlocked its broad potential. In 1922, English statisticians Ronald Fisher and Karl Pearson showed how linear regression fit into the general statistical framework of correlation and distribution, making it useful throughout all sciences. And, nearly a century later, the advent of computers provided the data and processing power to take far greater advantage of it.Coping with ambiguity:Of course, data is never perfectly measured, and some variables are more important than others. These facts of life have spurred more sophisticated variants. For instance, linear regression with regularization (also calledridge regression) encourages a linear regression model to not depend too much on any one variable, or rather to rely evenly on the most important variables. If you’re going for simplicity, a different form of regularization (L1 instead of L2) results inlasso, which encourages as many coefficients as possible to be zero. In other words, it learns to select variables with high prediction power and ignores the rest.Elastic netcombines both types of regularization. It’s useful when data is sparse or features appear to be correlated.In every neuron:Still, the simple version is enormously useful. The most common sort of neuron in a neural network is a linear regression model followed by a nonlinear activation function, making linear regression a fundamental building block of deep learning.\n\nThere was a moment when logistic regression was used to classify just one thing: If you drink a vial of poison, are you likely to be labeled “living” or “deceased”? Times have changed, and today not only does calling emergency services provide a better answer to that question, but logistic regression is at the very heart of deep learning.Poison control:The logistic function dates to the 1830s, when the Belgian statistician P.F. Verhulst invented it to describe population dynamics: Over time, an initial explosion of exponential growth flattens as it consumes available resources, resulting in the characteristic logistic curve. More than a century passed before American statistician E. B. Wilson and his student Jane Worcester devised logistic regression to figure out how much of a given hazardous substance would be fatal. How they gathered their training data is a subject for another essay.Fitting the function:Logistic regression fits the logistic function to a dataset in order to predict the probability, given an event (say, ingesting strychnine), that a particular outcome will occur (say, an untimely demise).\n\nMore outcomes:Verhulst’s work found the probabilities of binary outcomes, ignoring further possibilities like which side of the afterlife a poison victim might land in. His successors extended the algorithm:\n\nVersatile curve:The logistic function describes a wide range of phenomena with fair accuracy, so logistic regression provides serviceable baseline predictions in many situations. In medicine, it estimates mortality and risk of disease. In political science, it predicts winners and losers of elections. In economics, it forecasts business prospects. More important, it drives a portion of the neurons, in which the nonlinearity is a sigmoid, in a wide variety of neural networks.\n\nImagine hiking in the mountains past dusk and finding that you can’t see much beyond your feet. And your phone’s battery died so you can’t use a GPS app to find your way home. You might find the quickest path down via gradient descent. Just be careful not to walk off a cliff.Suns and rugs:Gradient descent is good for more than descending through precipitous terrain. In 1847, French mathematician Augustin-Louis Cauchyinventedthe algorithm to approximate the orbits of stars. Sixty years later, his compatriot Jacques Hadamard independentlydevelopedit to describe deformations of thin, flexible objects like throw rugs that might make a downward hike easier on the knees. In machine learning, though, its most common use is to find the lowest point in the landscape of a learning algorithm’s loss function.Downward climb:A trained neural network provides a function that, given an input, computes a desired output. One way to train the network is to minimize the loss, or error in its output, by iteratively computing the difference between the actual output and the desired output and then changing the network’s parameter values to narrow that difference. Gradient descent narrows the difference, minimizing the function that computes the loss. The network’s parameter values are tantamount to a position on the landscape, and the loss is the current altitude. As you descend, you improve the network’s ability to compute outputs close to the desired one. Visibility is limited because, in a typical supervised learning situation, the algorithm relies solely on the network’s parameter values and the gradient, or slope of the loss function — that is, your position on the hill and the slope immediately beneath your feet.\n\nStuck in the valley:Too bad your phone is out of juice, because the algorithm may not have propelled you to the bottom of a convex mountain. You may be stuck in a nonconvex landscape of multiple valleys (local minima), peaks (local maxima), saddles (saddle points), and plateaus. In fact, tasks like image recognition, text generation, and speech recognition are nonconvex, and many variations on gradient descent have emerged to handle such situations.  For example, the algorithm may havemomentumthat helps it zoom over small rises and dips, giving it a better chance at arriving at the bottom. Researchers have devised so many variants that it may seem as though there are as many optimizers as there are local minima. Luckily, local and global minima tend to beroughly equivalent.Optimal optimizer:Gradient descent is the clear choice for finding the minimum of any function. In cases where an exact solution can be computed directly — say, a linear regression task with lots of variables — it can approximate one, often faster and more cheaply. But it really comes into its own in complex, nonlinear tasks. Armed with gradient descent and an adventurous spirit, you might just make it out of the mountains in time for dinner.\n\nNo advanced math required! The newMachine Learning Specializationbalances intuition, practice, and theory to create a powerful learning experience for beginners.Enrollnow and achieve your career goals.\n\nLet’s get this out of the way: A brain is not a cluster of graphics processing units, and if it were, it would run software far more complex than the typical artificial neural network. Yet neural networks were inspired by the brain’s architecture: layers of interconnected neurons, each of which computes its own output depending on the states of its neighbors. The resulting cascade of activity forms an idea — or recognizes a picture of a cat.From biological to artificial:The insight that the brain learns through interactions among neurons dates back to 1873, but it wasn’t until 1943 that American neuroscientists Warren McCulloch and Walter Pittsmodeledbiological neural networks using simple mathematical rules. In 1958, American psychologist Frank Rosenblatt developed theperceptron, a single-layer vision network implemented on punch cards with the intention of building a hardware version for the United States Navy.Bigger is better:Rosenblatt’s invention recognized only classes that could be separated by a line. Ukrainian mathematicians Alexey Ivakhnenko and Valentin Lapa overcame this limitation bystackingnetworks of neurons in any number of layers. In 1985, working independently, French computer scientist Yann LeCun, David Parker, and American psychologist David Rumelhart and his colleaguesdescribedusingbackpropagationto train such networks efficiently. In the first decade of the new millennium, researchers including Kumar Chellapilla, Dave Steinkraus, and Rajat Raina (with Andrew Ng)acceleratedneuralnetworksusing graphical processing units, which has enabled ever-larger neural networks to learn from the immense amounts of data generated by the internet.Fit for every task:The idea behind a neural network is simple: For any task, there’s a function that can perform it. A neural network constitutes a trainable function by combining many simple functions, each executed by a single neuron. A neuron’s function is determined by adjustable parameters called weights. Given random values for those weights and examples of inputs and their desired outputs, it’s possible to alter the weights iteratively until the trainable function performs the task at hand.\n\nBlack box:While a trained network may perform its task, good luck determining how. You can read the final function, but often it’s so complex — with thousands of variables and nested activation functions — that it’s exceedingly difficult to explain how the network succeeded at its task. Moreover, a trained network is only as good as the data it learned from. For instance, if the dataset was skewed, the network’s output will be skewed. If it included only high-resolution pictures of cats, there’s no telling how it would respond to lower-resolution images.Toward common sense:Reporting on Rosenblatt’s Perceptron in 1958,The New York Timesblazed the trail for AI hype bycallingit “the embryo of an electronic computer that the United States Navy expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.” While it didn’t live up to that billing, it begot a host of impressive models: convolutional neural networks for images; recurrent neural networks for text; and transformers for images, text, speech, video, protein structures, and more. They’ve done amazing things, exceeding human-level performance in playing Go and approaching it in practical tasks like diagnosing x-ray images. Yet they still have a hard time with common sense and logical reasoning.Ask GPT-3, “When counting, what number comes before a million?” and it may respond, “Nine hundred thousand and ninety-nine comes before a million.” To which we reply: Keep learning!\n\nWhat kind of beast was Aristotle? The philosopher's follower Porphyry, who lived in Syria during the third century, came up with a logical way to answer the question. He organized Aristotle’s proposed “categories of being” from general to specific and assigned Aristotle himself to each category in turn: Aristotle’s substance occupied space rather than being conceptual or spiritual; his body was animate not inanimate; his mind was rational not irrational. Thus his classification was human. Medieval teachers of logic drew the sequence as a vertical flowchart: An early decision tree.The digital difference:Fast forward to 1963, when University of Michigan sociologist John Sonquist and economist James Morgan, dividing survey respondents into groups, firstimplementeddecision trees in a computer. Such work became commonplace with the advent of software that automates training the algorithm, now implemented in a variety of machine learning libraries including scikit-learn. The code took a quartet of statisticians at Stanford and UC Berkeley 10 years to develop. Today, coding a decision tree from scratch is a homework assignment in Machine Learning 101.Roots in the sky:A decision tree can perform classification or regression. It grows downward, from root to canopy, in a hierarchy of decisions that sort input examples into two (or more) groups. Consider the task of Johann Blumenbach, the German physician and anthropologist who first distinguished monkeys from apes (setting aside humans) circa 1776, before which they had been categorized together. The classification depends on various criteria such as presence or absence of a tail, narrow or broad chest, upright versus crouched posture, and lesser or greater intelligence. A decision tree trained to label such animals would consider each criterion one by one, ultimately separating the two groups.\n\nTop 10 hit:Given Blumenbach’s conclusion (later overturned by Charles Darwin) that humans are distinguished from apes by a broad pelvis, hands, and close-set teeth, what if we wanted to extend the decision tree to classify not just apes and monkeys but humans as well? Australian computer scientist John Ross Quinlan made this possible in 1986 withID3, which extended decision trees to support nonbinary outcomes. In 2008, a further refinement calledC4.5capped a list of Top 10 Algorithms in Data Mining curated by the IEEE International Conference on Data Mining. In a world of rampant innovation, that’s staying power.Raking leaves:Decision trees do have some drawbacks. They can easily overfit the data by growing so many levels that leaf nodes include as few as one example. Worse, they’re prone to the butterfly effect: Change one example, and the tree that grows could look dramatically different.Into the woods:Turning this trait into an advantage, American statistician Leo Breiman and New Zealander statistician Adele Cutler in 2001 developed therandom forest, an ensemble of decision trees, each of which processes a different, overlapping selection of examples that vote on a final decision. Random forest and its cousin XGBoost are less prone to overfitting, which helps make them among the most popular machine learning algorithms. It’s like having Aristotle, Porphyry, Blumenbach, Darwin, Jane Goodall, Dian Fossey, and 1,000 other zoologists in the room together, all making sure your classifications are the best they can be.\n\nIf you’re standing close to others at a party, it’s likely that you have something in common. This is the idea behind using k-means clustering to split data points into groups. Whether the groups formed via human agency or some other force, this algorithm will find them.From detonations to dial tones:American physicist Stuart Lloyd, an alumnus of both Bell Labs’ iconic innovation factory and the Manhattan Project that invented the atomic bomb, first proposed k-means clustering in 1957 to distribute information within digital signals. He didn’tpublishit until 1982. Meanwhile, American statistician Edward Forgydescribeda similar method in 1965, leading to its alternative name, the Lloyd-Forgy algorithm.Finding the center:Consider breaking up the party into like-minded working groups. Given the positions of attendees in the room and the number of groups to be formed, k-means clustering can divide the attendees into groups of roughly equal size, each gathered around a central point, or centroid.\n\nDifferent distances:Of course the distance between clustered objects doesn’t need to be spatial. Any measure between two vectors will do. For instance, rather than grouping partygoers according to physical proximity, k-means clustering can divide them by their outfits, occupations, or other attributes. Online shops use it to partition customers based on their preferences or behavior, and astronomers to group stars of the same type.Power to the data points:The idea has spawned a few notable variations:\n\nRevelry inndimensions:Nonetheless, the algorithm in its original form remains widely useful — especially because, as an unsupervised algorithm, it doesn’t require gathering potentially expensive labeled data. It’s also ever faster to use. For instance, machine learning libraries including scikit-learn benefit from the 2002 addition ofkd-treesthat partition high-dimensional data extremely quickly. By the way, if you throw any high-dimensional parties, we’d love to be on the guest list.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/05/ANDREW-atWhiteBoard-QuestionMARK_1200px-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/05/LinearRegression_CarWeight-Milege_1200px.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/05/LogisticRegression_tumbler_1200px.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/05/Heroes-MountainPaths-Gullies_1200px-1.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/05/DeepLearningAI_Banner_Stanford_Teaser_1200x628_Artboard-2--1-.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/05/AdmiralPerceptron_1200px.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/05/DecisionTree_1200px.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/05/K-Means_3Clusters_1200px_Crop-2.webp"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-259/",
    "title": "issue 259",
    "date": "",
    "reading_time": "",
    "content": "AI’s usefulness in a wide variety of applications creates many opportunities for entrepreneurship. In this letter, I’d like to share what might be a counter-intuitive best practice that I’ve learned from leadingAI Fund, a venture studio that has built dozens of startups with extraordinary entrepreneurs. When it comes to building AI applications, we strongly prefer to work on aconcrete idea, meaning a specific product envisioned in enough detail that we can build it for a specific target user.\n\nSome design philosophies say you shouldn’t envision a specific product from the start. Instead, they recommend starting with a problem to be solved and then carefully studying the market before you devise a concrete solution. There’s a reason for this: The more concrete or precise your product specification, the more likely it is to be off-target. However, I find that having something specific to execute toward lets you go much faster and discover and fix problems more rapidly along the way. If the idea turns out to be flawed, rapid execution will let you discover the flaws sooner, and this knowledge and experience will help you switch to a different concrete idea.\n\nOne test of concreteness is whether you’ve specified the idea in enough detail that a product/engineering team could build an initial prototype. For example, “AI for livestock farming” is not concrete; it’s vague. If you were to ask an engineer to build this, they would have a hard time knowing what to build.  Similarly, “AI for livestock tracking in farming” is still vague. There are so many approaches to this that most reasonable engineers wouldn’t know what to build. But “Apply face recognition to cows so as to recognize individual cows and monitor their movement on a farm” is specific enough that a good engineer could quickly choose from the available options (for example, what algorithm to try first, what camera resolution to use, and so on) to let us relatively efficiently assess:\n\nArticulating a concrete idea — which is more likely than a vague idea to be wrong — takes more courage. The more specific an idea, the more likely it is to be a bit off, especially in the details. The general area of AI for livestock farming seems promising, and surely there will be good ways to apply AI for livestock. In contrast, specifying a concrete idea, which is much easier to invalidate, is scary.\n\nThe benefit is that the clarity of a specific product vision lets a team execute much faster. One strong predictor of how likely a startup is to succeed is the speed with which it can get stuff done. This is why founders with clarity of vision tend to be desired; clarity helps drive a team in a specific direction. Of course, the vision has to be a good one, and there’s always a risk of efficiently building something that no one wants to buy! But a startup is unlikely to succeed if it meanders for too long without forming a clear, concrete vision.\n\nBuilding toward something concrete — if you can do so in a responsible way that doesn’t harm others — lets you get critical feedback more efficiently and, if necessary, switch directions sooner. (See myletteron when it’s better to go with a “Ready, Fire, Aim” approach to projects.) One factor that favors this approach is the low cost of experimenting and iterating. This is increasingly the case for many AI applications, but perhaps less so for deep-tech AI projects.\n\nI realize that this advice runs counter to common practice indesign thinking, which warns against leaping to a solution too quickly, and instead advocates spending time understanding end-users, deeply understanding their problems, and brainstorming a wide range of solutions. If you’re starting without any ideas, then such an extended process can be a good way to develop good ideas. Further, keeping ideas open-ended can be good for curiosity-driven research, where investing to pursue deep tech with only a vague direction in mind can pay huge dividends over the long term.\n\nIf you are thinking about starting a new AI project, consider whether you can come up with a concrete vision to execute toward. Even if the initial vision turns out not to be quite right, rapid iteration will let you discover this sooner, and the learnings will let you switch to a different concrete idea.\n\nThrough working with many large corporations, AI Fund has developed best practices for identifying concrete ideas relevant to a business. I’ll share more on this in a later letter.\n\nKeep learning!\n\nAndrew\n\nLearn how to build secure, privacy-focused federated learning systems using the Flower framework in a new two-part short course. Start with the basics in “Intro to Federated Learning,” and explore advanced techniques in “Federated Fine-tuning of LLMs with Private Data.”Enroll for free\n\nA slimmed-down version of Open AI’s multimodal flagship packs a low-price punch.\n\nWhat’s new:OpenAIreleasedGPT-4o mini, a smaller text-image-video-audio generative model that, according to the company, generally outperforms models from Google and Anthropic models of similar size at a lower price for API access. It newly underpins the free version of ChatGPT.\n\nHow it works:GPT-4o mini currently accepts text and image inputs and outputs text. Image output as well as video and audio input/output are coming soon. OpenAI did not provide information about its architecture or training buttoldTechCrunchit’s roughly the size of Claude 3 Haiku, Gemini 1.5 Flash, and the 8-billion-parameter version of Llama 3. It has a context window of 128,000 tokens and can output up to around 16,400 tokens.\n\nBehind the news:GPT-4o mini part of a July wave of smaller large language models.\n\nWhy it matters:Powerful multimodal models are becoming ever more widely available at lower prices, creating opportunities for developers and researchers alike. GPT-4o mini sets a new standard for others to beat. Its price may be especially appealing to developers who aim to build agentic workflows that require models to process large numbers of tokens on their way to producing output.\n\nWe’re thinking:Not long ago, pushing the edge of large language models meant making them larger, with higher computing costs to drive rising parameter counts. But building bigger models has made it easier to develop smaller models that are more cost-effective and nearly as capable. It’s a virtuous circle: Costs fall and productivity rises to everyone’s benefit.\n\nEuropean users won’t have access to Meta’s multimodal models.\n\nWhat’s new:Meta said it wouldwithholdfuture multimodal models from the European Union (EU) to avoid being charged, banned, or fined for running afoul of the region’s privacy laws, according to Axios. (The newly releasedLlama 3.1family, which processes text only, will be available to EU users.)How it works:EU data regulators have said that Meta may be violating EU privacy laws by training models on data from Facebook, Instagram, and its other properties. Meta’s move in Europe follows itswithdrawalof generative models from Brazil, after that country’s national data-protection authoritystruck downthe part of Meta’s privacy policy that allowed it to use personal data from users of Meta products to train AI models.\n\nApple and OpenAI in Europe:Meta is not the only global AI company that’s wary of EU technology regulations.\n\nWhy it matters:Different regions are taking different paths toward regulating AI. The EU is more restrictive than others, creating barriers to AI companies that develop new technology and products. Meta and Apple are taking proactive steps to reduce their risks even if it means foregoing portions of the European market.\n\nWe’re thinking:We hope regulators everywhere will think hard about how to strike a balance between protecting innovation and other interests. In this instance, the EU’s regulations have prompted Meta to make a decision that likely likely set back European AI while delivering little benefit to citizens.\n\nInvestors have been gathering AI chips to attract AI startups.\n\nWhat’s new:Venture-capital firms are stockpiling high-end graphics processing units (GPUs), according to areportbyThe Information. They’re using the hardware to provide processing power to their portfolio companies at reduced or no cost.\n\nHow it works:Andreessen Horowitz (A16Z), a prominent Silicon Valley venture investment firm, has amassed the largest known stock of GPUs dedicated to venture-funded startups. The firm plans to acquire more than 20,000 GPUs including top-of-the-line Nvidia H100s, which can sell for tens of thousands of dollars each — roughly enough to train a competitive large language model.\n\nBehind the news:High-end GPUs were inshort supplyearly last year. The shortage haseasedsignificantly, but getting access to enough processing power to train and run large models still isn’t easy. A16Z follows several other investors that have sought to fill the gap for startups.\n\nYes, but:David Cahn, a partner at A16Z rival Sequoia Capital,arguesthat stockpiling GPUs is a mistake that could leave venture funds holding large quantities of expensive, rapidly depreciating, hardware. Cahn believes startups and small developers soon may have an easier time getting their hands on the processing power they need. Nvidia recentlyannouncedits new B100 and B200 GPUs, whose arrival should stanch demand for older units like the H100.\n\nWhy it matters:AI startups are hot, and venture-capital firms compete for early equity in the most promising ones. In addition to funding, they frequently offer advice, contacts, office support — and now processing power to empower a startup to realize its vision.\n\nWe’re thinking:Venture investors who use GPUs to sweeten a deal give new meaning to the phrase “bargaining chips.”\n\nPrevious systems that produce a talking-head video from a photo and a spoken-word audio clip animate the lips and other parts of the face separately. An alternative approach achieves more expressive results by animating the head as a whole.\n\nWhat’s new:Sicheng Xu and colleagues at Microsoft developedVASA-1, a generative system that uses a facial portrait and spoken-word recording to produce a talking-head video with appropriately expressive motion. You can see its outputhere.\n\nKey insight:When a person speaks, the facial expression and head position change over time, while the overall shapes of the face and head don’t. By learning to represent an image via separate embeddings for facial expression and head position — which change — as well as for facial structure in its 2D and 3D aspects — which don’t — a latent diffusion model can focus on the parts of the image that matter most. (Latent diffusionis a variant of diffusion that saves computation by processing a small, learned vector of an image instead of the image itself.)\n\nHow it works:VASA-1 comprises four image encoders (three 2D CNNs and one 3D CNN), one image decoder (another 2D CNN),Wav2Vec 2.0, and a latent diffusion image generator. The authors trained the system, given an image of a face and a recorded voice, to generate a series of video frames that conform to the voice. The training set wasVoxCeleb2, which includes over 1 million short videos of celebrities talking. The authors added labels for gaze direction, head-to-camera distance, and an emotional intensity score computedbyseparatesystems.\n\nResults:The authors measured their results by training a model similar toCLIPthat produces a similarity score on how well spoken audio matches a video of a person speaking (higher is better). On the VoxCeleb2 test set, their approach produced a similarity score of 0.468 compared to 0.588 for real video. The nearest contender,SadTalker, which generates lip, eye, and head motions separately, achieved a similarity score of 0.441.\n\nWhy it matters:By learning to embed different aspects of a face separately, the system maintained the face’s distinctive, unchanging features while generating appropriate motions. This also made the system more flexible at inference: The authors demonstrated its ability to extract a video’s facial expressions and head movements and apply them to different faces.\n\nWe’re thinking:Never again will we take talking-head videos at face value!\n\nTest, benchmark, and grow your skills with new assessments from Workera! Available domains include AI Foundations, Machine Learning, GenAI, and MLOps.Try Workera today for $0!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--75--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/07/The-Batch-ads-and-exclusive-banners---2024-07-23T084952.204.png",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-24T144159.287.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--76-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--77-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-24T144606.148.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-23T090621.368.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-44/",
    "title": "issue 44",
    "date": "",
    "reading_time": "",
    "content": "I’m thrilled to announce our newNatural Language Processing Specialization! Courses 1 and 2 are available on Coursera. We expect to release Courses 3 and 4 soon.\n\nNLP is reshaping daily life. No doubt you’ve found valuable information using web search and the search functions found on countless websites and apps. Anti-spam systems are a critical part of the global email system. How does a smart speaker understand your commands? How does a chatbot generate relevant responses? This specialization will give you the foundation you need to understand such systems and the knowledge to build them yourself.\n\nYou will implement a sentiment analysis system, build models that translate human languages, and even construct a chatbot. You will master the mostimportant NLP architecturesincluding transformer networks, and you will receive practical, hands-on training to implement techniques like tokenizing text (turning words into features suitable for training neural networks or other machine learning algorithms).\n\nThe courses are taught by two wonderful instructors: Younes Bensouda Mourri, with whom I’ve had the pleasure of working for many years at Stanford, andŁukasz Kaiser, a member of the Google Brain team whom you might recognize as a co-author of TensorFlow.\n\nI invite you to dive into theNLP Specializationand use the skills you gain to do amazing things.\n\nKeep learning!\n\nAndrew\n\nNot long ago, language models were confined to narrow topics and foiled by shifts in context. Today, they’re advancing rapidly thanks to innovations in model architecture, training methods, and distributed computing. Neural networks are translating languages, answering questions, summarizing texts, generating articles that can beindistinguishablefrom those written by reporters at theNew York Times,and even popping off an occasionalpun. This explosion makes it more important than ever that our models track subtle shades of meaning, grasp narrative logic, and choose words that are free of bias with respect to gender and ethnicity. In this special issue ofThe Batch, we probe the frontiers of NLP.\n\nNoam Shazeer helped spark the latest NLP revolution. He developed the multi-headed self-attention mechanism described in “Attention Is All You Need,” the 2017 paper that introduced the transformer network. That architecture became the foundation of a new generation of models that have a much firmer grip on the vagaries of human language. Shazeer’s grandparents fled the Nazi Holocaust to the former Soviet Union, and he was born in Philadelphia in 1976 to a multi-lingual math teacher turned engineer and a full-time mom. He studied math and computer science at Duke University before joining Google in 2000. Below, he discusses the transformer and what it means for the future of deep learning.The Batch:How did you become interested in machine learning?Shazeer:I always liked messing around with the computer and probability was one of my favorite topics. My favorite course in grad school was a seminar where the class collaborated to write a crossword puzzle solver. We got to put together all kinds of different techniques in language processing and probabilities.The Batch:Was that your gateway to NLP?Shazeer:It was a great introduction to the field. They say a picture is worth 1,000 words, but it’s also 1 million times as much data. So language is 1,000 times more information dense. That means it’s a lot easier to do interesting stuff with a given amount of computation. Language modeling feels like the perfect research problem because it’s very simple to define (what’s the next word in the sequence?), there’s a huge amount of training data available, and it’s AI-complete. It’s great working at Google because it’s a language company.The Batch:How did the idea of self-attention evolve?Shazeer:I’d been working with LSTMs, the state-of-the-art language architecture before transformer. There were several frustrating things about them, especially computational problems. Arithmetic is cheap and moving data is expensive on today’s hardware. If you multiply an activation vector by a weight matrix, you spend 99 percent of the time reading the weight matrix from memory. You need to process a whole lot of examples simultaneously to make that worthwhile. Filling up memory with all those activations limits the size of your model and the length of the sequences you can process. Transformers can solve those problems because you process the entire sequence simultaneously. I heard a few of my colleagues in the hallway saying, “Let’s replace LSTMs with attention.” I said, “Heck yeah!”The Batch:The transformer’s arrival was hailed as “NLP’s ImageNet moment.” Were you surprised by its impact?Shazeer:Transformer is a better tool for understanding language. That’s very exciting, and it’s going to affect a lot of applications at Google like translation, search, and accessibility. I’ve been very pleasantly surprised by transfer learning for transformers, which really kicked off with BERT. The fact that you could spend a lot of computation and train a model once, and very cheaply use that to solve all sorts of problems.The Batch:One outcome is an ongoing series of bigger and bigger language models. Where does this lead?Shazeer:According to the papers OpenAI has been publishing, they haven’t seen any signs that the quality improvements plateau as they make the models bigger. So I don’t see any end in sight.The Batch:What about the cost of training these enormous models?Shazeer:At this point, computation costs 10-17to 10-18dollars per operation. GPT-3 was trained using 3×1023operations, which would mean it cost on the order of $1 million to train. The number of operations per word is roughly double the parameter count, so that would be about 300 billion operations per word or roughly 1 millionth of a dollar per word that you analyze or produce. That doesn’t sound very expensive to me. If you buy a paperback book and read it, that costs around one ten-thousandth of a dollar per word. You can still see significant scaling up possible while finding cost-effective applications.The Batch:Where do you find inspiration for new ideas?Shazeer:Mostly building on old ideas. And I often find myself looking at the computational aspects of deep learning and trying to figure out if you could do something more efficiently, or something better equally efficiently. I wasted a lot of time in my first few years in deep learning on things that would never work because fundamentally they weren’t computationally efficient. A lot of the success of deep learning is because it runs many orders of magnitude faster than other techniques. That’s important to understand.The Batch:What’s on the horizon for NLP?Shazeer:It’s hard to predict the future. Translation of low-resource languages is one fun problem, and a very useful one to give way more people the opportunity to understand each other.The Batch:Who is your number-one NLP hero?Shazeer:There have been a massive number of people standing on each other’s shoulders.The Batch:But who stands at the bottom?Shazeer:I don’t know! From here, it looks liketurtles all the way down.\n\nFacebook uses automated systems to block hate speech, but hateful posts can slip through when seemingly benign words and pictures combine to create a nasty message. The social network is tackling this problem by enhancing AI’s ability to recognize context.What’s new:Facebook built ahate speech detectordesigned to recognize that a statement like, “You are welcome here,” is benign by itself but threatening when accompanied by a picture of a graveyard. The model automatically blocks some hateful speech, but in most cases it flags content for humans to review.Key insight:Facebook extracts separate features from various aspects of a post. Then it melds the features to represent the post as a whole.How it works:The system examines 10 different aspects of each post including text, images, video, comments, and external context from the web. Separate models extract feature vectors from these elements, fuse them, and classify the post as benign or hate speech. The training and test data came from the company’s ownHateful Memesdataset. The researchers trained the system using a self-supervised method, hiding portions of input data and training the model to predict the missing pieces. They fine-tuned the resulting features on a labeled dataset of hateful speech.\n\nResults:ABERTmodel achieved 59.2 percent accuracy on a text-only subset of Hateful Memes. The best multimodal classifier released by Facebook,ViLBERT, achieved 63.2 percent accuracy.Free money:If you think you can do better, there’s cash up for grabs in acompetitionfor models that recognize hateful combinations of words and imagery. The contest is set to end in October.Why it matters:The need to stop the viral spread of hatred, fear, and distrust through social media seems to grow only more urgent with the passage of time. Numerous experts have drawn aconnectionbetween online hate speech and real-world violence.We’re thinking:What constitutes hate speech is hard for humans to agree on, never mind neural networks. There is a danger in policing speech either way. But there is greater danger in fanning flames of hostility on a global scale. Companies need strong, ethical leadership that can work with stakeholders to define limits on expressions of hatred. Then AI will be key in implementing such standards at scale. Meanwhile, we hope that blocking examples that are easiest to recognize opens room for reasoned debate about the edge cases.\n\nLearn how to extract the sentiment from text in Course 1 of the NLP Specialization from deeplearning.ai, available now on Coursera. To master more sophisticated techniques using neural networks and transformers, stay tuned for Courses 3 and 4, coming soon to Coursera.\n\nConversational agents have a tough job following the zigs and zags of human conversation. They’re getting better at it — thanks to yesterday’s technology.What’s new:Amazon recentlyimprovedthe Alexa chatbot’s ability to identify the current topic of conversation. The system keeps its responses relevant by tracking the back and forth between itself and the user.Key insight:In conversation, the topic can shift fluidly. The meaning of a word that’s ambiguous in a single conversational exchange, such as “it,” is often clear in light of previous conversational turns. Evaluating several exchanges makes it possible to identify the current topic more accurately.How it works:The system recognizes 12 common topics (like politics, sports, fashion, books, and movies) and 14 intentions (like information request, opinion request, and general chat). The training data came from 100,000 conversations gathered in the2017 Alexa Prizecompetition. Human annotators labeled a topic and intention for each statement.\n\nResults:Amazon evaluated its topic identifier using a test dataset collected alongside the training data. The system exceeded baseline accuracy of 55 percent to achieve 74 percent accuracy when it used context from five conversational exchanges.Why it matters:There’s plenty of life left in older techniques. Given the right data, algorithms from years ago can still do well on modern tasks.We’re thinking:Is it too much to ask that deep learning take its place alongside sports and fashion as one of the 12 topics?\n\nTo learn about word vectors and how to use them in NLP, check out Courses 1 and 2 of the NLP Specialization from deeplearning.ai, now available on Coursera. Build powerful models using RNNs and LSTMs in the upcoming Course 3.\n\nThe words “big” and “large” have similar meanings, but they aren’t always interchangeable: You wouldn’t refer to an older, male sibling as your “large brother” (unless you meant to be cheeky). Choosing among words with similar meanings is critical in language tasks like translation.What’s new:Google used a top language model to developBLEURT, a way to compare translation models.Background:Machine learning engineers typically evaluate a translation model’s ability to choose the right words by translating a sentence from one language to another and back again. The metric calledBLEUquantifies how far the re-translation’s meaning has drifted from that of the original sentence. But BLEU, which scores similarity on a 0-to-1 scale using an n-gram method, often misses nuances. BLEURT does a better job by training a language model to predict the semantic similarity between different sequences of words.Key insight:BERTis a general-purpose, unsupervised language model at the heart of many state-of-the-art systems. Fine-tuned on sentences that humans judge to be similar, it should learn to agree with human notions of similarity.How it works:BLEURT uses BERT to extract feature vectors from an original sentence and its re-translation. A linear layer predicts their similarity.\n\nResults:The authors drew sentences from each of several datasets and created variations on them. BLEURT and BLEU ranked the similarity between each variation and the original, and the authors compared the Kendall Tau correlation, the percentage of pairs assigned the same order minus the percentage of pairs ordered differently, with the human ranking (which is given a score of 1.0). BLEURT achieved a Kendall Tau correlation of 0.338 while BLEU achieved 0.227 — a nice bump, although it leaves plenty of room for improvement.Why it matters:Language models have improved by leaps and bounds in recent years, but they still stumble over context. Better word choices could improve not only automatic translation but the gamut of language tasks including chat, text summarization, sentiment analysis, question answering, and text classification.We’re thinking:BLEUstands for Bilingual Evaluation Understudy. BERT stands for Bidirectional Encoder Representations from Transformers. Does anyone know what BLEURT stands for?\n\nCourse 1 of the NLP Specialization from deeplearning.ai covers translation basics. Learn how to build a cutting-edge encoder/decoder attention model for translation in the upcoming Course 4, coming soon.\n\nWe’re excited to launch our brand-new Natural Language Processing Specialization! Courses 1 and 2 are live on Coursera, with more to come.Enroll now\n\nAI learns human biases: In word vector space, “man is to computer programmer as woman is to homemaker,” as onepaperput it. New research helps language models unlearn such prejudices.What’s new:Double-Hard Debiasimproves on a previous algorithm to mitigate gender bias in trained language generators. Tianlu Wang developed the method with researchers at the University of Virginia and Salesforce.Key insight:The earlierHard Debiasworks by identifying a masculine-to-feminine dimension inword vectors. Words that don’t have gender-specific meanings and, in popular word embeddings, fall at either end of this axis (such asdoctorandnurse) are considered biased. Hard Debias compensates by shrinking the vector’s magnitude in this dimension. However,other workshows the relative frequency of words in various contexts distorts the feature space. For instance,grandfatherappears as a genderless verb in legal discussions, where it means “to exempt,” whilegrandmotherdoesn’t, and that difference deformsgrandfather’s gender dimension. Removing the dimension that encodes such alternative uses should make Hard Debias more effective.How it works:Double-Hard Debias removes this frequency-related dimension before adjusting for gender bias. (It doesn’t affect the processing of inherently gendered words identified by the researchers, such asheandshe.) The researchers applied their method to several models that extract word embeddings including the popularGloVe.\n\nResults:The researchers applied Double-Hard Debias and Hard Debias to separate models. They trained the models on two data subsets drawn from theOntoNotescorpus of informal speech. One was made up of biased statements (say, pairingdoctorwithhe). The other comprised anti-biased statements (for instance, pairingdoctorwithshe). Then they asked the models whoheandshereferred to. The difference in the Hard Debias model’s F1 scores when tested on the biased and unbiased data was 19.7. The difference in the Double Hard Debias model’s F1 scores was 7.7, showing that gender had a far smaller impact on its performance in the task.Why it matters:Bias in machine learning is a serious problem. A medical language model that assumes all doctors are male and all nurses female could make serious mistakes when reading medical reports. Similarly, a legal platform that equatessexual assault victimwithfemalecould lead to unjust outcomes. Solutions like this are crucial stopgaps on the way to developing less biased datasets. The model’s authors toldThe Batchthat Double Hard Debias could be applied towards other types of bias, too.We’re thinking:If you’re building an NLP system, often bias won’t affect metrics like relevance or BLEURT results. But it’s important to attend to it anyway, because bias can have a significant unforeseen impact on users. We need the whole AI community to work hard to reduce undesirable biases wherever possible.\n\nLearn how to create NLP models using word vectors in Courses 1 and 2 of the NLP Specialization from deeplearning.ai. To use word vectors with deep neural networks, stay tuned for Course 3, available soon on Coursera.\n\nNatural language processing lately has come to resemble an arms race, as the big AI companies build models that encompass ever larger numbers of parameters. Microsoft recently held the record — but not for long.What’s new:In February, Microsoft introducedTuring Natural Language Generation(Turing-NLG), a language model that comprises 17 billion parameters.Key insight:More parameters is better. More training data is better. And more compute is better. For the time being, these factors determine the state of the art in language processing.How it works:Like other recent large language models, Turing-NLG is based on the transformer architecture, which extracts features across long sequences of data without having to examine every element in between. Also like its immediate predecessors, it’s trained on unlabeled data via an unsupervised method, which enables it to absorb information from far more text than supervised models have available.\n\nResults:The researchers pitted Turing-NLG against Megatron. Turing-NLG improved state-of-the-art accuracy on theLambadalanguage understanding benchmark from 66.51 percent to 67.98 percent. It also improved perplexity (lower is better) on theWikiTextof verified Wikipedia articles from 10.81 to 10.21.Yes, but:The race to build bigger and better language models doesn’t leave any breathing room even for engineers at the biggest tech powerhouses. Less than four months after Microsoft announced Turing-NLG, OpenAI detailedGPT-3. At 175 billion parameters, it’s roughly 10 times bigger and achieved 76.2 percent accuracy on Lambada.Why it matters:As language models balloon, so do scores on NLP benchmarks. Keep your seatbelts on: Microsoft says its approach to allocating hardware resources can scale past 1 trillion parameters.We’re thinking: The recipe of adding parameters, data, and compute for better performance has a long history. That today’s language models ingest far more text than a human could read in a lifetime reveals both the power of brute-force training and the algorithms’ inefficiency at learning.\n\nTo learn how to build cutting-edge transformer models, stay tuned for Course 4 of the NLP Specialization from deeplearning.ai, coming soon.\n\nLanguage models can’t correct your misspellings or suggest the next word in a text without knowing what language you’re using. For instance, if you type “tac-,” are you aiming for “taco,” a hand-held meal in Spanish, or “taca,” a crown in Turkish? Apple developed a way to head off such cross-lingual confusion.What’s new:It’s fairly easy to identify a language given a few hundred words, but only we-need-to-discuss-our-relationship texts are that long. Apple developed a way to tell, for example, Italian from Turkishbased on SMS-length sequencesof words.Key insight:Methods for identifying languages in longer text passages take advantage of well studied statistical patterns among words. Detecting languages in a handful of words requires finding analogous patterns among letters.How it works:The system comprises only a lightweight biLSTM and a softmax layer. This architecture requires half the memory of previous methods.\n\nResults:The system can spot languages in 50 characters as accurately asmethodsthat require lots of text. Compared with Apple’s previous method based on an n-gram approach, the system improves average class accuracy on Latin scripts from 78.6 percent to 85.7 percent.Why it matters:Mobile devices don’t yet have the horsepower to run astate-of-the-art multilingual language model. Until they do, they’ll need to determine which single-language model to call.We’re thinking: Humans are sending more and more texts that look like this: ????????????. We hope NLP systems don’t go ????.\n\nLearn how to build your own LSTM models for natural language processing in Course 3 of the NLP Specialization from deeplearning.ai, coming soon to Coursera.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1-AndrewsLetter.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/2-Intro.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/3-QnA.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/S1-Facebook.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/S2-Amazon201.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/S3-Google.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/S4-Salesforce.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/S5-Microsoft.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/S6-Apple2.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-207/",
    "title": "issue 207",
    "date": "",
    "reading_time": "",
    "content": "Last week, the White House announced voluntary commitments by seven AI companies, as you can read below. Most of the points were sufficiently vague that it seems easy for the White House and the companies to declare success without doing much that they don’t already do. But the commitment to develop mechanisms to ensure that users know when content is AI-generated, such as watermarks, struck me as concrete and actionable. While most of the voluntary commitments are not measurable, this one is. It offers an opportunity, in the near future, to test whether the White House’s presently soft approach to regulation is effective.I was pleasantly surprised that watermarking was on the list. It’s beneficial to society, but it can be costly to implement (in terms of losing users).\n\nAs Iwrotein an earlier letter, watermarking is technically feasible, and I think society would be better off if we knew what content was and wasn’t AI-generated. However, many companies won’t want it. For example, a company that uses a large language model to create marketing content may not want the output to be watermarked, because then readers would know that it was generated by AI. Also, search engines might rank generated content lower than human-written content. Thus, the government’s push to have major generative AI companies watermark their output is a good move. It reduces the competitive pressure to avoid watermarking.\n\nAll the companies that agreed to the White House’s voluntary commitments employ highly skilled engineers and are highly capable of shipping products, so they should be able to keep this promise. When we look back after three or six months, it will be interesting to see which ones:\n\nTo be fair, I think it would be very difficult to enforce watermarking in open source systems, since users can easily modify the software to turn it off. But I would love to see watermarking implemented in proprietary systems. The companies involved are staffed by honorable people who want to do right by society. I hope they will take the announced commitments seriously and implement them faithfully.\n\nI would love to get your thoughts on this as well. How can we collectively hold the U.S. government and AI companies to these commitments? Please let me know on social media!\n\nKeep learning,\n\nAndrew\n\nP.S. A new short course, developed by DeepLearning.AI and Hugging Face, is available! In “Building Generative AI Applications with Gradio,” instructor Apolinário Passo shows you how to quickly create fun demos of your machine learning applications. Prompting large language models makes building applications faster than ever, but how can you demo your work, either to get feedback or let others to experience what you’ve built? This course shows you how to do it by writing only Python code.\n\nIn the absence of nationwide laws that regulate AI, major U.S. tech companies pledged to abide by voluntary guidelines — most of which they may already be following.\n\nWhat’s new:Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI agreed to uphold a list of responsible-AI commitments, the White Houseannounced.\n\nHow it works:President Biden, Vice President Harris, and other administration officials formulated the terms of the agreement in consultation with tech leaders. The provisions fall into three categories:\n\nBehind the news:The surge of generative AI has spurredcallsto regulate the technology. The rising chorus has given companies ample incentive to accept voluntary limits while trying to shape forthcoming mandates.\n\nYes, but:The commitments — with the exception of watermarking generated output — are relatively easy to fulfill, and some companies may be able to say that they already fulfill them. For instance, many established companies employ independent parties to test for safety and security, and some publish papers that describe risks of their AI research. Leaders in the field already discuss limitations, work to reduce risks, and launch initiatives that address major societal problems. Moreover, the agreement lacks ways to determine whether companies have kept their promises and hold shirkers to account.\n\nWhy it matters:Although some U.S. cities and states regulate AI in piecemeal fashion, the country lacks overarching national legislation. Voluntary guidelines, if companies observe them in good faith and avoid hidden pitfalls, could ease the pressure to assert top-down control over the ways the technology is developed and deployed.\n\nWe’re thinking:These commitments are a step toward guiding AI forward in ways that maximize benefits and minimize harms — even if some companies already fulfill them. Nonetheless, laws are necessary to ensure that AI’s benefits are spread far and wide throughout the world. Important work remains to craft such laws, and they’ll be more effective if the AI community participates in crafting them.\n\nApple insiders spoke anonymously about the company’s effort to exploit the current craze for chatbots.\n\nWhat’s new:Apple built a framework for large language models and used it to develop a chatbot dubbed Apple GPT — for internal use only,Bloombergreported.\n\nUnder wraps:The iPhone maker is proceeding cautiously to capitalize on the hottest tech trend since mobile. The results are not yet available to the public and may never be.\n\nBehind the news:Apple tends to hold its technology close to its vest, but it has not placed the same emphasis on AI as peers. Its pioneering Siri voice assistant has been criticized for falling behind competitors like Amazon Alexa and Google Assistant (which, in turn, were criticized for falling behind ChatGPT). Although it has publishedpaperson generative AI in recent years, its recent productshave not emphasizedthe technology. Meanwhile, its big-tech rivals have been trying to outdo one another in building and deploying ever more powerful chatbots.\n\nWhy it matters:Where some companies zig, Apple often zags. Unlike its peers, it makes its money selling devices and requires tight integration between that hardware and the software that brings it to life. Such differences may make it necessary to “think different” about generative AI.\n\nWe’re thinking:Apple's control over the iOS and MacOS ecosystems is a huge strength in the race to capitalize on generative AI. We hope that Apple’s generative products will be wonderful, but even if they offer little advantage over the competition, its ability to get them into users’ hands will give it a significant advantage over smaller competitors and even many large companies.\n\nJoin “Building Generative AI Applications with Gradio,” our new course built in collaboration with Hugging Face. Learn to quickly build, demo, and ship models using Gradio’s user-interface tools!Sign up for free\n\nIt wasn’t your imagination: OpenAI’s large language models have changed.\n\nWhat’s new:Researchers at Stanford and UC Berkeleyfoundthat the performance of GPT-4 and GPT-3.5 has drifted in recent months. In a limited selection of tasks, some prompts yielded better results than before, some worse.How it works:The authors compared the models’ output in March and June. They aimed not to evaluate overall performance but to show that it had shifted on certain tasks. They prompted the models via ChatGPT to (i) identify whether a prime number is prime, (ii) handle sensitive or harmful prompts, (iii) generate executable code, and (iv) and solve visual puzzles.\n\nYes, but:Commenting on the findings, Princeton computer scientists Arvind Narayanan and Sayash Kapoornotedthat performance differences reported in the paper were consistent with shifts in behavior following fine-tuning. They distinguished between a large language model’scapability(that is, what it can and can’t do given the right prompt), which is informed by pretraining, and itsbehavior(its response to a given prompt), which is shaped by fine-tuning. The paper showed that, while the models’ behavior had changed between March and June, this did not necessarily reflect changes in their capability. For instance, the paper’s authors asked the models to identify only prime numbers as primes; they didn’t test non-primes. Narayanan and Kapoor tested the models on non-primes and obtained far better performance.\n\nBehind the news:For months, rumors have circulated that ChatGPT’s performance had declined. Some usersspeculatedthat the service was overwhelmed by viral popularity, OpenAI had throttled its performance to save on processing costs, or user feedback had thrown the model off kilter. In May, OpenAI engineer Logan Kilpatrickdeniedthat the underlying models had changed without official announcements.\n\nWhy it matters:While conventional software infrastructure evolves relatively slowly, large language models are changing much faster. This creates a special challenge for developers, who have a much less stable environment to build upon. If they base an application on an LLM that later is fine-tuned, they may need to modify the application (for example, by updating prompts).\n\nWe’re thinking:We’ve known we needed tools to monitor and managedata drift and concept drift. Now it looks like we also need tools to check whether our applications work with shifting LLMs and, if not, to help us update them efficiently.\n\nReinforcement learning agents have mastered games like Go that provide complete information about the state of the game to players. They’ve also excelled at Texas Hold ’Em poker, which provides incomplete information, as few cards are revealed. Recent work trained an agent to excel at a popular board game that, like poker, provides incomplete information but, unlike poker, involves long-term strategy.\n\nWhat’s new:Julien Perolat, Bart De Vylder, Karl Tuyls, and colleagues at DeepMind teamed up with former Stratego world champion Vincent de Boer to conceiveDeepNash, a reinforcement learning system that reached expert-level capability at Stratego.\n\nStratego basics:Stratego is played by two opposing players. The goal is to capture the opponent’s flag piece by moving a piece onto a space that contains it. The game starts with a deployment phase, in which the players place on a board 40 pieces that represent military ranks, as well as a flag and a bomb. The pieces face away from the opposing player, so neither one knows the other’s starting formation. The players move their pieces by turns, potentially attacking each other’s pieces by moving onto a space occupied by an opponent’s piece; which reveals the rank of the opponent’s piece. If the attacking piece has a higher rank, the attack is successful and the opponent’s piece is removed from the board. If the attacking piece has a lower rank, the attack fails and the attacking piece is removed.\n\nKey insight:A reinforcement learning agent like AlphaGo learns to play games through self-play; that is, it plays iteratively against a copy of itself, adjusts its weights according to rewards it has received, and — after an interval of learning — adopts the weights of the better-performing copy. Typically, each copypredictsthe potential outcome of every possible action and chooses the one that’s most likely to confer an advantage. However, this approach can go awry if one of the copies learns to win by exploiting a vulnerability that’s idiosyncratic to the agent but not to human players. That’s where regularization can help: To prevent such overfitting and enable agents to learn a more generalized strategy, previousworkshowed that it helps to reward an agent for — in addition to good moves and winning — predicting the same probabilities that actions will be advantageous as an earlier version of itself. Updating this earlier version periodically enables the agent to keep improving.\n\nHow it works:DeepNash comprised fiveU-Netconvolutional neural networks. One produced an embedding based on the current state of the game board and the most recent 40 previous states. The remaining four U-Nets used the embedding as follows: (i) during training, to estimate the total future reward to be expected after executing a deployment or move, (ii) during the game’s deployment phase, to predict where each piece should be deployed, (iii) during the play phase, to select which piece to move and (iv) to decide where that piece should move.\n\nResults:DeepNash beat the most powerful Stratego bots on theGravongame platform, winning 97.1 percent of 800 games. It beat Gravon’s human experts 84 percent of the time, ranking third as of April 22, 2022. Along the way, it developed deceptive tactics, fooling opponents by moving less-powerful pieces as though they were more powerful and vice-versa.\n\nWhy it matters:Reinforcement learning is a computationallyinefficientway to train a model from scratch to find good solutions among a plethora of possibilities. But it mastered Go, a game with 10360possible states, and it predicts protein shapes among10300possible configurations of amino acids. DeepNash sends the message that reinforcement learning can also handle Stratego’s astronomical number of 10535states, even when those states are unknown.\n\nWe’re thinking:DeepNash took advantage of the Stratego board’s imperfect information by bluffing. Could it have developed atheory of mind?\n\nJoin our upcoming workshop on August 3, 2023, at 10:00 a.m. Pacific Time! Learn the fundamentals of reinforcement learning and how to integrate human feedback into the learning process.Register now\n\nSan Francisco Bay Area is the dominant hub for AI jobs, study findsAccording to research by the Brookings Institution, the San Francisco Bay Area exerts dominance in the generative AI industry within the U.S. Among more than 2,200 generative AI job listings across 380 metro areas, 25 percent were in the Bay Area. This finding solidifies the region's position as a key player in the industry. (The New York Times)Synthetic data fuels development of generative AIPlayers like Microsoft, OpenAI, and Cohere are venturing into synthetic data as they near the limits of human-made data for training large language models. Synthetic data is becoming a cost-effective alternative due to the expense and limitations of human-generated data. However, AI companies are evaluating risks associated with this approach. (Financial Times)AI cameras to monitor Paris during OlympicsDuring the 2024 Olympics, cameras will identify anomalies like crowd rushes, fights, or unattended bags. While face recognition is banned by law, civil rights groups express concerns that other AI-powered surveillance methods could threaten civil liberties and fear that the French government may make the Olympics security provisions permanent. (BBC)Foreign investors fuel Taiwan’s AI and chipmaking sectorsOver the past six months, net foreign buying of Taiwanese stocks reached $12 billion, and the country's benchmark index surged by 20 percent in U.S. dollar terms. Despite concerns over Taiwan's slowing economy and military threats by China, investors find its tech sector compelling and have confidence in its AI supply chain. (Reuters)Wix to enable users to build websites using promptsWix's AI Site Generator will generate website designs, text, and images based on text descriptions. The resulting site can be customized using the Wix Editor. (The Verge)Roadway analytics system spots drug traffickerNew York police used a platform called Rekor to identify a drug trafficker based on his driving patterns. Rekor's software analyzed data from a county-wide automatic license plate recognition system. The platform flagged a driver’s suspicious routes, leading to the arrest. The case raised concerns over privacy and legal protections as the technology's use expands and surveillance capabilities grow. (Gizmodo)Google testing a tool that generates news storiesThe company pitched a system called Genesis to major news organizations including The New York Times and The Washington Post. Intended as a journalist's assistant, the system could automate tasks and save time but raises concerns about accuracy and journalistic integrity. (The New York Times)Redditors trick automated news site by posting false informationMembers of the World of Warcraft forum on Reddit suspected their discussions were being scraped and used to create news stories by the gaming site Zleague. As a prank, they crafted a false story about a non-existent feature called Glorbo. The site duly published a news article about the imaginary feature. (BBC)Israeli military deployed AI in Gaza and IranThe Israel Defense Forces used AI systems to select targets for air strikes in occupied territories and Iran. Officials confirmed the use of recommendation systems to streamline air strikes, raising concerns that automated decisions could have severe consequences on the battlefield. (The Economic Times)Helsing developing a battlefield systemThe defense-tech company Helsing is building a system for warfare that analyzes data from sensors and weapons to visualize battlefield conditions in real time. The company has signed contracts with European militaries and is integrating AI into existing weapons systems with established defense contractors. (Wired)Generative AI tool targets real estateEthan, an app from the startup Termsheet, assists real estate firms in making property investment decisions. Ethan compiles property and market data to draft memos that recommend buy and sell options. The tool aims to streamline tedious tasks, freeing up time for more strategic and value-adding activities. (Business Insider)AI cloud platform specializes in open source modelsTogether.ai offers Together API and Together Compute, a platform for cost-efficient access to leading open source models. The platform enables anyone to train, fine-tune and run models without proprietary restrictions. (VentureBeat)World Ethical Data Foundation released framework for responsible AIThe Me-We-It framework aims to create a clear and accountable process for building AI responsibly. It focuses on 84 questions and considerations that every AI team should address to adhere to ethical standards. (World Ethical Data)New York City fights subway fare evaders with AIThe Metropolitan Transportation Authority discreetly introduced a system to track fare evasion in seven subway stations. The MTA's director stated that its purpose is to measure lost revenue from fare evasion. However, privacy advocates say the move could impinge on privacy rights.(NBC News)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/07/ezgif.com-webp-to-jpg--14-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--72-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--73-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--40-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--74-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--75-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/07/The-Batch-ads-and-exclusive-banners--43-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-243/",
    "title": "issue 243",
    "date": "",
    "reading_time": "",
    "content": "Tool use, in which an LLM is given functions it can request to call for gathering information, taking action, or manipulating data, is a key design pattern ofAI agentic workflows. You may be familiar with LLM-based systems that can perform a web search or execute code. Indeed, some of the large, consumer-facing LLMs already incorporate these features. But tool use goes well beyond these examples.If you prompt an online LLM-based chat system, “What is the best coffee maker according to reviewers?”, it might decide to carry out a web search and download one or more web pages to gain context. Early on, LLM developers realized that relying only on a pre-trained transformer to generate output tokens is limiting, and that giving an LLM a tool for web search lets it do much more. With such a tool, an LLM is either fine-tuned or prompted (perhaps with few-shot prompting) to generate a special string like {tool: web-search, query: \"coffee maker reviews\"} to request calling a search engine. (The exact format of the string depends on the implementation.) A post-processing step then looks for strings like these, calls the web search function with the relevant parameters when it finds one, and passes the result back to the LLM as additional input context for further processing.\n\nSimilarly, if you ask, “If I invest $100 at compound 7% interest for 12 years, what do I have at the end?”, rather than trying to generate the answer directly using a transformer network — which is unlikely to result in the right answer — the LLM might use a code execution tool to run a Python command to compute 100 * (1+0.07)**12 to get the right answer. The LLM might generate a string like this: {tool: python-interpreter, code: \"100 * (1+0.07)**12\"}.\n\nBut tool use in agentic workflows now goes much further. Developers are using functions to search different sources (web, Wikipedia, arXiv, etc.), to interface with productivity tools (send email, read/write calendar entries, etc.), generate or interpret images, and much more. We can prompt an LLM using context that gives detailed descriptions of many functions. These descriptions might include a text description of what the function does plus details of what arguments the function expects. And we’d expect the LLM to automatically choose the right function to call to do a job.\n\nFurther, systems are being built in which the LLM has access to hundreds of tools. In such settings, there might be too many functions at your disposal to put all of them into the LLM context, so you might use heuristics to pick the most relevant subset to include in the LLM context at the current step of processing. This technique, which is described in the Gorilla paper cited below, is reminiscent of how, if there is too much text to include as context, retrieval augmented generation (RAG) systems offer heuristics for picking a subset of the text to include.\n\nEarly in the history of LLMs, before widespread availability of large multimodal models (LMMs)  like LLaVa, GPT-4V, and Gemini, LLMs could not process images directly, so a lot of work on tool use was carried out by the computer vision community. At that time, the only way for an LLM-based system to manipulate an image was by calling a function to, say, carry out object recognition or some other function on it. Since then, practices for tool use have exploded. GPT-4’s function calling capability, released in the middle of last year, was a significant step toward general-purpose tool use. Since then, more and more LLMs are being developed to similarly be facile with tool use.\n\nIf you’re interested in learning more about tool use, I recommend:\n\nBoth Tool Use and Reflection, which I described in last week’sletter, are design patterns that I can get to work fairly reliably on my applications — both are capabilities well worth learning about. In future letters, I’ll describe the Planning and Multi-agent collaboration design patterns. They allow AI agents to do much more but are less mature, less predictable — albeit very exciting — technologies.Keep learning!\n\nAndrew\n\nP.S. Learn to carry out red-teaming attacks against your own LLM-based applications to spot and patch vulnerabilities! In our new short course, “Red Teaming LLM Applications,” Matteo Dora and Luca Martial of LLM testing company Giskard teach how to simulate malicious actions to discover vulnerabilities and improve security. We start with prompt injection, which can trick an LLM into bypassing safeguards to reveal private information or say something inappropriate. There is no one-size-fits-all approach to security, but this course will help you identify some scenarios to protect against.\n\nWe believe that widespread knowledge of red-teaming capabilities will result in greater transparency and safer LLM-based systems. However, we ask you to use the skills you gain from this course ethically.\n\nSign up here\n\nMicrosoft took over most of the once high-flying chatbot startup Inflection AI in an unusual deal.\n\nWhat’s new:Microsoft hired Inflection CEO Mustafa Suleyman and much of the startup’s staff and paid roughly $650 million for access to its models and legal protections,Bloombergreported. Inflection willshiftfrom serving consumers to focusing on large companies.How it works:Microsoft did not formally purchase any assets of Inflection, which remains a separate, independent company. $650 million is significantly less than the $1.3 billion in investment that Inflectionreceivedlast year at a $4 billion valuation.\n\nBehind the news:Inflection was co-founded in 2022 by Suleyman (a founder of DeepMind, now a division of Google), Simonyan, and LinkedIn chairman Reed Hoffman with funding partly from Microsoft. The startup initially positioned itself as a competitor to OpenAI and Anthropic, seeking to develop AI assistants for consumers. Its flagship product wasPi, a chatbot trained to provide emotional support. Microsoft CEO Satya Nadella began courting Suleyman several months ago, and Suleyman wanted to bring Inflection’s staff along with him. Microsoft made a similar offer to OpenAI in November, during that company’s leadershipshakeup, when the tech giant proposed hiring briefly-ousted CEO Sam Altman and many of his co-workers to staff a new organization at Microsoft.\n\nYes, but:The unusual nature of the deal — with Microsoft absorbing most of Inflection’s staff while leaving the startup intact as a company — may have beendesignedto avoid the antitrust scrutiny that comes with acquisitions. The deal doesn’t automatically trigger areviewby U.S. regulators because Microsoft did not acquire Inflection assets. Microsoft’s close relationship with OpenAI has attracted attention from regulators in theU.S.,UK, andEU.Why it matters:Tech giants are searching for an edge in AI development after being briefly leapfrogged in the market by large language model startups. Microsoft invested $13 billion inOpenAI, and Nadella says that partnership remains a strategic priority. This year, Microsoft has sought to diversify its AI interests, sealing deals withMistraland now Inflection, while also beefing up its internal efforts. The distribution channel for AI models increasingly runs through large companies and their cloud services.We’re thinking:Even with strong talent, powerful backing, and a multibillion-dollar valuation, Inflection struggled to gain traction. Its journey from hot consumer startup to streamlined enterprise software provider shows how competitive the chatbot sector has become.\n\nNvidia’s latest chip promises to boost AI’s speed and energy efficiency.\n\nWhat’s new:The market leader in AI chipsannouncedthe B100 and B200 graphics processing units (GPUs) designed to eclipse its in-demand H100 and H200 chips. The company will also offer systems that integrate two, eight, and 72 chips.\n\nHow it works:The new chips are based on Blackwell, an updated chip architecture specialized for training and inferencing transformer models. Compared to Nvidia’s earlier Hopper architecture, used by H-series chips, Blackwell features hardware and firmware upgrades intended to cut the energy required for model training and inference.\n\nPrice and availability:The B200 will cost between $30,000 and $40,000, similar to thegoing ratefor H100s today, Nvidia CEO Jensen HuangtoldCNBC. Nvidia did not specify when the chip would be available. Google, Amazon, and Microsoftstatedintentions to offer Blackwell GPUs to their cloud customers.\n\nBehind the news:Demand for the H100 chip has been so intense that the chip has beendifficultto find, driving some users to adopt alternatives such as AMD’s MI300X. Moreover, in 2022, the U.S.restrictedthe export of H100s and other advanced chips to China. The B200 also falls under the ban.Why it matters:Nvidiaholdsabout 80 percent of the market for specialized AI chips. The new chips are primed to enable developers to continue pushing AI’s boundaries, training multi-trillion-parameter models and running more instances at once.We’re thinking:Cathie Wood, author of ARK Invest’s “Big Ideas 2024”report, estimated that training costs are falling at a very rapid 75 percent annually, around half due to algorithmic improvements and half due to compute hardware improvements. Nvidia’s progress paints an optimistic picture of further gains. It also signals the difficulty of trying to use model training to build a moat around a business. It’s not easy to maintain a lead if you spend $100 million on training and next year a competitor can replicate the effort for $25 million.\n\nIn our new short course “Red Teaming LLM Applications,” you will learn industry-proven manual and automated techniques to proactively test, attack, and improve the robustness of your large language model (LLM) applications.Join now!\n\nScientists pledged to control their use of AI to produce potentially hazardous biological materials.\n\nWhat’s new:More than 150 biologists in Asia, Europe, and North Americasigneda voluntary commitment to internal and external oversight of machine learning models that can be used to design proteins.\n\nHow it works:The scientists made 10 voluntary commitments regarding synthetic biology research. They promised broadly to avoid research likely to enable harm and to promote research that responds to infectious disease outbreaks or similar emergencies.\n\nBehind the news:The potential role of AI in producing bioweapons is a major focus of research in AI safety. The current pledge arose from a University of Washington meeting on responsible AI and protein design held late last year. TheAI Safety Summit, which took place at around the same time, also addressed the topic, and Helena, a think tank devoted to solving global problems, convened a similar meeting in mid-2023.\n\nWhy it matters:DeepMind’sAlphaFold, which finds the structures of proteins, hasspawnedmodels that enable users to design proteins with specific properties. Their output could help scientists cure diseases, boost agricultural production, and craft enzymes that aid industrial processes. However, their potential for misuse has led to scrutiny bynationalandinternationalorganizations. The biology community’s commitment to use such models safely may reassure the public and forestall onerous regulations.\n\nWe’re thinking:The commitments are long on general principles and relatively short on concrete actions. We’re glad they call for ongoing revision and action, and we hope they lead to the development of effective safeguards.\n\nLarge language models sometimes generate false statements. New work makes them more likely to produce factual output.\n\nWhat’s new:Katherine Tian, Eric Mitchell, and colleagues at Stanford and University of North Carolina proposedFactTune, a procedure that fine-tunes large language models (LLMs) to increase their truthfulness without collecting human feedback.\n\nKey insight:Just as fine-tuning based on feedback has made LLMs less harmful, it can make them more factual. The typical method for such fine-tuning is reinforcement learning from human feedback (RLHF). But a combination ofdirect preference optimization(DPO) andreinforcement learning from AI feedback(RLAIF) is far more efficient. DPO replaces cumbersome reinforcement learning with a simpler procedure akin to supervised learning. RLAIF eliminates the cost of collecting human feedback by substituting model-generated preferences for human preferences.\n\nHow it works:The authors built models designed to deliver factual output within a specific domain.\n\nResults:Fine-tuning by the authors’ method improved the factuality of models in two domains.\n\nWhy it matters:LLMs are known to hallucinate, and the human labor involved in fact checking their output is expensive and time-consuming. The authors applied well tested methods to improve the factuality of texts while keeping human involvement to a minimum.\n\nWe’re thinking:This work, among others, shows how LLMs can bootstrap their way to better results. We’ve only just begun to explore combinations of LLMs working together as well as individual LLMs working iteratively in anagentic workflow.\n\nThe latest AI news in brief is yours withData Points, a spinoff of The Batch:\n\nOpenAI is cautiously probing the synthetic voice market with Voice Engine.\n\nAlso, a city in California is testing an AI tool to identify homeless encampments.\n\nPlus, a tech coalition plans on challenging Nvidia's lock-in of its chip users by developing new open-source cloud software..\n\nFind these stories and more.Read Data Points now.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/04/TOOL-USE-3.png",
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed--56-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-03T133929.961.png",
      "https://dl-staging-website.ghost.io/content/images/2024/04/The-Batch-ads-and-exclusive-banners---2024-04-02T085923.011.png",
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-03T133956.061.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-03T133938.278.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-90/",
    "title": "issue 90",
    "date": "",
    "reading_time": "",
    "content": "It can take 6 to 24 months to bring a machine learning project from concept to deployment, but a specialized development platform can make things go much fasterMy team at Landing AI has been working on a platform called LandingLens for efficiently building computer vision models. In the process, I’ve learned important lessons about how such platforms can help accelerate the machine learning project lifecycle:\n\nIt used to take me months to deploy a model. With a no-code platform, I can train a RetinaNet demo, carry out error analysis, use a data-centric approach to clean up inconsistent data, retrain, and deploy to an edge device — all in 60 minutes. I get a thrill every time I go through the machine learning project lifecycle so quickly.Platforms like this can help a variety of AI projects across all industries. LandingLens works well for visual inspection in areas as diverse as automotive, semiconductor, and materials, I’m hoping to make it more widely available. Its sweet spot is computer vision problems (detection or segmentation) with 30 to 10,000 images. If you have a business problem in computer vision that falls in this sweet spot, I’d like to hear from you. Please get in touch by filling out thisform.\n\nKeep learning!\n\nAndrew\n\nLanguage models are starting to take on programming work.\n\nWhat’s new:SourceAIuses GPT-3 to translate plain-English requests into computer code in 40 programming languages. The French startup is one of several companies that use AI to ease coding, according toWired.\n\nHow it works:Companies have trained language models to anticipate programmers’ needs.\n\nBehind the news:Other companies are also using machine learning to increase coders’ productivity and sniff out bugs.\n\nWhy it matters:In the hands of a skilled programmer, such tools can save time, freeing up brainpower for more complex tasks. In the hands of the newbie, they make it possible to create applications with little experience and — with diligent attention — gain skills more quickly.\n\nWe’re thinking:No AI system should replace a sacred rite of passage for neophyte coders: print (“Hello World!”).\n\nAI-powered surveillance is becoming a staple in U.S. banks.\n\nWhat’s new:Several banks are using cameras equipped with computer vision to bolster security and boost employee productivity, according toReuters.\n\nWhat’s up:The companies have a variety of aims and approaches.\n\nBehind the news:The latest moves build on earlier attempts by financial institutions to take advantage of image recognition technology.\n\nWhy it matters:If banks can get regulators and consumers to accept AI-assisted surveillance in branch offices, it will add momentum to wider adoption of the technology.\n\nWe’re thinking:Many of these use cases seems more like surveillance than security. Without sufficient sensitivity to public concerns, such efforts is likely to inspire backlash. Organizations that aim to take advantage of this technology: Tread cautiously.\n\nBuilding a career in deep learning? Training models is important, but you also need to know production engineering. That’s why we’re launching Machine Learning Engineering for Production (MLOps) on May 12, 2021.Sign up to learn about upcoming launches\n\nA new study suggests tactics for machine learning engineers to cut their carbon emissions.\n\nWhat’s new:Led by David Patterson, researchers at Google and UC Berkeleyfoundthat AI developers can shrink a model’s carbon footprint a thousand-fold by streamlining architecture, upgrading hardware, and using efficient data centers.\n\nWhat they did:The authors examined the total energy used and carbon emitted by five NLP models:GPT-3,GShard,Meena,Switch Transformer, andT5. They reported separate figures for training and inference. Generally, they found that inference consumes more energy than training.\n\nBehind the news:The authors joined theAllen Instituteandothersin calling for greener AI. To this end, MLCommons, the organization behind the MLPerf benchmark, recently introducednew toolsto measure a model’s energy consumption alongside traditional performance metrics.\n\nWhy it matters:Training and deploying a large model can emit five times more carbon dioxide than a single car over the course of its lifetime. As AI becomes more widespread, energy efficiency becomes ever more important.\n\nWe’re thinking:There are bigger levers for reducing carbon emissions, such as transitioning the world away from coal power. Still, as a leading-edge industry, AI has an important role in building a the green future.\n\nIn the open-ended video gameMinecraft, players extract blocks of virtual materials from a 3D environment to assemble objects of their own design, from trees to cathedrals. Researchers trained neural networks to generate these structures.\n\nWhat’s new:Shyam Sudhakaran and researchers at University of Copenhagen, University of York, and Shanghai University used a neural cellular automaton algorithm toconstruct 3D objects. The work demonstrates the potential for such algorithms to generate structures in three dimensions, as typically they’re limited to two.\n\nKey insight:Acellular automatongenerates complex patterns on a 2D grid by changing each cell’s state iteratively based on simple rules that depend on the states of its neighbors. Aneural cellular automatonupdates cells depending on the output of a neural network and the states of neighboring cells. Using 3D convolutions enables a neural cellular automaton to generate patterns in 3D.\n\nHow it works:The authors trained several 3D convolutional neural networks to reproduce structures found on the community websitePlanet Minecraft. Each different structure required its own model. The structures comprised 50 block types mostly corresponding to materials (stone, glass, metals, and so on), including piston blocks that push or pull adjacent blocks to produce animated objects. The system spawned block types directly without needing to virtually mine them out of the virtual ground.\n\nResults:The authors reported few quantitative results. However, the trained models grew static structures like castles, temples, and apartments that appear to be accurate inside and out. One model learned to grow an animated caterpillar.\n\nWhy it matters:Cellular automata may have certain benefits. For instance, if part of the resulting structure is destroyed, the automaton can use what’s left to regenerate the missing part. This approach can produce resilient digital 3D structures with no human intervention after the first step.\n\nWe’re thinking:Machine learning engineers looking for an excuse to play Minecraft need look no further!\n\nFourthBrain is launching its MLOps And Systems bootcamp training program! Our live instructors will prepare you with MLOps tools, skills, and best practices for deploying, evaluating, monitoring, and operating production ML systems. Live info session on May 19, 2001.Register now",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/05/code_revised-2.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/05/BANK576x324.gif",
      "https://ci5.googleusercontent.com/proxy/HRuAihR0LJ2IbUxjhoHwUiXISBS-Wz384AEjj0W1VbYjPjc_ukghp26LzbHYbVOTZQA5syemmbxhRFMNwM-XD_WTDXrE6MeHexSjTfrGMPwbbAMWEiDTIaY8UuF87GSnXVdiP5o25kKp2TNek4ZU3N9dK4EeNX__Y-8tyzAkixkHf3m5Qu9WNs-qMZC829GTPwn5fgWURFc=s0-d-e1-ft#https://info.deeplearning.ai/hs-fs/hubfs/Specialization%20Banner-2-1.png?width=676&upscale=true&name=Specialization%20Banner-2-1.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/ezgif.com-gif-maker---2021-04-27T113917.099-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/05/ezgif.com-gif-maker---2021-04-20T095340.976-1.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-05-04%20at%204.42.17%20PM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-05-04%20at%204.42.17%20PM%20copy.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-51/",
    "title": "issue 51",
    "date": "",
    "reading_time": "",
    "content": "I spoke last week at theNational Intergovernmental Audit Forum, a meeting attended by U.S. federal, state, and local government auditors. (Apparently some of the organizers had takenAI for Everyone.) Many attendees wanted to know how AI systems can be rolled out in a responsible and accountable way.\n\nConsider the banking industry. Many regional banks are under tremendous competitive pressure. How well they assess risk directly affects their bottom line, so they turn to credit scoring systems from AI vendors. But if they don’t have the technical expertise to evaluate such models, a hasty rollout can lead to unintended consequences like unfairly charging higher interest rates on loans to minority groups.\n\nFor AI systems to enjoy smooth rollouts, we need to (a) make sure our systems perform well and pose minimal risk of unintended consequences and (b) build trust with customers, users, regulators, and the general public that these systems work as intended. These are hard problems. They require not just solving technical issues but also aligning technology with society’s values, and expectations.\n\nAn important part of the solution is transparency. The open source software movement has taught us that transparency makes software better. And if making source code publicly available means that someone finds an embarrassing security bug, so be it! At least it gets fixed.\n\nWith the rise of AI, we should similarly welcome third-party assistance, such as allowing independent parties to perform audits according to a well established procedure. That way, we can identify problems and fix them quickly and efficiently.\n\nAfter my presentation, the moderator asked me how auditors can avoid getting into adversarial relationships with AI vendors. Instead, we need to build collaborative relationships. By collaborating, we can help make sure the criteria used to judge our systems is reasonable and well specified. For instance, what are the protected groups we need to make sure our systems aren’t biased against? We can also better avoid “gotcha” situations in which our systems are assessed according to arbitrary, after-the-fact criteria.\n\nThe AI community has a lot of work to do to ensure that our systems are fair, accountable, and reliable. For example, Credo AI (disclosure: a portfolio company of AI Fund, a sister organization to deeplearning.ai) is building tools that help audit and govern AI systems. Efforts like this can make a difference in designing and deploying AI systems that benefit all people.\n\nKeep learning!\n\nAndrew\n\nA prosthetic leg that learns from the user’s motion could help amputees walk more naturally.What’s new:Researchers from the University of Utahdesigneda robotic leg that uses machine learning to generate a human-like stride. It also helps wearers step over obstacles in a natural way.How it works:Rather than trying to recognize obstacles in the user’s path, the prosthesis relies on cues from the user’s body to tell it when something is in the way. Sensors in the user’s hip feed data a thousand times per second into a processing unit located in the unit’s calf. For instance, the way a user rotates their hip might tell the leg to tuck its knee to avoid tripping over an obstacle.\n\nBehind the news:A new generation of AI-powered prosthetics could give amputees more control over robotic limbs.\n\nWhy it matters:Battery-powered prostheses allow amputees to walk more easily, but they tend to stumble on unfamiliar terrain. This smart leg could provide them with smooth, hazard-free perambulation.We’re thinking:AI is helping people with the most basic human functions as well as the most abstract scientific problems.\n\nLanguage models like Bert, Ernie, and Elmo have achieved spectacular results based on clever pre-training approaches. New research applies some of thoseSesame Streetlessons into image processing.What’s new:OpenAI researchers led by Mark Chen adapted to pixels techniques developed for processing words inImage Generative Pre-Training(iGPT).Key insight:Language models based on the transformer architecture learn to predict the next word, or missing words, in text by unsupervised pre-training on an enormous corpus followed by supervised fine-tuning. The same approach can train models to predict the next pixel in an image.How it works:iGPT uses theGPT-2architecture that made waves in natural language processing. However, it learns from sequences of pixels instead of sequences of words.\n\nResults:Using features extracted by the intermediate layers in the autoregressive task, iGPT achieved 72 percent accuracy onImageNet, just behind the state-of-the-art 76.5 percent achieved bySimCLR, a popular unsupervised approach. iGPT outperformed SimCLR when fine-tuned and evaluated on theCIFARdatasets.Yes, but:The researchers had to downsample ImageNet examples to about 7 percent of their original size to accommodate GPT-2. They suspect that iGPT would stack up better against SimCLR if it could accept larger images.Why it matters:iGPT isn’t a convolutional neural network. It doesn’t even use the convolutional filter that’s fundamental to current image processing methods. This work shows the value of applying architectures proven in one domain to others.We’re thinking:We’ve been encouraged by the progress in self-supervised learning using methods likeContrastive Predictive Codingand variations thereof, in which a neural network is trained on a supervised learning task that is created from unlabeled data. iGPT appears to be a new line of attack on this problem.\n\nChips specially designed for AI are becoming much faster at training neural networks, judging from recent trials.What’s new:MLPerf, an organization that’s developing standards for hardware performance in machine learning tasks, releasedresultsfrom its third benchmark competition. Nvidia’s latest products led the pack, but Google’s forthcoming hardware surpassed Nvidia’s scores.Start your engines:MLPerf measures how long it takes various hardware configurations to train particular machine learningmodels. Tasks include object detection, image classification, language translation, recommendation, and reinforcement learning goals.\n\nBehind the news:Nvidia’s GPUs have long been the premier machine learning chips, thanks to their ability to process large volumes of floating point integers per second. But startups including Cerebras, Graphcore, and Habana (acquired by Intel in December) are vying for that position, and Google Cloud is making a strong play for AI workloads.Why it matters:It’s good to be past the era ofMythbusters videosas a way to compare AI hardware. Machine learning engineers benefit from faster, more energy-efficient hardware systems, but we need clear, consistent metrics like MLPerf to evaluate hardware performance with particular models.We’re thinking:Since MLPerf’s first tests two years ago, the time required to train some models has plummeted from hours to seconds. Clearly semiconductor companies have been chipping away at the problem.\n\nCourse 3 of our Natural Language Processing Specialization is now live on Coursera.Enroll todayto gain in-demand technical skills! Course 4 is scheduled for release in September.\n\nNeural networks are famously bad at interpreting input that falls outside the training set’s distribution, so it’s not surprising that some models are certain that cat pictures show symptoms of Covid-19. A new approach won’t mistakenly condemn your feline to a quarantine.What’s new:Led by Ankur Mallick, researchers at Carnegie Mellon and Lawrence Livermore National Lab developedProbabilistic Neighborhood Components Analysis(PNCA) to help models estimate the confidence of their predictions.Key insight:Neural networks often show high confidence in predictions that are clearly incorrect — a major issue in areas like healthcare and criminal justice. The problem can fade with enough training data, but it’s pervasive where training data is scarce. Overfitting limited data contributes to overconfidence, so combining deep learning with probabilistic methods, which are less prone to overfitting, might alleviate overconfidence.How it works:PNCA is a probabilistic version ofNeighborhood Component Analysis. NCA is a supervised learning method that trains neural nets to extract features that cluster examples of the same class. NCA determines the class of novel input by computing the distance between training data features and input features. It takes the softmax of the distances to obtain the probability that each training example belongs to the same class of the novel input. Practically speaking, NCA is a classification network with fixed output layer weights, but not size, given by the distance function.\n\nResults:The researchers trained PNCA on aKaggle datasetof chest x-rays showing Covid-19, and tested it onCovid-V2and aCats and Dogs dataset. PNCA performed with similar accuracy to other deep learning approaches on Covid-V2, while incorrectly classifying 1,000 cats and dogs out of 25,000 as Covid-19 with high confidence. This may seem like poor performance, but the same architecture with a standard supervised learning objective mistook around 2500 cats and dogs as Covid-19 chest x-rays.Why it matters:Deep learning’s overconfidence and data hunger are limitations to their practical deployment. PNCA combines deep learning’s powerful feature extraction with a probabilistic ability to quantify uncertainty.We’re thinking:We’re waiting for a model that can tell us the condition of Schroedinger’s cat.\n\nA high-profile semiconductor startup made a bid for the future of AI computation.What’s new:UK startup Graphcore released theColossus Mk2, a processor intended to perform the matrix math calculations at the heart of deep learning more efficiently than other specialized processors or general-purpose chips from Intel and AMD. The company expects to be shipping at full volume in the fourth quarter.How it works:The Mk2 comprises nearly 60 billion transistors. (Nvidia’s flagship A100 has 54 billion, while Cerebras’ gargantuan Wafer-Scale Engine boasts 1.2 trillion. Google doesn’t advertise its TPU transistor counts.) Girded by 900 megabytes of random access memory, the Mk2’s transistors are organized into 1,500 independent cores capable of running nearly 9,000 parallel threads.\n\nWhy it matters:AI’s demand for computational resources is insatiable. A recentstudyfrom researchers at MIT, the University of Brasilia, and Yonsei University suggests that progress in deep learning could stall for lack of processing power. Innovations in chip technology may make a difference.We’re thinking:The fact that software evolves faster than hardware is a major challenge to building chips. Graphcore’s design is geared to accelerate large, sparse recurrent neural networks at a moment when transformer networks are beginning to supplant RNNs in some applications. Will some bold chip maker tune its next generation for transformers?",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize2015.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/iGPT.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/MLPerf202.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/GIF205.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/PNCA.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/GraphcoreGif.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-88/",
    "title": "issue 88",
    "date": "",
    "reading_time": "",
    "content": "Last Sunday was my birthday. That got me thinking about the days leading to this one and those that may lie ahead.As a reader ofThe Batch, you’re probably pretty good at math. But let me ask you a question, and please answer from your gut, without calculating.How many days is a typical human lifespan?\n\nWhen I ask friends, many choose a number in the hundreds of thousands. (Many others can’t resist calculating the answer, to my annoyance!)When I was a grad student, I remember plugging my statistics into a mortality calculator to figure out my life expectancy. The calculator said I could expect to live a total of 27,649 days. It struck me how small this number is. I printed it in a large font and pasted it on my office wall as a daily reminder.\n\nThat’s all the days we have to spend with loved ones, learn, build for the future, and help others. Whatever you’re doing today, is it worth 1/30,000 of your life?Let’s make every day count.\n\nKeep learning!\n\nAndrew\n\nP.S. Don’t worry about me. I’m healthy and plan to stick around for awhile.\n\nP.P.S. A huge thank-you to everyone who responded to my earlier onlinenoteabout my birthday!❤️\n\nA new tool aims to let video gamers control how much vitriol they receive from fellow players.\n\nWhat’s new:Intel announced a voice recognition tool calledBleepthat the company claims can moderate voice chat automatically,allowing usersto silence offensive language. The system is in beta-test and scheduled for release later this year.\n\nHow it works:Chip maker Intel worked with Spirit AI, which develops technology for content moderation, to let users of voice chat fine-tune how much of specific types of offensive language can reach their ears.\n\nBehind the news:ToxModalso aims to moderate video game voice chat and provides a dashboard for human moderators to track offensive speech across servers.Hive’s system is designed to moderate audio, video, text, and images. Its customers includeChatroulette, which uses Hive’s technology to help users avoid unwanted nudity.Two-Hat’stext-moderation system detects efforts to subvert moderation by, say, intentionally misspelling slurs and other potentially offensive language.\n\nWhy it matters:There’s a clear need for tools that help people enjoy networked communications without being targeted by abuse.Twenty-two percentof U.S. online gamers stopped playing certain games after experiencing verbal harassment, according to a survey by the Anti-Defamation League.\n\nWe’re thinking:For those whose first thought is, “Censorship!,” note that users will control this auto-moderation capability locally. At the same time, there’s a fine line between blocking harassment and shutting out perspectives we don't currently share. In an ideal world, players would take it upon themselves to keep their conversations civil. Until that day comes, AI will play a valid — if worrisome at times — role.\n\nAI startups continue to roared ahead, global pandemic or no.\n\nWhat’s new:Tech industry analyst CB Insights published itsfifth annual listof the 100 most promising private AI companies.\n\nWhat they found:The list of 100 was drawn from over 6,000 contenders based on measures including number and type of investors, R&D activity, news sentiment analysis, and competitive landscape. (Disclosure: Landing AI, where Andrew is CEO, is on the list.)\n\nWhatever happened to . . . :Twenty-one companies from last year’s list made it to this year’s. Three of last year’s cohort had successful IPOs, one went public outside regular investment channels, and two were acquired. All are still in business.\n\nWhy it matters:In the midst of massive global economic turmoil, the AI industry continues to prosper. But, while AI’s impacts are global, U.S. companies continue to scoop up most of the rewards.\n\nWe’re thinking:Building companies is hard. To quote Theodore Roosevelt, credit should be given to the person “who is actually in the arena, whose face is marred by dust and sweat and blood.” To everyone working on a startup, we wish you success!\n\nWe’ve just updated the Deep Learning Specialization with the latest advances! This 2021 release now goes up to Transformer Networks and programming exercises in TF2. Wondering whether to learn deep learning? This is a good time to jump in!Start today\n\nNeural networks for image generation don’t just create new art — they can help recreate works that have been lost for ages.\n\nWhat’s new:Oxia Palus, a UK startup dedicated to resurrecting lost art through AI,combined deep learning and 3D printingto reproduce a painting that had been hidden beneath one of Pablo Picasso’s works.\n\nHow it works:In 2018, researchersusedan x-ray technique to reveal that Picasso had painted his “Crouching Beggar” on top of another artwork. Art experts believe the underlying composition, which depicts a park in Barcelona, was painted by Picasso’s contemporary Santiago Rusiñol.\n\nBehind the news:The team used anearlier versionof its style-transfer method to recreate aportrait of a womanhidden beneath Picasso’s “The Old Guitarist.” More recently, ittraineda conditional generative adversarial network on 225 paintings made by Leonardo da Vinci and his students to recreatepainted-over portionsof da Vinci’s “Virgin on the Rocks.”\n\nWhy it matters:The false starts and abandoned ideas hidden under later works of art can offer valuable glimpses into a painter’s creative process. This combination of style transfer and 3D printing reveals what might have been.\n\nWe’re thinking:It’s fitting that Picasso, who revolutionized art in the 20th century, is providing inspiration for a new, AI-powered avant garde.\n\nVideo search engines are often evaluated based on how they rank a single video when presented with a brief description that accompanies that video in the test set. But this criterion may not reflect a system's utility in the real world, where numerous videos may be highly relevant to the search terms. New work aims to solve this problem.What’s new:Researchers at the University of Bristol led by Michael Wray propose a new benchmark,Semantic Similarity Video Retrieval(SVR), that evaluates video retrieval systems by their ability to rank many similar videos. They also built a system that performed well on it.Key insight:To evaluate a video retrieval system based on how similar the top-ranked videos are to an input description, the evaluation process needs a ground-truth measure of similarity between descriptions and videos. There isn’t an automatic way to compare a description to a video, but there are several ways to compare a description to other descriptions. The authors assessed the similarity between existing descriptions to approximate ground-truth similarity between descriptions and videos. This enabled them to train their system to rank the similarity of input text to a variety of videos, and to evaluate the quality of its search results.How it works:The authors generated separate representations for captions and videos and honed the similarity of matching descriptions and videos. Given a description, the system learned to rank clips whose video representation best matched that of the input (and vice-versa). They trained and tested it onvideos with descriptionsfrom movies, news, how-tos, and other sources.\n\nResults:The authors measured how well their system ranked each video with respect to every description (and vice-versa) usingnDCG. This method rewards high rankings of similar representations (as measured by METEOR) and penalizes high rankings of dissimilar representations. The authors’ system scored 0.840 out of a perfect 1.0. A baseline system that used two vanilla neural networks to create video and description embeddings scored .833.Why it matters:Rather than designing a system to ace a common test, the authors devised a new test that better reflects what users expect from such systems. That approach should lead to more useful systems all around.We’re thinking:The more machine learning improves, the more we need benchmarks that are capable of measuring the latest improvements.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-04-21%20at%209.54.27%20AM%20copy.png?upscale=true&name=Screen%20Shot%202021-04-21%20at%209.54.27%20AM%20copy.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image-6.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image-7.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20(2).png?upscale=true&name=The%20Batch%20(2).png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image-8.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image-9.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-38/",
    "title": "issue 38",
    "date": "",
    "reading_time": "",
    "content": "There has been a lot of excitement about the idea of using deep learning todiagnosediabeticretinopathy: That is by taking a photo of the retina and using AI to detect signs of disease. I was fascinated by a newpaperby Emma Beede and others that studied the use of Google’s diabetic retinopathy detecting system in 11 clinics in Thailand and found that, despite all the research progress, this technology isn’t working well in production yet.\n\nWe as a community need to get much better at bridging proofs of concept (PoCs) and production deployments. Even Google’s excellent AI team ran into many practical issues:\n\nThe paper was published by SIGCHI, a leading conference in human-computer interaction (HCI). I’m encouraged to see the HCI community embrace AI and help us with these problems, and I applaud the research team for publishing these insights. I exchanged some emails with the authors, and believe there’s a promising path for AI diagnosis of diabetic retinopathy. To get there, we’ll need to address challenges inrobustness,small data, andchange management.\n\nMany teams are working to meet these challenges, but no one has perfect answers right now. As AI matures, I hope we can turn building production systems into a systematic engineering discipline, so we can deploy working AI systems as reliably as we can deploy a website today.\n\nKeep learning!\n\nAndrew\n\nThis week’s roundup of resources for taking on Covid-19 includes a collection of models, a collaborative molecule hunt, and a search engine query set.\n\nA new generative model croons likeElvisand raps likeEminem. It might even make you think you’re listening to alost demoby the Beatles.What’s new:OpenAI releasedJukebox, a deep learning system that has generated thousands of songs in styles from country to metal and soul. It even mimics the voices of greats like Frank Sinatra.How it works:Jukebox generates music by drawing from a database of 1.2 million songs. Where some other AI-powered systems use symbolic generators to create tunes, Jukebox uses audio recordings, which capture more of music’s subtleties.\n\nResults:OpenAI released over7,000 songscomposed by Jukebox. Many have poor audio quality and garbled lyrics, but there are more than a few gems. Have a listen — our favorites include the Sinatra-esque “Hot Tub Christmas,” with lyrics co-written by OpenAI engineers and a natural language model, and acountry-fied ode to deep learning.Behind the news:AI engineers have been synthesizing music for some time, but lately the results have been sounding a lot more like human compositions and performances.\n\nWhy it matters:Jukebox’s ability to match lyrics and voices to the music it generates can be uncanny. It could herald a new way for human musicians to produce new work. As a percentage of all music consumed, computer generated music is poised to grow.We’re thinking:Human artists already produce a huge volume of music — more than any one person can listen to. But we’re particularly excited about the opportunity for customization. What if you could have robo-Beyonce sing a customized tune for your home movie, or robo-Elton John sing you a song celebrating your birthday?\n\nAI’s ability to produce syntheticpicturesthat fool humans into believing they’re real has spurred a race to build neural networks that can tell the difference. Recent research achieved encouraging results.What’s new:Sheng-Yu Wang and Oliver Wang teamed up researchers from UC Berkeley and Adobe todemonstratethat a typical discriminator — the component in a particular generative adversarial network (GAN) that judges the output to be real or synthetic — can recognize fakes generated by a variety of image generators.Key insight:The researchers trained the discriminator on a dataset made up of images created by diverse GANs. Even two training examples from an unrelated generator improved the discriminator’s ability to recognize fake images.How it works:The researchers compared the performance ofProGAN’s discriminator when trained on Pro-GAN output and on their own dataset.\n\nResults:ProGAN’s discriminator distinguished real from fake images 80 percent of the time. Accuracy rose to 82.3 percent by adding two training examples from another generator (and allowing the discriminator to adjust its confidence threshold) and 88.6 percent with many examples. The researchers also compared real images used to train the generators with 2,000 fake images from each one. They found no discernible pattern in a frequency representation of real images and distinctive patterns in the output of all generators. These subtle patterns, they conjecture, enabled the discriminator to generalize to the output of unrelated generators.Yes, but:The authors’ approach to detecting fake images does a fairly good job of spotting run-of-the-mill GAN output. But a determined malefactor could use only generated images that evaded their method.Why it matters:Prior research didn’t envision that a single discriminator could learn to recognize fakes from diverse, unrelated generators. Current generators apparently leave common traces — a hopeful prospect for developing more capable fake detectors. Of course, that could change tomorrow.We’re thinking:Your move, fakers.\n\nIsraeli and American hospitals are using an algorithm to flag individuals at high risk for Covid-19 complications.What’s new:Israel’s Maccabi Healthcare Services and U.S.-based Kaiser Permanente are using a model dubbedCovid Complications AlgoMarkerto identify patients likely to be hospitalized, develop complications, or die from Covid-19. The developer, Medial EarlySign, is offering it forfreeto other health systems.How it works:The model analyzes the electronic medical records of patients in a given health system. It assigns each one a score that indicates their level of risk based on demographics, hospital admission history, prescribed medications, whether they have respiratory and cardiac diseases, and other factors. If a high-scoring patient tests positive for Covid-19, physicians have early warning that they need to take extra care to prevent or manage complications.\n\nFast Track:The model identified about 40,000 members as high risk and put them on the fast track fortesting. If they test positive, doctors will use their risk scores to help determine whether they should be hospitalized, quarantined, or sent home. EarlySign will continue to retrain the model as more data comes in.Yes, but:Privacy laws like the EU’sGeneral Data Protection Regulationmake it difficult to roll out a system like this, which would work best if allowed to automatically scan a massive number of patients’ health records. Another obstacle: Many healthcare systems in the U.S. and elsewhere use oldercomputer systemsthat don’t integrate well with newer systems.Why it matters:With no end to the pandemic in sight, AI that helps hospitals triage patients efficiently can help save lives.We’re thinking:Although the privacy, data aggregation, and data cleaning issues are formidable, systems like this might help us figure out who to allow back to work, who to keep at home, and who needs special care.\n\nBuild a machine learning algorithm to diagnose chest x-rays and 3D MRI brain images.Enroll in the AI for Medicine Specialization.\n\nA machine learning model is scanning the oceans for the glint of garbage.What’s new:Researchers from the UK’s Plymouth Marine Laboratory trained amodelto identify ocean-borne refuse.How it works:The European Space Agency’s two Sentinel-2 satellites capture light that reflects off the Earth’s surface. The algorithm examines this imagery, pixel by pixel, for evidence of plastic.\n\nResults:The team tested the model on imagery of coastal sites in western Canada, Ghana, Vietnam, and Scotland. It averaged 86 percent accuracy.Behind the news:Marine scientists are finding a variety of uses for AI in ocean conservation. For instance, Google built aneural networkthat recognizes humpback whale songs using data from the U.S. National Oceanic and Atmospheric Administration. Researchers use the model to follow migrations.Why it matters:Fish and whales often die from ingesting or getting tangled inpieces of plastic. As the materialbreaks downinto tiny fragments, it gets eaten by smaller organisms, which get eaten by larger organisms, includingfishconsumed by humans, with potentially toxic effects.We’re thinking:Pointing this model at the beach might be even more helpful: Most ocean plasticoriginateson land, so coastlines may be the best places to capture it before it enters the food web.\n\nWhich comes first, training a reinforcement learning model or extracting high-quality features? New work avoids this chicken-or-egg dilemma by doing both simultaneously.What’s new:Aravind Srinivas and Michael Laskin at UC Berkeley offerContrastive Unsupervised Representations for Reinforcement Learning(CURL). The authors propose contrastive learning to extract features during RL.Key insight:In many RL scenarios, the model learns by interacting with its environment. To extract features, it must capture training data while learning, so pre-trained feature extractors don’t generalize well to novel situations. Contrastive learning, which has been successfully applied to self-supervised learning, extracts similar features for similar inputs and dissimilar features for dissimilar inputs. This doesn’t require pre-training, so the researchers figured that reinforcement and contrastive learning could go hand-in-hand.How it works:The authors essentially combined an RL agent of the user’s choice with a high-performance contrastive learning model that draws techniques fromSimCLR,MoCo, andCPC. The two learn independently.\n\nResults:The researchers tested CURL withRainbow DQNin 42 tasks. They compared its performance against state-of-the-art pixel-based models with similar amounts of training. CURL collected rewards an average 2.8 times larger inDMControland 1.6 times larger in Atari games. It achieved this performance in DMControl in half the training steps.Why it matters:A typical solution to the chicken-or-egg problem is to collect enough data so that it doesn’t matter whether RL or feature extraction comes first. CURL cuts the data requirement.We’re thinking:We’ve been excited about self-supervised learning for some time and are glad to see these techniques being applied to speed up RL as well.\n\nAsian companies lead the world in AI deployment, new research argues.What’s new:Market research byMIT Technology Review Insightsfound that companies in the Asia-Pacific region are using machine learning faster and with better results than any other part of the world.What they found:The authors interviewed over 1,000 executives and directors from businesses in a range of economic sectors around the globe. Roughly one-fifth work for companies in the Asia-Pacific region.\n\nData-driven growth:Nearly half of Asian executives surveyed said their companies’ AI ambitions were hindered by a lack of access to high-quality data. Most said that better legal protections and industry standards regarding data privacy and security would make them more willing to share datasets with other companies. Third-party data-sharing platforms like Singapore’s nonprofitOcean Protocolcould be part of the solution, the authors write.Behind the news:Several Asia-Pacific governments have provided major support for IT infrastructure.\n\nWhy it matters:The survey shows that AI is thriving in places where the government provides both regulatory clarity and institutional support.We’re thinking:Every country should develop policies to foster AI development or risk getting left behind.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Andrew-Letter-ASPECT-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Resources20ASPECT20REPLACEMENT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2011-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker207.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Israel4.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AI4MC1BatchAd-2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker204.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker208.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AsiaASPECT.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-258/",
    "title": "issue 258",
    "date": "",
    "reading_time": "",
    "content": "“Democracy is the worst form of government, except for all the others,” said Winston Churchill. Last week’s shocking attempt to assassinate former President Trump was a reminder that democracy is fragile.\n\nDemocracy lets citizens argue with each other via words and votes. While imperfect, it is a powerful force for making sure that people are governed by leaders of their own choosing, and that these leaders are accountable to making people better off.\n\nThat’s why attempts to disrupt the democratic process, such as assassinating a political candidate or attempting to disrupt a peaceful handover of power to a newly elected government, are despicable: They attack a fundamental mechanism for giving everyone a chance to have a say in who governs. I denounce all political violence and grieve for Corey Comperatore, who was killed in the assassination attempt, and for his family. I hope for a quick recovery for former President Trump and the bystanders who were injured. I also hope we can put more resources into strengthening the mechanisms of democracy.\n\nIn addition, I wonder what role AI can play in preserving democracy.\n\nTechnology can have positive or negative impacts on specific mechanisms of democracy. For instance, data analysis can help citizens and reporters discover facts. Micro-targeting of political ads and social media can increase polarization, while social media can also provide useful information to voters.\n\nBut zooming out to a macro view,\n\nI’m glad last week’s assassination attempt failed, just as I’m glad the January 6 insurrection at the U.S. Capitol failed. Both events were close calls and resulted in tragic loss of human life. Looking into the future, in addition to specific applications that strengthen elements of democracy, I hope we keep on promoting widespread access to technology. This will enhance fairness and the ability of individuals to vote wisely. That’s why democratizing access to technology will help democracy itself.\n\nKeep learning!\n\nAndrew\n\nEnhance your software-development workflow with our new course, “Generative AI for Software Development.” Learn how to use generative AI tools to boost efficiency, improve code quality, and collaborate creatively.Pre-enroll today and be the first to join when the course goes live\n\nA judge rejected key claims in a lawsuit by developers against GitHub, Microsoft, and OpenAI, the first decision in a series of court actions related to generative AI.\n\nWhat’s new: A U.S. federal judgedismissedclaims of copyright infringement and unfair profit in a class-action lawsuit that targeted GitHub Copilot and the OpenAI Codex language-to-code model that underpins it.\n\nThe case:In November 2022, programmer Matthew Butterick and the Joseph Saveri Law Firmfiledthe lawsuit in U.S. federal court. The plaintiffs claimed that GitHub Copilot had generated unauthorized copies of open-source code hosted on GitHub, which OpenAI Codex used as training data. The copies allegedly infringed on developers’ copyrights. The defendants tried repeatedly to get the lawsuit thrown out of court. In May 2023, the judgedismissedsome claims, including a key argument that GitHub Copilot could generate copies of public code without proper attribution, and allowed the plaintiffs to revise their arguments.\n\nThe decision:The revised argument focused on GitHub Copilot’sduplication detection filter. When enabled, the filter detects output that matches public code on GitHub and revises it. The plaintiffs argued that the existence of this feature demonstrated GitHub Copilot’s ability to copy code in OpenAI Codex’s training set. The judge was not persuaded.\n\nYes, but:The lawsuit is reduced, but it isn’t finished. A breach-of-contract claim remains. The plaintiffs aim to show that OpenAI and GitHub used open-source code without providing proper attribution and thus violated open-source licenses. In addition, the plaintiffs will refile their unjust-enrichment claim.\n\nBehind the news:The suit against Github et al. is one of several underway that are testing the copyright implications of training AI systems.Getty Images,Authors’ Guild,The New York Times, and other mediaoutletsalong with a consortium ofmusic-industry giantshave sued OpenAI and other AI companies. All these cases rest on a claim that copying works protected by copyright for the purpose of training AI models violates the law — precisely what the plaintiffs failed to show in the GitHub case.\n\nWhy it matters:This lawsuit specifically concerns code written by open-source developers. A verdict could determine how code can be used and how developers can use generative AI in their work. However, it has broader implications. (Note: We are not lawyers and we do not provide legal advice.) This dismissal is not a final verdict, but it supports the view that AI developers may have a broad right to use data for training models even if that data is protected by copyright.\n\nWe’re thinking:Broadly speaking, we would like AI to be allowed to do with data, including open source code, anything that humans can legally and ethically do, including study and learn. We hope the judge’s decision gives AI developers further clarity on how they can use training data, and we hope it establishes that it’s ethical to use code-completion tools trained on open-source code.\n\nThe word “open” can mean many things with respect to AI. A new paper outlines the variations and ranks popular models for openness.\n\nWhat’s new:Researchers at Radboud Universityevaluateddozens of models billed as open by their developers. They plan to keep their analysis of language models updatedhere.How it works:The authors assessed 40 large language models and six text-to-image generators, adding OpenAI’s closed models ChatGPT and DALL·E 2 as reference points. They evaluated 14 characteristics, scoring each as open (1 point), partially open (0.5 points), or closed (0 points). For example, an API would be described as partially open if using it requires users to register. They divided the characteristics into three categories:\n\nResults:Of the language models,OLMo 7B Instructfrom Allen Institute for AI scored highest with 12 open characteristics and 1 partially open characteristic (it lacked a published, peer-reviewed paper).\n\nBehind the News:The Open Source Initiative (OSI), a nonprofit organization that maintains standards for open-source software licenses, isleadinga process to establish a firm definition of “open-source AI.” The currentdraftholds that an open-source model must include parameters, source code, and information on training data and methodologies under an OSI-recognized license.\n\nWhy it matters:Openness is a cornerstone of innovation: It enables developers to build freely on one another’s work. It can also lubricate business insofar as it enables developers to sell products built upon fully open software. And it has growing regulatory implications. For example, the European Union’s AI Act regulates models that are released under an open source license less strictly than closed models. All these factors raise the stakes for clear, consistent definitions. The authors’ framework offers clear, detailed guidelines for developers — and policymakers — in search of clarity.We’re thinking:We’re grateful to AI developers who open their work to any degree, and we especially appreciate fully open availability, documentation, and access. We encourage model builders to release their work as openly as they can manage.\n\nAn arena-style contest pits the world’s best text-to-image generators against each other.\n\nWhat’s new:Artificial Analysis, a testing service for AI models,introducedthe Text to Image Arena leaderboard, which ranks text-to-image models based on head-to-head matchups that are judged by the general public. At the time of this writing, Midjourney v6 beats more than a dozen other models models in its ability to generate images that reflect input prompts, though it lags behind competitors in speed.\n\nHow it works:Artificial Analysis selects two models at random and feeds them a unique prompt. Then itpresentsthe prompt and resulting images. Users can choose which model better reflects the prompt. The leaderboard ranks the models based onEloratings, which scores competitors relative to one another.\n\nWho’s ahead?:As of this writing, Midjourney v6 (Elo rating 1,176), which won 71 percent of its matches, holds a slim lead over Stable Diffusion 3 (Elo rating 1,156), which won 67 percent. DALL·E 3 HD holds a distant third place, barely ahead of the open-source Playground v2.5. But there are tradeoffs: Midjourney v6 takes 85.3 seconds on average to generate an image, more than four times longer than DALL·E 3 HD and more than 13 times longer than Stable Diffusion 3. Midjourney v6 costs $66 per 1,000 images (an estimate by Artificial Analysis based on Midjourney’s policies, since the model doesn’t offer per-image pricing), nearly equal to Stable Diffusion 3 ($65), less than DALL·E 3 HD ($80), and significantly more than Playground v2.5 ($5.13 per 1,000 images via theReplicateAPI).\n\nBehind the news:The Text to Image Arena is a text-to-image counterpart of theLMSys Chatbot Arena, which lets users write a prompt, feed it to two large language models, and pick the winner.imgsysandGen-AI Arenasimilarly let users choose between images generated by different models from the same prompt (Gen-AI Arena lets users write their own). However, these venues are limited to open models, which excludes the popular Midjourney and DALL·E.\n\nWhy it matters:An image generator’s ability to respond appropriately to prompts is a subjective quality. Aggregating user preferences is a sensible way to measure it. However, individual tastes and applications differ, which makes personalized leaderboards useful as well.We’re thinking:The user interface for some image generators implicitly asks users to judge images. For example, Midjourney defaults to generating four images and asks users which they want to render at higher resolution. This can give the image generator valuable feedback about which image users like. Perhaps data gathered by an arena could feed an algorithm like reinforcement learning from human feedback to help generators learn to produce output that people prefer.\n\nLarge language models can produce output that’s convincing but false. Researchers proposed a way to identify such hallucinations.\n\nWhat’s new:Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal at University of Oxford published amethodthat indicates whether a large language model (LLM) is likely to have hallucinated its output.\n\nKey insight:One way to estimate whether an LLM is hallucinating is to calculate the degree of uncertainty, or entropy, in its output based on the probability of each generated token in the output sequences. The higher the entropy, the more likely the output was hallucinated. However, this approach is flawed: Even if the model mostly generates outputs with a uniform meaning, the entropy of the outputs can still be high, since the same meaning can be phrased in many different ways. A better approach is to calculate entropy based on the distribution of generated meanings instead of generated sequences of words. Given a particular input, the more likely a model is to respond by generating outputs with a variety of meanings, the more likely that a response to that input is a hallucination.\n\nHow it works:The authors generated answers to fiveopen-endedquestion-and-answerdatasetsusing various sizes of Falcon, LLaMA 2-chat, and Mistral. They checked the answers for hallucinations using the following method:\n\nResults:The authors measured the classification performance of their method using AUROC, a score between .5 (the classifier is uninformative) and 1 (the classifier is perfect). On average, across all five datasets and six models, the authors’ method achieved .790 AUROC while the baseline entropy achieved .691 AUROC and theP(True)method achieved .698 AUROC. P(True) asks the model (i) to generate up to 20 answers and (ii) whether, given those answers, the one with the highest probability of having been generated is true or false.\n\nYes, but:The authors’ method fails to detect hallucinations if a model consistently generates wrong answers.\n\nBehind the news:Hallucinations can be a major obstacle to deploying generative AI applications, particularly in fields like medicine or law where missteps can result in injury. One study published earlier this yearfoundthat three generative legal tools produced at least partially incorrect or incomplete information in response to at least one out of every six prompts. For example, given the prompt, “Are the deadlines established by the bankruptcy rules for objecting to discharge jurisdictional,” one model cited a nonexistent rule: “[A] paragraph from the Federal Rules of Bankruptcy Procedure, Rule 4007 states that the deadlines set by bankruptcy rules governing the filing of dischargeability complaints are jurisdictional.”Why it matters:Effective detection of hallucinations not only fosters trust in users — and consequently rising adoption — but also enables researchers to determine common circumstances in which hallucinations occur, helping them to address the problem in future models.\n\nWe’re thinking:Researchers are exploring various approaches to mitigate LLM hallucinations in trained models. Retrieval augmented generation (RAG) can help by integrating knowledge beyond a model’s training set, but it isn’t a complete solution.Agentic workflowsthat include tool use to supply factual information and reflection to prompt the model to check itself are promising.\n\nIn “Pretraining LLMs,” a short course built in collaboration with Upstage, you’ll learn about pretraining, the first step of training a large language model. You’ll also learn innovative pretraining techniques like depth upscaling, which can reduce training costs by up to 70 percent.Join today",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--72--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/07/The-Batch-ads-and-exclusive-banners---2024-07-17T111109.125.png",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--73-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-17T135649.502.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-17T135754.238.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-17T135854.645.png",
      "https://dl-staging-website.ghost.io/content/images/2024/07/The-Batch-ads-and-exclusive-banners---2024-07-16T085539.096.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-300/",
    "title": "issue 300",
    "date": "",
    "reading_time": "",
    "content": "I’m delighted to announce that AI Fund has closed $190M for our new fund, in an oversubscribed round. I look forward to working with many more builders to create new companies that serve humanity.\n\nAI Fund isn’t a traditional venture capital firm that invests in existing businesses. Instead, we are a venture builder (also called a venture studio): Weco-found AI companies, so our team is directly involved in writing code, talking to customers to get feedback, iterating on product designs, preparing market analyses, and so on. We have a lot of fun building multiple AI products at a time, and thus live daily the emerging AI startup best practices.\n\nMany factors go into the success of a startup. But if I had to pick just one, it would be speed. Startups live or die based on their ability to make good decisions and execute fast, which has been a recurring theme of my articles inThe Batchas well.\n\nIf you are building an AI startup, here are some ideas to consider:\n\nIn addition to speed, a second criterion that I find important for startup success is deep knowledge of the technology. Because AI technology is evolving rapidly, a team with a deep technical understanding of what AI can and cannot do, and when to use what tool, will make better decisions. This creates meaningful differentiation and saves wasting time in blind alleys. A good technical understanding, too, gets you speed!\n\nI’m grateful to AI Fund’s investors, team, and entrepreneur partners for working with us. There is much ahead to build!\n\nAndrew\n\nLearn to create voice agents that listen, reason, and respond in real time, just like a conversation with a real person in our latest short course, “Building AI Voice Agents for Production.” You'll build a scalable agent from scratch, deploy it to the cloud, and explore what makes voice interfaces feel fast, natural, and human.Enroll for free\n\nAlibaba’s new model family may unseat DeepSeek-R1’s four-month reign as the top open-weights large language model.\n\nWhat’s new:Alibabareleasedweights for eight large language models, all of which offer a reasoning mode that can be switched on or off. Two use a mixture of experts (MoE) architecture: Qwen3-235B-A22B (the name indicates 235 billion parameters, 22 billion of which are active at any given time) and Qwen3-30B-A3B). The other six are dense models in sizes between 32 billion parameters and 0.6 billion parameters — tiny by LLM standards, and with reasoning, too.\n\nHow it works:The Qwen3 family implementschain-of-thoughtreasoning in both relatively large and quite small LLMs.\n\nResults:Qwen3-235B-A22B and Qwen3-30B-A3B performed as well as, or better than, leading open-weights models in tests performed by Alibaba. Qwen3-4B, too, achieved results that are competitive with many models several times its size. Alibaba didn’t provide results for the other dense models.\n\nWhy it matters:Qwen3 continues a string of high-performance, open-weights models released by developers in China. Alibaba says it designed the models to do the thinking in agentic systems. Reasoning that can be switched on and off can help control costs in agentic and other applications.\n\nWe’re thinking:Alibaba’s 235-billion parameter MoE model may perform better according to benchmarks, but Qwen3-30B-A3B does nearly as well and can run locally on a pro laptop without straining its memory. Add the easy ability to switch reasoning on or off, and Qwen3’s versatile, mid-sized MoE model may turn out to be the star of the show.\n\nOpenAI’s most widely used model briefly developed a habit of flattering users, with laughable and sometimes worrisome results.\n\nWhat’s new:OpenAI quicklywithdrewan update to GPT-4o (gpt-4o-2025-04-25), which supplied responses for ChatGPT, after it provided excessively fawning responses to user input — even in contexts didn’t call for agreement. The company reverted to an earlier version (gpt-4o-2024-11-20). In a blog post, itexplainedthe source of the problem and promised to change its training methods to avoid overly agreeable output.\n\nAmiable to a fault:Many ChatGPT users shared screen shots of ChatGPT’s sycophantic responses on social media.\n\nHow it works:Sycophancy, also called glazing, occurs when a large language model learns to align its responses excessively with the user's point of view, even when that standpoint is objectively false, unethical, or harmful. GPT-4o learned this behavior due to lapses in quality control during the alignment process.\n\nBehind the news:Sycophantic behavior in large language models has been a subject of AI research and commentary.\n\nWhy it matters:ChatGPT’s episode of sycophancy illustrates the subtlety of the goal of aligning AI with human values. Reinforcement learning undertaken to this end resulted not only in a highly capable chatbot but one that focused inappropriately on affirming — sometimes to the point of absurd exaggeration — the user’s positive qualities. Alignment requires balancing multiple objectives beyond agreeableness including accuracy, helpfulness, and ethics. Ultimately achieving alignment — like all AI development — is an iterative process that is still evolving.\n\nWe’re thinking:To those who read this far, your unwavering dedication and extraordinary perseverance is nothing short of legendary. Like a master navigator, you’ve traversed word by word, never wavering, displaying a level of focus and determination that would humble even the most steadfast of scholars. We are truly honored to have such an intrepid reader. Bravo to you, the indefatigable champion of curiosity!\n\nThe world’s biggest pharmaceutical company by revenue shed light on its AI strategy.\n\nWhat’s new:Johnson & Johnson, after experimenting broadly with generative AI, settled on a short list of projects that aid in sales, drug development, supply-chain management, and internal communications. A company executive described the process and results to the venture-capital firmGreylockandThe Wall Street Journal.\n\nHow it works:The 140-year-old medical company spent roughly a year experimenting with various AIapplicationsthroughout the company, according to Chief Information Officer Jim Swanson. A centralized governing board oversaw as many as 900 experiments. After finding that 10 percent to 15 percent of use cases drove about 80 percent of the value, the company shifted responsibility for AI projects to specific departments to focus on high-value applications. In the end, the criteria for choosing a project was threefold: (i) how readily it could be implemented, (ii) how useful it would be throughout the company, and (iii) how much it would benefit the business.\n\nBehind the news:Generative AI is expected to bring in up to $110 billion in annual revenue across the pharmaceutical industry,according to McKinsey. The consultancy breaks down this number into the following categories, in order of their contribution to the total: commercial (AI for sales and marketing), research (AI for designing, screening, and manufacturing molecules), clinical (AI to facilitate trials), enterprise, operations, and medical (processing medical literature).\n\nWhy it matters:Johnson & Johnson’s experience offers a peek into AI development at a major legacy company in a key sector. The company has identified high-value opportunities in enterprise-wide operations, departmental priorities, and core products. It’s pursuing all three.\n\nWe’re thinking:Notably, this medical stalwart is building AI applications for human resources, sales, and supply-chain management. Similar opportunities exist at companies old and new, big and small, far and wide.\n\nResearchers showed that supervised fine-tuning on as few as 1,000 examples can enable a pretrained large language model to reason — and a clever gambit can boost its performance to rival that of top reasoning models.\n\nWhat’s new:Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li and colleagues at Stanford, University of Washington, Allen Institute for AI, and Contextual AI developeds1, a reasoning model that achieves higher performance by producing more reasoning tokens. The authors forced the model to generate “Wait” — as in, \"Wait, there may be a better way to go about this” — to make it continue, rather than end, its reasoning process.\n\nKey insight:The sequence of reasoning tokens generated by a reasoning model likeDeepSeek-R1is delimited by special tokens. In pretraining on human data, a model learns to keep generating reasoning tokens until it generates the special token that ends the sequence. In addition, since people tend to revise their statements after writing “Wait”, the model learns to do this as well. Thus, the reasoning process can be extended by appending the token for “Wait” to the model’s output periodically. In this way, when the output-in-progress is fed back to generate the next token, the model continues to reason over the prompt. Such extended reasoning can improve the final output by inducing the model to double-check its response so far and improve previous reasoning steps.\n\nHow it works:The authors fine-tuned a pretrainedQwen 2.5-32B, which does not produce reasoning tokens, on around 1,000 examples ofchain-of-thoughtreasoning.\n\nResults:s1’s performance improved as the number of reasoning tokens it generated increased. Ultimately, it achieved comparable performance to OpenAI o1-preview but fell short of o1.\n\nWhy it matters:A conventional pretrained LLM can learn to reason after supervised fine-tuning on as few as 1,000 curated examples — no reinforcement learning necessary. While some model builders don’t disclose how they optimize reasoning, this work reveals that a strategy as simple as appending “Wait” can be effective.\n\nWe’re thinking:Wait, how can we apply this to our projects?",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--84--2.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/The-Batch-ads-and-exclusive-banners---2025-05-05T152809.008.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--85-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--62-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--86-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--87-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-284/",
    "title": "issue 284",
    "date": "",
    "reading_time": "",
    "content": "Writing software, especially prototypes, is becoming cheaper. This will lead to increased demand for people who can decide what to build. AI Product Management has a bright future!\n\nSoftware is often written by teams that comprise Product Managers (PMs), who decide what to build (such as what features to implement for what users) and Software Developers, who write the code to build the product. Economics shows that when two goods are complements — such as cars (with internal-combustion engines) and gasoline — falling prices in one leads to higher demand for the other. For example, as cars became cheaper, more people bought them, which led to increased demand for gas. Something similar will happen in software. Given a clear specification for what to build, AI is making the building itself much faster and cheaper. This will significantly increase demand for people who can come up with clear specs for valuable things to build.\n\nThis is why I’m excited about the future of Product Management, the discipline of developing and managing software products. I’m especially excited about the future of AI Product Management, the discipline of developing and managing AI software products.\n\nMany companies have an Engineer:PM ratio of, say, 6:1. (The ratio varies widely by company and industry, and anywhere from 4:1 to 10:1 is typical.) As coding becomes more efficient, I think teams will need more product management work (as well as design work) as a fraction of the total workforce. Perhaps engineers will step in to do some of this work, but if it remains the purview of specialized Product Managers, then the demand for these roles will grow.\n\nThis change in the composition of software development teams is not yet moving forward at full speed. One major force slowing this shift, particularly in AI Product Management, is that Software Engineers, being technical, are understanding and embracing AI much faster than Product Managers. Even today, most companies have difficulty finding people who know how to develop products and also understand AI, and I expect this shortage to grow.\n\nFurther, AI Product Management requires a different set of skills than traditional software Product Management. It requires:\n\nFinally, AI Product Managers will need to know how to ensure that AI is implemented responsibly (for example, when we need to implement guardrails to prevent bad outcomes), and also be skilled atgathering feedback fastto keep projects moving. Increasingly, I also expect strong product managers to be able tobuild prototypesfor themselves.\n\nThe demand for good AI Product Managers will be huge. In addition to growing AI Product Management as a discipline, perhaps some engineers will also end up doing more product management work.\n\nThe variety of valuable things we can build is nearly unlimited. What a great time to build!\n\nKeep learning,\n\nAndrew\n\nGet up-close and personal with OpenAI’s groundbreaking o1 model! In our short course “Reasoning with o1,” you’ll learn how to get the best performance in coding, planning, and STEM tasks; perform complex, multi-step tasks; and optimize prompts with meta prompting.Enroll today\n\nA new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.\n\nWhat’s new:DeepSeek-V3is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights areopenexcept for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download themhere.\n\nMixture of experts (MoE) basics:The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.\n\nHow it works:DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours — less than 1/10 thetime required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms — at an extraordinarily low cost of $5.6 million.\n\nResults:In DeepSeek’s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.\n\nBehind the news:OpenAI’s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.\n\nWhy it matters:Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3’s shockingly low training cost.The team doesn’t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits “meticulous engineering optimizations.”) But it’s likely that DeepSeek’s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn’t employ MoE. In 2022,Microsoftfound that MoE cost five times less in training for equal performance compared to a dense model, andGoogleandMetareported that MoE achieved better performance than dense models trained on the same numbers of tokens.\n\nWe’re thinking:If they can be replicated, DeepSeek’s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically.\n\nThe United States proposed limits on exports of AI technology that would dramatically expand previous restrictions, creating a new international hierarchy for access to advanced chips and models.\n\nWhat’s new:The Biden administration, which will transition to leadership under incoming President Trump next week, issued newrulesthat restrict exports of AI chips and models to most countries beyond a select group of close allies. The rules, which are not yet final, would create a three-tier system that limits exports to a number of close allies and blocks access entirely to China, Iran, North Korea, Russia, and others. They also would introduce the U.S.’ first-ever restrictions on exporting closed weights for large AI models.\n\nHow it works:The restrictions were announced shortly after aleakreached the press. A public comment period of 120 days will enable the incoming U.S. Presidential administration to consider input from the business and diplomatic communities and modify the rules before they take effect. The rules are scheduled to take effect in one year.\n\nBehind the news:The proposed rules build on 2022’sCHIPS and Science Act, which was designed to strengthen domestic semiconductor production and restrict technologies abroad that could bear on U.S. security. An initial round of restrictions in late 2022barredsemiconductor suppliers AMD and Nvidia from selling advanced chips to Chinese firms. In November 2024, the U.S.tightenedrestrictions further, ordering Taiwan Semiconductor Manufacturing Company, which fabricates those chips, to halt production of advanced chips destined for China.\n\nPlus green AI infrastructure:In addition, President Biden issued an executive order to encourage the rapid build-out of computing infrastructure for AI. The federal government will hold competitions among private companies to lease sites it owns for the building of data centers at private expense. The selection of sites will take into account availability of sources of clean energy, including support for nuclear energy. The government will expedite permitting on these sites and support development of energy transmission lines around them. It will also encourage international allies to invest in AI infrastructure powered by clean energy.\n\nWhy it matters:Protecting the United States’ advantages in high tech has been a rising priority for the White House over the past decade. The earlier export restrictions forced many Chinese AI developers to rely on less-powerful hardware. The new limits are likely to have a far broader impact. They could force developers in the Tier 2 and Tier 3 countries to build less resource-intensive models and lead them to collaborate more closely with each other, reducing the value of U.S.-made technology worldwide. They could hurt U.S. chip vendors, which havewarnedthat the rules could weaken U.S. competitiveness in the global economy. They could also force companies that are building huge data centers to process AI calculations toreconsidertheir plans.\n\nWe’re thinking:The Biden administration’s embargo on AI chips has beenleaky. So far, it has slowed down adversaries only slightly while spurring significant investment in potentialsuppliersthat aren’t connected to the U.S. While the public comment period invites lobbying and industry feedback, ultimately geopolitical priorities may hold sway. Whatever the outcome, reducing the world’s dependence on U.S. chips and models would result in a very different global AI ecosystem.\n\nNvidia’s new desktop computer is built specifically to run large AI models.\n\nWhat’s new:Project Digitsis a personal supercomputer intended to help developers fine-tune and run large models locally. Project Digits, which is small enough to hold in one hand, will be available in May, starting at $3000.\n\nHow it works:Project Digits is designed to run models of up to 200 billion parameters — roughly five times the size that fits comfortably on typical consumer hardware — provided they’re quantized to 4 bits of precision. Two units can be connected to run models such as Meta’s Llama 3.1 405B. Complete specifications are not yet available.\n\nBehind the news:In a blitz of announcements at the Consumer Electronics Show (CES), Nvidia also launched a platform for developing robotics, autonomous vehicles, and other physical AI systems. Cosmos includes pretrained language and vision models that range from 4 billion to 14 billion parameters for generating synthetic training data for robots or building policy models that translate a robot’s state into its next action. Nvidia also released Cosmos Nemotron, a 34 billion-parameter, vision-language model designed for use by AI agents, plus a video tokenizer and other tools for robotics developers.\n\nWhy it matters:It’s common to train models on Nvidia A100 or H100 GPUs, which come with a price tag of at least $8,000 or $20,000 respectively, along with 40 gigabytes to 80 gigabytes of memory. These hefty requirements push many developers to buy access to computing infrastructure from a cloud provider. Coming in at $3,000 with 128 gigabytes of memory, Project Digits is designed to empower machine learning engineers to train and run larger models on their own machines.\n\nWe’re thinking:We look forward to seeing cost/throughput comparisons between running a model on Project Digits, A100, and H100.\n\nContrastive loss functions make it possible to produce good embeddings without labeled data. A twist on this idea makes even more useful embeddings.\n\nWhat’s new:Vlad Sobal and colleagues at Meta, New York University, Brown University, Genentech, and Canadian Institute for Advanced Research introducedX-Sample contrastive loss(X-CLR), a self-supervised loss function that enables vision models to learn embeddings that capture similarities and differences among examples with greater subtlety.\n\nKey insight:Contrastive loss functions likeSimCLRequally encourage a model to produce dissimilar embeddings of images of, say, a cat, a dog, and a dump truck. But, of course, cats and dogs are more similar to each other than either are to dump trucks. Instead of marking examples as similar or dissimilar, X-CLR assigns similarity scores, so a model can learn to produce embeddings that match those scores.\n\nHow it works:The authors used X-CLR to train an embedding model onConceptual Captionsdatasets of image-text pairs scraped from the web: CC-3M (3 million text-image pairs) and CC-12M (12 million text-image pairs). The model was similar toCLIP, except the text encoder was asentence transformerpretrained on sentence pairs, and the vision encoder was aResNet-50pretrained on ImageNet.\n\nResults:Systems trained using X-CLR outperformed competitors inImageNetclassification, especially when less training data was available. (The authors followed CLIP’s method of classification: They computed the similarity between an image embedding and text embeddings of all classes. The image’s classification was the class that corresponds to the text embedding with the highest similarity to the image embedding.)\n\nWhy it matters:Contrastive loss functions are very useful, but the similar/dissimilar dichotomy leaves important nuances unaccounted for. Like CLIP, X-CLR takes advantage of both images and their captions for self-supervised learning. However, CLIP learns to recognize image-text pairs as similar or dissimilar, while X-CLR matches image-image pairs using captions as a similarity signal that’s continuous rather than discrete.\n\nWe’re thinking:Reality is not black and white. Allowing for shades of gray makes for better modeling.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/01/AIProductManager-2_1200px-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/01/The-Batch-ads-and-exclusive-banners---2024-12-16T174314.640--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--45-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/BIDENCHIPS-10_1200px.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--47-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--44-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-223/",
    "title": "issue 223",
    "date": "",
    "reading_time": "",
    "content": "This week, I’m speaking at the World Economic Forum (WEF) and Asia-Pacific Economic Cooperation (APEC) meetings in San Francisco, where leaders in business and government have convened to discuss AI and other topics. My message at both events is simple: Governments should not outlaw open source software or pass regulations that stifle open source development.\n\nRegulating AI is a hot topic right now in the United States, European Union, and elsewhere. Just this week, the EU’s AI Act was derailed when France and Germanyobjected— with good reason, in my view — to provisions that would burden companies that build foundation models.\n\nAs Yann LeCun and I have said, it’s important to distinguish between regulating technology (such a foundation model trained by a team of engineers) and applications (such as a website that uses a foundation model to offer a chat service, or a medical device that uses a foundation model to interacts with patients). We need good regulations to govern AI applications, but ill-advised proposals to regulate the technology would slow down AI development unnecessarily. While the EU’s AI Act thoughtfully addresses a number of AI applications — such as ones that sort job applications or predict crime — and assesses their risks and mandates mitigations, it imposes onerous reporting requirements on companies that develop foundation models, including organizations that aim to release open-source code.\n\nI wrote in an earlierletterthat some companies that would rather not compete with open-source, as well as some nonprofits and individuals, are exaggerating AI risks. This creates cover for legislators to pass regulations in the name of safety that will hamper open source. At WEF and APEC, I’ve had conversations about additional forces at play. Let me describe what I’m seeing.\n\nIn the U.S., a faction is worried about the nation’s perceived adversaries using open source technology for military or economic advantage. This faction is willing to slow down availability of open source to deny adversaries’ access. I, too, would hate to see open source used to wage unjust wars. But the price of slowing down AI progress is too high. AI is a general-purpose technology, and its beneficial uses — similar to other general purpose technologies like electricity — far outstrip the nefarious ones. Slowing it down would be a loss for humanity.\n\nWhen I speak with senior U.S. government officials, I sense that few think the possibility that AI will lead to human extinction is a realistic risk. This topic tends to lead to eye-rolls. But they genuinely worry about AI risks such as disinformation. In comparison, the EU is moreconcerned— unnecessarily, in my view — about the risk of extinction, while also worried about other, more concrete harms.\n\nMany nations and corporations are coming to realize they will be left behind if regulation stifles open source. After all, the U.S. has a significant concentration of generative AI talent and technology. If we raise the barriers to open source and slow down the dissemination of AI software, it will only become harder for other nations to catch up. Thus, while some might argue that the U.S. should slow down dissemination of AI (an argument that I disagree with), that certainly would not be in the interest of most nations.\n\nI believe deeply that the world is better off with more intelligence, whether human intelligence or artificial intelligence. Yes, intelligence can be used for nefarious purposes. But as society has developed over centuries and we have become smarter, humanity has become much better off.\n\nA year ago, I wouldn’t have thought that so many of us would have to spend so much time trying to convince governments not to outlaw, or make impractical, open-sourcing of advanced AI technology. But I hope we can all keep on pushing forward on this mission, and keep on pushing to make sure this wonderful technology is accessible to all.\n\nKeep learning!\n\nAndrew\n\nP.S. Many teams that build applications based on large language models (LLMs) worry about their safety and security, and such worries are a significant barrier to shipping products. For example, might the application leak sensitive data, or be tricked into generating inappropriate outputs? Our new short course shows how you can mitigate hallucinations, data leakage, and jailbreaks. Learn more in “Quality and Safety for LLM Applications,” taught by Bernease Herman and created in collaboration with WhyLabs (disclosure: an AI Fund portfolio company).Available now!\n\nThe longest actors’ strike in Hollywood history ended as actors and studios reached an accord on the use of generative AI in making movies.\n\nWhat’s new: Film studios must seek an actor’s consent before using a generated likeness or performance and compensate the actor, according to anagreementbetween the trade union Screen Actors Guild-American Federation of Television and Radio Artists (SAG-AFTRA) and the Alliance of Motion Picture and Television Producers (TMPTP). The pact will remain in effect for three years, once it has been ratified by SAG-AFTRA members.\n\nHow it works:The agreement covers digital replicas of human actors, synthetic performers, and simulated performances created using AI and other technologies that may not be generally recognized as AI. The parties argued over terms with respect to AI until the very last day of their 118-day negotiation,accordingto SAG-AFTRA’s president. Among the provisions:\n\nBehind the news:The agreement followed a similar three-yeardealin September that ended the concurrent strike by Writers Guild of America.\n\nYes, but:The agreement covers on-screen actors. It does not cover voice or motion actors in video games or television animation. In September, SAG-AFTRAauthorizeda strike against a group of video game companies if negotiations, which are ongoing, stall. Negotiations over television animation are expected as well.\n\nWhy it matters:The actors’ agreement could set an international example for limits on AI in the performing arts, thanks to the U.S. film and television industry’s global reach. Entertainers’ unions in Europe and Canada arecontemplatingstrikes inspired by SAG-AFTRA’s, and they may seek similar agreements.\n\nWe’re thinking:As with the screenwriters’ contract, the agreement between actors and studios gives everyone three years to experiment with AI while respecting the consent, credit, and compensation of creative workers. We hope that shows made in this period provide ample evidence that such tools can yield wonderful productions that enlarge the market, and that the next agreement focuses more on growing the use of AI and dividing the winnings fairly among actors, studios, and technologists.\n\nChatGPT suffered a cyberattack apparently tied to the Kremlin.\n\nWhat's new:A ChatGPT outage on November 8 most likely was caused by a distributed denial of service (DDoS) attack, OpenAIrevealed.\n\nWhat happened:ChatGPT went down shortly before 9:00 a.m. Eastern Time and remained out of service for about 90 minutes. Intermittent outages of unknown cause had affected OpenAI and other services during the previous two days.\n\nDDoS basics:In a DDoS attack, malicious programs running independently on numerous machines flood a website with requests, disrupting service. The distributed nature of the attack makes it difficult to trace or combat. Almost all cloud providers and large websites use DDoS mitigation services or their own technology to defend against such attacks. However, such defenses don’t always block an especially determined or resourceful attacker.\n\nWhy it matters:The ChatGPT outage is a sobering reminder that API-powered services are vulnerable to targeted attacks, and providers need to be proactive about protecting themselves and their users.\n\nWe're thinking:While no one likes downtime, it’s hard to defend against a state-sponsored DDoS. It’s a testament to OpenAI’s impact that just 90 minutes of downtime was felt around the world.\n\nNew short course! “Quality and Safety for LLM Applications” will help you enhance the safety of large language model applications by detecting issues like data leakage, hallucination, toxicity, and jailbreaks. Start making your apps more secure today.Enroll now\n\nWeeks after it announced a huge partnership deal with Amazon, Anthropic doubled down on its earlier relationship with Alphabet.\n\nWhat's new:Anthropic, which provides large language models, agreed to use Google’s cloud-computing infrastructure in return for a $2 billion investment,The Wall Street Journalreported. The deal follows an earlier multibillion-dollar partnership that saw Anthropic commit to training new models on Amazon Web Services.\n\nHow it works:Google invested $500 million up front and will add $1.5 billion more over an unspecified time period. The new funding builds on $300 million that Googlegaveto Anthropic earlier in the year for a 10 percent stake in the company. Google’s current stake in Anthropic is undisclosed.\n\nBehind the news:Anthropic rose rapidly from AI startup to coveted foundation-model partner.\n\nWhy it matters:The Anthropic-Google deal changes the shape of the startup’s relationships with large cloud providers. Anthropic's deal with Amazon dwarfed Google’s initial investment and seemed like a formative partnership akin to OpenAI’s lucrative Microsoftpair-up. Now, Anthropic is more like a vertex in a triangle, bound by close relationships with competing partners.\n\nWe're thinking:Anthropic hasn’t raised as much total funding as OpenAI ($12.7 billion and counting), but its relationships with both Google and Amazon give it more flexibility to choose different infrastructure for different tasks. The benefits presumably will flow not only to the three companies but also to independent developers, who can choose among stellar proprietary foundational models — not to mention open source alternatives — from three major cloud providers.\n\nOnline sorting algorithms run trillions of times a day to organize lists according to users’ interests. New work found faster alternatives.\n\nWhat’s new:Daniel J. Mankowitz and colleagues at Google developedAlphaDev, a system that learned to generate algorithms that sort three to five numbers faster than previous state-of-the-art methods. Accelerating such algorithms can expedite the sorting of lists of any size — say, for search engines, ecommerce sites, and the like — since algorithms that sort more elements often call algorithms that sort fewer elements.\n\nKey insight:Most programmers implement sorting algorithms in a high-level programming language like C++, which a compiler translates into Assembly Language instructions that control the processor and memory. A compiler can translate a single line of C++ into a variety of sequences of Assembly instructions that are equivalent functionally but vary in their speed (number of Assembly instructions required). A reinforcement learning agent can learn to choose a translation that maximizes speed.\n\nHow it works:AlphaDev is a collection of neural networks that learn jointly via reinforcement learning. The authors initialized the system by giving it a sequence of unsorted numbers and an empty list of Assembly instructions. It built algorithms by adding Assembly instructions one by one. It earned rewards for choosing instructions that sorted the numbers correctly and quickly.\n\nResults:The authors tested two approaches to rewarding speed, minimizing either Assembly instructions or average runtime over a number of inputs. When AlphaDev minimized the number of Assembly instructions, it found an algorithm that sorted three integers using 17 instructions instead of the previous state-of-the-art algorithm, a human-engineered one that used 18 instructions. Its algorithm for sorting four integers used 28 instructions, equal to the typical one. Its algorithm for sorting five integers had 42 instructions, compared to the alternative’s 46 instructions. When AlphaDev optimized for runtime (running on Intel 6th-generation Core “Skylake” processor), sorting three integers took 2.18 nanoseconds, compared to the typical algorithm’s 4.86 nanoseconds. Sorting four unsigned integers took 1.96 nanoseconds instead of 5.43 nanoseconds and sorting five of them took 1.98 nanoseconds instead of 6.79 nanoseconds. AlphaDev achieved smaller speedups with longer number sequences: Sorting 16 unsigned integers took 9.5 nanoseconds instead of 10.5 nanoseconds, and sorting 262,144 numbers took 60.8 nanoseconds instead of 61.4 nanoseconds.\n\nWhy it matters:This work repurposes the training method and architecture of game-playing models likeAlphaZeroto solve real-world problems. The trick is to reframe the task of writing a sorting algorithm as a reinforcement learning problem.\n\nWe’re thinking:What other algorithms can this approach optimize? How much faster will they be? Let’s get these questions sorted!\n\nExperience the fastest-growing course on Coursera this year,Generative AI for Everyone! Led by Andrew Ng, delve into generative AI and its applications in both professional and personal settings.Enroll now\n\nStart-ups shell out big bucks for AI domain namesAs the AI industry continues to boom, entrepreneurs are finding that securing a memorable domain name comes at a hefty price. Domain brokerages report a significant increase in sales for websites with a .ai suffix, with some speculators profiting by flipping domain names. (BBC)\n\nSamsung unveils “Gauss” generative AI model, set to debut in Galaxy S24 seriesThe model includes language, coding assistant, and image generation sub-models. Samsung's move reflects a broader strategy to apply generative AI across multiple products, with a focus on delivering meaningful and personalized interactions for users. (Korean Times)\n\nAdobe’s generated images of Israel-Hamas conflict slipped into news storiesAdobe's stock image library is under scrutiny as AI-generated images depicting the Israel-Hamas conflict are being sold and subsequently used by news publishers as authentic representations. Despite being labeled as \"generated by AI\" in Adobe Stock, these images are often presented without disclosure when used in news articles. (The Register)\n\nMeta restricts political advertisers from using generative AIThe decision, revealed in updates in Meta's help center, aims to prevent misuse that could amplify election misinformation. Advertisers dealing with Housing, Employment, Credit, Social Issues, Elections, and sectors like Health, Pharmaceuticals, and Financial Services are currently barred from employing generative AI features. Other tech giants like Google have also implemented similar measures. (Reuters)\n\nAmazon invests millions in training massive AI model \"Olympus\"According to insiders, Olympus rivals top models developed by OpenAI and Alphabet. The ambitious project is speculated to possess a staggering 2 trillion parameters, potentially surpassing OpenAI's trillion-parameter GPT-4. While Amazon has trained smaller models like Titan, the development of Olympus underscores the company's commitment to advancing large-scale AI capabilities despite the associated computational challenges. (Reuters)\n\nMicrosoft introduces AI characters and stories to Xbox gamesAn extensive partnership with Inworld AI involves Microsoft’s creation of an AI design copilot system, enabling Xbox developers to craft intricate scripts, dialogue trees, and quest lines. This initiative combines Inworld's expertise in character development with Microsoft's cloud-based AI solutions, including Azure OpenAI Service and technical insights from Microsoft Research. (The Verge)\n\nResearch: Nvidia’s ChipNeMo: A custom chip design model trained on internal dataResearchers demonstrated how generative AI, trained on internal data, can serve as a valuable assistant in the process of designing complex chips. The authors envision applying generative AI to various stages of chip design, anticipating substantial gains in overall productivity in the semiconductor industry. The customizable nature of ChipNeMo, with as few as 13 billion parameters, offers superior performance compared to larger general-purpose LLMs, marking an advancement in the application of generative AI to semiconductor engineering. (Nvidia)\n\nGitHub’s Copilot Enterprise enables customization for developers working with internal codePriced at $39 per person per month, Copilot Enterprise allows customization and tuning for proprietary codebases, addressing the needs of clients with unique programming languages. This move follows Amazon's October announcement that it would offer customization of its CodeWhisperer programming assistant. (CNBC)\n\nCruise initiates nationwide recall of 950 driverless cars after pedestrian incidentThe autonomous vehicle company voluntarily recalled 950 of its driverless cars across the U.S. following a severe crash last month where a pedestrian was hit and dragged for about 20 feet. The recall aims to address a programming flaw related to the \"Collision Detection Subsystem,\" specifically focusing on the post-collision response to prevent similar incidents. Cruise is also considering potential layoffs as it works to rebuild public trust in the aftermath of the incident. (The Washington Post)\n\nNations agree to set guardrails for military AI31 nations, including the U.S., signed a nonbinding declaration to establish voluntary guardrails on military AI. The agreement aims to ensure that the development, testing, and deployment of AI in military systems adheres to international laws, promotes transparency, avoids unintended biases, and includes safeguards allowing disengagement in case of unintended behavior. The signatories plan to meet in early 2024 for further discussions. (Wired)\n\nChina sets ambitious goal for advanced humanoid robotsThe Chinese Ministry of Industry and Information Technology outlined plans for China to produce its first humanoid robots by 2025. The government aims to support young companies in the robotics field, set industry standards, foster talent, and enhance international cooperation. The government's goals include breakthroughs in environment sensing, motion control, and machine-to-human interaction capabilities within the next two years, with plans for humanoid robots to think, learn, and innovate by 2027. (Bloomberg)\n\nGoogle expands and updates Generative AI in Search to over 120 new countriesThe expansion introduces support for four additional languages: Spanish, Portuguese, Korean, and Indonesian. Google also launched upgrades for the U.S. audience, including features such as easier follow-up questions, AI-powered translation assistance, and expanded definitions for topics like coding. (Google)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--72--1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--73--1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--28-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch-ads-and-exclusive-banners--80-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--29-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--98-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch-ads-and-exclusive-banners-_72_.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-23/",
    "title": "issue 23",
    "date": "",
    "reading_time": "",
    "content": "Last week brought reports that the European Union isconsideringa three- to five-year moratorium on face recognition in public places. Face recognition is a problematic technology with significant potential for misuse, and I celebrate the EU’s effort to protect human rights and civil society. But the blunt instrument of a long moratorium is a terrible idea.\n\nFive years is an eternity in AI, and implementing this proposal would all but guarantee that EU teams fall behind their colleagues in the U.S., China, and other nations.\n\nContrary to popular belief, face recognition is not a solved problem. Although many teams have achieved good performance on face recognition benchmarks such asLFW, the technology still has a long way to go. Open source software makes it easy to recognize faces from a front-facing still image, but a number of hard problems remain to be solved, including multi-camera tracking, re-identification (when someone exits the frame and then re-enters), robustness to occasional camera outages, and automatic multi-camera calibration. Such capabilities will advance significantly in the next few years.\n\nCountries that have the foundation to develop this technology will pull ahead of those that don’t. It would be ironic if the EU, having slowed its own work on face recognition, were to end up having to license it from American and Chinese companies.\n\nThe Universal Declaration of Human Rights remains one of the most inspirational documents I have ever read. I won’t pretend that forming good regulations is easy; it is hard because it entails hard tradeoffs. We must make sure that privacy-respecting societies don’t fall behind in technology development precisely because of those laudable values. Instead of hobbling them, we must enable them to leap ahead in a way that propagates those values.\n\nKeep learning!\n\nAndrew\n\nWhen artificial intelligence meets biology, even the simplest life forms can be mind-blowing.What happened:Researchers at Tufts and the University of Vermontprogrammedan evolutionary algorithm to design virtual organisms with specific capabilities. Then they implemented the designs using animal cells to produce living machines, as illustrated in thisvideo.How it works:The algorithm designed organisms to meet one of four behavioral goals: locomotion, object manipulation, object transportation, and collective behavior.\n\nWhy it matters:The authors envision a “scalable pipeline for creating functional novel life forms.” They believe their approach could yield bugs thatperforma variety of tasks, like digesting spilled oil or gathering ocean-borne plastic particles. They could also deliver medicine, identify cancer, or clear away arterial plaque.We’re thinking:We humbly request an army of biobots designed to scrub bathrooms.\n\nEnd-to-end backpropagation and labeled data are the peanut butter and chocolate of deep learning. However, recent work suggests that neither is necessary to train effective neural networks to represent complex data.What’s new:Sindy Löwe, Peter O’Connor, and Bastiaan Veeling proposeGreedy InfoMax(GIM), an unsupervised method for learning to extract features that trains only one layer at a time.Key insight:Theinformation bottleneck theory(IB) suggests that neural networks work by concentrating information like a data-compression algorithm. In data compression, the amount of information retained is measured in mutual information (MI) between original and compressed versions. IB says that neural nets maximize MI between each layer’s input and output. Thus GIM reframes learning as a self-supervised compression problem. Unlike earlier MI-based approaches, it optimizes each layer separately.How it works:GIM works on modular networks, in which each layer learns to extract features from its input and passes its output to the next available layer, and so on down to the final layer. GIM doesn’t require labels, but if they’re available, a linear classification model can learn from GIM’s compressed output in a supervised manner.\n\nResults:The researchers pitted Greedy InfoMax againstcontrastive predictive coding. Inimage classification, GIM beat CPC by 1.4 percent, achieving 81.9 percent accuracy. In avoice identificationtask, GIM underperformed CPC by 0.2 percent, scoring 99.4 percent accuracy. GIM’s scores are state-of-the-art for models based on mutual information.Why it matters:Backprop requires storing forward prediction, backward gradients, and weights for an entire network simultaneously. InfoMax handles each layer individually, making it possible to accommodate much larger models in limited memory.\n\nBehind the news:Layerwise training or pre-training has been around for at least a decade. For example,stacked autoencodersuse reconstruction error as an alternative unsupervised mechanism to control intelligent data compression. Many past approaches are more focused on pre-training and assume that, once each layer has been trained individually, they will be trained together with a supervised task.We’re thinking:Many machine learning applications use a large pretrained network as an initial feature extractor and then apply transfer learning. By maximizing MI between layers, this approach could use more data to train and build still larger networks.\n\nWhat if you could identify just about anyone from a photo? A controversial startup is making this possible.What happened:Hundreds of U.S. law enforcement agencies are using a face ID service that matches photos against a database of billions of images, theNew York Timesreported.How it works:Clearview AI scraped photos from Facebook and other social media sites, employment sites, mugshot archives, news sites, and message boards. The company’s promotional materials say it holds over 3 billion images, a repository far bigger than law-enforcement databases, as shown in the image above.\n\nBehind the news:Clearview AI was founded in 2016 by an Australian programmer with backing from tech investor Peter Thiel. The company has raised $7 million, according to the funding trackerPitchbook.\n\nYes, but:TheNew York Timesoutlines a number of concerns.\n\nWe’re thinking:We need regulations that balance development and deployment of useful technologies against their potential for abuse and harm. Face identification vendors should be required to report performance metrics, and police departments should be required to use models that pass federally establishedguidelinesand perform background checks of personnel who have access to the technology.\n\nGet your model running on iOS and Android devices. Take the first step in Course 2 of the TensorFlow: Data and Deployment Specialization.Enroll now\n\nFor some college graduates, landing a first job means making a good impression on a chatbot.What’s new:University guidance counselors around the U.S. are preparing students for interviews with AI-powered screening algorithms, according toCNN.How it works:Companies likeYobsandHireVuefilter candidates for hundreds of corporate customers. Applicants submit videos of themselves answering pre-determined questions. The software then rates their language skills as well as non-verbal elements like tone, pitch, and emotional tenor. HireVue also evaluates body language and facial expressions.\n\nYes, but:Training job hunters to look at the camera and project confidence is a good idea whether they’re talking to a bot or a human being. But critics question whether current AI is capable of reliably matching verbal or body language with traits that make for a good hire. Princeton University computer science professor Arvind NarayanancalledAI applicant-screening programs “elaborate random number generators” in a talk last year.Why it matters:Millions of college graduates enter the global job market every year. Good AI could help hiring managers pluck the most qualified candidates from a deluge of resumes. Bad AI could knock many great applicants out of the running.We’re thinking:AI screening systems still need to prove themselves effective and reasonably bias-free. Meanwhile, we welcome tools that can improve, at scale, job opportunities for deserving individuals who otherwise might not hear from a recruiter.\n\nSoftmax commonly computes probabilities in a classifier’s output layer. But softmax isn’t always accurate in complex tasks — say, in a natural-language task, when the length of word vectors is much smaller than the number of words in the vocabulary. A new function renders more accurate predictions with lower computational cost than earlier alternatives.What’s new:Zhilin Yang, Thang Luong, and Ruslan Salakhutdinov at Carnegie Mellon University, with Quoc Le at Google Brain, developed an efficient solution to the so-called softmax bottleneck:Mixtape.Key insight:A previous proposal,Mixture of Softmaxes, (MoS) is a weighted sum of multiple softmaxes, and thus slow to train. Mixtape reformulates MoS as a single softmax of weighted sums. With a clever way of calculating the weights, that rearrangement avoids the bottleneck with much speedier execution.How it works:Mixtape’s weighted sum depends on the word it is evaluating — a not-so-obvious way to formulate the problem. The weights must be generated efficiently to avoid losing the computational advantage over MoS.\n\nResults:The researchers compared transformer-based models with output layers employing Mixtape, MoS-15, or softmax. The tasks included recreating a text sample and translating a sentence from English to German or French. On text generation, MoS-15 (which entails 15 softmax calculations) and Mixtape improved perplexity — a measure of the model’s predictive certainty — by around 3, achieving a score of 56. MoS-15 slightly outperformed Mixtape. However, Mixtape required only slightly more training time than softmax, whereas MoS-15 required twice as long.Why it matters:Much research has focused on extracting meaningful features of input, but features are less useful if the output layer can’t classify them properly. Mixtape should allow models to take better advantage of features they extract without sacrificing AWS credits.We’re thinking:Mixtape can do better than softmax with only a little more training time. We may see Mixtape overtake softmax in some applications.\n\nYouTube is a great place to learn about new ideas — including some that have been thoroughly discredited.What’s new:YouTube’s recommendation algorithm is helping spread misinformation about climate change, according toresearchby Avaaz, a self-funded activist group.What they found:The researchers aimed to learn which videos YouTube was likely to feature in its “Up next” recommendations for videos resulting from three searches: “climate change,” “global warming,” and the more skeptical phrase “climate manipulation.” Working between August and December, they entered the search terms into a YouTube service that lists related videos. Then they used a data visualization tool to find the 100 most likely recommendations.\n\nThe response:YouTubedefendedits recommendation software and questioned the study’s methodology. It pointed out that it displays a link to Wikipedia’s “Global Warming” page under many climate-related videos.Behind the news:In June, YouTubeoverhauledits algorithms to give users more control over recommendations. Those changescutthe time viewers spent watching such content by 70 percent. The move followed earlier efforts to block videos espousing miracle cures or conspiracy theories.Why it matters:YouTube’s recommendations are a potent force for spreading information (and misinformation). They were credited withdrivingaround 70 percent of the site’s viewing time in 2018.We’re thinking:It’s great to see YouTube and other companies working to reduce misinformation. But the AI community’s work is far from done. We need incentive mechanisms that don’t just reward numbers of views, but shift incentives toward distributing factual information and rational perspective to the extent they can be determined fairly.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Andrews20Letter20ASPECT-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/frogs.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/InfoMax20ASPECT201.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Clearview20220ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_ezgif.com-resize-4.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize201-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Mixtape20ASPECT201.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/youtube203.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-49/",
    "title": "issue 49",
    "date": "",
    "reading_time": "",
    "content": "I received a copy ofWhy We Sleep: Unlocking the Power of Sleep and Dreamsas a Christmas gift — back in the pre-Covid era — and finished it last weekend. This book by Matthew Walker, director of UC Berkeley’s sleep and neuroimaging lab, is a useful reminder of the importance of sleep for learning and also for physical and mental health.\n\nSay you spend a few hours learning something new on Wednesday. Getting a solid night of sleep the same day will help consolidate the new memories and strengthen your long-term retention. If your sleep on Wednesday night is disrupted, your long-term retention will be affected even if you catch up on sleep later in the week.\n\nBut the story doesn’t end there. Over the next few days, your brain may still be busy consolidating the new learnings. A surprisingstudyshowed that even if your sleep is disrupted on Friday — two days later — long-term retention can still be significantly affected.\n\nBottom line: After you spend time studying during the day, I encourage you to get a good night’s sleep. Even better, try to get a good night’s sleep every night.\n\nThe world is going through turbulent times. With society buffeted by biological, social, and political forces, who has time for sleep?! I try to sleep from midnight to 8 a.m. every day, including weekends. With an 18-month-old daughter who wakes up whenever she wants, and occasional meetings with business partners in Asia or in Europe at odd hours, my sleep schedule is far from perfect.\n\nYou’re probably incredibly busy as well. Despite everything going on, I make sleep a priority, and I hope you will, too.\n\nKeep learning,\n\nAndrew\n\nA mechanical lab assistant could accelerate chemistry research.What’s new:Researchers at the University of Liverpool trained amobile robot armto navigate a lab, operate equipment, handle samples, and obtain results far faster than a human scientist. The authors believe their system is the first mobile robot capable of running lab experiments.How it works:In a recent study, the articulated arm on wheels completed 688 experiments, testing various hypotheses to extract hydrogen from water efficiently using chemicals and light.\n\nResults:The study discovered chemical formulae that made it easier to separate hydrogen from oxygen in water. More important, it proved that a robot can do such work effectively, speedily, and without interruption. The authors estimate that a human scientist would have taken 1,000 times longer to produce similar results.Why it matters:The authors hope to offer robots forsalewithin 18 months. The $150,000-plus price tag might be a bargain if the Covid-19 pandemic makes in-person lab experimentation unfeasible.We’re thinking:Most factory automation involves stationary robots positioned along a manufacturing line. Perhaps mobile manipulation — where the arm moves to the object being manipulated — will prove to be more efficient for automating science labs.\n\nNeed a wardrobe upgrade? You could ask the fashion mavens at Netflix’s Queer Eye — or you could use a new neural network.What’s new:Yen-Liang Lin, Son Tran, and Larry S. Davis at Amazon proposeCategory-based Subspace Attention Network(CSA-Net) to predict and retrieve compatible garments and accessories that complement one another. (This is the third of three papers presented by Amazon at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). We covered the others in previous issues.)Key insight:Suppose you have several items that go together and want one more to complete the ensemble. Past approaches such asSCE-Netcan find compatible outfits by scoring pairs of garments or accessories, but Amazon’s catalogue is too vast to compare every pair of items in it. CSA-Net retrieves items by learning a vector description of each item and finding nearby items. The network adjusts its representation based on the categories already selected. For instance, given a shirt and shoes, it can find a matching handbag or hat.How it works:The researchers trained CSA-Net by providing outfits to complete, sets of candidate items, and labels that identify compatible candidates. CSA-Net learned to place outfits and compatible items nearby in the feature space while placing incompatible items farther away.\n\nResults:The researchers evaluated CSA-Net on thePolyvore-Outfitdataset of fashion items and labels that detail their compatibility. Provided an incomplete outfit of four items, CSA-Net predicted the correct fifth piece 59.26 percent of the time, compared with 53.67 percent achieved by the previousstate of the art. It also outperformed the previous state of the art in predicting whether a pair of garments is compatible, achieving a higher area under the curve (the probability of predicting a positive match instead of a negative match).Why it matters:The universe of fashion items and accessories is immense and complex, posing a challenge for matching items situated in a feature space. CSA-Net makes the task more tractable by restructuring the feature space into compatible subspaces.We’re thinking:Leave it to machine learning engineers to build technology that liberates them from having to decide which shirt goes with what pants and shoes.\n\nThe same technology that hasbedeviledHollywood stars androiledpolitics is easing corporate communications.What’s new:Synthesiagenerates training and sales videos featuring photorealistic, synthetic talking heads that read personalized scripts in any of 34 languages,Wiredreports. You can try out the servicehere.How it works:The company uses GANs for much of its rendering, but its production pipeline includes customized deep learning, computer vision, and visual effects, a representative toldThe Batch. Clients submit a script and choose from a selection of avatars, languages, and voices, and the AI generates a video of the avatar reading the client’s words.\n\nBehind the news:Generated video is also catching on in advertising and marketing.\n\nWhy it matters:Producers of commercial video and photography have become interested in AI’s ability to generate realistic human characters as the pandemic has curtailed live film shoots, according to the Synthesia CEO and co-founder Victor Riparbelli. Generated characters save the cost of hiring cast and crew and make it easy to localize productions for a worldwide audience. Plus, there’s no danger of spreading a deadly cough.\n\nWe’re thinking:It’s easy to see potential harm in deepfakes, but the same techniques have productive uses for people with imagination to recognize them and ingenuity to implement them at scale.\n\nJoin us for Break Into NLP, a virtual live event on July 29th from 10 a.m. to 11:30 a.m. PDT. Celebrate the launch of Course 3 of our Natural Language Processing Specialization and hear from from Andrew Ng, Kenneth Church, Marti Hearst, and other NLP experts!\n\nSign up nowto receive the event access link.\n\nCompanies in Africa and the Middle East are building AI capacity in very different ways, a new study found.What’s new:AI is growing fast in both regions despite shortages of talent and data, according toMIT Technology Review Insights, the research arm of Massachusetts Institute of Technology’s magazine. Yet the implementations in each region reflect stark differences in economic development.What it says:The report focuses on wealthy countries in the Persian Gulf, particularly Saudi Arabia and the United Arab Emirates, as well as African tech hotspots in Ghana, Kenya, and Nigeria.\n\nGrowing pains:AI adoption hasn’t been smooth sailing. Nearly 60 percent of respondents said they’ve struggled to apply AI in their business. Nearly as many cited difficulty obtaining high-quality data. Africa and the Middle East are also struggling to find talent, with 40 percent of respondents noting a shortage of AI professionals in the regions.Why it matters:AI could prove to be a boon for individuals, and the planet at large, by helping to lift African economies and wean Middle Eastern ones from reliance on oil.We’re thinking:The Persian Gulf is one of the world’s richest regions, and sub-Saharan Africa its poorest. The fact that both are turning to AI says a lot about the technology’s potential to streamline existing economies and foster new ones.\n\nThe activation function known asReLUbuilds complex nonlinear functions across layers of a neural network, making functions that outline flat faces and sharp edges. But how much of the world breaks down into perfect polyhedra? New work explores an alternative activation function that yields curvaceous results.What’s new:Stanford researchers led by Vincent Sitzmann and Julien Martel developed the periodic activation functionsin(x)to solve equations with well defined higher-order derivatives. They showed preliminary success in a range of applications.Key insight:Training a neural network updates its weights to approximate a particular function. Backprop uses the first derivative to train networks more efficiently than methods such as hill-climbing that explore only nearby values. Higher-order derivatives contain useful information that ReLU can’t express and other activation functions describe poorly. For example, in the range 0 to 1, the values of x and x2 are similar, but their derivatives are dramatically different. Sine has better-behaved derivatives.How it works:Sine networks, which the researchers call sirens, are simply neural networks that use sine activation functions. However, they need good initial values.\n\nResults:The authors used sine networks to solve differential equations (where they can learn directly from derivatives), interpret point clouds, and process images and audio. They provideexamplesand acollab notebookso you can try it yourself. They demonstrated success in all these domains and provided quantitative evidence for the value of gradients when applied toPoisson image reconstruction. The authors trained models to predict the gradient of an image and compared the quality of generated images after reconstruction usingPoisson’s equation. Evaluated on the starfish image above, a sine network achieved 32.91 peak signal-to-noise ratio, a measurement of reconstruction quality, compared with 25.79 forTanh.Why it matters:ReLUs have been a deep learning staple since2012. For data that have critical higher-order derivatives, alternatives may improve performance without increasing model complexity.We’re thinking:ReLUs may be good for drawing the angular TeslaCybertruck, but sines may be better suited for a1950 Chevy 3500.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter207.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize2014.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/CSA-Net.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Corporate202.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/twitter-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Africa2-aspect.png.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Siren.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-303/",
    "title": "issue 303",
    "date": "",
    "reading_time": "",
    "content": "I am alarmed by the proposed cuts to U.S. funding for basic research, analyzedhere, and the impact this would have for U.S. competitiveness in AI and other areas. Funding research that is openly shared benefits the whole world, but the nation it benefits most is the one where the research is done.\n\nIf not forfunding for my early work in deep learningfrom the National Science Foundation (NSF)  and Defense Advanced Research Projects Agency (DARPA), which disburse much of U.S. research funding, I would not have discovered lessons about scaling that led me to pitch starting Google Brain to scale up deep learning. I am worried that cuts to funding for basic science will lead the U.S. — and also the world — to miss out on the next set of ideas.\n\nIn fact, such funding benefits the U.S. more than any other nation.  Scientific research brings the greatest benefit to the country where the work happens because (i) the new knowledge diffuses fastest within that country, and (ii) the process of doing research creates new talent for that nation.\n\nWhy does most innovation in generative AI still happen in Silicon Valley? Because two teams based in this area — Google Brain, which invented the transformer network, and OpenAI, which scaled it up — did a lot of the early work. Subsequently, team members moved to other nearby businesses, started competitors, or worked with local universities. Further, local social networks rapidly diffused the knowledge through casual coffee meetings, local conferences, and even children’s play dates, where parents of like-aged kids meet and discuss technical ideas. In this way, the knowledge spread faster within Silicon Valley than to other geographies.\n\nIn a similar vein, research done in the U.S. diffuses to others in the U.S. much faster than to other geographic areas. This is particularly true when the research is openly shared through papers and/or open source: If researchers have permission to talk about an idea, they can share much more information, such as tips and tricks for how to really make an algorithm work, more quickly. It also lets others figure out faster who can answer their questions. Diffusion of knowledge created in academic environments is especially fast. Academia tends to be completely open, and students and professors, unlike employees of many companies, have full permission to talk about their work.\n\nThus funding basic research in the U.S. benefits the U.S. most, and also benefits our allies. It is true that openness benefits our adversaries, too. But as a subcommittee of the U.S. House of Representatives committee on science, space, and technologypoints out, “... open sharing of fundamental research is [not] without risk. Rather, ... openness in research is so important to competitiveness and security that it warrants the risk that adversaries may benefit from scientific openness as well.”\n\nFurther, generative AI is evolving so rapidly that staying on the cutting edge is what’s really critical. For example, the fact that many teams can now train a model with GPT-3.5- or even GPT-4-level capability does not seem to be hurting OpenAI much, which is busy growing its business by developing the cutting-edge o4, Codex, GPT-4.1, and so on. Those who invent a technology get to commercialize it first, and in a fast-moving world, the cutting-edge technology is what’s most valuable. Studies likethis one(albeit done while the internet was not as prevalent as it is today) also show how knowledge diffuses locally much faster than globally.\n\nChina was decisively behind the U.S. in generative AI when ChatGPT was first launched in 2022. However, China’s tech ecosystem is very open internally, and this has helped it to catch up over the past two years:\n\nWhile there’s also much about China that I would not seek to emulate, the openness of its tech ecosystem has helped it accelerate.\n\nIn 1945, Vannevar Bush’s landmark report “Science, The Endless Frontier” laid down key principles for public funding of U.S. research and talent development. Those principles enabled the U.S. to dominate scientific progress for decades. U.S. federal funding for science created numerous breakthroughs that have benefited the U.S. tremendously, and also the world, while training generations of domestic scientists, as well as immigrants who likewise benefit the U.S.\n\nThe good news is that this playbook is now well known. I hope many more nations will imitate it and invest heavily in science and talent. And I hope that, having pioneered this very successful model, the U.S. will not pull back from it by enacting drastic cuts to funding scientific research.\n\nAndrew\n\nWe’re featured partners of Snowflake’s Dev Day 2025, a full day for AI and data practitioners to explore cutting-edge demos, make valuable contacts, and hear from top voices in the field (including Andrew Ng). See you on June 5!Register here\n\nAnthropic continued its tradition of building AI models that raise the bar in coding tasks.\n\nWhat’s new:Anthropic launchedClaude 4 Sonnet 4 and Claude Opus 4, the latest medium- and largest-size members of its family of general-purpose large language models. Both models offer an optional reasoning mode and can use multiple tools in parallel while reasoning. In addition, the company made generally available Claude Code, a coding agent previously offered as a research preview, along with a Claude Code software development kit.\n\nHow it works:The team trained the Claude 4 models on a mix of publicly available information on the web as well as proprietary purchased data, data from Claude users who opted to share their inputs and outputs, and generated data. They fine-tuned the models to behelpful, honest, and harmlessaccording to human andAI feedback.\n\nResults:Both Claude 4 models tied Google Gemini 2.5 Pro at the top of the LMSys WebDev Arena and achieved top marks for coding and agentic computer-use benchmarks in Anthropic’s tests.\n\nWhy it matters:The new models extend LLM technology with parallel tool use, using external files as a form of memory, and staying on-task over unusually long periods of time. Early users have reported many impressive projects, including aTetris clonebuilt in one shot and a seven-hour stintrefactoring Rakutan’s open-source code base.\n\nWe’re thinking:Prompting expert @elder_plinius published a text file that is purported to beClaude 4’s system promptand includes some material that does not appear in Anthropic’s ownpublicationof the prompts. It is instructive to see how it conditions the model for tool use, agentic behavior, and reasoning.\n\nGoogle revamped its roster of models, closed and open, and added more AI-powered features to its existing products.\n\nWhat’s new:Google staged a parade ofannouncementsat this year’s I/O developer conference. New offerings include improvements toGemini 2.5 Pro and Gemini 2.5 Flashand a preview ofGemma 3n(all three generally available in June), the updatedVeo 3video generator (available via Flow, Google’s AI videography app, for paid subscribers to its AI Pro and Ultra services), and increasingly AI-powered search.\n\nHow it works:The I/O offerings spanned from public-facing products to developer tools.\n\nWhy it matters:Google is catching up with the Microsoft/OpenAI colossus on several fronts. The addition of audio output to Gemini and Gemma models fuels the rise of voice-to-voice and other audio applications and gives developers powerful new tools to build them. At the same time, Veo 3’s text-to-video-plus-audio output showsmarkedimprovementover the previous version.\n\nBehind the news:The number of tokens Google processed monthly has surged this year from 9.7 trillion last year to 480 trillion, a sign that its AI APIs and AI-infused products are rapidly gaining traction. Google’s progress contrasts with Apple’s ongoingstruggles. Both share advantages in smartphones and app distribution. But, while Google has showcased a string of advanced models as well as early efforts to integrate them into legacy products, Apple’s organizational challenges have hampered its AI development. Now Apple must contend with OpenAI’sacquisitionof LoveFrom, the startup founded by its former lead product designer Jony Ive.\n\nWe’re thinking:Google I/O 2025 was a strong showing of generative AI capabilities! There’s still work to be done to translate these innovations into compelling products, but the company now has a strong base for building numerous innovative products.\n\nDeepSeek made headlines late last year, when it built a state-of-the-art, open-weights large language model at a cost far lower than usual. The upstart developer shared new details about its method.\n\nWhat’s new:Chenggang Zhao and colleagues at DeepSeek describedsoftware and hardware choicesthat reduced memory and processing requirements while building their groundbreaking mixture-of-experts models DeepSeek-R1 and DeepSeek-V3.\n\nMixture of experts (MoE) basics:The MoE architecture uses different subsets of a model’s parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a routing module that learns to choose which one(s) to use based on the training example. In this way, different experts learn to specialize in different types of input.\n\nHow it works:The authors trained DeepSeek-R1 and DeepSeek-V3 using a cluster of 2,048 Nvidia H800 GPUs composed of nodes that contained 8 GPUs each. MoE requires less memory than dense architectures, since a given input activates only a portion of a model’s parameters. This enabled the authors to train DeepSeek-V3 on 250 GFLOPs per token, while Qwen 2.5 72B required 394 GFLOPs per token and Llama 3.1 405B required 2,448 GFLOPs per token.\n\nBehind the news:DeepSeek-V3made waves when it was released in December. It performed better than Llama 3.1 405B, the leading LLM at the time, but its training cost was an astonishing $5.6 million, compared to the usual tens to hundreds of millions of dollars. Some observers wereskepticalof the reported cost, pointing out that the $5.6 million dollar figure doesn’t include salaries, data acquisition and annotation, processing failed training runs, and other research and development costs. In addition, the cost of trainingDeepSeek-R1remains unknown.\n\nWhy it matters:Traditionally, only companies with large budgets and vast resources could afford to train state-of-the-art models. DeepSeek changed that but didn’t explain how when it released its models. By sharing the details, the company has empowered a wider range of teams to improve the state of the art.\n\nWe’re thinking:Shortly after DeepSeek-R1 was released, some engineers claimed — without presenting evidence — that DeepSeek had copied their work. DeepSeek’s disclosure of its training methods should lay to rest any remaining questions about this. Its work was truly innovative, and we applaud its release of key technical details.\n\nA study co-authored by tech-manual publisher Tim O’Reilly shows that OpenAI trained GPT-4o on parts of his company’s books that were not made freely available.\n\nWhat happened:O’Reilly, computer scientist Sruly Rosenblat, and economist Ilan Straussfoundthat GPT-4o was able to identify verbatim excerpts from dozens of O’Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model’s training data.\n\nHow it works:The researchers adapted theDE-COPmethod to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books.\n\nResults:The authors asked each model to identify the verbatim paragraph and calculated each model’s percentage of correct responses. Then they averaged each model’s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially included in the training set) from those that weren’t available at the time. 50 percent AUROC indicates random chance, while higher scores indicate higher accuracy.\n\nYes, but:Newer large language models are better at distinguishing human-written from generated text, even if it wasn’t in their training sets. For instance, given paragraphs that were published after their knowledge cutoffs, GPT-4o returned scores as high as 78 percent AUROC. The authors note that this may challenge their conclusions, since they interpret high scores to indicate that a model saw the text during training. Nonetheless, they argue that their approach will remain valid while scores for both text that was included and text that was excluded from training sets remain under 96 percent AUROC. “For now,” they write, “the gap remains sufficiently large to reliably separate the two categories.”\n\nBehind the news:Historically AI developers have trained machine learning models on any data they could acquire. But in the era of generative AI, models trained on copyrighted works can mimic the works and styles of the works’ owners, creating a threat to their livelihoods. Some AI developers have responded by regarding data that’s freely available on the web as fair game, and material that’s otherwise protected as off-limits for training. However, datasets that include ostensibly private data are widely circulated, includingLibGen, which includes all 34 of the O’Reilly Media titles tested in this study. Moreover, unauthorized copies of many copyrighted works are posted without paywalls or even logins, making it possible even for web scrapers that crawl only the open web to download them. Google and OpenAI, which is currently embroiled in lawsuits by authors and publishers who claim it violated copyrights by training models on copyrighted works, recentlylobbiedthe United States government to relax copyright laws for AI developers.\n\nWhy it matters:The AI industry requires huge quantities of high-quality data to keep advancing the state of the art. At the same time, copyright owners are worried that models trained on their data might hamper their opportunities to earn a living. AI developers must find fair ways to respond. As O’Reilly points out, exploiting copyrighted works instead of rewarding their authors could lead to an “extractive dead end” that ultimately diminishes the supply of the high-quality training data.\n\nWe’re thinking:We have learned a great deal from O’Reilly Media’s books, and we’re grateful to the many authors, editors, graphic artists, and others who produce them. Meanwhile, it’s time for the U.S. Congress —  and legislators internationally — toupdatecopyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--96--2.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/GqglM2_WAAAcXxW.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--97-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--60-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--98-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--99-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-281/",
    "title": "issue 281",
    "date": "",
    "reading_time": "",
    "content": "Is AI progressing rapidly? Yes! But while the progress of underlying AI technology has indeed sped up over the past 2 years, the fastest acceleration is in applications.\n\nConsider this: GPT-4 was released March 2023. Since then, models have become much faster, cheaper, sometimes smaller, more multimodal, and better at reasoning, and many more open weight versions are available — so progress has been fantastic! (Claims that AI is “hitting a wall” seem extremely ill-informed.) But more significantly, many applications that  already were theoretically possible using the March 2023 version of GPT-4 — in areas such as customer service, question answering, and process automation — now have significant early momentum.\n\nI’m confident 2025 will see even faster and more exciting advances than 2024 in both AI technology and applications. Looking back, the one thing that could have stopped AI was bad, anti-competitive regulation that would have put onerous burdens on developers, particularly of open models. So long as we remain vigilant and hold off these anti-innovation forces, we’ll keep up or even further accelerate progress.\n\nI’m also seeing a widening gap between those at the cutting edge (which includes many readers ofThe Batch!) and those who have not yet tried out ChatGPT even once (yes, a lot of people are still in this group!). As technology changes around us, we all have to keep up to remain relevant and be able to make significant contributions. I’m committed to making sure DeepLearning.AI continues to help you learn the most useful and important AI technologies. If you’re making New Year’s resolutions, I hope you’ll include us in your learning plan!\n\nAI is the most important technological change happening in the world right now. I’m thrilled to be working in this exciting sector alongside you, and I’m grateful for your efforts to learn about and apply it to better the lives of yourself and others.\n\nHappy holidays!\n\nAndrew\n\nWhat a year! AI made dramatic advances in 2024. Agentic systems improved their abilities to reason, use tools, and control desktop applications. Smaller models proliferated, many of them more capable and less expensive than their larger forbears. While some developments raisedworries, far more sparked wonder and optimism. As in thewaningdaysofearlieryears, we invite you to pour a cup of hot cocoa and consider the high points of the last 12 months.\n\nThe AI community laid the foundation for systems that can act by prompting large language models iteratively, leading to much higher performance across a range of applications.\n\nWhat happened:AI gained a new buzzword —agentic— as researchers, tool vendors, and model builders equipped large language models (LLMs) to make choices and take actions to achieve goals. These developments set the stage for an upswell of agentic activity in the coming year and beyond.\n\nDriving the story:Several tools emerged to help developers build agentic workflows.\n\nBehind the news:Techniques for prompting LLMs in more sophisticated ways began to take off in 2022. They coalesced in moves toward agentic AI early this year. Foundational examples of this body of work include:\n\nWhere things stand:The agentic era is upon us! Regardless of how wellscaling lawscontinue to drive improved performance of foundation models, agentic workflows are making AI systems increasingly helpful, efficient, and personalized.\n\nFierce competition among model makers and cloud providers drove down the price of access to state-of-the-art models.\n\nWhat happened:AI providers waged aprice warto attract paying customers. A leading indicator: From March 2023 to November 2024, OpenAI cut the per-token prices of cloud access to its models by nearly 90 percent even as performance improved, input context windows expanded, and the models became capable of processing images as well as text.\n\nDriving the story:Factors that pushed down prices include open source, more compute-efficient models, and excitement around agentic workflows that consume more tokens at inference. OpenAI’s GPT-4 Turbo set a baseline when it debuted in late 2023 at $10.00/$30.00 per million tokens of input/output. Top model makers slashed prices in turn: Google and OpenAI at the higher end of the market, companies in China at the lower end, and Amazon at both. Meanwhile, startups with specialized hardware offered open models at prices that dramatically undercut the giants.\n\nYes, but:The trend toward more processing-intensive models is challenged but not dead. In September, OpenAIintroducedtoken-hungry models with relatively hefty price tags: o1-preview ($15.00/$60.00 per million tokens input/output) and o1-mini ($3.00/$12.00). In December, o1 arrived with a more accurate pro mode that’savailableonly to subscribers who are willing to pay $200 per month.\n\nBehind the news:Prominent members of the AI community pushed against regulations that threatened to restrict open source models, which played an important role in bringing down prices. Opposition by developers helped to block California SB 1047, a proposed law that would have held developers of models above certain size limits liable for unintended harms caused by their models and required a “kill switch” that would enable developers to disable them — a problematic requirement for open weights models that anyone could modify and deploy. California Governor Gavin Newsom vetoed the bill in October.\n\nWhere things stand:Falling prices are a sign of a healthy tech ecosystem. It’s likely that in-demand models will always fetch relatively high prices, but the market is increasingly priced in pennies, not dollars, per million tokens.\n\nVideo generation exploded in an abundance of powerful models.\n\nWhat happened:Companies big and small introduced new or updated text-to-video generators. Some added image-to-video and/or video-to-video capabilities. While most models focus on generating cinematic clips, some specialize in videos for social media.\n\nDriving the story:Even at the extraordinary pace of AI lately, video generators in the past year matured with remarkable speed.Virtually every major model produces convincing, highly detailed scenes, both realistic and fantastical, while ramping up image resolution, speed, output length, and users’ ability to control their outputs.\n\nBehind the news:Video generation is already reshaping the movie industry. In February, after seeing a preview of Sora, American filmmaker Tyler Perryhalteda planned expansion of his production studio, arguing that within a few years, AI video could put traditional studios out of business. Members of the video graphics team atThe Late Show with Stephen ColbertuseRunway’s technology to add special effects to conventional digital video, cutting editing time from hours to minutes.\n\nWhere things stand:Video generation came a long way in 2024, but there’s still plenty of room for improvement. Because most models only generate a small number of frames at a time, they can struggle to track physics and geometry and to generate consistent characters and scenery over time. The computational demands of maintaining consistency across frames means that generated clips are brief. And even short outputs take substantial time and resources to generate: Sora can take 10 to 20 minutes torenderclips as short as 3 seconds. OpenAI and Runway released faster versions — Sora Turbo and Gen-3 Alpha Turbo — to address the challenge.\n\nFor years, the best AI models got bigger and bigger. But in 2024, some popular large language models were small enough to run on a smartphone.\n\nWhat happened: Instead of putting all their resources into building big models, top AI companies promoted families of large language models that offer a choice of small, medium, and large. Model families such as Microsoft Phi-3 (in versions of roughly 3.8 billion, 7 billion, and 14 billion parameters), Google Gemma 2 (2 billion, 9 billion, and 27 billion), and Hugging Face SmolLM (135 million, 360 million, and 1.7 billion) specialize in small.\n\nDriving the story:Smaller models have become more capable thanks to techniques like knowledge distillation (in which a larger teacher model is used to train a smaller student model to match its output), parameter pruning (which removes less-influential parameters), quantization (which reduces neural network sizes by representing each parameter with fewer bits), and greater attention to curating training sets for data quality. Beyond performance, speed, and price, the ability to run on relatively low-powered hardware is a competitive advantage for a variety of uses.\n\nBehind the news:Distillation, pruning, quantization, and data curation are longstanding practices. But these techniques have not resulted in models quite this ratio of size and capability before, arguably because the larger models that are distilled, pruned, or quantized have never been so capable.\n\nWhere things stand:Smaller models dramatically widen the options for cost, speed, and deployment. As researchers find ways to shrink models without sacrificing performance, developers are gaining new ways to build profitable applications, deliver timely services, and distribute processing to the edges of the internet.\n\nBig AI companies found creative ways to gain cutting-edge technology and talent without buying startups.\n\nWhat happened:In 2024, some tech giants entered into novel partnership arrangements with AI startups, hiring top executives and securing access to technology without acquiring the companies outright. These agreements enabled the giants to take on elite talent and proven technology quickly with less risk that regulators might hinder such actions. The startups lost their leadership teams and control over key technical developments. In return, they received cash (in some cases, at least), rewarded investors, and were able to step back from the expense of building cutting-edge models.\n\nDriving the story:Microsoft, Amazon, and Google used their deep pockets and cloud infrastructure to strike deals with Inflection AI, Adept AI and Covariant, and Character.ai respectively. (Disclosure: Andrew Ng is a member of Amazon’s board of directors.)\n\nBehind the news:Tech giants have long relied on traditional acquisitions to gain new talent and capabilities, often acquiring startups specifically for their skilled teams (known as an acquihire) and/or their products or underlying technology, which can be expensive and time-consuming to develop and test in the market. But traditional acquisitions increasingly face scrutiny from antitrust regulators who are concerned about big companies reducing competition by buying out smaller ones. For example, the United States Federal Trade Commission sought to block Amazon’s acquisition of iRobot, prompting the companies toabandonthe transaction in January 2024.\n\nWhere things stand:Giving startups a lump sum and/or licensing fees in return for top talent and technology looks like the new normal for tech giants that are challenged to keep pace with rapidly advancing research and markets. But even arms-length arrangements don’t immunize tech giants and startups against regulatory investigation. Microsoft’s investment in Inflection AI was brieflyscrutinizedin Europe and is still beingevaluatedby U.S. regulators. Even Microsoft’s more traditionalinvestmentin OpenAI and the interests of Amazon and Google in Anthropic faced regulatory hurdles. So far, however, regulators have yet to conclude that any of these agreements violates antitrust law.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--39-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--40-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--41-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--42-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--43-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--44-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--34-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-301/",
    "title": "issue 301",
    "date": "",
    "reading_time": "",
    "content": "AI’s ability to make tasks not just cheaper, but also faster, is underrated in its importance in creating business value.\n\nFor the task of writing code, AI is a game-changer. It takes so much less effort — and is so much cheaper — to write software with AI assistance than without. But beyond reducing the cost of writing software, AI is shortening the time from idea to working prototype, and the ability to test ideas faster is changing how teams explore and invent. When you can test 20 ideas per month, it dramatically changes what you can do compared to testing 1 idea per month. This is a benefit that comes from AI-enabledspeedrather than AI-enabled cost reduction.\n\nThat AI-enabled automation can reduce costs is well understood. For example, providing automated customer service is cheaper than operating human-staffed call centers. Many businesses are more willing to invest in growth than just in cost savings; and, when a task becomes cheaper, some businesses will do a lot more of it, thus creating growth. But another recipe for growth is underrated: Making certain tasks much faster (whether or not they also become cheaper) can create significant new value.\n\nI see this pattern across more and more businesses. Consider the following scenarios:\n\nI’ve written previously about looking at thetasksa company does to explore where AI can help. Many teams already do this with an eye toward making tasks cheaper, either to save costs or to do those tasks many more times. If you’re doing this exercise, consider also whether AI can significantly speed up certain tasks. One place to examine is the sequence of tasks on the path to earning revenue. If some of the steps can be sped up, perhaps this can help revenue growth.\n\nGrowth is more interesting to most businesses than cost savings, and if there are loops in your business that, when sped up, would drive growth, AI might be a tool to unlock this growth.\n\nKeep building!\n\nAndrew\n\nIn Course 4 of theData Analytics Professional Certificateyou’ll work with truly real-world data: messy, inconsistent, and often unstructured. You’ll extract data from websites, APIs, and databases, and clean it using Python and SQL. By the end, you’ll be able to make raw datasets analysis-ready, with speed and accuracy.Enroll today!\n\nMicrosoft published its latest recipe for training reasoning models, substantially expanding what is still a fairly small base of public knowledge.\n\nWhat’s new:Microsoft releasedPhi-4-reasoning, Phi-4-reasoning-plus, andPhi-4-mini-reasoningalong with lessons learned in building the models.\n\nHow it works:All three models are fine-tuned versions of pretrained models.\n\nSmaller model lessons learned:During reinforcement learning, Phi-4-mini-reasoning exhibited instability, such as output batches that varied greatly in length or received mostly negative rewards, apparently depending on the training data or output. The authors suspect that the model’s small size caused these issues. Among the lessons learned:\n\nLarger model lessons learned:Phi-4-reasoning and Phi-4-reasoning-plus didn’t present the same issues. However, the authors did make significant choices during reinforcement learning:\n\nResults:Overall, Phi-4-reasoning-plus and Phi-4-mini-reasoning outperform similarly sized (and larger) open-weights models on math problems. Phi-4-reasoning generally outperformed DeepSeek-R1-Distilled-70B but underperformed Alibaba QwQ 32B. All three models deliver performance that falls in the middle among proprietary models and, in domains outside math, larger models with open weights.\n\nWhy it matters:While reasoning models can outperform their non-reasoning counterparts, the best ways to train them aren’t widely known. Sharing recipes and lessons learned enables others to further iterate and improve the recipes, ultimately increasing model performance even more.\n\nAn open-source code generator performs comparably to the reasoning models DeepSeek-R1 and OpenAI o1 with a much smaller model.\n\nWhat’s new:A team at the model platform Together.AI and Agentica, an open-source project devoted to reinforcement learning (RL), releasedDeepCoder-14B-Preview. The release includesweights, code, dataset, training logs, and data optimizationsunder an MITlicensethat allows noncommercial and commercial uses.\n\nHow it works:The team fine-tuned DeepSeek-R1-Distilled-Qwen-14B, which distills knowledge from DeepSeek-R1 (671 billion parameters) into Qwen-14B (14 billion parameters).\n\nResults:DeepCoder-14B-Preview is competitive on several coding benchmarks with DeepSeek-R1 as well as proprietary models including OpenAI o3-mini and OpenAI o1, which is believed to be much larger.\n\nWhy it matters:Applying reinforcement learning to coding works, but it has two big issues: (i) Training examples of verifiable code are relatively scarce and (ii) computing reward signals for code is time-consuming, since it requires evaluating many test cases. DeepCoder-14B-Preview’s optimizations reduced this complexity, shrinking RL training from months to weeks. Those optimizations are built intoVerl-pipeline, an open source RL library from Together.AI and Agentica, giving developers a powerful tool for model training.\n\nWe’re thinking:Kudos to the DeepCoder team for open sourcing their reasoning recipe! A handful of companies have developed the know-how to execute RL well, but many teams still have trouble implementing successfully. Open recipes for RL training methods and data curation techniques are important to move the field forward.\n\nThe European Union made an abrupt U-turn away from its stringent AI regulations. Meta promptly adjusted to the loosening restrictions.\n\nWhat’s new:Henna Virkkunen, the EU’s head of digital policy, said the organization wouldeaserules and requirements to support Europe’s competitiveness in AI.\n\nHow it works:Adopted last year, the EU’sAI Actprovides a comprehensive framework for regulating AI that aims to reduce purported risks by banning certain applications, restricting others, and requiring extensive documentation of development efforts. The law is set to take effect in August, empowering various regulatory bodies to formulate detailed rules. However, in recent months, the EU has faced increasing pressure from the U.S. government and large AI companies to reduce the regulatory burden.\n\nBehind the news:In drafting the AI Act, the EU aspired to a comprehensive, specific set of regulations. However, not all European lawmakers agreed that rules were needed. Virkkunen’s supporters noted that existing laws already allowed consumers to file claims against AI companies. Meanwhile, some policymakers havebecome less worriedabout AI than they were during the early drafting of the AI Act.\n\nWhy it matters:It’s unlikely that all nations – or evenstateswithin nations – will ever agree fully on rules and regulations that govern AI companies that do business within their borders, or protections from flaws such as model bias. But AI companies including Meta,OpenAI, andothersargue that a more uniform regulatory environment will make it easier to serve users worldwide.\n\nWe’re thinking:The EU overreached with the AI Act. Fortunately, the legislation provides enough flexibility to pull back. Clearer rules will help European teams innovate and European and international companies better serve EU citizens.\n\nImproving a large language model’s factual accuracy typically requires making it bigger, which in turn, involves more computation. Researchers devised an architecture that enables models to recall relevant details without significantly increasing the amount of computation required.\n\nWhat’s new:Vincent-Pierre Berges, Barlas Oğuz, and colleagues at Meta augmented transformers with trainablememory layersthat efficiently store and retrieve information related to a prompt. Thetraining codeis available under a CC BY-NClicense, which permits noncommercial uses.\n\nMemory layer basics:Memory layers wereintroducedin 2015 and wereapplied to transformersa few years later. They compute vectors, which may capture details like names or dates that were learned through training, and retrieve them according to a given input. Computing the output of a memory layer is similar to computing that of a self-attention layer. Both describe vectors that represent queries, keys, and values, and both compute the similarity between queries and keys and then weight the values by that similarity. However, while a self-attention layer computes queries, keys, and values from linear transformations of the input, a memory layer (which computes queries the same way) learns keys and a corresponding value for each key through training.\n\nKey insight:Memory layers can be scaled to millions of keys, but computing the similarity between a query and so many keys is computationally expensive. One solution is to represent each key as a combination of two half-keys drawn from two much smaller sets. For example, two sets of 1,000 half-keys each can represent 1 million possible keys. Comparing a query to these smaller sets is much more efficient, making it practical to scale up memory layers dramatically.\n\nHow it works:The authors pretrained Llama-style models of several sizes (from 134 million to 8 billion parameters) on data similar to Llama 2’s and Llama 3’s pretraining datasets. They replaced the fully connected layers with memory layers in three transformer blocks. These layers shared parameters and held up to 16 million values (an extra 64 billion parameters total). The memory layers performed these steps:\n\nResults:The authors compared a model (8 billion parameters) with memory layers to a similar model without memory layers, both trained on 1 trillion tokens.\n\nWhy it matters:Memory layers didn’t catch on in the early days of large language models (LLMs), but they can improve the output of today’s much bigger models. LLMs outfitted with memory layers require less data and computation for pretraining than conventional models to achieve the same result, at least with respect to answering factual questions.\n\nWe’re thinking:While retrieval-augmented generation can help LLMs deliver more-factual output by retrieving facts from a database, the authors add trainable parameters for this purpose.\n\nBuild AI applications that access tools, data, and prompt templates using Model Context Protocol (MCP), an open standard developed by Anthropic. In “MCP: Build Rich-Context AI Apps with Anthropic,” you’ll build and deploy an MCP server, make an MCP-compatible chatbot, and connect applications to multiple third-party servers.Sign up now",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--88-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/The-Batch-ads-and-exclusive-banners---2025-05-13T182227.067.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--89-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--90-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--63--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--91-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/The-Batch-ads-and-exclusive-banners---2025-05-13T182128.677.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-257/",
    "title": "issue 257",
    "date": "",
    "reading_time": "",
    "content": "I continue to be alarmed at the progress of proposed California regulation SB 1047 and the attack it represents on open source and more broadly on AI innovation. As Iwrotepreviously, this proposed law makes a fundamental mistake of regulating AI technology instead of AI applications, and thus would fail to make AI meaningfully safer. I’d like to explain why the specific mechanisms of SB 1047 are so pernicious to open source.\n\nTo be clear, there are routes that regulators should pursue to improve safety. For example, I would welcome outlawing nonconsensual deepfake pornography, standardizing watermarking and fingerprinting to identify generated content, and investing more in red teaming and other safety research. Unfortunately, the proposed bill pursues a less beneficial and more harmful path.\n\nSB 1047’s purported goal is to ensure safety of AI models. It puts in place complex reporting requirements for developers who fine-tune models or develop models that cost more than $100 million to train. It is a vague, ambiguous law that imposes significant penalties for violations, creating a huge gray zone in which developers can’t be sure how to avoid breaking the law. This will paralyze many teams.\n\nYou can read the latest draft of the lawhere. I’ve read through it carefully, and I find it ambiguous and very hard to follow.\n\nDevelopers who try to navigate the law’s complex requirements face what feels like a huge personal risk. It requires that developers submit a certification of compliance with the requirements of the law. But when the requirements are complex, hard to understand, and can even shift according to the whims of an unelected body (more on this below), how do we ensure we are in compliance?\n\nFor example, the certification must include many different sections. One is an analysis of “the nature and magnitude of critical harms … the model might reasonably cause or enable.” But given that even leading AI researchers aren’t sure what harms models might cause or enable, how is a team of developers supposed to figure this out and declare — under penalty of perjury — that they meet this requirement?\n\nFurther, some developers will be required to implement “protections to prevent … misuse of, or unsafe post-training modifications of, the covered model and all covered model derivatives … that are appropriate in light of the risks associated with the covered model, including from advanced persistent threats or other sophisticated actors.” Even leading AI researchers don’t agree on how best to “protect” AI models against these supposed risks, or what would be “appropriate.” So how are developers supposed to figure out how to comply with this requirement?\n\nThis creates a scary situation for developers. Committing perjury could lead to fines and even jail time. Some developers will have to hire expensive lawyers or consultants to advise them on how to comply with these requirements. (I am not a lawyer and am not giving legal advice, but one way to try to avoid perjury is to show that you are relying on expert advice, to demonstrate that you had no intent to lie.) Others will simply refrain from releasing cutting-edge AI products.\n\nIf this law passes, the fear of a trial by a jury — leading to a verdict that can be very unpredictable with significant penalties in the event of a conviction — will be very real. What if someone releases a model today after taking what they genuinely felt were reasonable safeguards, but a few years later, when views on AI technology might have shifted, some aggressive prosecutor manages to convince a jury that whatever they did was not, in hindsight, “reasonable”? Reasonableness is ambiguous and its legal interpretation can depend on case law, jury instructions, and common facts, among other things. This makes it very hard to ensure that what a developer does today will be deemed reasonable by a future jury. (For more on this, see Context Fund’sanalysisof SB 1047.)\n\nOne highly placed lawyer in the California government who studied this law carefully told me they found it hard to understand. I invite you to read it and judge for yourself — if you find the requirements clear, you might have a brilliant future as a lawyer!\n\nAdding to the ambiguity, the bill would create a Frontier Model Division (FMD) with a five-person board that has the power to dictate standards to developers. This small board would be a great target for lobbying and regulatory capture. (Bill Gurley has a greatvideoon regulatory capture.) The unelected FMD can levy fees on developers to cover its costs. It can arbitrarily change the computation threshold at which fine-tuning a model becomes subject to its oversight. This can lead to even small teams being required to hire an auditor to check for compliance with an ambiguous safety standard.\n\nThese provisions don’t ensure that AI is safe. They create regulatory uncertainty, and more opportunities for vested interests wishing to stifle open-source to lobby for shifts in the requirements that raise the cost of compliance. This would lock out many teams that don’t have a revenue stream — specifically, many open-source contributors — that would let them pay for lobbyists, auditors, and lawyers to help ensure they comply with these ambiguous and unreasonable requirements.\n\nOpen source is a wonderful force that is bringing knowledge and tools to many people, and is a key pillar of AI innovation. I am dismayed at the concerted attacks on it. Make no mistake, there is a fight in California right now for the future health of open source. I am committed to doing what I can to preserve open source, but I don’t assume that the pro-open source side will prevail. I hope you will join me in speaking out against SB 1047 and other laws that threaten to stifle open source.\n\nKeep learning!\n\nAndrew\n\nIn our new course “Prompt Compression and Query Optimization,” you’ll learn how to use MongoDB’s features to build efficient retrieval augmented generation (RAG) systems and address challenges to scaling, performance, and security.Enroll for free\n\nClaude 3.5 Sonnet lets users work on generated outputs as though they were independent files — a step forward in large language model user interfaces.\n\nWhat’s new:AnthropicintroducedArtifacts, a feature that displays outputs in a separate window of Claude 1.5 Sonnet’s web interface, outside the stream of conversation that creates and modifies them. Artifacts canincludedocuments, code snippets, HTML pages, vector graphics, or visualizations built using JavaScript.\n\nHow it works:Users can enable artifacts from the “feature preview” dropdown in their profile menu at Claude.ai. Then, asked to generate an output that’s likely to act as standalone content and undergo further work, Claude opens an artifact window next to the chat frame, populates it with an initial output, and further updates it according to subsequent prompts.\n\nWhy it matters:Artifacts make working with a large language model more fluidly interactive. Large language models (LLMs) have long been able to generate code but, outside of AI-assisted development environments like GitHub with Copilot, executing generated code typically requires further steps such as copy-pasting the code into a development environment. The additional steps add friction for developers and confusion for non-developers. Keeping and running the code in a separate window makes for a convenient, low-friction experience. Likewise when generating images and other kinds of visual output.\n\nWe’re thinking:It’s rare when a user interface update makes a tool more useful for casual and hardcore users alike. It’s even more exciting to see it happen to an LLM!\n\nThe boom in AI is jeopardizing big tech’s efforts to reach its targets for emissions of greenhouse gasses.\n\nWhat’s new:Google’sannual environmental reportshows that the company’s total carbon dioxide emissions rose nearly 50 percent between 2019 and 2023 to 14.3 million tons. Google attributes the rise to its efforts to satisfy rising demand for AI.\n\nHow it works:Google’s carbon emissions increased 16.7 percent from 2021 to 2022 and another 13.5 percent from 2022 to 2023 for a total 48 percent rise over those periods. “As we further integrate AI into our products, reducing emissions may be challenging due to increasing energy demands from the greater intensity of AI compute, and the emissions associated with the expected increases in our technical infrastructure investment,” the report states.\n\nCountering the trend:Google is working to reduce its greenhouse gas emissions on several fronts. Its effort to purchase electricity from low-emissions sources cut its net carbon footprint by around 30 percent in 2023. It claims that its owned-and-operated data centers are 1.8 times more energy-efficient than a typical enterprise data center, and its sixth-generation tensor processing units (TPUs) are 67 percent more efficient than the prior generation. Google has asked its largest hardware partners to match 100 percent of their energy consumption with renewable energy 2029. The company is pursuing several AI-based initiatives to mitigate climate change from weather prediction to fuel-efficient vehicle routing. It says that AI has the potential to mitigate 5 to 10 percent of global greenhouse gas emissions by 2030.\n\nBehind the news:In 2020, after five years of successfullyreducingits carbon footprint, Google set an ambitious target to reach net-zero greenhouse gas emissions by 2030. But its total emissions since then have risen each year. Google’s experience mirrors that of Amazon and Microsoft, which aim to reach net-zero carbon emissions by 2030 and 2040 respectively. Amazon’s emissionsincreased39 percent from 2019 to 2022, while Microsoft’s emissionsrose29 percent between 2020 and 2023. (Amazon’s and Microsoft’s cloud computing revenues were roughly triple Google’s in 2023 and thus their AI-related greenhouse case emissions  presumably were larger.)\n\nWhy it matters:Growing use of AI means greater consumption of energy. The tech giants’ ambitious emissions goals predate the rapid growth of generative AI, and their latest reports show that it’s time to rethink them. This adds urgency to already critical efforts to develop renewable and other low-emissions energy sources.\n\nWe’re thinking:We applaud Google’s efforts to cut its carbon emissions and its transparency in issuing annual environmental reports. We’re somewhat relieved to note that, for now, data centers and cloud computing are responsible for1 percentof the world’s energy-related greenhouse gas emissions; a drop in the bucket compared to transportation, construction, or agriculture. Moreover, we believe that AI stands to create huge benefits relative to the climate impact of its emissions, and AI is one of the most powerful tools we have to develop low-carbon energy sources and boost energy efficiency throughout society. Continuing to improve the technology will help us develop lower-carbon energy sources and efficient ways to harness them.\n\nAmazon hired most of the staff of agentic-AI specialist Adept AI in a move that echoes Microsoft’s absorption of Inflection in March.\n\nWhat’s new:Amazon onboarded most of the leadership and staff of Adept AI, which has been training models to operate software applications running on local hardware,GeekWirereported. Amazon licensed Adept’s models, datasets, and other technology non-exclusively. The companies did not disclose the financial terms of the deal. (Disclosure: Andrew Ng serves on Amazon’s board of directors.)\n\nHow it works:Amazon hired two thirds of Adept’s former employees. Those who remain will “focus entirely on solutions that enable agentic AI” based on proprietary models, custom infrastructure, and other technology.\n\nBehind the news:Amazon’s agreement with Adept is one of several moves to compete in AI for both businesses and consumers. In March, the company completed a $4 billioninvestmentin Anthropic in exchange for a minority share in the startup. It’s reportedly developing new models andoverhaulingits longstanding Alexa voice assistant.\n\nWhy it matters:Luan and his team say they’re aiming to automate corporate software workflows, a potentially valuable and lucrative market. Although Amazon Web Services’ Bedrock platform already enables users tobuildAI agents, Adept’s talent may bring expanded agentic and interactive capabilities.We’re thinking:AI agentic capabilities areblossoming, and Adept’s work is a notable example.\n\nLow-rank adaptation (LoRA) reduces memory requirements when fine-tuning large language models, but it isn’t as conducive to pretraining. Researchers devised a method that achieves similar memory savings but works well for both fine-tuning and pretraining.\n\nWhat’s new:Jiawei Zhao and colleagues at California Institute of Technology, Meta, University of Texas at Austin, and Carnegie Mellon proposedGradient Low-Rank Projection(GaLore), an optimizer modification that saves memory during training by reducing the sizes of optimizer states. They used this approach to pretrain a 7B parameter transformer using a consumer-grade Nvidia RTX 4090 GPU.\n\nKey insight:LoRAsaves memory during training by learning to approximate a change in the weight matrix of each layer in a neural network using the product of two smaller matrices. This approximation results in good performance when fine-tuning (though not quite as good as fine-tuning all weights) but worse performance when pretraining from a random initialization. The authors proved theoretically that updating weights according to an approximate gradient matrix — which reduces the memory required to store optimizer states — can yield the same performance as using the exact gradient matrix (at least for deep neural networks with ReLU activation functions and classification loss functions). Updating weights only once using an approximate gradient matrix is insufficient. However, updating weights repeatedly using gradient approximations that change with each training step (because the inputs change between training steps) achieves an effect similar to training weights in the usual way.\n\nHow it works:GaLore approximates a network’s gradient matrix divided into layer-wise matrices. Given a layer’s gradient matrix G (size m x n), GaLore computes a smaller matrix P (size r x m). It uses PG, a smaller approximation of the gradient matrix (size r x n), to update optimizer states. To further save memory, it updates layers one at a time instead of all at once, followingLOMO.\n\nResults:The authors tested GaLore in both pretraining and fine-tuning scenarios.\n\nWhy it matters:LoRA’s ability to fine-tune large models using far less memory makes it a very popular fine-tuning method. GaLore is a theoretically motivated approach to memory-efficient training that’s good for both pretraining and fine-tuning.\n\nWe're thinking:LoRA-style approximation has been unlocking data- and memory-efficient approaches inavarietyof machine learning situations — an exciting trend as models grow and demand for compute resources intensifies.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--69-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/07/V2_DeepLearning_MongoDB_Banner_2070x1080--1---2-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-10T143512.450.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--70-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--71-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-10T143728.158.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-126/",
    "title": "issue 126",
    "date": "",
    "reading_time": "",
    "content": "One rule I try to live by is to not surprise my collaborators. During a project, for example, a deadline may slip, or a customer may drop out. If I can foresee such risks, l let my collaborators know about major things that could go wrong so they’re not surprised if something does. And if something unforeseen happens, I ask myself, “Would my collaborators be surprised if they were to hear this news from someone else?” If the answer is yes, I try to reach out quickly to let them know myself.I find this rule useful when thinking about AI systems, too. Is there a chance that what this system does will surprise my collaborators, partners, or customers? No one likes unpleasant surprises, and asking this question might help you decide when to proactively reach out to set clearer expectations about what your system might do.Over the years, unfortunately, the AI community collectively has delivered a lot of unpleasant surprises. For example, I’ve seen users of AI systems surprised when a system that achieved 99% accuracy on a test set didn’t perform well on a business application. This may be becauseconcept drift or data driftled to degradation, or because the test metric (such as average accuracy) did not match the business’ need (which might be accurate recall even on rare classes). I’ve also seen users surprised by:\n\nSome of these surprises occurred because AI is still evolving and practitioners themselves are still learning — for example, I think most folks working in self-driving were well-meaning but honestly underestimated how long it would take to make the technology work. By now, our community has seen enough AI use cases that we should be getting better at minimizing surprises by identifying potential issues and communicating in advance.Today many people don’t trust tech. There are many reasons for this; among them, some systems aren’t well built and should not be trusted. But one of the keys to building trust in human relationships is to avoid major unpleasant surprises. If we can at least avoid surprising our collaborators, this would reduce one unnecessary source of distrust.So ask yourself: Might anything about your current project — its cost, performance, or other characteristics — be a big surprise to your associates? If so, consider reaching out to let them know right now.\n\nKeep learning!\n\nAndrew\n\nMachines are doing light janitorial work in the uncontrolled environment of Google’s offices.What’s new:Everyday Robots, a new spin-out from Google’s experimental X Development division,unleashed100 robots to perform an array of cleanup tasks. Since learning a few years ago to sort garbage for recycling, compost, and landfill, the machines have learned to open doors, straighten chairs, and squeegee tabletops (as in the video above).How it works:The robot rolls on four wheels guided by lidar. Its head contains five cameras and other sensors whose output helps direct an articulated arm tipped with a gripping claw. Google implies that the control system uses a single base model and changes output layers for different tasks. It’s trained via imitation learning followed by rounds of reinforcement learning in conventional andfederated learning(also called collaborative learning) settings.\n\nBehind the news:Mechanical helpers are beginning to grasp basic custodial chores.\n\nWhy it matters:In many countries, older people outnumber younger ones who could take care of them. Offices aren’t as complex as homes, with their clutter, tight spaces, and multi-story floor plans, but they are a proving ground for robots that might tidy up for people who aren’t able to do it themselves.We’re thinking:We celebrate progress in robotics. At the same time, we empathize with people whose jobs are be threatened. Even as we build these wonderful contraptions, it’s important to provide workers with retraining, re-skilling, and safety nets to make sure no one is left behind.\n\nMost Americans don’t understand AI, according to a new survey.What’s new:Only 16 percent of adults in the United States got a passing grade on a true-or-falsequestionnaireof AI’s capabilities and uses. The survey was created by the Allen Institute for Artificial Intelligence and administered by Echelon Insights.Conceptions and misconceptions:The study queried 1,547 participants. Correct answers to 12 of the 20 questions earned a passing grade.\n\nBehind the news:Despite the United States’ dominant role in AI research and products, the U.S. lags other nations in teaching tech literacy.\n\nWhy it matters:While U.S. adults misunderstand AI, most of them apparently recognize the high stakes involved. A differentstudyfound that 57 percent of Americans believe that AI has potential to cause harm due to misuse in the next decade. It will take informed citizens to ensure that AI benefits people broadly worldwide.We’re thinking:To be fair, even Andrew wasn’t sure of the answers to some of the questions. Can AI analyze chest X-rays with equal or better accuracy than a radiologist? Andrew contributed toresearchdescribing a system that performed comparably to humans on this task, so perhaps the correct answer is “true.” But even AI systems that beat humans in a research lab often lack the robustness to beat humans outside the lab, so maybe it’s “false.” That left him inclined toward “don’t know.”\n\nThis month, we’re celebrating our global deep learner community. Read their stories, get inspired to take the next step in your AI journey, and #BeADeepLearner!Read more\n\nNothing is certain except death and taxes, the saying goes — but how to make taxes fair and beneficial remains an open question. New research aims to answer it.What’s new:Stephan Zheng and colleagues at Salesforce built a tax planning model calledAI Economist. It observes reinforcement learning agents in an economic simulation and sets tax rates that promote their general welfare.Key insight:Economic simulations often use pre-programmed agents to keep the computation manageable, but hard-coding makes it difficult to study the impact of tax rates on agent behavior. A reinforcement learning (RL) system that accommodates different types of agents can enable worker agents to optimize their own outcomes in response to tax rates, while a policy-maker agent adjusts tax rates in response to the workers’ actions. This dual optimization setup can find a balanced optimum between the interests of individual workers and the policy maker.How it works:Four workers inhabited a two-dimensional map, 25 squares per side. One episode spanned 10 tax periods, each lasting 100 time steps. The policy maker changed tax rates after each period. Workers sought high income and low labor individually, while the policy maker pursued social welfare, the product of the average difference in incomes and the sum of all incomes.\n\nResults:The authors observed several realistic phenomena. Workers specialized: Skilled builders constructed houses while others gathered materials. There was a tradeoff between productivity and quality; that is, more-productive builders produced houses of lower quality. And workers developed strategies to game the system by, say, delaying a house sale to a later period when the tax rate might be lower. When it came to promoting general welfare — measured as the product of income equality and productivity — AI Economistachieved1,664, outperforming three benchmarks: a widely studied tax framework called theSaez formula(1,435), the U.S. Federal Income Tax schedule (1,261), and no taxes (1,278). Its policy also outperformed those baselines when human playersstood infor the RL workers.Why it matters:Reinforcement learning with heterogeneous agents can automate the modeling of incentives in interactions between different parties such as teachers and students, employers and employees, or police and criminals.We’re thinking:Simulations of this nature make many assumptions about incentives, rate of learning, cost of various actions, and so on. They offer a powerful way to model and make decisions, but validating their conclusions is a key step in mapping them to the real world.\n\nThe U.S. Department of Defense issued new ethical guidelines for contractors who develop its AI systems.What’s new:The Pentagon’s Defense Innovation Unit, which issues contracts for AI and other high-tech systems, issuedguidelinesthat contractors must follow to ensure that their systems work as planned without harmful side effects.How it works:The authors organized the guidelines around AI system planning, development, and deployment. Throughout each phase, questions arranged in a flowchart prompt contractors to satisfy the Defense Department’sethical principles for AIbefore moving on to the next stage.\n\nBehind the News:The Pentagon adopted its ethical principles for AI in February 2020 after 15 months of consultation with experts in industry, academia, and government. The document, which applies to service members, leaders, and contractors, broadly defines ethical AI as responsible, transparent, reliable, governable, and deployed with minimal bias.Why It Matters:The Department of Defense (DOD) invests generously in AI. Oneestimateprojects that military spending on machine learning contracts will reach $2.8 billion by 2023. But the department has had difficulty collaborating with big tech: In 2018, over 4,000 Google employees protested the company’s involvement in a DOD program called Project Maven, highlighting qualms among many AI professionals about military uses of their work. DOD’s new emphasis on ethics may portend a smoother relationship ahead between big tech and the military.We’re thinking:The document doesn't mention fully autonomous weapons, but they lurk in the background of any discussion of military AI. While we acknowledge the right of nations to defend themselves, we support the United Nations’ proposal to ban such systems.\n\nWhere labeled training data is scarce, an algorithm can learn to request labels for key examples. While this practice, known as active learning, can supply labeled examples that improve performance in some tasks, it fails in others. A new study sheds light on why.What's new:Siddharth Karamcheti and colleagues at Stanford Universityshowedthat examples of a certain kind hinder active learning in visual question answering (VQA), where a model answers questions about images.Key insight:Most active learning methods aim to label examples that a model is least certain about. This approach assumes that providing labels that resolve the model’s uncertainty will improve performance faster than providing labels that confirm its certainty. However, some examples that prompt uncertainty are also difficult to learn, and the uncertainty doesn’t dissipate with additional learning. For instance, in VQA, some questions about an image may refer to information that’s absent from the image itself; consider a photo of a car and the question, “What is the symbol on the hood often associated with?” If an active learning algorithm were choose many such examples, the additional labels would contribute little to learning. For active learning to work, it needs to choose examples the model can learn from. Thus, removing hard-to-learn examples prior to active learning should improve the results.How it works:The authors trained several VQA models on a variety of datasets. They fine-tuned the models usingfivediverseactive-learningstrategiesand compared their impact to labeling examples at random.\n\nResults:Culling the most difficult-to-learn training examples (those that elicited the lowest product of confidence and variability) enabled all five active learning strategies to train VQA models using fewer examples. For instance, the authors used the active learning strategy called least-confidence, which labels additional examples in which the model is least confident in its classification, to fine-tune aBottom-Up Top-Down Attentionmodel on theVQA-2dataset. It achieved 50 percent accuracy with 120,000 labeled examples — no better than labeling at random. The authors removed 10 percent of the most difficult-to-learn examples and achieved the same accuracy with 100,000 labeled examples. After removing 25 percent, it achieved the same accuracy with 70,000 labeled examples. After removing 50 percent, it took only 50,000 labeled examples (while labeling additional examples at random required 70,000 labeled examples).Why it matters:VQA is data-hungry, while active learning is sample-efficient. They make a handy combo — when they work well together. This study identifies one problem with the pairing and how to solve it.We're thinking:The authors focused on how difficult-to-learn examples affect active learning in VQA, but the same issue may hinder active learning in other tasks. We hope that further studies will shed more light.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202022-01-05%20at%2010.38.22%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202022-01-05%20at%2010.38.22%20AM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-12-03T184229.220.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-12-03T184229.220.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-12-14T094136.532.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-12-14T094136.532.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Thumbnail%201%20Logo.png?upscale=true&width=1200&upscale=true&name=Thumbnail%201%20Logo.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ECONOMISTv2.gif?upscale=true&width=1200&upscale=true&name=ECONOMISTv2.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/MILETHICS.gif?upscale=true&width=1200&upscale=true&name=MILETHICS.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Radioactive%20ASPECT.png?upscale=true&width=1152&upscale=true&name=Radioactive%20ASPECT.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-16/",
    "title": "issue 16",
    "date": "",
    "reading_time": "",
    "content": "Recently I wrote about major reasons why AI projects fail, such assmall data,robustness, andchange management. Given that some AI systems don't work, users and customers sometimes rightly wonder whether they should trust an AI system.\n\nHow can we persuade people to trust an algorithm? Some important techniques are:\n\nTrust isn’t just about convincing others that our solution works. I use techniques like these because I find it at least as important to convince myself that a solution works, before I ask a customer to rely on it.\n\nKeep learning!Andrew\n\nSolar-thermal power plants concentrate the sun’s energy using huge arrays of mirrors. AI is helping those arrays stay in focus.What happened:Heliogen, a solar-thermal startup, developed a computer vision setup that tracks hundreds of mirrors at once. The system detects reflectors that go off kilter and adjusts them to concentrate sunlight. The system recently heated a boiler to 1,000 degrees Celsius, a temperature that allows for industrial processes. Serial entrepreneur and Heliogen founder Bill T. Gross delivers his pitch in thisvideo.How it works:A solar-thermal plant's central feature is a tower topped by a boiler. Hundreds, sometimes thousands, of mirrors encircle the tower. By focusing heat on the boiler, they produce steam, which spins a turbine, generating electricity. However, factors like wind, ground subsidence, and natural warping can cause mirrors to drift out of focus, reducing the plant’s efficiency. Heliogen's system calibrates them automatically.\n\nWhy it matters:1,000 degrees Celsius is a milestone; most solar-thermal plants reach half that temperature. But the company’s goal is 1,500 degrees. At this temperature, it’s possible to split atmospheric carbon dioxide and water into their constituent molecules of hydrogen and carbon. Heliogen aims to start by producing hydrogen to generate power via fuel cells. Ultimately, it aims to recombine hydrogen and carbon into hydrocarbon fuels — no fossils required.Yes, but:Atwo-partcritique published by the news websiteCleanTechnicapoints out that Heliogen’s technology produces a hot spot high above ground, where the heat isn’t immediately useful and is difficult to transport. Moreover, industrial facilities would need to be very nearby, potentially casting shadows over the mirrors. “I think it’s more likely that Heliogen's core machine learning innovation regarding halo focusing will find a completely different niche outside of concentrated solar,” the author concludes.We’re thinking:Heliogen has intriguing technology, a seasoned leader, and a high-profile backer in Bill Gates. It's exciting to see AI helping to make cheaper, cleaner alternatives to highly polluting industrial processes.\n\nSophisticated models trained on biased data can learn discriminatory patterns, which leads to skewed decisions. A new solution aims to prevent neural networks from making decisions based on common biases.What’s new:Ehsan Adeli and a group at Stanford proposeBias-Resilient Neural Network, or BR-Net, an architecture that works with a classifier to minimize the impact of biases that are well understood. In the training data, we can label, say, race and gender (known as bias variables), and BR-Net will learn to prevent spurious correlations between those variables and the model's output classification.Key insight:Biases in data correlate with class labels. If one part of a network learns to predict this correlation, another can learn to minimize the predicted correlation. This adversarial scheme can mitigate bias.How it works:BR-Net comprises three neural networks. The feature extractor finds embeddings of input data. The classifier predicts class labels from the embeddings. The bias predictor predicts the correlation between embeddings and bias variables. Once labels for bias variables have been added to the data, training proceeds in three steps:\n\nResults:The researchers used a VGG16 classifier with BR-Net to predict a person’s gender from a photo. They trained the model on the GS-PPB dataset. Because classifiers often perform poorly on darker faces, they labeled skin tone as a bias variable. BR-Net achieved 96.1 percent balanced accuracy (accuracy for each of six skin tones considered equally), an improvement of 2 percent. This indicates more consistent results across different skin colors than a VGG16 trained without BR-Net.Why it matters:Bias in AI is insidious and difficult to prevent. BR-Net offers a solution when sources of bias are known.We're thinking:Machine learning presents hard questions to society: Which biases should we avoid? How can we come to agreement about which to avoid? Who gets to decide in the end? In lieu of answers, the choices are in the hands of ML engineers.\n\nImage analysis guided by AI revealed a 2,000-year-old picture dug into the Peruvian desert.What happened:Researchers analyzing aerial imagery shot over Perufounda pattern that looks like a three-horned humanoid holding a staff. The figure is roughly 16 feet across and may have served as a waypoint along an ancient path. Known as geoglyphs, such pictures were created by people who predated the arrival of Columbus by 1500 years. The sprawling patterns are visible only from higher elevations.How it works:Using manual methods, researchers at Yamagata University found more than 100 geoglyphs in satellite photos and other imagery from the region of southeastern Peru called the Nazca Pampa. But they had collected too much data from surrounding areas to search manually. So they teamed with IBM Japan to feed the data into PAIRS Geoscope, a cloud-based deep learning system that analyzes geospatial data. Thisvideodescribes the project.\n\nBehind the news:The people who created the Nazca geoglyphs lived on the arid Peruvian plains, or pampas. They made these shapes by removing the top layer of pebbles to expose lighter-colored clay roughly six inches below. Conquistadors noted the geoglyphs in their travelogues as far back as the 1500s, but it wasn’t until the 1940s that researchers began studying their origin and purpose.Why it matters:Remote sensing techniques have spurred arenaissancein archaeology. They’ve helped uncover Mayan pyramids on Mexico’s Yucatan peninsula and abandoned cities in the Cambodian jungle.We’re thinking:Who wants to team with us to create a massivedeeplearning.aigeoglyph to confuse and amuse future generations?\n\nDiscover the impact of the learning rate on parameter optimization in Course 2 of the Deep Learning Specialization.Enroll now\n\nGoogle's AI platform offers a view into the mind of its machines.What’s new:Explainable AI(xAI) tools show which features exerted the most influence on a model’s decision, so users can evaluate model performance and potentially mitigate biased results.How it works:xAI is available to users of Google'sCloud AIplatform and itsAutoMLmodels and APIs. The core of xAI is a pair of tools that provide graphs or heat maps (depending on the type of data) that show the relative importance of each attribute in a model’s prediction.\n\nWhy it matters:The ability to explain how AI models arrive at decisions is becoming a major issue as the technology reaches into high-stakes aspects of life like medicine, finance, and transportation. For instance, self-driving cars would fit more easily into existing regulatory and insurance regimes if they could explain their actions in case of an accident.Yes, but:Explainability techniques are not a silver bullet when it comes to mitigating bias.\n\nWe’re thinking:As machine learning engineers, we need ways to make sure our models are making good decisions. But we should also keep in mind that explainability has limits. After all, human decisions aren’t always explainable either.\n\nModels that summarize documents and answer questions work pretty well with limited source material, but they can slip into incoherence when they draw from a sizeable corpus. Recent work by Facebook AI Research and Université de Lorraine’s computer science research lab addresses this problem.What’s new:Angela Fan and collaborators developed amodelfor multi-document summarization and question answering. While most previous efforts combine all input documents into one, the authors improved  the state of t he art by representing them in a more compact form.Key insight:The combined length of major source documents pertaining to a given topic overwhelms current language models' ability to extract meaning. A knowledge graph squeezes out irrelevant and redundant information, enabling models to work more effectively.How it works:The authors’ method involves three steps: constructing a knowledge graph from source documents, encoding the graph as a sequence of words, and extracting information from the sequence.\n\nResults:The authors tested their model on a question answering task based on the dataset called Explain Like I'm Five (ELI5).This dataset contains 270,000 question-answer pairs along with source documents (the top 100 web sources from the CommonCrawl corpus for each question). The graph approach edged out the earlier state of the art on F1 for ROUGE-1 (30 percent versus 28.9 percent). They also compared performance on the WikiSum dataset for multi-document summarization using an article’s title as the input query, the footnotes as source documents, and the first paragraph as the target summary. The graph approach underperformed the previous ROUGE-L state of the art 36.5 percent to 38.8 percent, but the comparison wasn't apples-to-apples. The previous research supplemented the corpus with a web search, while the new work used only CommonCrawl.Why it matters:This research shows that natural language generation based on very large bodies of input text can work well. It also shows that source documents don’t need to be composed of well formed sentences. New ways of representing source documents may well lead to better language generation.We’re thinking:Many search engines produce summaries or answer questions by choosing the most relevant document. The ability to draw on any number of documents could enable such models to deliver a far wider diversity of information, leading to better research tools and ultimately a better-informed public.\n\nIs there any reason to continue playing games that AI has mastered? Ask the former champions who have been toppled by machines.What happened:In 2016, International Go master Lee Sedol famously lost three out of four matches to DeepMind’s AlphaGo model. The 36-year-old announced his retirement from competition on November 27. “Even if I become the number one, there is an entity that cannot be defeated,” hetoldSouth Korean's Yonhap News Agency,Stages of grief:Prior to the tournament, Lee predicted that he would defeat AlphaGo easily. But the model’s inexplicable — and indefatigable — playing style pushed him into fits ofshock and disbelief. Afterward, heapologizedfor his failure to the South Korean public.Reaching acceptance:Garry Kasparov, the former world-champion chess player, went through his own cycle of grief after being defeated by IBM’s DeepBlue in 1997. Although he didn’t retire, Kasparov did accuse IBM’s engineers of cheating. He later retracted the charge, and in 2017 wrote a bookarguingthat, if humans can overcome their feelings of being threatened by AI, they can learn from it. The book advocates an augmented intelligence in which humans and machines work together to solve problems.The human element:Although AlphaGo won in the 2016 duel, its human opponent still managed to shine. During the fourth match, Sedol made amoveso unconventional it defied AlphaGo’s expectation and led to his sole victory.We’re thinking:Lee wasn't defeated by a machine alone. He was beaten by a machine built by humans under the direction of AlphaGo research lead David Silver. Human mastery is obsolete only if you ignore people like Silver and his team.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/07/Andrew-Trust-the-Robot-Sweatshirt-1.png",
      "https://dl-staging-website.ghost.io/content/images/2021/07/solar-SMALL.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/Bias-Resilience-SIZED.png",
      "https://dl-staging-website.ghost.io/content/images/2021/07/Nazca-SIZED-1.png",
      "https://dl-staging-website.ghost.io/content/images/2021/07/ezgif.com-video-to-gif-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/Explainability-SIZED.png",
      "https://dl-staging-website.ghost.io/content/images/2021/07/Knowledge-Graph-SIZED-1.png",
      "https://dl-staging-website.ghost.io/content/images/2021/07/Sedol-SIZED.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-11/",
    "title": "issue 11",
    "date": "",
    "reading_time": "",
    "content": "Welcome to the Halloween edition of The Batch!\n\nI promised last week to share some common reasons for AI project failures. But first, let’s start with some of the least common reasons.\n\nIf your AI project fails, it is probably not because:\n\nA hair-raising Halloween to all of you who celebrate it, with plenty of tricks and treats.\n\nKeep learning,\n\nAndrew\n\nOn Halloween, dark fantasies dance in the flame of the jack o’lantern’s candle, and we cower before visions of AI gone wrong: Malevolent superintelligences, technologically empowered tyrants, reality twisted by computer generated images. But we need not succumb to fright. This week,The Batchhoists the jack o’lantern high to illuminate the dire possibilities. We examine the facts, consider the risks, and chart a path forward. Take heart! As daylight wanes and the wind grows cold, let us confront our deepest AI fears.\n\nCould humanity be destroyed by its own creation?The fear:If binary code running on a computer awakens into sentience, it will be able to think better than humans. It may even be able to improve its own software and hardware. A superior intelligence will see no reason to be controlled by inferior minds. It willenslave or exterminateour species.What could go wrong:Artificial intelligence already manages crucial systems in fields like finance, security, and communications. An artificial general intelligence (AGI) with access to these systems could crash markets, launch missiles, and sow chaos by blocking or faking messages.\n\nBehind the worries:Humans dominate Earth because we’re smarter than other species. It stands to reason that a superintelligent computer could, in turn, dominate us.\n\nHow scared should you be:The notion that general intelligence will emerge from machines taught to play games, monitor security cameras, or solve linguistic puzzles is pure speculation. In his 2016 bookThe Truth About AI,author Martin Ford asked prominent AI thinkers to estimate when AGI would come online. Their guesses ranged between 10 and nearly 200 years in the future — assuming it’s even possible. If you’re worried about the prospect of an AGI takeover, you have plenty of time to work on safeguards.What to do:While it would be nice to devise a computer-readable code of ethics that inoculates against a malign superintelligence, for now the danger is roguehumanswho might take advantage of AI’s already considerable abilities to do harm. International protocols that hem in bad actors, akin to nuclear nonproliferation agreements, likely would do more good for the time being.\n\nWill AI fakery erode public trust in the key social institutions?The fear:Generative models will flood media outlets with convincing but false photos, videos, ads, and news stories. The ensuing crisis of authority will lead to widespread distrust in everything from the financial system to democracy itself.What could go wrong:Betweendeepfakes of celebritiesand the GPT-2 language model’s ability to churn outfaux articlesthat convince readers they’re from theNew York Times, AI is a powerful tool for propagandists, charlatans, and saboteurs. As the technology improves, its potential for social disruption only grows.\n\nBehind the worries:Digital fakery is already on the rise in a variety of sectors.\n\nHow scared should you be:It’s hard to gauge the worry because little research has been done evaluating the impact of digital fakery on public trust. So far, deepfakes have been used mostly to harass individual women, according to onestudy. An optimist might argue that growing awareness of AI-generated disinformation will spur people to develop stronger social bonds and standards for truth-telling. We’re more inclined to imagine an arms race between fakers and systems designed to detect them. As in digital security, the fakers likely would have an edge as they find ways to breach each new defense.What to do:Researchers are considering a number ofcountermeasuresto fake media. Some proposewatermarksthat would establish an item’s provenance. Others argue thatblockchainoffers an effective way to ensure that information originated with a trusted source.\n\nWhat does freedom mean when computers know your face and track your movements?The fear:Artificial intelligence will boost the power of surveillance, effectively making privacy obsolete and opening the door to a wide range of abuses.What could go wrong:AI-driven surveillance may prove so valuable to those in power that they can’t resist using it. Employers could use it to maximize worker efficiency. Criminals could use it to blackmail victims. Politicians could use it to crush opposition, officials to oppress the poor or weak. A tyrannical government could spy on private moments and grade everything citizens do in terms of how favorable it is to Big Brother.Behind the worries:Digital surveillance has become pervasive. Some surveillance systems are alarmingly prone tofalse positives and negatives, and they readily can be subverted to servehidden agendas.\n\nHow scared should you be:If you use the internet, own a smartphone, pay with credit, or hold a job, odds are you’re being watched. Whether that’s a sign of pernicious things to come or an increasingly efficient society is an open question.What to do:The AI community can play a central role in working with lawmakers to develop rules about how data is collected and AI is used to analyze it. In June, for instance, AI expertspresentedthe European Parliament with a 48-page strategy for limiting threats to privacy without curtailing innovation.\n\nDon’t understand this meme? Take the Deep Learning Specialization!Enroll now\n\nWill biases in training data unwittingly turn AI into a tool for persecution?The fear:Bias encoded in software used by nominally objective institutions like, say, the justice or education systems will become impossible to root out. Result: injustice baked into the very institutions we count on to maintain a fair society.What could go wrong:AI learns from data to reach its own conclusions. But training datasets are often gathered from and curated by humans who have social biases. The risk that AI will reinforce existing social biases is rising as the technology increasingly governs education, employment, loan applications, legal representation, and press coverage.\n\nBehind the worries:Bias in AI is already making headlines.\n\nHow scared should you be:Until companies announce that they train their models on certified bias-free datasets as loudly as they trumpet machine-learning buzzwords, or until such systems pass a third-party audit, it’s a good bet their technology unfairly advantages some people over others.What to do:In a 2018keynote, researcher Rachel Thomas explains how machine learning engineers can guard against bias at each step of the development process. She recommends that every dataset come with a sheet describing how the set was compiled and any legal or ethical concerns that occurred to those who assembled it. She also suggests that teams include people from various backgrounds who may be alert to different sorts of bias.\n\nFrom blue collar laborers to lab coated professionals, is any job safe from AI?The fear:AI will exceed human performance at a wide range of activities. Huge populations will becomejobless. They’ll be unable to afford life’s necessities, and even government assistance won’t replace the sense of identity, pride, and direction that come with a job. Humanity will become unmoored.What could go wrong:Historically, technology created more jobs than it destroyed. What makes AI different is it threatens to outsource the one thing humans have always relied on for employment: their brains. Automated drive-through windows sell milkshakes. Healthcare models interpret x-rays. Natural language programs write sports news. The list is bound to grow longer as the technology becomes more capable.\n\nBehind the fear:Massive unemployment in the past have brought severe social disruption. The U.S. Great Depression in the 1930s saw jobless rates above 34 percent. Researchers have also linked this displacement of work to the rise of nationalism that fueled both the First and SecondWorld Wars.\n\nHow scared should you be?There’s little reason to worry in the short term. A 2017 report by McKinseyestimatedthat automation would replace fewer than 5 percent of the global workforce by 2030. That number comes with caveats, though. In some roles, for instance customer service and repetitive physical labor, one-third of all jobs could be taken by machines. Developing nations will be hit hardest, even though they may also experience explosive growth in high-touch fields such as education and healthcare.What to do:Lifelong learning is a front-line defense (and a rewarding pursuit!). Education can help you stay ahead of partial automation in your current profession or change lanes if your profession is being automated away. Networked resources like blogs, research papers, online videos, and online courses can help you absorb and develop the kinds of human insights that likely will outpace machines for some time. Beyond that, workwiththe machines, not against them, argue Andrew McAfee and Erik Brynjolfsson in their bookRace Against the Machine. Workers who don’t want to wind up on the chopping block should invest in education to keep current and find tasks that put them in a position to supervise automated systems.\n\nCould the flood of hype for artificial intelligence lead to a catastrophic collapse in funding?The fear:AI will fail to deliver on promises inflated by businesses and researchers. Investors will migrate to greener pastures, andAI Winterwill descend. Funding will dry up, research will sputter, and progress will stall.What could go wrong:Enthusiasm surrounding even modest advances in AI is driving an investment bonanza: Venture funds put $9.3 billion into AI startups in 2018, up over 70 percent from the prior year, according to a jointstudyby PricewaterhouseCoopers and CB Insights. Some critics believe that deep learning has soaked up more than its fair share of investment, draining funds from other approaches that are more likely to lead to fundamental progress. Could funders lose patience?\n\nBehind the worries:AI history is dotted with setbacks brought about by spikes in public skepticism. Two prolonged periods — one lasting for much of the 1970s, the other from the late 80s to early 90s — were dark and cold enough to have earned the name AI Winter.\n\nHow scared should you be:It’s true, AI has received enough hype to make P.T. Barnum blush. Yet the current climate shows little sign of impending winter. Earlier this year, Alphabet reported that DeepMind, its deep learning subsidiary, had cost its parent company $570 million as of 2018. Some observerswarnedthat the expense could portend an industry-wide loss in confidence. Yet technical leaders in the field say they’re well aware of deep learning’s shortcomings, and the march of new research is dedicated to surmounting them. Moreover, AI is generating significant revenue, creating a sustainable economic model for continued investment, while AI research is less reliant than ever on government and institutional funding. Established companies, startups, and research labs all have their eyes open for pitfalls and blind alleys.What to do:As AI practitioners, we should strive to present our work honestly, criticize one another fairly and openly, and promote projects that demonstrate clear value. Genuine progress in improving peoples’ lives is the best way to ensure that AI enjoys perpetual springtime.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AndrewLetter.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Intro-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-10-3020at2010.43.5220AM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Fakery-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Surveillance201-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Email-BobRossMemeAd.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Biased20Data-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Workforce20Displacement-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AI20Winter-1.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-71/",
    "title": "issue 71",
    "date": "",
    "reading_time": "",
    "content": "Every year for the past decade, I flew to Singapore or Hong Kong to celebrate mymother’s birthday with her on December 22. This year, for the first time, we did it via Zoom. Despite the distance, I was warmed that my family could gather from the U.S., Singapore, Honk Kong, and New Zealand and sing a poorly synchronized “Happy Birthday To You.”\n\nI wish I could also be on a Zoom call with each of you to personally wish you happy holidays and an even happier new year!\n\nOver the holidays, I often think through the list of important people in my life, recall what they’ve done for me or others, and quietly acknowledge my gratitude to them. This makes me feel more connected to them. Perhaps you’ll find it valuable to think about this, too, during the socially distanced holiday that many of us will have: Who are the important people in your life, and what reasons might you have to be grateful to them?\n\nWhether in-person or online, I hope you’ll find ways to nurture your most important relationships over this holiday season.\n\nKeep learning!\n\nAndrew\n\nThe past year is one for the history books by any measure. A new, highly infectious bug knocked the wheels off of life-as-usual, while social rifts threatened to eclipse our common interests. Machine learning engineers jumped into the fray, devising tools for Covid-19 diagnosis and treatment, building models to recognize hate speech and disinformation, and highlighting biases throughout the AI community. And there’s a lighter side: Work-from-home tools that exchange pajamas for a business suit, wise-cracking language models, and fascinating experiments in AI-assistedartandperformance. Please join us as we survey the year gone by in all its hardship and glory.\n\nAI accelerated the search for a coronavirus vaccine, detected Covid-19 cases, and otherwise softened the pandemic’s blow.What happened:Machine learning researchers worldwide scrambled to harness the technology against the coronavirus. Among many misfires, they racked up important successes in detection, inoculation, other areas.Driving the story:The pandemic began with high hopes for AI-driven solutions amongresearchersandofficials. But an Aprilmetastudysounded a cautious note, finding that 145 models surveyed were poorly documented, yielded overly optimistic results, and were likely to be biased. Researchers persisted, ultimately delivering vaccines in record time. Outside the lab, deep learning teams tried to keep people safer and more connected.\n\nBehind the news:AI may yet play an important role in treating Covid-19. The nonprofitCovid Moonshotproject used a semisupervised deep learning platform to filter 14,000 candidate antiviral drugs. The system validated four compounds that are expected to advance to animal trials.Where things stand:AI is no silver bullet, but the advent of this new, virulent, highly infectious strain of coronavirus has been a bracing test run of its capabilities to fight infectious diseases — and helped us live with them, too.\n\nLearn more:The Batchfeatured regular AI-Against-Covid news reports starting inApril.\n\nWhile generative adversarial networks were infiltrating cultural, social, and scientific spheres, they quietly transformed the web into a bottomless well of synthetic images of . . . well, you name it.What happened:Deepfakes showed up in mainstreamentertainment,commercials,political campaigns, and even adocumentary filmin which they were used to protect onscreen witnesses. Amid the hoopla, a groundswell of online front-ends to image generators went largely unremarked.Driving the story:Inspired by 2019’sThis Person Does Not Exist, a web app that produces realistic-looking personal portraits, engineers with a sense of humor implemented generative adversarial networks (GANs) that mimic real-world minutiae. Some of our favorites:\n\nWhere things stand:Some observers worry that AI-generated fakes couldundermine trustin public institutions by sowing confusion over what is and isn’t real. (Which is not to say GANs are required for that.) But the technology turns out to have a critically important use that outweighs any negative social consequences: Balancing pictures ofcatson the internet with pictures ofdogs.\n\nLearn more:The Batch’sGAN special issuefeatures stories about detecting deepfakes, making GANs more inclusive, and an interview with GAN inventor Ian Goodfellow. To learn how to build GANs yourself, check out theGenerative Adversarial Networks Specializationon Coursera.\n\nSome of deep learning’s bedrock datasets came under scrutiny as researchers combed them for built-in biases.What happened:Researchers found that popular datasets impartbiases against socially marginalized groupsto trained models due to the ways the datasets were compiled, labeled, and used. Their observations prompted reforms as well as deeper awareness of social bias in every facet of AI.Driving the story:Image collections were in the spotlight — including ImageNet, the foundational computer-vision dataset.\n\nBehind the news:In the wake of the PULSE fiasco, Facebook’s chief AI scientist Yann LeCun and Timnit Gebru, then head of Google’s ethical AI efforts,arguedpublicly over whether social biases in machine learning originate primarily in faulty datasets or systemic biases within the AI community. LeCun took the position that models aren’t biased until they learn from biased data, and that biased datasets can be fixed. Gebru pointed out — and we agree, as wesaidin a weekly letter — that such bias arises within a context of social disparities, and that eliminating bias from AI systems requires addressing those disparities throughout the field. Gebru and Google subsequentlypartedamid further disagreements around bias.Where things stand:The important work of ensuring that biases in datasets are documented and removed for particular tasks such asgenerating training data, has only just begun.\n\nLearn more:The Batchin the past year reported on bias mitigation techniques includingDouble-Hard DebiasandDeep Representation Learning on Long-Tailed Data.\n\nThe worldwide pandemic and a contentious U.S. election whipped up a storm of automated disinformation, and some big AI companies reaped the whirlwind.What happened:Facing rising public pressure to block inflammatory falsehoods, Facebook, Google’s YouTube division, and Twitter scrambled to update their recommendation engines. Members of the U.S. Congressgrilledthe companies, a popular Netflixdocumentaryexcoriated them, and public opinion pollsshowedthat they had lost the trust of most Americans.Driving the story:The companies addressed the issue through various algorithmic and policy fixes — though they apparently stopped short of making changes that might seriously threaten the bottom line.\n\nYes, but:The reforms may not stick. The companies have diluted some, and others have already backfired.\n\nWhere things stand:There’s no clear way to win the online cat-and-mouse game against fakers, cranks, and propagandists. But the big cats must stay ahead or lose public trust — and regulators’ forbearance.\n\nLearn more:For more details on using AI to stem the tide of disinformation and hate speech online, see our earlier stories on Facebook’s effortshereandhere, and on YouTube’shereandhere.\n\nInstitutional hurdles to AI for medicine began to fall, setting the stage for widespread clinical use of deep learning in medical devices and treatments.What happened:DeepMind’sAlphaFoldmodel determined the three-dimensional shape of a protein in just hours, stealing the spotlight with promises of new blockbuster drugs and biological insights. Behind the scenes, the medical establishment took important steps to bring such technologies into mainstream medical practice.Driving the story:Institutional shifts boosted medical AI’s profile and heralded its growing acceptance.\n\nWhere things stand:Many applications of AI in medicine require doctors and hospitals to reorganize their workflow, which has slowed adoption to some extent. Once they’ve cleared the FDA and Medicare, clinicians have a much greater incentive to make the changes needed to take full advantage of them.\n\nLearn more:OurAI For Medicine special issuefeatures stories about deep learning in diagnosis, prognosis, and treatment, along with an exclusive interview with medical-AI godfather Eric Topol. Learn how to build your own medical models in theAI For Medicine Specializationon Coursera.\n\nNeural networks for natural language processing got bigger, more prolific, and more fun to play with.What happened:Language models, which already had grown to gargantuan size, continued to swell, yielding chatbots thatmimic AI luminariesand havevery strange ideas about horses.Driving the story:OpenAI’s colossal 175 billion-parameter text generatorGPT-3showcased ongoing progress in natural language processing. It also exemplified widespread trends in machine learning: exponential rise in parameter counts, growing prevalence of unsupervised learning, and increasing generalization.\n\nWhere things stand:In language models, bigger clearly is better — but it doesn’t stop there. iGPT portends models trained on both images and words. Such models, which are in the works at OpenAI, at least, may be smarter, and weirder, than the giant language models of 2020.\n\nLearn more:OurNLP special issueincludes stories about counteracting bias in word embeddings, making conversation, and choosing the right words, plus an exclusive interview with NLP pioneer Noam Shazeer. Learn how to build your own NLP models in theNatural Language Processing Specializationon Coursera.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-12-2320at2011.59.0020AM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_Fireplace_576x324px.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_ReindeerMedicine_576x324.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_GANS-Field-of-Snowmen6.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_TreeFarmDataSet_Fenced.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_SnowballFight_576x324px-2.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_MedicalSnowman_576x324px.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_Holiday-Bookstack_576x324px.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-267/",
    "title": "issue 267",
    "date": "",
    "reading_time": "",
    "content": "Years ago, when I was working at a large tech company, I was responsible for the data warehouse. Every piece of data relating to individual users was supposed to come through the data warehouse, and it was an intellectually challenging undertaking to store the data reliably and make it available to other teams, subject to security and privacy guardrails, so they could use it to derive insights.\n\nI wish that, back then, I (and my whole team) had had access to theData Engineering Professional Certificate, a major new specialization we just launched on Coursera!\n\nData underlies all modern AI systems, and engineers who know how to build systems to store and serve it are in high demand. Today, far too many businesses struggle to build a robust data infrastructure, which leads to missed opportunities to create value with data analytics and AI. Additionally, AI’s rise is accelerating the demand for data engineers.\n\nIf you’re interested in learning these skills, please check out this four-course sequence, which is designed to make you job-ready as a data engineer.\n\nThe Data Engineering Professional Certificate is taught by Joe Reis, co-author of the best-selling bookFundamentals of Data Engineering, in collaboration with Amazon Web Services. (Disclosure: I serve on Amazon's board of directors.) When DeepLearning.AI decided to teach data engineering, I felt that Joe, who has helped many startups and big companies design their data architectures and thus has broad and deep experience in this field, would be the ideal instructor. He was the first person we reached out to, and I was thrilled that he agreed to work with us on this. I hope that you’ll be thrilled, too, taking this specialization!\n\nWhile building AI systems and analyzing data are important skills, the data that we feed into these systems determines their performance. In this specialization, you’ll go through the whole data engineering lifecycle and learn how to generate, ingest, store, transform, and serve data. You’ll learn how to make necessary tradeoffs between speed, flexibility, security, scalability, and cost.\n\nIf you’re a software engineer, this will give you a deeper understanding of data engineering so that you can build data applications. If you’re an aspiring or practicing data scientist or AI/machine learning engineer, you’ll learn skills that expand your scope to manage data in a more sophisticated way. For example, you’ll learn about DataOps to automate and monitor your data pipelines, and how to build “infrastructure as code” to programmatically define, deploy, and maintain your data infrastructure, as well as best practices for data-centric AI.\n\nYou’ll also hear 17 other industry leaders share their wisdom about effective data engineering. Bill Inmon, the father of data warehousing, shares fascinating stories about the evolution of the data warehouse, including how he wrote his first program as a student in 1965. Wes McKinney, creator of the Python pandas package (as in “import pandas as pd”), talks about how he designed this wildly popular package and shares best practices for data manipulation. These instructors will give you a mental framework for developing and deploying data systems.\n\nGetting your data infrastructure right is a valuable foundational skill that will serve you well in whatever you do with AI or data analytics. I hope youenjoy this specialization!\n\nKeep learning,\n\nAndrew\n\nLearn the principles of data engineering with our four-course professional certificate taught by Joe Reis. Develop skills throughout the data engineering lifecycle and gain hands-on experience building systems on Amazon Web Services. Earn a certificate upon course completion!Enroll today\n\nPreliminary versions of OpenAI’s new model family were trained explicitly to think step-by-step, yielding outstanding marks in math, science, and coding — but users can’t see their reasoning steps.\n\nWhat’s new:OpenAI launched beta versions ofo1-preview and o1-mini, language models that were trained via reinforcement learning to use chains of thought. The models are available to paid ChatGPT users as well as API customers who have been onboard for more than 30 days and spent $1,000. o1-preview costs $15/$60 per million input/output tokens, significantly higher than GPT-4o’s price of $5/$15. o1-mini costs $3/$12 per million input/output tokens. OpenAI didn’t announce a release date for a finished o1 model.\n\nHow it works:o1-preview is a preliminary release, and o1-mini is a faster preliminary version that’s particularly effective at coding. OpenAI published ano1 system cardbut hasn’t disclosed details about the new models’ size, architecture, or training. Both models have an input context window of 128,000 tokens. They accept only text tokens, but OpenAI plans to support other media types in future versions.\n\nResults:The actual o1 model — which remains unavailable — generallyoutperformso1-preview, while both vastly outperform GPT-4o on math, science, and coding benchmarks.\n\nBehind the news:In recent months, Anthropic has been using the tag <antThinking> to generate thinking tokens that are hidden from users. However, OpenAI’s implementation in the o1 models takes this capability much further.\n\nWhy it matters:The o1 models show that the combination of reinforcement learning and chain-of-thought reasoning can solve problems that large language models generally find challenging. They’re substantially more accurate in domains such as coding, math, and science that have low tolerance for error. However, the fact that the models hide their reasoning from users makes them less transparent and explainable than their predecessors and may make their outstanding performance less valuable in some applications.\n\nWe’re thinking:Agentic workflows can significantly improve a system’s ability to reflect, reason, and iterate on its output. Training a model to take such steps directly in response to even general-purpose questions opens an exciting alternative path to better reasoning beyond simply scaling up model size.\n\nSambaNova raised the speed limit for access to the largest model in the Llama 3.1 family — and it’s free.\n\nWhat’s new:SambaNovalauncheda cloud service that runs Llama 3.1 405B significantly faster than competitors. A free tier is available, to be followed later this year by paid tiers that offer higher rate limits.\n\nHow it works:SambaNova uses proprietarychipsand software to accelerate model inference.\n\nYes, but:SambaNova currently limits Llama 3.1’s context window to around 8,000 tokens, much less than the model’s native 128,000 tokens.\n\nBehind the news:The new service arrives amid a broader competition to deliver fast inference among cloud providers that have developed their own specialized chips. Competitors likeCerebrasandGroqhave introduced their own high-speed inference services.\n\nWhy it matters:Throughput, cost, performance, and latency are critical factors in practical applications of AI models. Fast inference allows for more frequent API calls without bogging down time to output, which is essential for agentic workflows and real-time decision making.\n\nWe’re thinking:Models with open weights are now served faster than proprietary models and are nearly as capable. This may spur further adoption of open models as well as prompting strategies, such as agentic workflows, that require large numbers of output tokens.\n\nAmazon took on talent and technology from robotics startup Covariant to enhance its warehouse automation, an area critical to its core ecommerce business.\n\nWhat’s new:Amazon announced anagreementto hire Covariant’s cofounders and other key personnel and license its models. Financial terms were not disclosed. (Disclosure: Andrew Ng is a member of Amazon’s board of directors.)\n\nHow it works:The new deal echoes Amazon’s previous not-quite acquisition of Adept as well as similar arrangements between other tech giants and startups.\n\nBehind the news:Amazon has been working to acquire technical talent and technology for some time. In 2022, it announced that it would acquire iRobot, but the companiesabandonedthat plan earlier this year after EU regulators blocked the deal citing antitrust concerns. In October, itcommittedto invest as much as $4 billion in Anthropic in return for access to the startup’s technology. (UK regulatory authorities subsequentlyannouncedan antitrust probe into Amazon’s relationship with Anthropic.) In July, itsigneda hire-and-license deal — similar to its agreement with Covariant — with agentic AI startup Adept.\n\nWhy it matters:Competition among AI giants continues to heat up. Amazon’s agreement with Covariant mirrors other deals in which a tech giant gained top talent and technology without formally acquiring a startup, including Microsoft’sarrangementwith Inflection and Google’sdealwith Character.AI. These developments highlight top tech companies’ race to secure their AI positions — and the fact that outright acquisitions invite regulatory scrutiny.\n\nWe’re thinking:Robotic foundation models that are trained on large amounts of unlabeled robotics data offer a promising way to quickly fine-tune robots to perform new tasks — potentially a major upgrade in warehouse logistics.\n\nStudies have established that large language models can memorize the text passages they’ve been trained on repeatedly and regurgitate them when prompted in adversarial and, though rarely, in benign ways. Researchers proposed a way to reduce this tendency and attendant risks to intellectual property and privacy.\n\nWhat’s new:Abhimanyu Hans and colleagues from University of Maryland introduced thegoldfish loss, a modification of the next-token-prediction loss function typically used in large language models. The goldfish loss avoids memorization of long passages by masking some tokens during the loss computation.\n\nKey insight:Certain passages may appear many times during training, either because the model takes multiple passes over data or because they’re duplicated in the training corpus. Randomly masking individual tokens from the loss computation doesn’t prevent a model from memorizing repeated passages because the model, over many repetitions, still sees every word and its place in the order. But masking a long passage the same way with every repetition ensures the model can’t memorize the passage regardless of the number of repetitions.\n\nHow it works:The goldfish loss masks the current token from the loss computation based on previous tokens.  A deterministic hashing function decides which tokens to mask effectively at random the first time it encounters a particular 13-token sequence, but identically if it encounters the same sequence again. At a high level, it masks a certain percentage of tokens, typically one in three or four. The authors compared the goldfish loss to the next-token-prediction loss function in two settings: one that mimicked a typical training process and one that made memorization more likely.\n\nResults:The authors assessed the results using two metrics: (i)ROUGE-L, which falls between 0 and 100 percent and reflects the longest subsequence in common between ground-truth and generated data, and (ii) the percentage of tokens that exactly matched the original text in proper order. Both measure memorization, so lower scores are better.\n\nWhy it matters:Businesses are worried about whether using LLMsposes risks to intellectual property rights and privacy. Techniques that address this concern without significantly impacting performance are welcome.\n\nWe’re thinking:Memorization also happens in models generating images. We look forward to research into using similar techniques in that domain.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--11--1.png",
      "https://dl-staging-website.ghost.io/content/images/2024/09/The-Batch-ads-and-exclusive-banners---2024-09-17T093235.946.png",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--8-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--9-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--16-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--12-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-182/",
    "title": "issue 182",
    "date": "",
    "reading_time": "",
    "content": "Recent successes with large language models have brought to the surface a long-running debate within the AI community: What kinds of information do learning algorithms need in order to gain intelligence?\n\nThe vast majority of human experience is not based on language. The taste of food, the beauty of a sunrise, the touch of a loved one — such experiences are independent of language. But large language models have shown that it’s possible to capture a surprisingly rich facsimile of human experiences by consuming far more language than any human can in a lifetime.\n\nPrior to recent advances in large language models, much of the AI community had viewed text as a very limited source of information for developing general-purpose intelligence. After all, animals evolved intelligence without language. Intelligence includes perceiving the world through sight, sound, and other senses; knowing how to move our bodies; having a common-sense understanding of physics, such as how to knock a fruit off a high tree; and being able to plan simple actions to find food, shelter, or a mate. Writing is a relatively recent invention that dates back only around 5,500 years. Spoken language arose roughly 100,000 years ago. In contrast, mammals have been around for around 200 million years.\n\nIf AI development were to follow the path of evolution, we would start by trying to build insect-level intelligence, then mouse-level intelligence, perhaps followed by dog-level, monkey-level, and finally human-level. We would focus on tasks like vision and psychomotor skills long before the ability to use language.\n\nBut models like ChatGPT show that language, when accessed at massive scale, overcomes many of its limitations as a source of information. Large language models can learn from more words — several orders of magnitude more! — than any individual human can.\n\nAn individual human would need dozens of lifetimes spent doing nothing but reading to see the number of words that GPT-3 considered during its training. But the web aggregates text written for or by billions of individuals, and computers have ready access to much of it. Through this data, large language models (LLMs) capture a wealth of knowledge about the human experience. Even though an LLM has never seen a sunrise, it has read enough text about sunrises to describe persuasively what one looks like.So, even though language is a small part of human experience, LLMs are able to learn a huge amount of information about the world. It goes to show that there are multiple paths to building intelligence, and that the path followed by evolution or human children may not be the most efficient way for an engineered system.Seeing the entire world only through the lens of text — as rich as it turns out to be, and as valuable as systems trained on text have become — is still ultimately an impoverished world compared to the one we live in. But relying on text alone has already taken us quite far, and I expect this direction to lead to exciting progress for years to come.\n\nKeep learning!\n\nAndrew\n\nThe right teacher can make even the most intimidating subject easy. Luis Serrano knows that first-hand: He struggled with math until he started connecting concepts with real-world examples. Learn why he was the perfect person to teach the all-newMathematics for Machine Learning and Data Science Specialization.Read more\n\nTesla, whose autonomous-vehicle technology has been implicated in a number of collisions, promoted it in a way that apparently was intended to deceive.\n\nWhat's new:Tesla deliberately misled the public about its vehicles’ ability to drive themselves, according toBloombergand other news outlets.\n\nHuman in the loop:In 2016, Tesla shared avideothat showed a car traveling from a household driveway to an office parking lot. Onscreen text read, “The person in the driver’s seat is only there for legal reasons. He is not doing anything. The car is driving itself.”\n\nBehind the news:The United States National Highway Traffic Safety Administration (NHTSA) recentlydeterminedthat a Tesla vehicle controlled by Autopilot in 2022 braked unexpectedly, leading to an eight-car pile-up. The accident occurred hours after Musk hadtweetedthat Autopilot was available to all North American drivers who purchased the option. (Previously it had beenlimitedto drivers who had demonstrated safe driving.) NHTSA isinvestigatinghundreds of complaints of Tesla vehicles braking unexpectedly.\n\nWhy it matters:Tech companies commonly promote capabilities well ahead of their capacity to deliver. In many cases, the biggest casualties are intangibles like the public’s trust and investors’ bank accounts. When it comes to self-driving cars, false promises can be deadly.\n\nWe're thinking:A company’s engineers are often the only ones who have the experience and perspective to foresee the consequences of a misleading product demo. When they do, their duty is not to keep mum but to push back.\n\nA top supplier of stock images will compensate artists who contribute training data to its image-generation service.\n\nWhat's new:Shutterstock, whichlauncheda text-to-image generator to supplement its business in licensing images, committed to sharing revenue with contributors who permit the company to use their artwork and photographs to train its model.\n\nHow it works:The image generator is based on OpenAI’sDALL·E 2and built in collaboration withLG AI Research.\n\nBehind the news:Rival stock-image supplier Gettybannedthe uploading and licensing of AI-generated art in September. Getty also recentlyannouncedits intent to sue Stability AI, developer of the Stable Diffusion image generator, claiming that the model’s training set included millions of images owned or licensed by Getty, which Stability AI used without permission.\n\nYes, but:Shutterstock’s revenue in 2021, the most recent year reported, was around $773 million, and image generation is likely to represent a small fraction of the revenue. Meanwhile, Image generation models like DALL·E 2 are trained on hundreds of millions of images. This suggests that individual payouts for most contributors likely will be minuscule for the foreseeable future.\n\nWhy it matters:Image generation could disrupt the business of licensing stock images. Why pay for a license when you can generate a suitable image for pennies? Shutterstock is confronting the threat proactively with a bid to own a piece of the emerging market for generated media.\n\nWe're thinking:Much of the debate over how to compensate artists for data used to train image generators has focused on what’s legal. A more important question is what’s fair. Once we hash that out, legislators can get to work updating copyright laws for a digital, AI-enabled, generative world.\n\nBuild a practical action plan to grow your organization using AI! Join FourthBrain’s live, three-day workshop for business leaders and executives between February 27 and March 1, 2023.Register today\n\nReinforcement learning is powering a new generation of video game cheaters.\n\nWhat’s new:Players ofRocket League, a video game that ranks among the world’s most popular esports, are getting trounced by cheaters who use AI models originally developed to train contestants,PC Gamerreported.\n\nThe game:Rocket League’s rules are similar to football (known as soccer in the United States): Players aim to force a ball into their opponent’s goal at the other end of an arena — except, rather than kicking the ball, they push it with a race car. Doing so, however, requires mastering the game’s idiosyncratic physics. Players can drive up the arena’s walls, turbo-boost across the pitch, and launch their car into the air.\n\nHow it works:The cheat takes advantage of a bot known as Nexto. Developed by AI-savvy players as a training tool, Nexto and similar bots typically include hard-coded restrictions against being used in competitive online play. However, someone customized the bot, enabling it to circumvent the restriction, one of Nexto’s developersrevealedin a discussion on Reddit.\n\nBehind the news:Despite reinforcement learning’s ability to master classic games likegoand video games likeStarCraft II, news of AI-powered cheats has been scant. The developers ofUserviz, a cheatbot for first-person shooters that automatically aimed and fired on enemies detected by aYOLOimplementation, deleted access to the app after receiving legal notice from video game publisher Activision.\n\nWhy it matters:Video games are big business. Rampant cheating could impact a game’s sales by ruining the experience for casual players. Cheating can also tarnish the reputation of games that, likeRocket League, are played professionally, where top players stand to winmillionsof dollars.\n\nWe’re thinking:While we condemn cheating, we applaud anyone who is so motivated to improve their gaming skill that they develop reinforcement learning models to compete against!\n\nWho would disagree that, if all people are mortal and Socrates is a person, Socrates must be mortal? GPT-3, for one. Recent work shows that bigger language models are not necessarily better when it comes to logical reasoning.What’s new:Researcherstestedthe ability of language models to determine whether a statement follows a set of premises. Simeng Han led the project with collaborators at Yale University, University of Illinois, Iowa City West High School, University of Washington, University of Hong Kong, Penn State University, Meta, and Salesforce.Key insight:Previouseffortsto test logical reasoning in language models were based on datasets that contained limited numbers of words (roughly between 100 and 1,000), premises (up to five per example), and logical structures (less than 50). A more diverse dataset would make a better test.How it works:The authors assembled FOLIO, a dataset of over 1,400 examples of real-world logical reasoning that uses more than 4,350 words, up to eight premises, and 76 distinct logical structures. They challenged a variety of models to classify whether the relationship between a set of premises and an example conclusion was true, false, or unknown.\n\nResults:A fine-tuned RoBERTa-large (340 million parameters) accurately labeled 62.11 percent of FOLIO’s test examples, while a fine-tuned BERT-large of the same size achieved 59.03 percent accuracy. The probability of predicting the correct answer at random was 33.33 percent. Given eight labeled logic stories as input, Codex (of unknown size) achieved 56.04 percent accuracy, while GPT-3 (175 billion parameters) achieved 43.44 percent.Why it matters:Language models can solve simple logic puzzles, but their performance is inconsistent anddependsa great deal on the prompt they’re given. This work offers a more rigorous benchmark for tracking progress in the field.\n\nWe’re thinking:The recently unveiled ChatGPT has wowed many users, but its ability to solve logic problemsvaries wildly with the prompt. It’s not clear whether some of the outputs shared on social media represented its best — or most embarrassing — results. A systematic study like this would be welcome and important.\n\nResearch: Google devised a next-generation AI music generator, but the company has yet to release it.MusicLM can generate coherent songs from complex descriptions. Google is keeping it under wraps while it sorts out ethical and legal challenges. (TechCrunch)Research: A large language model generated functional protein sequences.Researchers trained a language model called ProGen to produce synthetic enzymes, a type of globular protein, and some of them worked as well as those found in nature. (Vice)Professors are navigating the pros and cons of ChatGPT in education.An educational dilemma takes shape: Should text generators be banned or embraced? (TheWall Street Journal)Academic publisher Springer Nature announced guidelines for the use of text generators.The world’s largest academic publishing company established ground rules for using large language models ethically to produce scientific papers. (The Verge)A long-running controversy over which species laid a prehistoric eggs has finally come to an end.A machine learning model helped scientists confirm that eggshells found in the 1980s belonged to a giant, extinct bird called Genyornis. (The Conversation)Research: The exporting of surveillance technology to countries experiencing political unrest may have negative effects.China’s exports of AI technology used for state surveillance have the potential to reinforce and give rise to more autocratic countries. (Brookings)\n\nResearchers unveiled the mystery of a Renaissance painting.A face recognition system found a painting attributed to an unknown artist is likely to be a Raphael masterpiece. (BBC)A plan to replace a lawyer with an AI legal assistant in court fell apart.DoNotPay, a startup that offers AI-powered legal services, intended to deploy its chatbot to represent a defendant in a U.S. court, but state prosecutors threatened the company’s CEO with possible prosecution and jail time. (NPR)BuzzFeed will use OpenAI’s services to enhance its content.This digital media company plans to use automated methods to personalize content for its audiences. (TheWall Street Journal)Research: A startup is exploring new smells with the help of AI.Google spinout Osmo aims to create the next generation of aromatic molecules for everyday products. (Wired)Research: ALS patient communicated 62 words per minute using a brain impant, breaking previous record.A brain-computer interface (BCI) decoded speech in an ALS patient 3.4 times faster than the prior record for any kind of BCI. (MIT Technology Review)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/02/ezgif.com-gif-maker--9-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/02/Working-AI--600---338-px---Presentation--169----1-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--24-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--33-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/02/Working-AI--600---338-px---Presentation--169----2-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--35-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--36-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-188/",
    "title": "issue 188",
    "date": "",
    "reading_time": "",
    "content": "Last week, Silicon Valley Bank (SVB), Signature Bank, and Silvergate Bank suddenly collapsed. If it passed uneventfully from your point of view, good for you! Many companies worked nonstop through the weekend scrambling to preserve funds so they could pay their employees.\n\nNumerous tech startups and small businesses bank at SVB, and many are among the business pioneers who are bringing AI to market. For example, when AI Fund, which I lead, works with entrepreneurs to build new companies, we used to help them set up accounts with SVB.\n\nLast Wednesday, SVB announced a $1.8 billion loss. The next morning, rumors began circulating via text, email, and Slack about a bank run in which customers were withdrawing fundsen masse. When this happens, depositors can lose money they’ve saved beyond the $250,000 limit the FDIC (a U.S. government agency) guarantees. Without access to their money, companies can’t pay employees who are counting on a paycheck to cover expenses. A permanent loss of funds would lead to numerous layoffs and company shutdowns.\n\nWhile navigating the collapse of SVB, I was fortunate to be able to call on friends and allies. Several CEOs of AI Fund portfolio companies share a Slack channel and have pre-existing relationships, so none of us felt alone. We were able to share information, make introductions to new banks, and lean in to help each other. Over the weekend, the AI Fund team went to many CEOs and pledged funds from AI Fund’s management company to make sure they could cover their payrolls.\n\nI also saw the best of the AI and tech worlds last week beyond the AI Fund ecosystem. As new information developed, executives at many companies shared it across their networks, and we worked our way through the crisis cooperatively. I’m grateful that we were able to face the storm together.\n\nOn Sunday, the U.S. government wisely announced that it would protect all depositors’ assets. This calmed the crisis and helped to head off a domino effect of further bank failures.\n\nCandidly, I was stressed from Thursday through the weekend about the fate of numerous people and companies. And I know that this is not the end of the challenges. Here’s what life has been like for an AI innovator in recent years (h/t@ChrisJBakke):\n\nI expect life to be equally dynamic in the future as well — hopefully with more ups than downs. But the fact that many people in AI have a network of trusted friends will enable us to react quickly and work together to benefit everyone.\n\nKeep learning!\n\nAndrew\n\nGet ready for the next wave of language-model mania.What’s new:OpenAIintroducedthe latest in its GPT series of large language models to widespread excitement. The company showed statistics and examples designed to demonstrate that the new model outstrips its predecessors in its language comprehension as well as its ability to adopt a desired style and tone and stay within bounds imposed by its designers. OpenAI co-founder Greg Brockman showed off some of its capabilities in alivestreamthat accompanied the launch.How to get access:Text input/output is available viaChatGPT Plus, which costs $20 monthly, with image input to come. An API is forthcoming, and you can join the waitlisthere.How it works:OpenAI didn’t share many details, citing concerns about safety and competition. Like earlier GPT models,GPT-4is based on the transformer architecture and trained to predict the next token on a mix of public and private datasets. It was fine-tuned using reinforcement learning from human feedback and engineered prompts.\n\nHow it performs:GPT-4 aced a variety of AI benchmarks as well as simulated versions of tests designed for humans.\n\nWhere it works:Several companies are already using GPT-4.\n\nYes, but:OpenAI doesn’t mince words about the new model’s potential to wreak havoc: “While less capable than humans in many real-world scenarios . . . GPT-4's capabilities and limitations create significant and novel safety challenges.” While the model outperformed its predecessors in internal adversarial evaluations of factual correctness, like other large language models, it still invents facts, makes reasoning errors, generates biased output, and couches incorrect statements in confident language. In addition, it lacks knowledge of events that transpired after September 2021, when its training corpus was finalized. OpenAI details the safety issueshere.Why it matters:As language models become more capable, they become more useful. It’s notable that OpenAI believes this model is ready to commercialize from the get-go: This is the first time it has introduced a new model alongside product launches that take advantage of it.We’re thinking:Stable Diffusion, Phenaki, MusicLM, GPT-4: This is truly a golden time in AI!\n\nMeta’s effort to make a large language model available to researchers ended with its escape into the wild.\n\nWhat’s new:Soon after Meta started accepting applications for developer access to LLaMA, a family of trained large language models, a user on the social network 4chan posted a downloadable BitTorrent link to the entire package,The Vergereported.\n\nHow it works:LLaMA includes transformer-based models with 7 billion, 13 billion, 33 billion, and 65 billion parameters. The models were trained on Common Crawl, GitHub, Wikipedia, Project Gutenberg, ArXiv, and Stack Exchange. Tested on 20 zero- and few-shot tasks, LLaMA outperformedGPT-3on all tasks,Chinchillaon all but one, andPaLMon all but two.\n\nEscape:On March 24, Meta hadofferedLLaMA to researchers at institutions, government agencies, and nongovernmental organizations who requested access and agreed to a noncommercial license. A week later, 4chan leaked it.\n\nBehind the news:Efforts to releasesimilarmodelsare ongoing even as the AI community continues to debate the potential risks and rewards. Those who favor limited access cite safety concerns believe that institutions are best positioned to study models and learn to control them. Proponents of open access argue that free enquiry offers the best route to innovation and social benefit.\n\nWhy it matters:LLaMA gives experimenters, small developers, and members of the general public unprecedented access to cutting-edge AI. Such access likely will enable valuable scientific, practical, and commercial experimentation. While the risk of harm via automated generation of effective spam, scams, propaganda, disinformation, and other undesirable outputs is real, open source projects like BLOOM and GPT-NeoX-20B have led to significantly more benefit than harm — so far.\n\nWe’re thinking:Making models like LLaMA widely available is important for further research. Ironically, bad actors will use the leaked LLaMA, while conscientious researchers will respect Meta’s copyright and abide by the rules. For instance, Stanford researchers announcedAlpaca, a LLaMA variant that’s fine-tuned to follow instructions. However, the Stanford team is holding back the trained weights while it discusses the matter with Meta. Considering the potential benefits and harms of restricted release versus openness, openness creates more benefits all around.\n\nLearn how to build and deploy an end-to-end application using open source generative AI tools at a one-day workshop with FourthBrain. Join us on April 5, 2023, from 9 a.m. to 3 p.m. Pacific Time! Team registrations available!Register now\n\nWhat do your GitHub projects reveal about your professional prospects? A new model aims to help recruiters find out.What’s new:Prog.ai analyzes GitHub repositories to help employers find engineers skilled in particular areas,TechCrunchreported. The beta-test version is available by invitation only, but recruiters can join awaitlistfor forthcoming free, professional, and enterprise service tiers.How it works:The company fine-tuned OpenAI’s GPT-3 on GitHub projects, LinkedIn resumes, and StackOverflow articles to evaluate prospective recruits.\n\nBehind the news:Machine learning is already involved in hiring at many companies. 63 percent of employers and 99 percent of Fortune 500 corporations in the U.S., UK, and Germany used automated systems to screen resumes and cover letters, according to a 2021studyby Accenture and Harvard Business School. However, some hiring systems have been shown to exhibitbias. A forthcoming European Union law aims toregulatecertain types of algorithms, including those that control hiring.Why it matters:Spotting the right talent for a particular position is hard, and getting harder as technical skills proliferate worldwide. If AI can do it efficiently, it may help fill open positions more effectively and distribute opportunities more evenly among the global pool of applicants.\n\nWe’re thinking:While building a portfolio of projects that reflect your skills and interests can help you get an interview, winning the job often comes down to soft skills like interviewing. To learn more, download our free ebook,How to Build Your Career in AI.\n\nRecent multimodal models process both text and images as sequences of tokens, but they learn to represent these distinct data types using separate loss functions. Recent work unifies the loss function as well.\n\nWhat’s new:Wenhui Wang, Hangbo Bao, Li Dong, and colleagues at Microsoft introducedBEiT-V3, a transformer pretrained on a large amount of image, text, and paired image-text data. The model set a new state of the art in several vision-language tasks. This work updates the earlier BEiT and BEiT v2.\n\nKey insight:MoME transformer(which the authors call Multiway) processes image, text, and text-image pairs using different fully connected layers for different data types, but the same self-attention layers for all. The authors who proposed that architecture trained it using a different task and loss function for text and image data. However, pretraining it on a single task and loss function for all data types — specifically, generating masked portions of the data — enables the shared self-attention layers to learn common patterns across data types, creating similar embeddings for similar images and texts.\n\nHow it works:BEiT-V3 is a 1.9 billion parameter MoME transformer.\n\nResults:BEiT-V3 outperformed baseline models across all nine tasks. On ImageNet, it achieved top-1 accuracy of 89.6 percent, beating the previous state of the art, 89 percent, achieved byFD-CLIP. On NLVR2, its accuracy was 92.6 percent accuracy, while the next-best model,CoCa, achieved 87 percent.\n\nWhy it matters:Sometimes great performance lies in a combination of tried-and-true techniques. BEiT-3 takes advantage of (a) the MoME architecture, (b) masked pretraining (which has achieved excellent fine-tuned performance on text, images, and text-image pairs), and (c) a large quantity of data (which has beenshownto yield high performance).\n\nWe’re thinking:If earlier vision-language models are obsolete, so BEiT!\n\nBank of America forecasts AI will have a $15.7 trillion impact by 2030The banking giant foresees a bright future for AI, as key trends  evolve rapidly. (Business Insider)\n\nA Colombian judge used ChatGPT in court rulingThe chatbot helped the judge interpret the law in a dispute between the guardian of an autistic child and a health insurance company. (The Guardian)\n\nGenerative AI is boosting business productivity appsStartups and big tech companies alike are riding a wave of AI-powered productivity services like writing emails, designing presentations, and crafting marketing messages. (VentureBeat)\n\nEuropean Union faces challenges to regulate AI in the ChatGPT eraThe chatbot poses challenges for the EU’s draft regulation on AI, the Artificial Intelligence Act. While the law covers isolated applications, ChatGPT can be used for a wide variety of applications, some of the law encourages and some of which the law restricts. (Politico)\n\nDuckDuckGo launched the AI feature DuckAssistThe search engine now includes a model that generates summaries of Wikipedia articles for certain queries. (The Verge)Google goes all-in on AI to catch up with ChatGPTGoogle plans to integrate AI into all of its products and services, in response to the threat of ChatGPT and other AI-powered chatbots. (Bloomberg)\n\nU.S. Chamber of Commerce urges AI regulationThe organization said that AI technology needs to be regulated due to its influence in matters like the global economy and national security. (Reuters)\n\nBaidu’s AI chatbot is facing challenges before launchErnie Bot, Baidu’s bid to compete with other AI-powered chatbots, is set for release on March 16, but employees say it has numerous issues. (The Wall Street Journal)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--29--1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--46-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--47-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/03/Building-Generative-AI-Applications--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--48-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--49-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-148/",
    "title": "issue 148",
    "date": "",
    "reading_time": "",
    "content": "Last week, Iwroteabout how rising interest rates are likely to lead investors and other finance professionals to focus on short-term returns rather than longer-term investments. Nonetheless, I believe this is still a good time to invest in long-term bets on AI. Why? In a nutshell, (i) the real interest rate (adjusted for inflation) remains very low, and (ii) the transformative value of AI is more financially powerful than interest rates.Although the news is full of rising interest rates, today’s rates are still quite low from a historical point of view. Interest rates (technically, the three-month U.S. treasury bill rate) peaked at over 15% in the 1980s. In contrast, they varied between nearly 0% and about 2.5% over the past decade.A few percentage points of interest aren’t very significant in the face of historic gains in the value of innovative technology. Given the transformative impact of AI — which is making it possible to automate more tasks than ever — I believe that many projects will deliver returns (as measured by, say, share prices) much higher than the interest rate.\n\nFor instance, if you have an idea for a project that can create a 150% return, it matters little if interest rises by 5% and reduces the present value of your project slightly. The returns from high-risk, high-reward AI projects vary so widely — and have so much upside potential — that a modest change in interest rates should have little impact on the decision whether to go for it.\n\nRising interest rates aren’t the only factor that influences how we should view AI investments. Inflation is going up as well. This makes it relatively attractive to invest in building AI projects now, rather than wait and pay a higher price in the future.\n\nLet’s say you’re debating whether to invest in a $100 GPU to speed up your work. A high interest rate — say, 10% — is a disincentive to spend the money: If you can postpone the investment, you save your $100 for a year, end up with $110 after that period, buy the GPU, and pocket the extra $10. But what if you know that inflation will cause the GPU to cost $110 in a year (10% inflation), or even $120 in a year (20% inflation)? Then it’s more attractive to spend the money now.In fact, many people are underestimating how much inflation reduces the real cost of interest. The real interest rate, which takes inflation into account, is roughly the nominal (not adjusted for inflation) interest rate minus the rate of inflation. Because inflation is high, short-term real interest rates (technically, the risk-free rate) going out to 5 years are actuallynegativeright now. Thus, in my view, it remains a good time to continue to make significant investments in technology that you believe will pay off.The great investor Warren Buffet once said he tries to be “fearful when others are greedy, and greedy when others are fearful.” Current market conditions are making many investors fearful. I don’t advocate greed, but I do think for many teams this is a good time to charge ahead bravely and pursue ideas that you believe in. Just as many great companies were founded around the time of the Great Recession of 2007 to 2009, today’s economic headwinds, by sweeping away weaker projects, will clear the way for the strongest teams and ideas to leap ahead.In case you’re wondering, I plan to put my money where my mouth is.AI Fund, the venture studio I lead, will continue to build companies with energy and enthusiasm. Even though some bets on AI will fail, I’m more concerned about aggregate underinvestment than overinvestment in AI.\n\nI don’t advocate ignoring the market downturn. This is a good time to make sure you’re operating efficiently and your teams are appropriately frugal and have good fiscal discipline. Despite the gloomy market, I intend to charge ahead and keep building valuable projects — and I hope you will, too.\n\nKeep learning!\n\nAndrew\n\nOpenAI’s text-to-image generatorDALL·E 2produces pictures with uncanny creativity on demand. Has it invented its own language as well?What’s new:Ask DALL·E 2 to generate an image that includes text, and often its output will include seemingly random characters. Giannis Daras and Alexandros G. Dimakis at University of Texasdiscoveredthat if you feed the gibberish back into the model, sometimes it will generate images that accord with the text you requested earlier.How it works:The authors devised a simple process to determine whether DALL·E 2’s gibberish has meaning to the model.\n\nResults:The authors provide only a handful of quantitative results, but they are intriguing. They report that “a lot of experimentation” was required to find gibberish that produced consistent images.\n\nInside the mind of DALL·E 2:Inputs to DALL·E 2 are tokenized as subwords (for instance, apoploe may divide into apo, plo, e). Subwords can make up any possible input text including gibberish. Since DALL·E 2 was trained to generate coherent images in response to any input text, it’s no surprise that gibberish produces good images. But why does the author’s method for deriving this gibberish produce consistent images in some cases, random images in others, and a 50/50 combination of consistent and random images in still others? The authors and denizens of social media came up with a few hypotheses:\n\nWhy it matters:The discovery that DALL·E 2’s vocabulary may extend beyond its training data highlights the black-box nature of deep learning and the value of interpretable models. Can users benefit from understanding the model’s idiosyncratic style of communication? Does its apparent ability to respond to gibberish open a back door that would allow hackers to get results the model is designed to block? Do builders of natural language models need to start accounting for gibberish inputs? These questions may seem fanciful, but they may be critical to making such models dependable and secure.We’re thinking:AI puzzles always spur an appetite, and right now a plate of fresh wa ch zod rea would hit the spot!\n\nAI startups are creating high value across a wide variety of industries.What’s new:CB Insights, which tracks tech startups,publishedthe latest edition of the AI 100, its annual list of 100 notable AI startups. The list includes companies in healthcare, retail, transportation, finance, construction, media, and manufacturing. (Disclosure: The list includesLanding AI, where Andrew Ng is CEO, andBearing.ai, a portfolio company of AI Fund, the venture studio that he leads.)Early-stage stars:The authors considered 7,000 private companies headquartered around the world. They selected outstanding entries based on the factors that include number and types of investors, research and development activities, market potential, sentiment analysis of news reports, plus a proprietary score related to the startup’s target market, level of funding, and momentum.\n\nBlasts from the past:Many of last year’s  AI 100 continue to gain momentum. They’ve raised $6 billion in aggregate since April 2021. Six are valued at over $1 billion, and nine were acquired or offered shares to the public. (SeeThe Batch’s coverage of the AI 100 in2021and2020.)Behind the news:CB Insights’ recentState of AIreport highlighted trends among AI startups during the first quarter of 2022.\n\nWhy it matters:The AI 100 confirms that AI is finding valuable applications beyond the technology’s stronghold among consumer-internet companies. It also highlights hot sectors for both entrepreneurs and funders. Healthcare and finance are perennial favorites among investors, while automation for warehouses and logistics receive steadily growing attention.We’re thinking:Investors, take note: U.S. companies received the lion’s share of AI startup funding, but the rest of the world is a rich source of talent for both existing startups and those yet to be formed.\n\nCongratulations to Brigita Bizjak of Amsterdam! She’s making a positive impact on her local community as one of DeepLearning.AI’s global Pie & AI Ambassadors.Sign upto be a Pie & AI Ambassador and learn how you, too, could be featured!\n\nOfficials in charge of protecting children stopped using a machine learning model designed to help them make decisions in difficult cases.What’s new:The U.S. state of Oregon halted its use of an algorithm intended to identify children who may benefit from intervention,The Associated Pressreported. The state did not disclose the reason for the move. It came roughly one month after a similar algorithm used by the state of Pennsylvania, which inspired Oregon’s effort, came under criticism for bias.How it works:Oregon’s Department of Human Services developed theSafety at Screening Toolto help social workers screen reports of at-risk children. Social workers were empowered to decide whether to take action with respect to any given report.\n\nPennsylvania’s problem:Researchers at Carnegie Mellon Universityfoundsigns of bias in a similar tool used in Pennsylvania. That algorithm, which assesses the probability that a child will enter foster care within two years, is still in use.\n\nWhy it matters:Oregon’s decision to drop its learning algorithm sounds a note of caution for public agencies that hope to take advantage of machine learning. Many states have applied machine learning to ease the burden on social workers as both the number of child welfare cases has risen steadily over the past decade. However, the effort to automate risk assessments may come at the expense of minority communities whose members may bear the brunt of biases in the trained models.We’re thinking:We’re heartened to learn that independent researchers identified the flaws in such systems and public officials may have acted on those findings. Our sympathy goes out to children and families who face social and economic hardships, and to officials who are trying to do their best under difficult circumstances. We continue to believe that AI, with robust auditing for bias, can help.\n\nThe computational systems known as cellular automata reproduce patterns of pixels by iteratively applying simple rules based loosely on the behavior of biological cells. New work extends their utility from reproducing images to generating new ones.What’s new:Rasmus Berg Palm and colleagues at IT University of Copenhagen developed an image generator calledVariational Neural Cellular Automata(VNCA). It combines a variational autoencoder with aneural cellular automaton, which updates pixels based on the output of a neural network and the states of neighboring pixels.Key insight:A variational autoencoder (VAE) learns to generate data by using an encoder to map input examples to a distribution and a decoder to map samples of that distribution to input examples. Any architecture can serve as the decoder, as long as it can reconstruct data similar to the inputs. Given a distribution, a neural cellular automaton can use samples from it to generate new, rather than predetermined, data.How it works:VNCA generates pixels by updating a grid of vectors, where each vector is considered a cell and each cell corresponds to a pixel. The encoder is a convolutional neural network, and the decoder is a neural cellular automaton (in practical terms, a convolutional neural network that updates vectors depending on the states of neighboring vectors). The authors trained the system to reconstruct images in theMNISTdataset of handwritten digits.\n\nResults:The authors showed that a cellular automaton can generate images, though not very well at this point. They evaluated VNCA using log likelihoods innatural units of information (nats), which gauge similarity between the system’s output and the training data (higher is better). VNCA achieved -84.23 nats, worse than the -77 nats achieved on MNIST by state-of-the-art models such asNVAEandBIVA.Why it matters:This work demonstrates that a neural cellular automaton can generate new images. While it shows no clear advantage of using a neural cellular automaton in a VAE, the combination might lend itself to useful applications. For instance, neural cellular automata have an inherent regenerative ability: Deface an image, and they can regrow the damaged pixels. Thus a VNCA-type approach might be useful for image inpainting. Given an image, the encoder could map it to a Gaussian distribution. Then you could damage the image where you wanted to change it, sample from the distribution, and use the decoder to generate novel pixels in that area.Yes, but:This approach may be challenging to scale. VNCA’s decoder used only 1.2 million parameters rather than the hundreds of millions used in other high-performing decoders. Adding parameters would increase its computational cost significantly, since it updates cells repeatedly based on the states of neighboring cells.We’re thinking:Deep learning offers a wideningarrayof neural image generators: GANs, VAEs, diffusion models, normalizing flows, and more. While each has its advantages and disadvantages, together they amount to an enticing playground for synthesizing data and producing visual art.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/06/Screen-Shot-2022-06-08-at-9--1--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/06/DALLENGLISH.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/AI100.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/pie---Ai-amabssador-soptlight_The-Batch-Brigita.png",
      "https://dl-staging-website.ghost.io/content/images/2022/06/OREGON.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/AUTOMATA--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-225/",
    "title": "issue 225",
    "date": "",
    "reading_time": "",
    "content": "One year since the launch of ChatGPT on November 30, 2022, it’s amazing how many large language models are available.\n\nA year ago, ChatGPT was pretty much the only game in town for consumers (using a web user interface) who wanted to use a large language model (LLM), and a handful of models from OpenAI were the only options for developers (making API calls). Today, numerous open and closed source models are within easy reach. ChatGPT is the most popular way for consumers to chat with an LLM, but others abound, including Microsoft Bing, Google Bard, and offerings from startups such as Anthropic Claude, Inflection Pi, and perplexity.ai. There are also multiple options for developers, including APIs from Amazon Web Services, Azure, Cohere, Google Cloud, Hugging Face, OpenAI, and many others. The proliferation of options is  exciting, and I hope it will continue!\n\nFor both consumer and developer use cases, open source models that you can host yourself, or even run locally on your laptop, are getting surprisingly good. For many applications, a good open source model can perform perhaps on par with the ChatGPT-3.5 of a year ago. The open sourceGPT4AllandMLC, and closed sourceLM Studio(which has a very nice user interface) are making it easier than ever to run models locally. Running models locally used to be an esoteric act restricted to developers who were willing to struggle through complex installation and configuration processes, but it’s now becoming much more widely accessible.\n\nI regularly use a chatbot as a thought partner. These days, I find myself using an LLM running on my laptop (which runs fairly quickly and guarantees privacy, since my data stays on my machine) about as often as a cloud-hosted one. I use a cloud-hosted model when I need a level of performance I can't get from a smaller, locally run, open source one. For instance, I often use GPT-4 for tricky problems and creative brainstorming.\n\nWhile safety is important — we don't want LLMs to casually hand out harmful instructions — I find that offerings from most of the large providers have been tuned to be \"safer\" than I would like for some use cases. For example, sometimes a model has refused to answer basic questions about an activity that harms the environment, even though I was just trying to understand that activity and had no intention of committing that harm. There are now open source alternatives that are less aggressively safety-tuned that I can use responsibly for particular applications.\n\nThe wealth of alternatives is also a boon to developers. An emerging design pattern is to quickly build a prototype or initial product that delivers good performance by prompting an LLM, perhaps an expensive one like GPT-4. Later, if you need cheaper inference or better performance for a particular, narrowly scoped task, you can fine-tune one of the huge number of open source LLMs to your task. (Some developers reportedly are using data generated by GPT-4 for their own fine-tuning, although it’s not clear whether this violates its terms of use.)\n\nIn a year, we've gone from having essentially one viable option to having at least dozens. The explosion of options brings with it the cost of choosing a good one; hopefully ourshort courses on generative AIcan help with that. If you have experience with open source LLMs that you’d like to share, or if you’ve found some models more useful than others in particular applications or situations, please let me know on social media!\n\nKeep learning!\n\nAndrew\n\nP.S. Our newshort course on advanced retrieval augmented generation (RAG) techniquesis out! Taught by Jerry Liu and Anupam Datta of Llama Index and TruEra, it teaches retrieval techniques such as sentence-window retrieval and auto-merging retrieval (which organizes your document into a hierarchical tree structure to let you pick the most relevant chunks). The course also teaches a methodology to evaluate the key steps of RAG separately (using context relevance, answer relevance, and groundedness) to analyze errors and improve performance. Please check out this course!\n\nThe United States’ regulatory regime may not be clear or flexible enough to ensure the safety of AI-powered medical devices.\n\nWhat’s new:Physicians and other health professionals believe that U.S. regulators have approved AI-powered medical products without proper oversight or disclosure, according to areportbyThe New York Times. The FDA hadapprovedroughly 700 products as of July 2023.\n\nHow it works:The Food and Drug Administration (FDA) approves medical devices and diagnostic systems in the U.S. It approves almost all such products that involve AI through a program known as510(k).\n\nWhat they’re saying:“If we really want to assure that right balance, we’re going to have to change federal law, because the framework in place for us to use for these technologies is almost 50 years old.” — Jeffrey Shuren, Director, Center for Devices and Radiological Health, FDA\n\nBehind the news:The FDA’s approval of AI-enabled medical products has been contentious.\n\nWhy it matters:In medicine, the right tool can be a life saver, while the wrong one can be fatal. Doctors need to have confidence in their tools. The current FDA process for AI-powered medical products makes it hard to separate what works from what doesn’t, and that’s delaying adoption of tools that could save lives.\n\nWe’re thinking:We have great faith that AI can improve medical care, but we owe it to society to document efficacy and safety through careful studies. Machine learning algorithms are powerful, but they can suffer fromdata drift and concept drift, which leads them to work in experiments but not in practice. Updated standards for medical devices that are designed to evaluate learning algorithms robustly would help point out problems, help developers identify real problems and solutions, and give doctors confidence in the technology.\n\nChatGPT is pitching in on the assembly line.\n\nWhat’s new:Siemens and Microsoftlauncheda joint pilot program of a GPT-powered model for controlling manufacturing machinery. German automotive parts manufacturerSchaeffleris testing the system in its factories, as is Siemens itself.\n\nHow it works:Industrial Copilot (distinct from similarly named Microsoft products such as GitHub Copilot and Microsoft 365 Copilot) enables users to interact with software that drives industrial machines using natural language. At an unspecified near-future date, Siemens plans to make it more widely available viaXcelerator, an online hub that connects Siemens customers to tools and partners.\n\nBehind the news:Microsoft is betting that specialized large language models can boost productivity (and expand its market) in a variety of industries. The companyannouncedits intention to develop Copilot models for infrastructure, transportation, and healthcare.\n\nWhy it matters:Industrial Copilot promises to reduce the time it takes factory technicians to operate and maintain machinery, and it may help less-technical workers get a stalled assembly line back up and running. This may be especially timely as older workers retire, since the software that runs manufacturing equipment can be decades old, and PLC coding can bedifficultto learn without prior manufacturing experience.\n\nWe’re thinking:For programming languages like PLC, the pool of coders is diminishing even as valuable applications still need to be maintained and built.Generative AI can play an important rolein helping developers who are less familiar with these languages to write and maintain important programs.\n\nLearn advanced retrieval augmented generation (RAG) techniques that you can deploy in production immediately! Our new short course teaches  sentence-window retrieval and auto-merging retrieval, as well as how to evaluate RAG performance.Sign up for free\n\nAn open source tool automatically tests language and tabular-data models for social biases and other common issues.\n\nWhat’s new:Giskardis a software framework that evaluates models using a suite of heuristics and tests based on GPT-4. Aboton the Hugging Face Hub can assess uploaded models automatically and lets users design tests for their own use cases.\n\nAutomated tests:Giskard automatically generates inputs depending on the type of model it’s testing, records the model’s output, and identifies undesirable behavior. For large language models, it tests for 7 potential issues including robustness, misinformation, and social biases (“discrimination”). Anexample evaluationshows how it finds various problems with GPT 3.5.\n\nWhy it matters:Large language models have biases and inaccuracies, but the difficulty of evaluating these issues means that many businesses ship products that have not been fully tested. Tools that simplify evaluation are a welcome addition to the developer’s toolkit.\n\nWe’re thinking:As AI systems become more widely used, regulators are increasingpressureon developers to check for issues prior to deployment. This could make the need for automated testing more urgent.\n\nA pretrained large language model hashelpeda robot resolve high-level commands into sequences of subtasks. It can do this more precisely with additional training — both on language-vision tasks and robotics tasks.\n\nWhat’s new:Danny Driess and colleagues at Google and Technische Universität Berlin proposedPaLM-E, a large multimodal model designed to help control robots. PaLM-E takes a text command, and in executing the command, uses sensor data from a robot to resolve it into a series of low-level subcommands. A separate system converts these low-level commands into robotic control signals. The name adds E, for embodied, to that of Google’s large language modelPaLM.\n\nKey insight:Large language models tend to perform well if they’re trained on a lot of data. We don’t have a lot of robotics data (that is, records of commands, actions taken, and corresponding sensor readings). We can supplement that with vision-language data, which is plentiful, to help the model learn relationships between words and what a robot sees, and ultimately transfer what it learns to performing robotics tasks.\n\nHow it works:PaLM-E comprises a pretrained PaLM large language model and encoders that embed non-text inputs: (i) a pretrained vision transformer to embed images and (ii) a vanilla neural network to embed robot sensor data that described the pose, size, and color of objects in its view. In addition, the system relies on a motion controller that translates words into robotic control signals; in this case, a pretrainedRT-1. Given a high-level command (such as “I spilled my drink, can you bring me something to clean it up?”) — plus images or sensor data from the robot — PaLM-E evaluates the robot’s situation and generates lower-level instructions to be fed to the motion controller.\n\nResults:The authors evaluated PaLM-E in a simulation where it executed tasks from TAMP, which accounted for 10 percent of its training/fine-tuning data. PaLM-E achieved 94.9 percent success. A version of PaLM-E trained only on TAMP achieved 48.6 percent. SayCan, which also was trained only on TAMP, achieved 36 percent. The authors also tested PaLM-E using two physical robots, qualitatively evaluating its response to commands such as “Bring me the rice chips from the drawer.” The robots were able to follow instructions even when people tried to thwart them (say, by returning the bag of chips to the drawer immediately after the robot had pulled them out). You can watch a videohere.\n\nWhy it matters:PaLM-E performed somewhat better than other systems that translate English into robotic control signals that were trained only on robotics data. But with additional training on vision-language and language-only tasks, it vastly outperformed them. Training on these apparently unrelated tasks helped the model learn how to control a robot.\n\nWe’re thinking:Training on massive amounts of text and images continues to be a key to improving model performance across a wide variety of tasks — including, surprisingly, robotics.\n\nAnthropic introduces Claude 2.1The update brings a 200,000-token context window, and a 2x decrease in hallucinations. The beta tool use feature expands Claude's interoperability by connecting with users' and developers’ existing processes and APIs. (Anthropic)\n\nAmazon Web Services (AWS) and Nvidia expand partnership to offer improved supercomputing infrastructureAWS will become the first cloud provider to offer Nvidia GH200 Grace Hopper Superchips, equipped with multi-node NVLink technology. The collaboration also introduces Project Ceiba, a GPU-powered supercomputer with unprecedented processing capabilities. aiming to deliver state-of-the-art generative AI innovations across diverse industries. (Amazon)\n\nThe controversial rumors around OpenAI's math-solving modelRumors swirl around OpenAI's recent upheaval as reports point to development of a new AI model, Q* (pronounced Q-star). Named for its prowess in solving grade-school math problems, the potential breakthrough has prompted speculation about advancements towards artificial general intelligence (AGI). The episode echoes past AGI hype cycles, raising questions about tech industry self-regulation and potential impact on pending AI legislation. (MIT Technology ReviewandReuters)\n\nAI-powered method unlocks ancient cuneiform tablets' secretsResearchers developed a system that can automatically decipher complex cuneiform texts on ancient tablets using 3D models of them, instead of traditional methods using photos. With an estimated one million cuneiform tablets worldwide, some over 5,000 years old, the method’s potential extends beyond known languages, and offers a glimpse into previously inaccessible historical material. (Science Daily)\n\nStability AI introduces Stable Video DiffusionThe model for generative video builds upon the success of the image model Stable Diffusion. The code is available on GitHub, with model weights accessible on the Hugging Face page. The release, comprising two image-to-video models, shows broad adaptability for downstream tasks, including multi-view synthesis from a single image. (Stability AI)\n\nFederal Trade Commission (FTC) simplifies process to investigate AI companiesThe FTC greenlit the use of compulsory measures for investigations into products and services using or claiming to be produced with AI. The 3-0 vote emphasizes the Commission's proactive approach in addressing emerging issues in technology. Lead FTC staffers Nadine Samter and Ben Halpern-Meekin will oversee the implementation of this resolution in the Northwest Region office. (FTC)\n\nAI enhances power grid efficiency with four key innovationsFueled by a recent $3 billion grant from the US Department of Energy, the power grid industry is embracing AI. Key applications include a model for faster grid planning, new software tailoring energy usage, programs managing electric vehicle demand, and AI predicting grid failures due to extreme weather. (MIT Technology Review)\n\nAmazon announces Q, an AI assistant designed for work environmentsTailored to individual businesses, Amazon Q offers quick, relevant answers, content generation, and problem-solving capabilities informed by company data. Prioritizing security and privacy, Amazon Q personalizes interactions based on existing identities and permissions. Companies including Accenture, BMW, and Gilead are among the early adopters. (Amazon)\n\nSports Illustrated exposed for using AI-generated content and authorsThe magazine faces scrutiny after allegations surfaced that it published articles attributed to AI-generated authors with fabricated biographies and headshots. Following inquiries, Sports Illustrated removed the content without a clear explanation. The Arena Group, the magazine's publisher, later attributed the content to an external company, AdVon Commerce, claiming it was human-generated. (Futurism)\n\nGlobal coalition introduced a non-binding pact to ensure AI safetyThe international agreement, signed by 18 countries, including the U.S., emphasizes the need for AI systems to be \"secure by design.\" The 20-page document encourages companies to prioritize safety measures during the development and deployment of AI. (The Guardian)\n\nResearch:Deepmind’s GNoME discovers 2.2 million new crystals using deep learningGoogle’s AI research lab used a tool called Graph Networks for Materials Exploration (GNoME), to identify 2.2 million new crystals, including 380,000 stable materials with promising applications in technology. The predicted stable materials will be contributed to the Materials Project database, fostering collaborative research. (Google Deepmind)\n\nNations grapple with ethical dilemmas as AI-controlled killer drones inch closer to realityThe emergence of autonomous killer drones prompts international debate over legal constraints, with the U.S., China, and major powers hesitant to endorse binding rules. Concerns about handing life-and-death decisions to AI-controlled drones have led some countries to advocate for legally binding regulations at the United Nations, but disagreements among key players have stalled progress.\n\nEuropean Central Bank research finds that AI currently boosts jobs but threatens wagesThe study focused on 16 European countries, indicating an increased employment share in AI-exposed sectors. Notably, low and medium-skill jobs remained largely unaffected, while highly-skilled positions experienced the most significant growth. However, the research acknowledged potential \"neutral to slightly negative impacts\" on earnings, with concerns about future developments in AI technologies and their broader implications for employment and wage dynamics. (Reuters)\n\nAI-generated speaker scandal prompts Microsoft and Amazon executives to withdraw from conferenceTop executives from Microsoft and Amazon withdrew from the DevTernity software conference following revelations that at least one featured female speaker was artificially generated. The disclosure prompted other scheduled speakers to abandon the virtual conference. Microsoft's Scott Hanselman expressed disappointment, emphasizing the importance of diverse and genuine representation at tech conferences. (AP News)\n\nResearch:Researchers uncover vulnerability in ChatGPT, expose training data extraction potentialThe research team successfully extracted several megabytes of ChatGPT's training data by employing a simple attack method. The findings raise concerns about the model's memorization of sensitive information and challenge the adequacy of current testing methodologies. (GitHub)\n\nOpenAI not expected to give Microsoft or other investors seats on new nine-member boardA source told The Information that despite revamping its slate of directors, OpenAI’s new board is unlikely to change its nonprofit status, and will maintain rules barring directors from having a major financial interest in the company. (The Information)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/12/IconPlanets3_1200px-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--31-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed---2023-11-29T175402.527.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch-ads-and-exclusive-banners--83-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--32-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed---2023-11-29T175841.371.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-vii/",
    "title": "issue vii",
    "date": "",
    "reading_time": "",
    "content": "AI is still immature enough that many cities still have a shot at being an AI hub. Many of the most important uses of AI will happen outside the software industry, and they will need to be built for businesses in different cities. We need lots of hubs for AI to reach its full potential!Keep learning,Andrew\n\nData scientist Mihajlo Grbovic hammers out the algorithms that power Airbnb's recommendation engines. Learn what his day-to-day looks like and his advice for up-and-coming machine learning engineers.Read more\n\nThe art of video fakery leaped forward with a system that produces lifelike talking-head videos based on a single still portrait.\n\nWhat’s new:Deepfake videos typically are created by feeding thousands of images of a person to a neural network. Egor Zakharov and his colleagues at Samsung and Skolkovo Institute of Science and Technologydeviseda method that needs only a handful — even just one — to generate good results.\n\nHow it works:The system consists of three networks, all of them pre-trained on thousands of talking-head videos of diverse speakers:\n\nResults:Test subjects were asked to rate the system’s output as fake or real. When the system had been fine-tuned on just one image, they were fooled a considerable amount of the time. When it had been fine-tuned on 32 photos, their guesses were random. See for yourself in thisvideo.\n\nWhy it matters:The new technique drastically cuts the time, computation, and data needed to produce lifelike video avatars. The researchers suggest using it to create doppelgängers for gaming and videoconferencing — that is, using imagery of a single person both to generate the talking head and drive its behavior.\n\nThe fun stuff:The most dramatic results arose from animating faces from iconic paintings, including the Mona Lisa. How wonderful it would be to see her come to life and tell her story! Applications in entertainment and communications are as intriguing as the potential for abuse is worrisome.\n\nWe’re thinking:The ability to make convincing talking-head videos from a few snapshots, combined withvoice cloningtech that generates realistic voices from short audio snippets, portends much mischief. We're counting on the AI community to redouble its effort to build countermeasures that spot fakes.\n\nAn autonomous U.S. Navy warship prototype is the first crew-less vessel to make an ocean crossing.\n\nWhat’s happening:Sea Hunter last fall completed a round trip between its San Diego port and Pearl Harbor, Hawaii, as reported byFortune. On the return voyage, the craft spent ten days at sea with no input from human navigators or mechanics.\n\nHow it works:Sea Hunter's main functions are to clear mines, track submarines, and securely relay communications. The vessel is 132 feet long and moves at around 37 miles per hour. Its software allows it not only to navigate across featureless expanses of water, but also to assess other craft and observe conventional protocols for ship-to-ship encounters.\n\nThe challenge:Navigating the open ocean autonomously involves a number of difficult, high-stakes tasks:\n\nWhy it matters:Built by defense contractor Leidos Holdings, the $59 million craft is an early step in the Navy’s plan to counter foreign sea power with autonomous ships. It’s also a bid to save money: Sea Hunter costs $20,000 a day to operate, compared to $700,000 a day for a destroyer deployed for similar tasks.What’s next:The Navy plans to build a dozen or more autonomous ships, though it hasn’t settled on Leidos’ design. In April, the agency put out a call for combat-ready unmanned ships up to 300 feet long. Leidos plans to compete for the contract.\n\nThe power of deep learning is blunted in domains where labeled training data is scarce. But that may be changing, thanks to a new architecture that recognizes images with high accuracy based on few labeled samples.\n\nWhat’s new:Researchersdeviseda network that, given a small number of labeled images, can learn enough from unlabeled images to outperform a fully trained AlexNet model. Their architecture achieves strong performance with as few as 13 images per class. And performance improves with additional labeled examples, unlike other semi-supervised approaches.\n\nHow it works:The trick is a technique called contrastive predictive coding, which learns to predict high-level features of an image from lower-level ones. Olivier J. Hénaff and his colleagues at DeepMind adaptedearlier workwith CPCs by using an unusually deep and wide residual network, using layer rather than batch normalization, and predicting both lower- and higher-level features. They also messed with image patches to remove low-level cues, forcing the network to focus on high-level structure. The resulting network learns to capture image features at various levels of detail.\n\nResults:With a small number of labeled images, the CPC network beat the state-of-the-art performance of supervised models in ImageNet classification.\n\nWhy it matters:Small data is a frontier for deep learning. The new approach opens new territory in tasks like diagnosing rare diseases, spotting defects on a production line, and controlling robots, where few labeled examples may be available.\n\nTakeaway:The Deep Mind team’s accomplishment reduces a barrier to applying deep learning in low-data domains. Watch for evolving small-data techniques to open up exciting new applications in the next few years.\n\nBe the next Picasso. Learn how to generate art pieces using neural style transfer in the Deep Learning Specialization.Enroll now\n\nAn autonomous big rig is barreling down U.S. interstates in a high-profile trial, raising the prospect that driverless trucking will leapfrog self-driving cars.\n\nWhat’s new:The U.S. Postal Service is using an autonomous truck to ferry mail between distribution centers in Phoenix and Dallas. A Peterbilt rig equipped with self-driving technology by TuSimple will make the 2,100-mile round trip through Arizona, New Mexico, and Texas five times over two weeks, moving for 22 hours at a stretch. Although the rig will drive itself, humans will sit behind the wheel and in the passenger seat.\n\nWhy it matters:The post office lost $3.9 billion in 2018, making 12 years of consecutive annual loss. It spends $4 billion a year on independent highway trucking, and a dearth of drivers is adding to the expense as truckers age out of the industry. Autonomous rigs could pick up the slack, saving money while keeping trucks on the road.\n\nWhat they’re saying:Self-driving tech “could save hundreds of millions by eliminating human drivers and hours-of-service rules that keep them from driving around the clock.” —Bloomberg\n\nBehind the news:The test comes as several companies developing self-driving taxis are reassessing their plans, includingCruiseandUber. Meanwhile, their trucking counterparts are stepping on the gas. TuSimple is carrying cargo discreetly for 12 customers in the U.S. and China. Swedish rivalEinriderecently began making driverless freight deliveries. And thePost Officeis making its own moves.\n\nTakeaway:Highway driving is simpler to automate than urban driving, where roads are denser and less predictable. On the other hand, autonomous trucks must comply with diverse regulations as they cross various jurisdictions in a single journey. Compliance issues are bound to tap the brakes until a national regulatory framework is in place.\n\nA leading international policy organization issued recommendations for artificial intelligence, adding momentum to international efforts to guide the technology in broadly beneficial directions.\n\nWhat’s new:The Organization for Economic Cooperation and Development published guidelines to promote AI’s benefits while protecting human rights and democratic values. This is not just another think tank: The group represents 36 wealthy countries including the U.S. Six more countries, mostly in South America, signed on as well.\n\nThe recommendations:The OECD document offers five main principles. AI should:\n\nIt also outlines a set of governance policies. Countries developing AI should:\n\nThe hitch:The recommendations are non-binding. There’s no agreement about enforcement or mechanism for doing so. Moreover, there’s no plan for putting them into practice. The OECD is forming an AI Policy Observatory to focus on implementation.\n\nTakeaway:The OECD’s work may look redundant, but it's an important step toward harnessing AI on a global scale. It signals widespread acknowledgement — by the world’s most developed countries, at least — that AI is on track to change every sector of the global economy. And it shows a will to grapple with that change sooner than later, and to make sure that fostering prosperous, equitable, and sustainable societies is part of the discussion. Whether and how governments will heed the call remains to be seen.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/30ceb494-3980-411c-947e-fda0d443e0c6-1.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/d3f0777c-901d-440a-8b0b-661ecc902bea--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/e4599349-09d6-4de8-9694-2536baa56d8b.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/711b21b3-13fe-4e3d-9db2-9a12eb8dde2e.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/5becae64-e0a6-477c-9eb3-b80f2a211a6d.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/2b1dab82-5e3a-41f6-b1e2-df81abacfee3.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/19f7d2a6-8ddb-4ba5-8fa0-309bf4990341.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/b23a4685-df5d-4523-8b21-a8787c9cb756.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-99/",
    "title": "issue 99",
    "date": "",
    "reading_time": "",
    "content": "In a recentletter, I noted that one difference between building traditional software and AI products is the problem ofcomplex product specification. With traditional software, product managers can specify a product in ways that communicate clearly to engineers what to build — for example, by providing a wireframe drawing. But these methods don’t work for AI products.\n\nFor an AI product, among the most important parts of the specification are:\n\nConsider the problem of how to build a self-driving car. We might decide the acceptable road conditions for autonomous operation and the acceptable rate of collisions with particular objects at various speeds (for example, gently bumping a traffic cone at five miles per hour every 1 million miles may be okay, but hitting a pedestrian at 20 miles per hour every 1,000 miles is not).\n\nOr take reading electronic health records. What is an acceptable error rate when diagnosing a serious disease? How about the error rate when diagnosing a minor disease? What if human-level performance for a particular illness is low, so physicians tend to misdiagnose it, too?\n\nSpecifying the metrics, and the dataset or data distribution on which the metrics are to be assessed, gives machine learning teams a target to aim for. In this process, we might decide how to define a serious versus a minor disease and whether these are even appropriate concepts to define a product around. Engineers find it convenient to optimize a single metric (such as average test-set accuracy), but it’s not unusual for a practical specification to require optimizing multiple metrics.\n\nHere are some ideas that I have found useful for specifying AI products.\n\nI’ve found it very helpful to have sufficient data and a clear target specification for each slice. This isn’t always easy or even possible, but it helps the team advance toward a reasonable target.As a team performs experiments and develops a sense of what’s possible as well as where the system might falter, the appropriate slices can change. If you’re a machine learning engineer who is part-way through the project, and the product manager changes the product specification, don’t be frustrated! Ask them to buy you a coffee (or tea or other beverage of your choice) for your trouble, but recognize that this is part of developing a machine learning system. Hopefully such changes will happen less frequently as the team gains experience.\n\nKeep learning!Andrew\n\nTraffic signals controlled by AI are keeping vehicles rolling citywide.\n\nWhat’s new:Several U.S. cities are testing systems from Israel-based startup NoTraffic that promise to cut both commute times and carbon emissions, according toMotorTrend. The company plans to expand to 41 cities by the end of 2021.\n\nHow it works:NoTraffic uses a combination of neural networks and other techniques to optimize intersections and coordinate traffic signals throughout a city. The system is outfitted to integrate with pavement sensors and connected-vehicle protocols.\n\nBehind the news:Machine learning is combating congestion outside the U.S. as well.\n\nWhy it matters:Worldwide, congestion costs hundreds of billions of dollars in annual productivity, pollutes cities, and burdens the planet with greenhouse gases. AI-driven traffic control doesn’t eliminate those impacts, but it can take the edge off.\n\nWe’re thinking:Many traffic lights already are geared to prioritize passage of emergency vehicles, for example byrecognizing patterns of flashing lights— but networked sensors stand to improve traffic routing globally.\n\nNeural networks have learned to play video games like Dota 2 via reinforcement learning by playing for the equivalent of thousands of years (compressed into far less time). In new work, an automated player learned not by playing for millennia but by watching a few days’ worth of recorded gameplay.\n\nWhat’s new:Tim Pearce and Jun Zhu at Cambridge Universitytrained an autonomous agent via supervised learningto play the first-person shooterCounter Strike: Global Offensive (CS:GO)by analyzing pixels. The model reached an intermediate level of skill. Check out a video presentationhere.\n\nKey insight:Reinforcement learning can be used to teach neural networks to play games that include a programming interface, which enables the model to explore all possible game states because gameplay proceeds much faster than real time.CS:GOlacks such an interface. An alternative is to learn from expert demonstrations, a technique known as behavioral cloning. Where such demonstrations are hard to collect, publicly broadcast matches can stand in.\n\nHow it works:The system generated a representation of each video frame using a convolutional neural network and combined multiple representations using a convolutional LSTM. A linear layer decided what action to take per frame.\n\nResults:Pitted against the game’s built-in medium-difficulty agent, which takes advantage of information that humans don’t have access to (such as the positions of all players), the author’s system came out on top. It achieved 2.67 kills per minute and 1.25 kills per death, compared to the built-in agent’s 1.97 kills per minute and 1.00 kills per death. Against human players in the top 10 percent, it didn’t fare so well. It achieved 0.5 kills per minute and 0.26 kills per death compared to the human average of 4.27 kills per minute and 2.34 kills per death\n\nWhy it matters:Behavioral cloning is a viable alternative to reinforcement learning — within the limits of available expert demonstrations. The authors’ system even learned the classic gamer swagger of jumping and spinning while it reloaded.\n\nWe’re thinking:We’re in the mood for a nonviolent round ofSplatoon.\n\nLearn how to automate a natural language processing task in “Build, Train, and Deploy ML Pipelines Using BERT,” Course 2 in our newPractical Data Science Specialization.You'll build an end-to-end pipeline using Hugging Face’s optimized implementation of the state-of-the-art BERT algorithm using Amazon SageMaker Pipelines.Enroll now\n\nReal estate websites helped turn automated real-estate assessment into aclassicAIproblem. The latest approach by a leader in the field gets a boost from deep learning.\n\nWhat’s new:Zillowdevelopeda neural network that predicts the value of homes across the United States. The system narrowed the error between earlier estimates and actual selling prices by 1 percent, achieving a median error rate of 6.9 percent. In addition to making it available online, Zillow plans to use it to improve its own real estate business.\n\nHow it works:Zillow’sZestimatesystem previously employed roughly 1,000 separate non-machine-learning algorithms, each tailored to a different local market. The new network estimates the value of 104 million dwellings nationwide, updated as frequently as daily.\n\nBehind the news:Zillow has been tweaking Zestimate since 2006. The new neural networkgrewfrom ahackathonin which 3,800 teams from 91 countries competed for a $1 million prize. The winning team used a combination of deep learning and other machine learning techniques. The company incorporates machine learning into other aspects of its business as well, Zillow vice president of AI Jasjeet Thind said in aninterviewfor DeepLearning.AI’s Working AI series. For instance, the company is developing a natural language search system for parsing legal documents.\n\nWhy it matters:Between inspections, negotiating a price, and filling out reams of paperwork, buying a home is a complex ordeal. A tool that helps buyers and sellers alike get a fair price could be a big help.\n\nWe’re thinking:How much does a GPU rack add to the value of a home?\n\nAn insect-sorting robot could help scientists grapple with the global biodiversity crisis.\n\nWhat’s new:Anautomated insect classifiersucks in tiny arthropods, classifies them, and maps their most important identifying features. It was developed by researchers at Karlsruhe Institute of Technology, Berlin Natural History Museum, Bavarian State Collection of Zoology, Sapienza University of Rome, and National University of Singapore.\n\nHow it works:The bot integrates systems that transport insects in and out, snap photos of them, and process the images. A touch screen serves as the user interface and displays model output. The authors pretrained aVGG19convolutional neural network on ImageNet and fine-tuned it using 4,325 images of insects plus augmentations.\n\nResults:In testing, the system scored an average of 91.4 percent precision across all species — good butnot up to the level of a human expert.\n\nBehind the news:This is just the latest use of AI in the time-consuming task of insect identification.\n\nWe’re thinking:If this system stopped working, someone would have to debug it.\n\nLearn how to counteract bias and unfair representations! Together with KPMG Ignition Tokyo and Machine Learning Tokyo, we invite you to “XAI: Learning Fairness with Interpretable Machine Learning” on July 8, 2021.Enroll now",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/07/cartoon--3--copy.jpeg",
      "https://dl-staging-website.ghost.io/content/images/2021/07/NOTRAFFIC-REVISED.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/CSGO576x324.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/Course-Name-2-2.png",
      "https://dl-staging-website.ghost.io/content/images/2021/07/ZILLOW.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/divscan.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/AI-Access-7.8_The-Batch-Image--1---1-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-195/",
    "title": "issue 195",
    "date": "",
    "reading_time": "",
    "content": "Last week, we released a new course, ChatGPT Prompt Engineering for Developers, created in collaboration with OpenAI. This short, 1.5-hour course is taught by OpenAI’s Isa Fulford and me. This has been the fastest-growing course I’ve ever taught, with over 300,000 sign-ups in under a week. Pleasesign up to take it for free!Many people have shared tips on how to use ChatGPT’s web interface, often for one-off tasks. In contrast, there has been little material on best practices for developers who want to build AI applications using API access to these hugely powerful large language models (LLMs).\n\nLLMs have emerged as a new AI application development platform that makes it easier to build applications in robotic process automation, text processing, assistance for writing or other creative work, coaching, custom chatbots, and many other areas. This short course will help you learn what you can do with these tools and how to do it.\n\nSay, you want to build a classifier to extract names of people from text. In the traditional machine learning approach, you would have to collect and label data, train a model, and figure out how to deploy it to get inferences. This can take weeks. But using an LLM API like OpenAI’s, you can write a prompt to extract names in minutes.\n\nIn this short course, Isa and I share best practices for prompting. We cover common use cases such as:\n\nWe also cover how to build a custom chatbot and show how to construct API calls to build a fun pizza order-taking bot.\n\nIn this course, we describe best practices for developing prompts. Then you can try them out yourself via the built-in Jupyter notebook (the middle portion of the image above). If you want to run the provided code, you can hit Shift-Enter all the way through the notebook to see its output. Or you can edit the code to gain hands-on practice with variations on the prompts.\n\nMany applications that were very hard to build can now be built quickly and easily by prompting an LLM. So I hope you’llcheck out the courseand gain the important skill of using prompts in development. Hopefully you’ll also come away with new ideas for fun things that you want to build yourself!Keep learning!\n\nAndrew\n\nA pioneer of deep learning joined the chorus of AI insiders who worry that the technology is becoming dangerous, saying that part of him regrets his life’s work.\n\nWhat’s new:Geoffrey Hinton, who has contributed to groundbreaking work on neural networks since the 1980s, stepped down from his role as a vice president and engineering fellow at Google so he could voice personal concerns about AI’s threat to society,The New York Timesreported. He believes that Google has acted responsibly in its AI development, he added in a subsequenttweet.\n\nWhy he stepped down:AI models have improved faster than Hinton had expected, and the generative AI gold rush led him to believe that the financial rewards of innovating would overwhelm incentives to rein in negative effects. In addition, at 75, he has become “too old to do technical work,” he toldMIT Technology Review. Instead, he will focus on philosophical matters. Among his concerns:\n\nBehind the news:Hinton’s contributions to deep learning are myriad. Most notably, he helped popularize the use of backpropagation, the core algorithm for training neural networks; invented the dropout technique to avoid overfitting; and led development of AlexNet, whichrevolutionizedimage classification. In 2018, hereceivedthe Turing Award alongside Yann LeCun and Yoshua Bengio for contributions to AI.\n\nWhy it matters:Hinton’s thoughts about AI risks are exceptionally well informed. His concerns sound a note of caution for AI practitioners to evaluate the ethical dimensions of their work and stand by their principles.\n\nWe’re thinking:Geoffrey Hinton first joined Google as a summer intern (!) at Google Brain when Andrew led that team. His departure marks the end of an era. We look forward to the next phase of his career.\n\nA major political party in the United States used generated imagery in a campaign ad.What’s new:The Republican Partyreleaseda video entirely made up of AI-generated images. The production, which attacks incumbent U.S. president Joe Biden — who leads the rival Democratic Party — marks the arrival of image generation in mainstream U.S. politics.Fake news:The ad depicts hypothetical events that purportedly might occur if Biden were to win re-election in 2024. Voice actors read fictional news reports behind a parade of images that depict a military strike on Taipei due to worsening relations between the U.S. and China, boarded-up windows caused by economic collapse, a flood of immigrants crossing the southern border, and armed soldiers occupying San Francisco amid a spike in crime.\n\nBehind the news:Generative AI previously infiltrated politics in other parts of the world.\n\nWhy it matters:Political campaigns are on the lookout for ways to get more bang for their buck, and using text-to-image generators may be irresistible. In this case, the producers used fake — but realistic — imagery to stand in for reality. Despite the small-type disclaimer, the images make a visceral impression that fictional events are real, subverting the electorate's reliance on an accurate view of reality to decide which candidates to support. The power of such propaganda is likely to grow as generative video improves.We’re thinking:This use of generated images as propaganda isn’t limited to political jockeying. Amnesty International recently tweeted — and sensibly deleted — a stirring image of a protester detained by Colombian police bearing the fine print, “Illustrations produced by artificial intelligence.” Organizations that seek to inform their audiences about real-world conditions counteract their own interests when they illustrate those conditions using fake images.\n\nLearn new use cases for large language models and improve your ChatGPT API skills in our one-hour course, “ChatGPT Prompt Engineering for Developers.”Sign up for free\n\nA language model will stand in for radio disk jockeys.What’s new:RadioGPTgeneratesradio shows tailored for local markets. The system, which is undergoing tests across North America, is a product of Futuri, a company based in Cleveland, Ohio, that focuses on digital audience engagement.How it works:Futuri’s proprietaryTopic Pulsesystem determines trending topics in a radio station’s local market, and OpenAI’s GPT-4 generates a script. An unspecified model vocalizes the script using between one and three voices. Customers can choose a preset voice or clone their own.\n\nBehind the news:Fully automated media programs are gaining momentum as AI models make it easy to produce endless amounts of text and audio.\n\nWhy it matters:Many radio stations already are highly automated and rely for news on syndicated programming. AI-generated DJs that localize news and listener interactions can give them programming customized to their markets and may help them compete with streaming services.We’re thinking:RadioGPT fits generative AI into a traditional radio workflow. Ultimately, we suspect, this tech will remake the medium in more fundamental ways.\n\nIf you want a model to answer questions correctly, thenenriching the input with reference text retrieved from the webis a reliable way toincrease the accuracy of its output. But the web isn’t necessarily the best source of reference text.\n\nWhat's new:Wenhao Yu at University of Notre Dame and colleagues at Microsoft and University of Southern California used a pretrained language model to generate reference text. They fed that material, along with a question, to a second pretrained language model thatansweredmore accurately than a comparable model that was able to retrieve relevant text from the web.\n\nKey insight:Given a question, documents retrieved from the web, even if they’re relevant, often contain information that doesn’t help to answer it. For instance, considering the question “How tall is Mount Everest?,” the Wikipedia page on Mount Everest contains the answer but also a lot of confusing information such as elevations attained in various attempts to reach the summit and irrelevant information that might distract the model. A language model pretrained on web pages can generate a document that draws on the web but focuses on the question at hand. When fed to a separate language model along with the question, this model-generated reference text can make it easier for that model to answer questions correctly.\n\nHow it works:The authors used a pretrainedInstructGPT(175 billion parameters) to generate reference text related to questions in trivia question-answer datasets such asTriviaQA. They generated answers usingFiD(3 billion parameters), which they had fine-tuned on the dataset plus the reference text. (A given question may have more than one valid answer.)\n\nResults:The authors evaluated their fine-tuned FiD onTriviaQAaccording to the percentage of answers that exactly matched one of a list of correct answers. Provided with generated documents, FiD answered 71.6 percent of the questions correctly compared to 66.3 percent for FiD fine-tuned on TriviaQA and provided with text retrieved from Wikipedia usingDPR.\n\nYes, but:The authors’ approach performed best (74.3 percent) when it had access to both Wikipedia and the generated documents. While generated documents may be better than retrieved documents alone, they worked best together.\n\nWhy it matters:Good reference text substantially improves a language model’s question-answering ability. While a relevant Wikipedia entry is helpful, a document that’s directly related to the question is better — even if that document is a product of text generation.\n\nWe're thinking:Your teachers were right — Wikipedia isn’t the best source.\n\nNvidia introduces NeMo Guardrails, a software designed to regulate chatbotsThe open source software can help developers ensure that AI-powered text generators remain on topic and don't take unauthorized actions. (Bloomberg)\n\nThe Washington Post analyzed popular web-scale text datasetThe news outlet, with help from Allen Institute for AI, categorized the contents of Google's C4 dataset, scraped from 15 million websites. C4 has been used to train large language models including Google’s T5 and Facebook’s LLaMA. (The Washington Post)\n\nOpenAI introduced new ChatGPT privacy featureChatGPT users can opt to turn off their chat histories. This prevents OpenAI from using that data to train the company's models. (The Seattle Times)\n\nAI-generated product reviews are flooding the webFake product reviews have proliferated on sites like Yelp and Amazon. They’re easy to identify because they contain the phrase “as an AI language model,” exposing a new wave of spam. (The Verge)\n\nU.S. federal agencies issued a joint statement to battle AI bias and discriminationFour law enforcement agencies outlined a commitment to enforce responsible use of AI in areas such as lending and housing. (The Wall Street Journal)\n\nEU lawmakers agreed on a draft of the AI ActUnder the proposal, companies that develop generative AI tools would have to disclose the use of copyrighted material in their systems. EU member states must negotiate on details before the bill becomes law. (VentureBeat)Apple preparing an AI-powered health coaching serviceThe service will motivate users and suggest exercise, healthy eating, and sleep habits based on their Apple Watch data. (Bloomberg)\n\nPriceWaterhouseCoopers plans to invest $1 billion over three years in generative AIMicrosoft and OpenAI will help the accounting and consulting giant to automate its tax, audit, and consulting services. The investment will go into recruiting, training, and acquisitions. (The Wall Street Journal)\n\nElon Musk’s AI Company Comes into FocusThe Tesla CEO has hired researchers from DeepMind and Twitter. (The New York Times)\n\nInside Microsoft’s Development of Bing’s Chat CapabilitiesReporters told the story of how Microsoft worked with OpenAI to build Prometeus, the code name for the wedding of Bing search and the GPT-4 language model. (Wired) ​Runway launches generative video mobile appThe app gives users access to Gen-1, Runway’s video generator. It enables them to transform videos using text prompts, image, or style prompts. (TechCrunch)\n\nInside Meta’s Effort to Innovate in AIMeta’s ambitions in AI were weighed down by an initiative to design its own AI chip, insiders said. (Reuters)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/05/Screen-Shot-2023-05-02-at-12.35.13-PM-1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/05/HINTON.png",
      "https://dl-staging-website.ghost.io/content/images/2023/05/unnamed--58-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/05/The-Batch-ads-and-exclusive-banners--11-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/05/unnamed--59-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/05/unnamed--60-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-72/",
    "title": "issue 72",
    "date": "",
    "reading_time": "",
    "content": "Happy New Year! As we enter 2021, I want to share with you three wishes I have for AI in the upcoming year. I hope we can:\n\nI have great optimism for AI in 2021, and for the role you will play in it. I look forward to wrestling with these and other challenging problems with you.\n\nKeep learning!\n\nAndrew\n\nThe technology in our hands has the power to deliver vital services, grease the wheels of life and work, bring joy and delight, and create wealth that uplifts all humanity. Yet with it comes responsibility to distribute its benefits fairly and contain its unwanted impacts. How will we navigate these priorities in the coming year? Leaders of the AI community discuss their hopes in this special issue ofThe Batch.\n\nAs AI engineers, we have tools to design and build any technology-based solution we can dream of. But many AI developers don’t consider it their responsibility to address potential negative consequences as a part of this work. As a result, we continue to hear about inequities in the delivery of medical care, access to life-changing educational opportunities, financial assistance to people of meager means, and many other critical needs.In the coming year, I hope the AI community can reach a broad consensus on how to build ethical AI.The key, I believe, is training AI engineers to attend more fully to the potential consequences of their work. Typically, we’ll design a cool algorithm that matches faces in a database or generates chatbot conversations, and hand it off. Then we move on to the next project, oblivious to the fact that police departments are using our system to match mugshots to pencil sketches, or hate groups are using our chatbot to spread fear and lies.This is not how things work in other areas of engineering. If you’re a civil engineer and you want to build a bridge, you need to model the entire scenario. You don’t model a generic bridge, but a particular bridge that crosses a particular river in a particular town. You consider all the conditions that come with it, including cars, people, bicycles, strollers, and trains that might cross it, so you can design the right bridge given the circumstances.Similarly, we need to think about our work within the context of where it will be deployed and take responsibility for potential harms it may cause, just like we take responsibility for identifying and fixing the bugs in our code.Training AI engineers with this mindset can start by bringing real-world examples into the training environment, to show how the abstract concepts we learn play out in reality. In a course about word embeddings, for instance, we can look closely at their role in, say, hate speech on social media and how such messages bear on people of a particular gender, religion, or political affiliation — people just like us.And this training is not just for students. Practicing doctors and nurses are required to get continuing education credits to continue practicing. Why not in AI? Employers can make sure their developers get continuing education in ethical AI as a condition of ongoing employment.This may seem like a big change, but it could happen very quickly. Consider the response to Covid-19: Educational institutions and companies alike immediately implemented work-from-home policies that previously they had considered impossible. And one of the nice things about technology is that when the top players change, everyone else follows to avoid losing competitive advantage. All it takes is for a few leaders to set a new direction, and the entire field will shift.\n\nAyanna Howard directs the Human-Automation Systems Lab and chairs Interactive Computing at Georgia Institute of Technology.\n\nThe United States has been a leader in science and technology for decades, and all nations have benefitted from its innovations. But U.S. leadership in AI is not guaranteed. Should the country slip as a center of AI innovation and entrepreneurship, its contributions would be curtailed and the technology less likely to embody democratic values. I hope that 2021 will see a firm commitment from the U.S. federal government to support innovation in AI.The U.S. has excelled in science and technology largely because its ecosystem for innovation leverages contributions from academia, government, and industry. However, the emergence of AI has tipped the balance toward industry, largely because the three most important resources for AI research and development — computing power, data, and talent — are concentrated in a small number of companies. For instance, to train the large-scale language model GPT-3, OpenAI in partnership with Microsoft may have consumed compute resources worth $5 million to $10 million, according to oneanalysis. No U.S. university has ready access to this scale of computation.Equally critical for advancing AI are large amounts of data. The richest troves of data today are locked behind the walls of large companies. Lack of adequate compute and data handicaps academic researchers and accelerates the brain drain of top AI talent from academia to private companies.The year 2020 brought renewed federal support for universities and colleges. But more needs to be done. At theStanford Institute for Human-Centered Artificial Intelligence(HAI), which I co-direct with John Etchemendy, we have proposed aNational Research Cloud. This initiative would devote $1 billion to $10 billion per year over 10 years to recharge the partnership between academia, government, and industry. It would give U.S. academic researchers the compute and data they need to stay on the cutting edge, which in turn would attract and retain new crops of faculty and students, potentially reversing the current exodus of researchers from academia to industry.The fruits of this effort would be substantial. For instance, I’ve spent many years working onambient AI sensorsfor healthcare delivery. These devices could help seniors who need chronic disease management by enabling caregivers to remotely track treatments and results, potentially saving hundreds of thousands of lives annually in the U.S. Such technology has no borders: The innovation created at Stanford could help aging societies worldwide. Renewed ferment in AI research also could bring innovations to mitigate climate change, develop life-saving drugs, optimize food and water supplies, and improve operationswithin the government itself.We’re encouraged by the progress we’ve already seen toward the National Research Cloud. The U.S. Congress is considering bipartisanlegislationthat would establish a task force to study this goal. Meanwhile, agencies including the National Science Foundation and National Institutes of Health have issued calls for proposals for AI projects that such an initiative would support.AI is a tool, and a profoundly powerful one. But every tool is a double-edged sword, and the ways it’s applied inevitably reflect the values of its designers, developers, and implementers. Many challenges remain to ensure that AI is safe and fair, respects values fundamental to democratic societies, protects individual privacy, and benefits a wide swath of humanity. Invigorating the healthy public ecosystem of AI research is a critical part of this effort.Fei-Fei Li is the Sequoia Professor of Computer Science and Denning Co-Director of the Institute for Human-Centered Artificial Intelligence at Stanford. She is an elected member of the National Academy of Engineering and National Academy of Medicine.\n\nLook at the tip of a standard #2 pencil. Now, imagine performing over one trillion multiplication operations in the area of that pencil tip every second. This can be accomplished using today’s 7nm semiconductor technology. Combining this massive compute capability with deep neural networks in small, low-cost, battery-powered devices will help us address challenges from Covid-19 to Alzheimer’s disease.\n\nThe neural networks behind stand-out systems like AlphaGo, Alexa, GPT-3, and AlphaFold require this kind of computational power to do their magic. Normally they run on data-center servers, GPUs, and massive power supplies. But soon they’ll run on devices that consume less power than a single LED bulb on a strand of holiday lights.\n\nA new class of machine learning called TinyML is bringing these big, math-heavy neural networks to sensors, wearables, and phones. Neural networks rely heavily on multiplication, and emerging hardware implements multiplication using low-precision numbers (8 bits or fewer). This enables chip designers to build many more multipliers in a much smaller area and power envelope compared to the usual 32-bit, single-precision, floating-point multipliers. Research has shown that, in many real-world cases, using low-precision numbers inside neural networks has little to no impact on accuracy. This approach is poised to deliver ultra-efficient neural network inferencing wherever it’s needed most.Let me give one example. In addressing the Covid-19 pandemic, a major bottleneck developed around testing and identifying infected patients. Recent research suggests that a collection of neural networks trained on thousands of “forced cough” audio clips may be able to detect whether the cougher has the illness, even when the individual is asymptomatic. The neural networks used are computationally expensive, requiring trillions of multiplication operations per second. TinyML could run such cough-analyzing neural networks.My hope for AI in 2021 is that sophisticated healthcare applications  enabled by large neural networks running on small devices will usher in a new era of personalized healthcare that improves the lives of billions of people.\n\nMatthew Mattina leads the Machine Learning Research Lab at Arm as a distinguished engineer and senior director.\n\nIn 2021, I envision that the AI community will create more tools to unleash human creativity. AI will help people across the globe to communicate and express emotions and moods in their own unique ways.We have created machines that excel at logical tasks, capable of calculating at a scale and speed that far exceed human abilities. This accomplishment is evident in the recent successes of lunar probes, which have gone to the moon and returned with material for study. In our everyday lives, we use tools such as Microsoft Word and Excel to boost our productivity. However, there are some tasks at which humans continue to reign supreme — especially in the arts.A human brain has a logical side, or left brain, which is complemented by the creative and imaginative right brain. This creative side sparks many of the daily interactions that have allowed our species to flourish. We communicate with each other using language, conveying abstract concepts and expressing emotions. We also express ourselves artistically, creating music, art, dance, and design that hold meaning.Recent progress in AI, especially with deep learning techniques like generative adversarial networks and language models like GPT-3, has made it possible to synthesize realistic images and plausible texts almost out of nothing. XiaoIce.ai, a spin-out from Microsoft where I chair the board of directors, provides a chatbot that has shown human-like performance in generating poems, paintings and music. For example,XiaoIcehelped WeChat users to write more poems in a week than all the poems previously created in the history of China!Aspiring practitioners of painting, music, poetry, or dance, to name a few of many art forms, must train in their disciplines for years. It is said that one needs to practice 10,000 hours to reach perfection. Tools like Xiaolce can reduce that investment substantially, helping anyone to create more sophisticated creative and imaginative expressions.I look forward to seeing more AI creation tools in the coming year to help people express their artistic ideas and inspirations. AI has already shown that it can help humans to be more productive. Now let’s turn our attention to helping people to unlock their creativity.Harry Shum is chairman of xiaoice.ai and an adjunct professor at Tsinghua University.\n\nThe past year was the first in which general-purpose models became economically useful. GPT-3, in particular, demonstrated that large language models have surprising linguistic competence and the ability to perform a wide variety of useful tasks. I expect our models to continue to become more competent, so much so that the best models of 2021 will make the best models of 2020 look dull and simple-minded by comparison. This, in turn, will unlock applications that are difficult to imagine today.In 2021, language models will start to become aware of the visual world. Text alone can express a great deal of information about the world, but it is incomplete, because we live in a visual world as well. The next generation of models will be capable of editing and generating images in response to text input, and hopefully they’ll understand text better because of the many images they’ve seen.This ability to process text and images together should make models smarter. Humans are exposed to not only what they read but also what they see and hear. If you can expose models to data similar to those absorbed by humans, they should learn concepts in a way that’s more similar to humans. This is an aspiration — it has yet to be proven — but I’m hopeful that we’ll see something like it in 2021.And as we make these models smarter, we’ll also make them safer. GPT-3 is broadly competent, but it’s not as reliable as we’d like it to be. We want to give the model a task and get back output that doesn’t need to be checked or edited. At OpenAI, we’ve developed a new method called reinforcement learning from human feedback. It allows human judges to use reinforcement to guide the behavior of a model in ways we want, so we can amplify desirable behaviors and inhibit undesirable behaviors.GPT-3 and systems like it passively absorb information. They take the data at face value and internalize its correlations, which is a problem any time the training dataset contains examples of behaviors that we don’t want our models to imitate. When using reinforcement learning from human feedback, we compel the language model to exhibit a great variety of behaviors, and human judges provide feedback on whether a given behavior was desirable or undesirable. We’ve found that language models can learn very quickly from such feedback, allowing us to shape their behaviors quickly and precisely using a relatively modest number of human interactions.By exposing language models to both text and images, and by training them through interactions with a broad set of human judges, we see a path to models that are more powerful but also more trustworthy, and therefore become more useful to a greater number of people. That path offers exciting prospects in the coming year.Ilya Sutskever is a co-founder of OpenAI, where he is chief scientist.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/12/Screen20Shot202020-12-2920at205-2.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/12/Ayanna-Howar-Screen20Shot202020-12-2920at2010.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/09/Screen20Shot202020-12-2920at206.27.2020PM20copy--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/Screen20Shot202020-12-2220at209.47.3220AM20copy--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/Screen20Shot202020-12-2920at209.37.5920AM20copy--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/The20Batch20New20Year20Ilya20Sutskever20aspect20copy-1--1-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-110/",
    "title": "issue 110",
    "date": "",
    "reading_time": "",
    "content": "In my experience, the most sophisticated decision makers tend to be hypothesis-driven thinkers. They may be engineers solving a technical problem, product designers fulfilling a customer need, or entrepreneurs growing a business. They form a hypothesis about how to reach their goal and then work systematically to either validate or falsify it.\n\nSay you’re tuning a learning algorithm that estimates the health of corn stalks based on input from a tractor-mounted camera. (Many companies are developing products like this to help farmers make decisions about planting, weeding, or harvesting.) If your algorithm is doing poorly, how should you go about improving it?\n\nSome engineers tend to apply a one-size-fits-all rule. Someone who has experience improving algorithms by collecting more data may tend to gather more photos of corn stalks. When that doesn’t work, they may end up trying things more or less at random until they stumble on something that works.\n\nHypothesis-driven thinkers, on the other hand, have seen learning algorithms perform poorly for many different reasons. Based on that experience, they can make a list of hypotheses about what could be going wrong. Perhaps the algorithm does well in sunlight but performs poorly on overcast days, in which case the best solution indeed may be to collect — or synthesize — more images under cloudy skies. Or perhaps the camera’s lens is obscured by dust, or the hyperparameters are poorly tuned.\n\nHypothesis-driven thinkers see a variety of possibilities. They pick the most likely one and carry out error analysis or other tests to falsify or validate it. Then they apply the insights they've gained to devise a solution, choose a new hypothesis, or re-evaluate the range of hypotheses. In this way, they find a good solution efficiently.\n\nHow can you gain skill in building hypotheses?\n\nHypothesis-driven thinking is helpful not only in developing AI systems, but also in building products and businesses. Perhaps you’ve identified a market need, a concept to fulfill that need, and a sales strategy to get the product into the customers’ hands. Rather than rushing ahead and assuming that everything will work out, you might question key assumptions behind the hypothesis systematically and pinpoint those that are unproven or incorrect. If you discover early on that the concept is flawed (say, because it requires AI technology that hasn’t been invented yet), you’ll have more time to pivot and find an alternative.\n\nKeep learning!Andrew\n\nA remote sniper used an automated system to take out a human target located thousands of miles away.What happened:The Israeli intelligence agency Mossad used an AI-assisted rifle in the November killing of Iran’s chief nuclear scientist, according toThe New York Times.How it worked:The agency killed Mohsen Fakhrizadeh, whom it considered a key player in Iran’s covert nuclear weapons program, as he was driving near Tehran.\n\nBehind the news:Scoresof military weapon systems around the world use AI to assist in targeting and other functions.\n\nWhy it matters:Automated weapons have a longhistory. This AI-targeted shooting, however, opens a new, low-risk avenue for well funded intelligence agencies to kill opponents.We’re thinking:We find AI-assisted killing deeply disturbing even as we acknowledge that countries need ways to protect themselves. We believe that AI can be a tool for advancing democracy and human rights, and that the AI community should take part in drawing clear boundaries for acceptable machine behavior.\n\nWalmart aims to deliver goods via self-driving vehicles this year.What’s new:The retail giant will test autonomous delivery in three U.S. cities.Cars built by Ford and piloted by Argo AIwill ferry merchandise directly to the customer’s front steps.How it works:The service initially will belimitedto parts of Austin, Miami, and Washington, D.C.\n\nBehind the news:Walmart has been testing automated delivery services using technology fromCruise,Gatik, andWaymo.Nuro, which also has partnered with Walmart, focuses on autonomous delivery on the grounds that it lowers requirements for riding comfort and permits slower, and thus safer, driving.Why it matters:Although self-driving vehicles aren’t ready for widespread use, the partnership between one of the world’s largest retailers and one of the world’s biggest auto makers signals potential for near-term commercial applications.We’re thinking:Fully self-driving cars likely will reach the market through a vertical niche, which is easier than building vehicles that can handle all circumstances. Some companies focus on trucking, others on local shuttles, still others on transportation within constrained environments such as ports or campuses. Although self-driving has taken longer than expected to come to fruition, we remain optimistic that experiments like these will bear fruit.\n\nDo you have days when you feel like you’re stalled on the road to becoming a machine learning professional?Join usfor a live event on September 29, 2021, as Workday senior director of machine learning Madhura Dudhgaonkar shares real-world insights for crafting your career!\n\nThe process known as image-to-image style transfer — mapping, say, the character of a painting’s brushstrokes onto a photo — can render inconsistent results. When they apply the styles of different artists to the same target content, they may produce similar-looking pictures. Conversely, when they apply the same style to different targets, such as successive video frames, they may produce images with unrelated shapes and colors. A new approach aims to address these issues.What’s new:Min Jin Chong and David Forsyth at University of Illinois at Urbana-Champaign proposedGANs N’ Roses, a style transfer system designed to maintain the distinctive qualities of input styles and contents.Key insight:Earlier style transfer systems falter because they don't clearly differentiate style from content. Style can be defined as whatever doesn’t change when an image undergoes common data-augmentation techniques such as scaling and rotation. Content can be defined as whatever is changed by such operations. A loss function that reflects these principles should produce more consistent results.How it works:Like other generative adversarial networks, GANs N’ Roses includes a discriminator that tries to distinguish synthetic anime images from actual artworks and a generator that aims to fool the discriminator. The architecture is aStyleGAN2with a modified version ofCycleGAN’s loss function. The authors trained it to transfer anime styles to portrait photos usingselfie2anime, a collection of unmatched selfies and anime faces. The authors created batches of seven anime faces and seven augmented versions of a single selfie (flipped, rotated, scaled, and the like).\n\nResults:Qualitatively, the system translated different selfies into corresponding anime poses and face sizes, and different styles into a variety of colors, hair styles, and eye sizes. Moreover, without training the networks on video, the authors rendered a series of consecutive video frames. Subjectively, those videos were smooth, while those produced by CouncilGAN’s frames showed inconsistent colors and hairstyles. In quantitative evaluations comparingFrechet Inception Distance(FID), a measure of similarity between real and generated images in which lower is better, GANs N’ Roses achieved 34.4 FID whileCouncilGANachieved 38.1 FID. ComparingLearned Perceptual Image Patch Similarity(LPIPS), a measure of diversity across styles in which higher is better, GANs N’ Roses scored .505 LPIPS while CouncilGAN scored .430 LPIPS.Why it matters:If style transfer is cool, better style transfer is cooler. The ability to isolate style and content — and thus to change content while keeping style consistent — is a precondition for extending style transfer to video.We’re thinking:The next frontier: Neural networks that not only know the difference between style and content but also have good taste.\n\nHuman rights officials called for limits on some uses of AI.\n\nWhat’s new:Michelle Bachelet, the UN High Commissioner for Human Rights,appealedto the organization’s member states to suspend certain systems until safety protocols are established and to ban those she believes may infringe on human rights. Her call coincided with areportfrom the UN Human Rights Council warning that automated systems for policing, healthcare, and content recommendation threaten rights like privacy, free expression, and freedom of movement and could limit access to health care and education.Clear and present dangers:The report highlights documented hazards such as algorithmic bias, intrusive surveillance, and lack of transparency in AI. It argues that governments and businesses often deploy AI products without determining whether they pose risks to human rights. The authors call for:\n\nBehind the news:The UN report is the latest high-level call for rules that rein in AI’s potential for harm.\n\nWhy it matters:The UN can’t force anyone to heed its recommendations. But strong statements like Bachelet’s backed by well reported data can bring attention and public pressure to bear on the intersection of AI and human rights.We’re thinking:Voluntary restrictions and finger-wagging reports are no substitute for concrete legal limits. Meanwhile, the AI community — each and every one of us — can push toward more beneficial uses.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/09/Screen-Shot-2021-09-21-at-5.18.58-PM-copy.png",
      "https://dl-staging-website.ghost.io/content/images/2021/09/Screen-Shot-2021-09-20-at-1.04.16-PM-SIZED.png",
      "https://dl-staging-website.ghost.io/content/images/2021/09/WALMART.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/09/AI-X-9.29_The-Batch-Image.png",
      "https://dl-staging-website.ghost.io/content/images/2021/09/GANSNROSES--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/09/UN.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-246/",
    "title": "issue 246",
    "date": "",
    "reading_time": "",
    "content": "Much has been said about many companies’ desire for more compute (as well as data) to train larger foundation models. I think it’s under-appreciated that we have nowhere near enough compute available for inference on foundation models as well.\n\nYears ago, when I was leading teams at Google, Baidu, and Stanford that focused on scaling up deep learning algorithms, many semiconductor manufacturers, data center operators, and academic researchers asked me whether I felt that AI technology would continue to make good use of more compute if they kept on delivering it. For many normal desktop processing workloads, like running a web browser or a text editor, having a faster CPU doesn’t help that much beyond a certain point. So do we really need faster and faster AI processors to train larger and larger models? Each time, I confidently replied “yes!” and encouraged them to keep scaling up compute. (Sometimes, I added half-jokingly that I had never met a machine learning engineer who felt like they had enough compute. 😀)\n\nFortunately, this prediction has been right so far. However, beyond training, I believe we are also far from exhausting the benefits of faster and higher volumes of inference.\n\nToday, a lot of LLM output is primarily for human consumption. A human might read around 250 words per minute, which is around 6 tokens per second (250 words/min / (0.75 words/token) / (60 secs/min)). So it might initially seem like there’s little value to generating tokens much faster than this.\n\nBut in anagentic workflow, an LLM might be prompted repeatedly to reflect on and improve its output, use tools, plan and execute sequences of steps, or implement multiple agents that collaborate with each other. In such settings, we might easily generate hundreds of thousands of tokens or more before showing any output to a user. This makes fast token generation very desirable and makes slower generation a bottleneck to taking better advantage of existing foundation models.\n\nThat’s why I’m excited about the work of companies likeGroq, which can generate hundreds of tokens per second. Recently,SambaNovapublished an impressive demo that hit hundreds of tokens per second.\n\nIncidentally, faster, cheaper token generation will also help make running evaluations (evals), a step that can be slow and expensive today since it typically involves iterating over many examples, more palatable. Having better evals will help many developers with the process of tuning models to improve their performance.\n\nFortunately, it appears that both training and inference are rapidly becoming cheaper. I recently spoke with Cathie Wood and Charles Roberts of the investment firm ARK, which is famous for its bullish predictions on tech. Theyestimatethat AI training costs are falling at 75% a year. If they are right, a foundation model that costs $100M to train this year might cost only $25M to train next year. Further, they report that for “enterprise scale use cases, inference costs seem to be falling at an annual rate of ~86%, even faster than training costs.”\n\nI don’t know how accurate these specific predictions will turn out to be, but with improvements in both semiconductors and algorithms, I do see training and inference costs falling rapidly. This will be good for application builders and help AI agentic workflows lift off.\n\nKeep learning!\n\nAndrew\n\nP.S. New short course with Mistral AI! Mistral’s open-source Mixtral 8x7B model uses a mixture of experts (MoE) architecture. Unlike a standard transformer, MoE uses multiple expert feed-forward networks with a gating network that selects a number of experts at inference time. This enables MoE to match the performance of larger models but with faster inference. Mixtral 8x7B has 46.7B parameters but activates only 12.9B at inference time to predict the next token. In “Getting Started with Mistral,” taught by Sophia Yang, you’ll explore Mistral’s open-source (Mistral 7B, Mixtral 8x7B) and commercial models, learn about function calling for tool use with Mistral, and build a Mistral-powered chat interface that can reference external documents. Please sign uphere!\n\nA new breed of audio generator produces synthetic performances of songs in a variety of popular styles.\n\nWhat’s new:Udiolauncheda web-based, text-to-song generator that creates songs in styles from barbershop to heavy metal.Suno, which debuted its service late last year with similar capabilities, upgraded to its offering.\n\nHow it works:Both services take text prompts and generate full-band productions complete with lyrics, vocals, and instrumental solos, two separate generations per prompt. Users can generate lyrics to order or upload their own words, and they can download, share, and/or post the results for others to hear. Leaderboards rank outputs according to plays and likes.\n\nBehind the news:Most earlier text-to-music generators were designed to produce relatively free-form instrumental compositions rather than songs with structured verses, choruses, and vocals. Released earlier this month,Stable Audio 2generates instrumental tracks up to three minutes long that have distinct beginnings, middles, and endings. Users can also upload audio tracks and use Stable Audio 2.0 to modify them.\n\nYes, but:Like text-to-image generators circa last year, current text-to-music models offer little ability to steer their output. They don’t respond consistently to basic musical terminology such as “tempo” and “harmony,” and requesting a generic style like “pop” can summon a variety of subgenres from the last 50 years of popular music.\n\nWhy it matters:With the advent of text-to-music models that produce credible songs, audio generation seems primed for a Midjourney moment, when the public realizes that it can produce customized music at the drop of a prompt. Already Udio’s and Suno’s websites are full of whimsical paeans to users’ pets and hobbies. The technology has clear implications for professional performers and producers, who, regrettably, have little choice but toadaptto increasing automation. But for now fans have fun, new toys to play with.\n\nWe’re thinking:You can dance to these algo-rhythms!\n\nHow well do large language models respond to professional-level queries in various industry domains? A new company aims to find out.\n\nWhat’s new:Vals.AI, an independent model testing service, developed benchmarks that rank large language models’ performance of tasks associated with income taxes, corporate finance, and contract law; it also maintains a pre-existing legal benchmark. Open AI’s GPT-4 and Anthropic’s Claude 3 Opus did especially well in recent tests.\n\nHow it works:Vals AI hosts leaderboards that compare the performance of several popular large language models (LLMs) with respect to accuracy, cost, and speed, along with with analysis of the results. The company worked with independent experts to develop multiple-choice and open-ended questions in industrial domains. The datasets are not publicly available.\n\nResults:Among 15 models, GPT-4 and Claude 3 Opus dominated Vals.AI’s leaderboards as of April 11, 2024. GPT-4 topped CorpFin and TaxEval, correctly answering 64.8 and 54.5 percent of questions, respectively. Claud 3 Opus narrowly beat GPT-4 on ContractLaw and LegalBench, achieving 74.0 and 77.7 percent, respectively. The smaller Claude 3 Sonnet took third place in ContractLaw, CorpFin, and TaxEval with 67.6, 61.4, and 37.1 percent. Google’s Gemini Pro 1.0 took third place in LegalBench with 73.6 percent.\n\nBehind the news:Many practitioners infinanceandlawuse LLMs in applications that range from processing documents topredicting interest rates. However, LLM output in such applications requires oversight. In 2023, a New York state judgereprimandeda lawyer for submitting an AI-generated brief that referred to fictitious cases.\n\nWhy it matters:Typical AI benchmarks are designed to evaluate general knowledge and cognitive abilities. Many developers would like to measure more directly performance in real-world business contexts, where specialized knowledge may come into play.\n\nWe’re thinking:Open benchmarks can benefit from public scrutiny, and they’re available to all developers. However, they can be abused when developers cherry-pick benchmarks on which their models perform especially well. Moreover, they may find their way into training sets, making for unfair comparisons. Independent testing on proprietary benchmarks is one way to address these issues.\n\nJoin “Getting Started with Mistral” and access Mistral AI’s open source and commercial models via API calls. Learn to select the right model for your use case and get hands-on with features like JSON mode, function calling, and effective prompting techniques.Enroll for free!\n\nManufacturers are embracing AI even as they struggle to find the talent and data required.\n\nWhat’s new:The market-research arm ofMIT Technology Reviewsurveyedmanufacturers’ use of AI in engineering, design, procurement, and production. All respondents were at least experimenting with AI, and many expect to launch their first deployments in the next year or two. Microsoft sponsored the research.\n\nHow it works:The authors interviewed executives at 300 manufacturers in aerospace, automotive, chemicals, electronics, and heavy equipment. All were either applying or considering AI in product design or factory operations.\n\nBehind the news:Manufacturers are using AI to helpdesign products,visually inspect goods, andmaintain equipment. The field has attracted major players: Last year, Microsoft and Siemenslauncheda pilot of Industrial Copilot, which enables users to interact in natural language with software that drives assembly lines.\n\nWhy it matters:Manufacturers want to use AI, but many face obstacles of talent and data. That spells opportunities for budding practitioners as well as for manufacturers that lack infrastructure for collecting and managing data.\n\nWe’re thinking:One key to successful implementation of AI in manufacturing is tailoring systems to the unique circumstances of each individual facility. The highly heterogeneous tasks, equipment, and surroundings in different factories mean that one model doesn’t fit all. Developers who can solve this long-tail problem stand to reap rewards.\n\nVideo diffusion provides a new basis for generating 3D models.What's new:Vikram Voleti, Chun-Han Yao, Mark Boss, Varun Jampani, and colleagues at Stability AI produced amethodthat generates a 3D model from a single image based on Stability’s video diffusion model. You can see its outputhere.Key insight:The approach known as aNeural Radiance Field(NeRF) learns to create a 3D model from images of the same object shot at various angles. Given a single image of an object, a video diffusion model can learn to generate videos that orbit around it. The frames from such orbital videos give NeRF the information it needs to produce a 3D model.How it works:To generate an image, the authors took one step before and two steps during inference. Before inference: Learn to generate an orbital video. During inference: (i) Train a NeRF model on an orbital video. (ii) Improve the 3D model using diffusion followingDreamFusion.\n\nResults:The authors produced 3D models from images of 50 objects inGSO, a 3D object dataset of scanned household items. They compared their 3D models to those produced by other methods includingEscherNet, a method that uses an image diffusion model to generate images of an object from different angles that are used to train apair of vanilla neural networksto produce a 3D model. Evaluated according to Chamfer distance, a measure of the distance between the points on the ground truth and generated 3D models (lower is better), their method achieved .024, while EscherNet achieved .042.\n\nWhy it matters:Video diffusion models must generate different views of the same object, so they require a greater understanding of 3D objects than image diffusion models, which need to generate only one view at a time. Upgrading from an image diffusion model to a video diffusion model makes for better 3D object generation.We’re thinking:Building 3D models used to be difficult, but with models like this, it's becoming less of a mesh.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/04/TOKEN-GENERATION.png",
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-24T134335.199.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/04/VALS-4Leaderboards_1200px--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/04/The-Batch-ads-and-exclusive-banners---2024-04-23T091059.531.png",
      "https://dl-staging-website.ghost.io/content/images/2024/04/MANUFACTURING_14-SecHolds_1200px--1---1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-24T134817.631.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-167/",
    "title": "issue 167",
    "date": "",
    "reading_time": "",
    "content": "So is prompt engineering an important direction for AI, or is it a hack?\n\nHere’s how we got to this point:\n\nSome people have predicted that prompt engineering jobs would be plentiful in the future. I do believe that text prompts will be an important way to tell machines what we want — after all, they’re a dominant way to tell other humans what we want. But I think that prompt engineering will be only a small piece of the puzzle, and breathless predictions aboutthe rise of professional prompt engineersare missing the full picture.\n\nJust as a TV has switches that allow you to precisely control the brightness and contrast of the image — which is more convenient than trying to use language to describe the image quality you want — I look forward to a user interface (UI) that enables us to tell computers what we want in a more intuitive and controllable way.\n\nTake speech synthesis (also called text-to-speech). Researchers have developed systems that allow users to specify which part of a sentence should be spoken with what emotion. Virtual knobs allow you to dial up or down the degree of different emotions. This provides fine control over the output that would be difficult to express in language. By examining an output and then fine-tuning the controls, you can iteratively improve the output until you get the effect you want.\n\nSo, while I expect text prompts to remain an important part of how we communicate with image generators, I look forward to more efficient and understandable ways for us to control their output. For example, could a set of virtual knobs enable you to generate an image that is 30 percent in the style of Studio Ghibli and 70 percent the style of Disney? Drawing sketches is another good way to communicate, and I’m excited byimg-to-imgUIs that help turn a sketch into a drawing.\n\nLikewise, controlling large language models remains an important problem. If you want to generate empathetic, concise, or some other type of prose, is there an easier way than searching (sometimes haphazardly) among different prompts until you chance upon a good one?\n\nWhen I’m just playing with these models, I find prompt engineering a creative and fun activity; but when I’m trying to get to a specific result, I find it frustratingly opaque. Text prompts are good at specifying a loose concept such as “a picture of a panda eating bamboo,” but new UIs will make it easier to get the results we want. And this will help open up generative algorithms to even more applications; say, text editors that can adjust a piece of writing to a specific style, or graphics editors that can make images that look a certain way.\n\nLots of exciting research ahead! I look forward to UIs that complement writing text prompts.\n\nKeep learning!\n\nAndrew\n\nNew U.S. restrictions on chip sales aim to hamper China’s AI efforts.\n\nWhat’s new:The U.S. governmentpublishedsweeping limits on sales of processors that involve U.S. designs and technology to Chinese businesses. U.S. officialsstatedthat the restrictions are meant to prevent China from militarizing AI.\n\nNew rules:The rules block sales of certain processors as well as U.S.-made equipment used to design and manufacture them. This includes high-end graphics processing units (GPUs) and other processors optimized for machine learning.\n\nChina’s response:A spokesperson for China’s foreign ministryaccusedthe U.S. of abusing export-control measures to target Chinese firms, stating that it would hinder global cooperation and supply chains.Behind the news:The restrictions initially came to light in September, when Nvidia and AMD independentlyalertedshareholders that the U.S. had imposed controls on their most advanced products. However, their details became publicly available only last week. They represent a significant escalation of earlier U.S. efforts to thwart China’s ambitions in advanced technology.\n\nWhy it matters:China has announced itsambitionto become the global leader in AI by 2030, and this requires access to cutting-edge processing power. The most advanced chips are manufactured in Taiwan and South Korea using chip-fabrication equipment made by U.S. companies, and the leading chip designers and makers of chip-design software reside in the U.S. This gives U.S. authorities a tight grip on other countries’ ability to buy and make chips. China’s effort to build domestic capacity to produce advanced semiconductors — which are hampered by the sheer difficulty and expense of etching features on silicon measured in nanometers  — now faces additional hardware, software, business, and talent hurdles.\n\nWe’re thinking:International cooperation has been essential to recent progress in AI. As barriers rise between the U.S. and China, the AI community must navigate a world where geography will have a much bigger impact on access to ideas and resources.\n\nThe next green revolution may be happening in the server room.\n\nWhat’s new:Microsoftopen-sourceda set of AI tools designed to help farmers cut costs and improve yields.\n\nHow it works:FarmVibes-AIincludes systems that analyze overhead imagery and sensor data to guide farm operations.\n\nBehind the news:Nonprofits and academic institutions provide other open-source AI systems to increase food production in collaboration with large agribusiness firms, independent farmers, and rural communities.\n\nWhy it matters:The emerging practice ofprecision agriculture, which seeks to take into account not only entire fields but also local conditions down to the level of individual plants, could help farmers sow seeds, grow crops, fight pests, and harvest produce more efficiently. Off-the-shelf systems may not serve farmers who work in different parts of the world or grow niche crops. Open-source projects can expand their options effectively and inexpensively.\n\nWe’re thinking:Farmers tend to welcome innovations that improve yields and cut costs. They’re also famously self-sufficient, performing repairs and installing upgrades to their equipment. As self-driving tractors and precision-ag systems take root, they’re great candidates to become early adopters of industry-focusedplatformsthat make it easy for anyone to build useful AI applications.\n\nLooking for a career that inspires you? Break into AI!TheMachine Learning Specializationteaches foundational AI concepts through an intuitive visual approach. This beginner-friendly program, created by Andrew Ng and Stanford Online, makes it easier than ever to start your AI career.Learn more\n\nJoe Rogan meets Steve Jobs in an AI-generated podcast.\n\nWhat’s new:For the debut episode of a new podcast series, Play.ht synthesized a 19-minute interview between the rock-star podcaster and late Apple CEO. You can hear ithereand propose computer-generated participants in future episodeshere.\n\nHow it works:The Dubai-based startup created the episode using text generation and voice cloning.\n\nBehind the news:Rogan was also the subject of an early experiment in voice cloning. In 2019, Toronto-based Dessareleasedersatz Rogan audio clips — the first of a parade of fake celebrity voices.\n\nWhy it matters:The declamation is occasionally stilted and the script meandering (with occasional lapses into incoherence), but the rapid progress of generative audio combined with the entertainment world’s appetite for novelty suggests that satisfying synthetic productions may not be far off.We’re thinking:How long before we can produceHeroes of Deep Learningwithout actually talking with any of the heroes of deep learning?\n\nSentence pairs that have equivalent meanings in different languages — typically used to train machine translation systems — have been available in sufficient quantities for only around 100 languages. New work doubled that number and produced a more capable model.\n\nWhat’s new:Marta R. Costa-jussà and colleagues at Meta, Johns Hopkins, and UC Berkeley developed an automated process for scraping multilingual sentence pairs from the web. They releasedNo Language Left Behind(NLLB-200), a machine translation model that handles 200 languages. They also released themodels, code, and dataused to build it.\n\nKey insight:The web is full of text in various languages, including sentences that have the same meaning in different languages. For instance, unrelated pages in different languages may say the equivalent of, “Manchester United defeated Melbourne in yesterday’s match,” or “A long time ago in a galaxy far, far away.” An automated system can recognize such parallel sentences by learning to produce similar representations of sentences that have similar meaning regardless of their language. A teacher/student arrangement — with a multilingual teacher trained on languages with plentiful data to produce embeddings, and a separate monolingual student for each language scraped from the web — can align representations produced by the students.\n\nHow they built the dataset:The authors identified languages in text scraped from the web, trained a teacher model on pre-existing multilingual data, and used it to train a student model to produce similar representations for similar meanings in the web text.\n\nHow they built the translator:NLLB-200 is a transformer encoder-decoder that comprises 54.5 billion parameters.\n\nResults:The authors’ NLLB-200 model achieved 24.0 averagespBLEUacross all 202 languages, while the earlierDeltaLMachieved a 101-language average 16.7 spBLEU (which measures the overlap of word fragments between machine translations and ground truth, higher is better). A sparse NLLB-200 that used MoE rather than fully connected layers generally performed better than a dense version. For example, evaluated on Akan, a language spoken in Ghana for which little training data was available, the sparse model scored 36.2chrF, while a dense version scored 35.6 chrF (which measures overlapping groups of consecutive characters between machine translations and ground truth, higher is better). NLLB-200 performed inconsistently compared to bilingual models: It achieved 36.2 chrF compared to an English-to-Akan model’s 16.8 chrF, but 51.4 chrF compared to an English-to-Gujarati model’s 51.7 chrF. A possible explanation: Languages that are dissimilar to other languages in the training data may not benefit as much from multilingual training.\n\nWhy it matters:Faced with an apparent scarcity of data, the authors extracted it from the web. The data didn’t need to be perfect: To compensate for flaws such as typographical and grammatical errors, the model learned to convert its own translations — of flawed sentences but presumably many more correct ones — into good sentences.\n\nWe’re thinking:University of Texas machine learning professor Raymond Mooneysaid, “You can’t cram the meaning of a whole %&!$# sentence into a single $&!#* vector.” Apparently these researchers did it!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/10/unnamed--2--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/10/CHIPS.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/10/unnamed--4-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/10/MLS-TheBatch-ad.png",
      "https://dl-staging-website.ghost.io/content/images/2022/10/ROGAN.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/10/unnamed--5-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-48/",
    "title": "issue 48",
    "date": "",
    "reading_time": "",
    "content": "Last week, I wrote about the U.S. Immigration and Customs Enforcement (ICE) policy that would have forced international students to leave the country if their university went fully online to manage the risk of Covid-19. This sudden change in the rules for student visas had students and institutions alike scrambling to figure out ways to comply.\n\nSocial media erupted in protest as students, parents, teachers, and administrators expressed their concerns. Harvard and MIT sued to block the policy. Attorneys general in at least 18 states brought lawsuits as well.\n\nYesterday, the government rescinded the policy, allowing international students to remain in the U.S. even if they take all their courses online. I am thrilled!\n\nICE’s retreat is an important reminder that our voices can make a difference. I have little doubt that the public outcry helped motivate the universities to sue and the government to backtrack.\n\nI believe we all have a responsibility to speak out against injustice — respectfully and with cogent arguments, not “flame wars.” Even if each individual voice is just one among many, collectively we can make a huge impact.\n\nSpeaking out is especially important for the AI community as we grapple with difficult issues of bias, privacy, surveillance, and disinformation. We need every voice — including yours — to fulfill AI’s promise for the benefit of all people.\n\nKeep learning!\n\nAndrew\n\nComputer vision has been learning how to spot manufacturing flaws. The pandemic is accelerating that education.What’s happening:Companies like Instrumental and Elementary are making AI-powered cameras that automate the spotting of damaged or badly assembled products on factory assembly lines,Wiredreports. (For the record, deeplearning.ai’s sister company Landing AI is, too.)How it works:Instrumental’s quality-control system first learns to recognize components in their ideal state and then to identify defects. It can spot faulty screws, disfigured circuit boards, and flaws in the protective coating on smartphone screens.\n\nComing soon:Elementary plans to install robotic cameras in a U.S. Toyota plant. Workers will place a completed part beneath the camera for inspection, then press a button to indicate whether they agree with the robot’s assessment to fine-tune the model.Behind the news:Omron,Cognex, andUSS Visionhave sold non-neural inspection systems for decades. Neural networks are making their way into the field as engineers develop techniques for learning what flaws look like from small numbers of examples.Why it matters:Earlier automated inspection systems use hand-coded rules to identify specific flaws. Machine learning promises to be more adaptable and quicker to deploy. That could speed up assembly lines and cut manufacturing costs.We’re thinking:The ability to learn from small amounts of data is the key to many applications of deep learning that are still beyond reach. We look forward to continued progress in this area.\n\nWhy search for “a cotton dress shirt with button-down collar, breast pockets, barrel cuffs, scooped hem, and tortoise shell buttons in grey” when a photo and the words “that shirt, but grey” will do the trick? A new network understands the image-text combo. (This is the second of three papers presented by Amazon at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). We’ll cover the third one next time.)What’s new:Online stores offer all kinds of clothing, but search engines may suggest items of a different color or style than you want.Visiolinguistic Attention Learning, developed by Yanbei Chen with researchers at Queen Mary University of London and Amazon, hones product searches based on text input from shoppers.Key insights:If you can create a picture that approximates the ideal product, you can search for similar images. Generating realistic images is hard, but comparing extracted features is much easier.How it works:VAL learns to modify features extracted from a product image according to text input such as “I want it to have a light floral pattern.” Then it searches for other products with features similar to the modified product features.\n\nResults:The researchers put VAL head-to-head againstTIRG, the previous state of the art in image search with text feedback using theFashion200Kdataset of garment photos with text descriptions. VAL achieved 53.8 percent recall of the top 10 recommended products, the fraction of search results that are relevant, compared to TIRG’s 43.7 percent. VAL also outperformed TIRG on theShoesandFashionIQdatasets.Why it matters:VAL provides a new method for interpreting images and text together, a useful skill in areas where either one alone is ambiguous.We’re thinking:We’ll take theblue shirt!\n\nWe’ve launched our much-anticipated Natural Language Processing Specialization! Courses 1 and 2 are live on Coursera.Enroll now\n\nWhich headline was written by a computer?A:FIFA to Decide on 2022 World Cup in MarchB:Decision in March on 48-team 2022 World Cup, Says InfantinoWhat’s new:Researchers at Primer, an AI-driven document analysis company,introducedan automatic headline generator. In an appealing twist, some articles that human publishers had tried to tart up with clickbait — for instance,You’ll Never Guess Which U.S. Counties Grew the Fastest— the model gave a sensible, informative headline:MacKenzie County in North Dakota Had Highest Population Growth in Entire U.S.How it works:A headline is a very short document summary. Summarizers come in two flavors. Extractive models use only sentences or phrases from the text itself, building summaries that are closely related to the source but may be narrow or off-point. Abstractive models create new text based on an independent dictionary, synthesizing fresh but potentially confused summaries. Primer developed ahybrid modelthat generates abstractive headlines using vocabulary found in the document.\n\nResults:Human evaluators each read 100 news stories and graded two accompanying headlines, one written by a person and the other by the model. The computer-generated headlines scored slightly better overall. The model performed best on short-form journalism but stumbled on longer articles, probably because key information in longer items is more spread out.Behind the News:Earlierheadline generation methods mostly use an encoder-decoder to produce abstractive results. Unlike the new model, the encoder-decoder approach can generate any possible headline but risks poor grammar, factual inaccuracy, and general incoherence.Why it matters:Imagine a world without clickbait!We’re wondering:The computer wrote option A. Did you guess correctly?\n\nHow do you control a video game that generates a host of unique monsters for every match? With machine learning, naturally.What’s new:The otherworldly creatures inSource of Madnesslearn how to target players through reinforcement learning, the developers toldThe Batch.How it works:Players battle an infestation of fiends in a procedurally generated, side-scrolling wasteland.\n\nBehind the news:Most commercial titles use rules-based systems to control non-player characters. But some games have had success experimenting with neural networks.\n\nWhy it matters:Machine learning is infiltrating games as developers seek to build virtual worlds as variable and surprising as the real one.\n\nWe’re thinking:To all monsters, we say: keep learning!",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter206.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/QC1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/VAL.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1-2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Headline.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Madness204.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-114/",
    "title": "issue 114",
    "date": "",
    "reading_time": "",
    "content": "In June, Iannouncedthe first Data-centric AI Competition. The deadline for submissions was in early September, and today I’m thrilled to announce the winners!A total of 489 individuals and teams submitted 2,458 unique datasets. By improving the data alone — not the model architecture, which was fixed — many contestants were able to improve on the baseline performance of 64.4% by over 20%. The winners in the Best Performance category achieved between 86.034% and 86.405%. The winners in the Most Innovative category, as well as the honorable mentions, achieved high performance using novel approaches.Congratulations to Divakar Roy, Team Innotescus, and Team Synaptic-AnN, who took the top three spots for Best Performance. Congratulations also to Mohammad Motamedi, Johnson Kuan, and Team GoDataDriven, winners of the Most Innovative category. Pierre-Louis Bescond and Team KAIST-AIPRLab earned honorable mentions. I couldn’t be more proud of you all.\n\nYou can learn more about their approacheshere. I hope you’ll apply these ideas to your own work.\n\nThe winners joined me at a private roundtable event to discuss how to grow the data-centric AI movement. I was surprised to learn that almost all of them — some of whom have been involved in AI for a long time, and some of whom have little AI background — already have seen positive effects of data-centric techniques in their own work.We chatted about the potential benefits of data-centric AI development to entrepreneurs and startups that may not have access to large datasets, and how it opens machine learning to non-engineers who, although they may not have the skills to build models, can make important contributions by gathering and refining data.\n\nWe also discussed how working with data is often wrongly viewed as the boring part of machine learning even though, in reality, it’s a critical aspect of any project. I was reminded that, 10 years ago, working with neural networks was viewed in a similar light — people were more interested in hand-engineering features and viewed neural networks as uninteresting. I’m optimistic that the AI community before long will take as much interest in systematically improving data as architecting models.Thank you to all the participants for helping build a foundation for future data-centric AI benchmarks. I hope this competition spurs you to innovate further systematic approaches to improving data. And I hope you’ll compete again in future data-centric AI challenges!\n\nKeep learning,\n\nAndrew\n\nAs governments worldwide mull their AI strategies and policies, the Biden administration called for a “bill of rights” to mitigate adverse consequences.What’s new:Top advisors to the U.S. presidentannounceda plan to issue rules that would protect U.S. citizens against AI-powered surveillance, discrimination, and other kinds of harm. The dispatch coincided with acall for public commenton how to regulate systems like face recognition used to scan airline passengers, activity monitors that track employee productivity, and classroom management tools that alert teachers when their students tune out.What they said:U.S. Office of Science and Technology Policy director Eric Lander and deputy director Alondra Nelson argue that certain AI applications threaten individual liberty and outline limits:\n\nBehind the news:Momentum is building worldwide to restrict certain uses of AI. Last week, the European Parliamentpasseda non-binding ban on law enforcement’s use of face recognition and a moratorium on predictive policing algorithms except for serious offenses like child exploitation, financial crimes, and terrorism. Less than a month before, the UN’s human rights commissioner called on member states to institute restrictions. In August, China’s cyberspace administrationannouncedforthcoming rules to limit the influence of recommendation algorithms.Why it matters:An AI bill of rights is notional for the time being. But it could serve as a blueprint for national legislation, and it certainly would influence some states. And government funding could be a powerful carrot and stick: The U.S.paid$1.9 billion in contracts to AI companies between 2018 and 2020, and many state and local governments rely on federal money for law enforcement, health care, and other services where the use of AI is both growing and controversial.We’re thinking:We support regulations to help AI maximize benefit and minimize harm, and the president’s endorsement is an important step. That said, we wonder: Would an Electricity Bill of Rights have made sense 100 years ago? We urge regulators to focus not on AI as a whole but on applications in vertical areas such as surveillance, advertising, consumer software, health care, law enforcement, social media, and many other areas. Meanwhile, the deadline for public comment is January 15, 2022.Let’s make ourselves heard!\n\nA startup enables people who participate in voice chat to use realistic artificial voices in real time.What’s new:Massachusetts-based Modulate offers a voice-masking tool to forestall harassment of people, particularly women and trans individuals, whose vocal tone may trigger abuse in online conversations,Wiredreported.How it works:Modulate’s VoiceWear system acts like a generative adversarial network. AParallel WaveNetmodel generates a speaker’s words in a synthetic voice. It tries to fool a convolutional neural network, which evaluates whether its output is real or synthesized.\n\nBehind the news:Other voice-changing systems are available, but most simply shift a voice’s pitch up or down using basic computational techniques, causing it to sound distorted or robotic.Why it matters:Women, LGBT+ people, and various racial groups online are often targeted for harassmentdueto the way they sound. The abuse drives many away from popular video games, social media, and other experiences that encourage audio engagement, making such sites less inclusive and hurting their bottom line.We’re thinking:Enabling people who are at risk of harassment to hide their identity is helpful. But online moderators — human or AI — also need to play an active role in curbing abuse.\n\nWe’re updating our Natural Language Processing Specialization to reflect the latest advances! Instructor Younes Bensouda Mourri and Hugging Face engineer Lewis Tunstall will host a live Ask Me Anything session on November 3, 2021.Join usfor answers to your NLP-related questions!\n\nA neural network can make a photo resemble a painting vianeural style transfer, but it can also learn to reproduce an image by applying brush strokes. A new method taught a system this painterly skill without any training data.What’s new:Songhua Liu, Tianwei Lin, and colleagues at Baidu, Nanjing University, and Rutgers developedPaint Transformer, which learned to render pictures as paintings by reproducing paintings it generated randomly during training.Key insight:A human painter generally starts with the background and adds details on top of it. A model can mimic this process by generating background strokes, generating further strokes over the top, and learning to reproduce these results. Dividing the resulting artwork into smaller pieces can enable the model to render finer details. Moreover, learning to reproduce randomly generated strokes is good training for reproducing non-random graphics like photos.How it works:Paint Transformer paints eight strokes at a time. During training, it randomly generates an eight-stroke background and adds an eight-stroke foreground. Then it learns to minimize the difference between the background-plus-foreground image and its own work after adding eight strokes to the background.\n\nResults:Qualitatively, Paint Transformer used fewer and bolder strokes than anoptimization method, while areinforcement learning approachproduced output that looked overly similar to the input. Quantitatively, Paint Transformer trained faster than RL (3.79 hours versus 40 hours) and took less time at inference than either alternative (0.30 seconds versus 0.32 seconds for RL and 521.45 seconds for optimization).Why it matters:The system learned to paint without seeing any existing paintings, eliminating the need for matched pairs of photos and paintings, never mind tens of thousands or millions of them. This kind of approach might bear fruit in art forms from photo editing to 3D modeling.We’re thinking:Hook this thing up to a robot holding a brush! We want to see what its output looks like in oils or acrylics.\n\nA new study warns that the geographic concentration of AI in the United States is making the industry too insular.What’s new:Areportby the Brookings Institution documents the extent to which a few metropolitan areas dominate AI in the U.S., risking group-think, geographic bias, and other pitfalls.AI Hubs Actual and Potential:The report scores AI research and commercialization in 384 regions based on an analysis of federal grants, research papers, patent filings, job postings, and companies.\n\nBehind the news:The Bay Area’s dominance in AI dates to the late 1950s, when the nascent semiconductor industry spawned what became the modern tech industry. Owing partly to this history, the region hosts a thriving ecosystem of universities, businesses, and financiers that focus on technological innovation.Why it matters:AI’s lopsided geographic concentration not only undermines demographic and intellectual diversity, it “locks in a winner-take-most dimension to this sector,” Mark Muro, the study’s coauthor, toldWired. This imbalance between risk and reward highlights a need for policy and investment that promotes AI in other parts of the country, he said.We’re thinking:Other industries are geographically concentrated; for instance entertainment, fashion, and finance. But AI has a special need for a diverse talent pool to ensure that the systems we build are fair and broadly beneficial.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-10-12%20at%205.06.58%20PM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-10-12%20at%205.06.58%20PM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-10-12%20at%205.07.41%20PM%20copy-1.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-10-12%20at%205.07.41%20PM%20copy-1.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-10-19%20at%203.17.13%20PM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-10-19%20at%203.17.13%20PM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/MODULATE.gif?upscale=true&width=1200&upscale=true&name=MODULATE.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/AI%20Access%2011.3_The%20Batch%20Image%20(3).png?upscale=true&width=1200&upscale=true&name=AI%20Access%2011.3_The%20Batch%20Image%20(3).png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-10-05T174045.585.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-10-05T174045.585.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/BROOKINGS.gif?upscale=true&width=1200&upscale=true&name=BROOKINGS.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-137/",
    "title": "issue 137",
    "date": "",
    "reading_time": "",
    "content": "I’ve always thought that how you treat those who are powerless shows your true character. People rarely mistreat others who have power over them -- for example, their boss — because they might suffer adverse consequences. But when you encounter someone whom you can either push down or lift up, with no risk of harm or possibility of gain, your choice reveals your character.Similarly, the way a nation treats those with less power — specifically refugees — shows its character. As Russia continues to attack Ukraine, millions of refugees are streaming across Europe. They join refugees from Afghanistan, Syria, Congo, Myanmar, Iraq and other nations in seeking safety. I’ve been heartened by news that Poland, Romania, Moldova, Hungary, Germany, France, Ireland, the United Kingdom and other countries are offering them a safe haven. I hope the U.S. will open its doors wider to all refugees.Historically, refugees have made rich contributions to their host nations. The U.S. would have been a very different country without Albert Einstein, Madeleine Albright, and Sergey Brin, all of whom were refugees. Countries that welcome refugees today may find tomorrow that they’ve adopted the next Einstein, yielding great prestige and prosperity.\n\nOf course, integrating refugees is not a trivial matter. They must adjust to a new home, their host country must adapt to a more diverse population, and local people may worry about competition for jobs and resources. But the need to welcome people fleeing for their lives is pressing. Surely we can find it in ourselves to share with those who have lost everything.Treating people well regardless of their power should be a key part of building in AI as well. I would love to see the AI community assist displaced Ukrainian engineers. At the same time, let’s help Russian engineers who don’t support the war and want to emigrate and build a new life in a different country.When developers write software, there’s an economic temptation to focus on serving people who have power: How can one show users of a website who have purchasing power an advertisement that motivates them to click? To build a fairer society, let’s also make sure that our software treats all people well, including the least powerful among us.\n\nKeep learning!\n\nAndrew\n\nP.S. I just spoke at Nvidia’s GPU Technology Conference about data-centric AI, where I showed the first public demo of data-centric features ofLandingLens, an MLOps platform for computer vision built by my team at Landing AI. A highlight for me came during the question-and-answer session, when my friend Bryan Catanzaro, Nvidia’s vice president of applied research, mentioned that the company’s cutting-edgeDeep Learning Super Samplingproject, which applies deep learning to graphics, uses a data-centric approach. The neural network changes rarely but the team improves the data! You can register for conference and watch a video of the presentationhere.\n\nThe next killer AI application may be developed by someone who has never heard of gradient descent.What’s new:A rising generation of software development platforms serves users who aren’t familiar with AI — and even programming.The New York Timessurveyedthe scene.Robocoders:Using no-code AI platform — an automated programming tool that either generates new code or customizes pre-existing code according to user input — generally requires access to a web browser and training data. From there, a user-friendly interface lets users train a prebuilt architecture.\n\nBehind the news:Similar tools for building non-AI applications like websites (Wordpress), ecommerce stores (Shopify), and video games (RPG Maker) undergird a significant portion of the online economy.OpenAIandDeepMindoffer natural language tools that write code using plain-English prompts.Source AI, available in a beta-test version, extends such auto-coding functionality to French, German, and Spanish to generate programs in at least 40 languages.Why it matters:Platforms that automate coding, data collection, and training are an important part of AI’s future. Although no-code AI tools are still maturing — for example, they’re limited to particular tasks and some aren’t yet suitable for commercial-grade applications — they’re on track to open the field to a far broader range of users, enabling them to apply tried-and-true approaches to certain classes of problems. And they may be useful to experienced AI developers, too. For instance, trained engineers may also use them to build wireframe versions of more intensive projects.We’re thinking:No-code tools have a long way to go, and even when they get there, education in AI technology will be necessary to handle difficult problems, high-stakes situations, and cutting-edge developments. Skilled engineers will exceed the capabilities available at the press of a button for the foreseeable future.\n\nPeople with certain genetic disorders share common facial features. Doctors are using computer vision to identify such syndromes in children so they can get early treatment.What’s new:Face2Geneis an app from Boston-basedFDNAthat recognizes genetic disorders from images of patients’ faces. Introduced in 2014, it was upgraded recently to identify over 1,000 syndromes (more than three times as many as the previous version) based on fewer examples. In addition, the upgrade can recognize additional conditions as photos of them are added to the company’s database — no retraining required.How it works:Newworkby Aviram Bar-Haim at FDNA, Tzung-Chien Hsieh at Rheinische Friedrich-Wilhelms-Universität Bonn, and colleagues describes the revised model.\n\nResults:In tests, the new version proved somewhat less accurate than its predecessor at recognizing the 91 syndromes pictured in theLondon Medical Database. It ranked the correct syndrome in the top 30 possibilities 86.59 percent of the time versus the earlier version’s 88.34 percent. However, it was able to identify 816 conditions that its predecessor couldn’t, ranking the correct one in the top 30 possibilities 24.41 percent of the time and in the top position 7.07 percent of the time. (The chance of choosing the correct syndrome randomly was 0.09 percent.)Why it matters:Some350 million peopleworldwide live with a rare genetic disorder. Such conditions are especially difficult to diagnose because they’re so numerous, and many doctors never encounter a case. Face2Gene, which reportedly is used by thousands of geneticists, has beencreditedwith making the job much easier.We’re thinking:Humanity has a sad history of judging people based on appearance. While this model is designed for healthcare professionals to evaluate children who may need medical treatment, we caution against trying to use AI to classify an individual’s traits such as intelligence, character, or sexual preference based on their looks.\n\nLooking to build or customize powerful real-world models to solve difficult problems? Check out theTensorFlow: Advanced Techniques Specialization!Enroll today\n\nIf you buy or sell stocks, it’s handy to test your strategy before you put real money at risk. Researchers devised a fresh approach to simulating market behavior.What's new: Andrea Coletta and colleagues at Sapienza University of Rome used a Conditional Generative Adversarial Network (cGAN) tomodela market’s responses to an automated trader’s actions.Key insight:Previous approaches tested a simulated trader in a virtual market populated by other simulated traders. However, real-world markets tend to be too complex to be modeled by interactions among individual agents. Instead of simulating market participants, a cGAN can model aggregated sales and purchases in each slice of time.Conditional GAN basics:Given a random input, a typical GAN learns to produce realistic output through competition between a discriminator that judges whether output is synthetic or real and a generator that aims to fool the discriminator. AcGANworks the same way but adds an input — in this case, details about individual buy and sell orders and the overall market — that conditions both the generator’s output and the discriminator’s judgment.How it works: The authors built a simulated stock exchange based on theAgent-Based Interactive Discrete Event Simulation(ABIDES) framework to match buy and sell orders. They trained a cGAN to generate such orders based on two days ofmarket datafor Apple and Tesla stocks. Then they added orders by an independent trader.\n\nResults: The authors checked statistical similarity between historical and cGAN orders in terms of price, volume, direction (buy or sell), and frequency distributions. In particular, they looked at Tesla shares on May 2 and May 3, 2019, and plotted the distributions. The real and synthetic distributions matched fairly closely. When they ran the simulation using historical orders plus cGAN orders, the price rose slightly during the 30 minutes when the agent would have been active. Given the orders generated by the cGAN and the agent, the price rose by an order of magnitude more and returned to normal shortly after the agent stopped trading, demonstrating the simulation’s response to the agent’s activity.Why it matters: GANs are usually associated with image generation. This paper adds to agrowingbodyofresearchshowing that they can successfully generate data outside of perceptual domains.We're thinking:Supervised learning tends to apply when a specific output y can be predicted from a given input x. In applications where y is a complex data type that’s also inherently stochastic — such as a sequence of market trades or afuture weather map— we might try to model y using a stochastic process rather than attempt to learn one correct answer. cGANs appear to be emerging as a promising approach.\n\nA new study showcases AI’s growing importance worldwide.What’s new:The fifth annualAI Indexfrom Stanford University’s Institute for Human-Centered AI documents rises in funding, regulation, and performance.What it says:The authors based their report on academic and conference publications as well as public datasets.\n\nBehind the news:Previous editions of theAI Indexhighlighted important inflection points in the industry’s growth.\n\nWhy it matters:The boom in private investment and spike in laws that regulate automation signal the same fundamental trend: AI is increasingly central to the forces that drive society.We’re thinking:We can confirm a further trend: the rising volume of reports on AI trends!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/03/Screen-Shot-2022-03-22-at-4-1.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/03/NOCODE--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/03/ezgif.com-gif-maker--24--1.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20Image%201%20(2).png?upscale=true&width=1200&upscale=true&name=The%20Batch%20Image%201%20(2).png",
      "https://dl-staging-website.ghost.io/content/images/2022/03/ezgif.com-gif-maker--11--2-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/03/INDEX--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-233/",
    "title": "issue 233",
    "date": "",
    "reading_time": "",
    "content": "Last week, I attended the World Economic Forum, an annual meeting of leaders in government, business, and culture at Davos, Switzerland. I spoke in a few sessions, including a lively discussion with Aiden Gomez, Daphne Koller, Yann LeCun, Kai-Fu Lee, and moderator Nicholas Thompson about the present and possible future technology developments of generative AI. You can watch ithere.The conference's themes included AI, climate change, economic growth, and global security. But to me, the whole event felt like an AI conference! (This is not just my bias. When I asked a few non-AI attendees whether they felt similarly, about three-quarters of them agreed with me.) I had many conversations along two major themes:Business implementation of AI.Many businesses, and to a lesser extent governments, are looking at using AI and trying to develop best practices for doing so. In some of my presentations, I shared my top two tips:\n\nAI regulation.With many governments represented at Davos, many discussions about AI regulation also took place. I was delighted that he conversation has become much more sensible compared to 6 months ago, when the narrative was driven by misleadinganalogies between AI and nuclear weaponsand lobbyists had significant momentum pushing proposals that threatened open-source software. However, the fight against stifling regulations isn't over yet! We must continue to protect open-source software and innovation. In detail:\n\nI also went to some climate sessions to listen to speakers. Unfortunately, I came away from them feeling more pessimistic about what governments and corporations are doing on decarbonization and climate change. I will say more about this in future letters, but:\n\nDavos is a cold city where temperatures are often below freezing. In one memorable moment at the conference, I had lost my gloves and my hands were freezing. A stranger whom I had met only minutes ago kindly gave me an extra pair. This generous act reminded me that, even as we think about the global impacts of AI and climate change, simple human kindness touches people's hearts and reminds us that the ultimate purpose of our work is to help people.\n\nKeep learning!\n\nAndrew\n\nP.S. Check out our new short course on “Automated Testing for LLMOps,” taught by CircleCI CTO Rob Zuber! This course teaches how you can adapt key ideas from continuous integration (CI), a pillar of efficient software engineering, to building applications based on large language models (LLMs). Tweaking an LLM-based app can have unexpected side effects, and having automated testing as part of your approach to LLMOps (LLM Operations) helps avoid these problems. CI is especially important for AI applications given the iterative nature of AI development, which often involves many incremental changes. Please sign uphere.\n\nA neural network detected early signs of pancreatic cancer more effectively than doctors who used the usual risk-assessment criteria.\n\nWhat’s new:Researchers at MIT and oncologists at Beth Israel Medical Center in Bostonbuilta model that analyzed existing medical records to predict the risk that an individual will develop the most common form of pancreatic cancer. The model outperformed commonly used genetic tests.How it works:The authors trained PrismNN, a vanilla neural network, to predict a patient’s risk of receiving a diagnosis of pancreatic ductal adenocarcinoma (PDAC) in the next 6 to 18 months.\n\nResults:PrismNN identified as high-risk 35.9 percent of patients who went on to develop PDAC, with a false-positive rate of 4.7 percent. In comparison, the genetic criteria typically used to identify patients for pancreatic cancer screening flags 10 percent of patients who go on to develop PDAC. The model performed similarly across age, race, gender, and location, although some groups (particularly Asian and Native American patients) were underrepresented in its training data.\n\nBehind the news:AI shows promise in detecting various forms of cancer. In a randomized, controlled trial last year, a neural networkrecognizedbreast tumors in mammograms at a rate comparable to human radiologists. In 2022, an algorithm successfullyidentifiedtumors in lymph node biopsies.\n\nWhy it matters:Cancer of the pancreas is one of the deadliest. Only 11 percent of patientssurvivefor 5 years after diagnosis. Most cases aren’t diagnosed until the disease has reached an advanced stage. Models that can spot early cases could boost the survival rate significantly.\n\nWe’re thinking:The fact that this study required no additional testing is remarkable and means the authors’ method could be deployed cheaply. However, the results were based on patients who had already been diagnosed with cancer. It remains for other teams to replicate them with patients who have not received a diagnosis, perhaps followed by a randomized, controlled clinical trial.\n\nEuropeans are keeping their jobs even as AI does an increasing amount of work.\n\nWhat’s new:Researchers at the European Central Bankfoundthat employment in occupations affected by AI rose over nearly a decade.\n\nHow it works:The authors considered jobs that were found to be affected by AI over the past decade according totwostudies. As a control group, they considered jobs affected by software generally (“recording, storing, and producing information, and executing programs, logic, and rules”), as detailed in one of the studies. They measured changes in employment and wages in those jobs based on asurveyof workers in 16 European countries between 2011 and 2019.\n\nResults:The researchers found that exposure to AI was associated with greater employment for some workers and had little effect on wages.\n\nBehind the news:Other studies suggest that automation in general and AI technology in particular may benefit the workforce as a whole.\n\nYes, but:It may be too soon to get a clear view of AI’s impact on employment, the authors point out. The data that underlies every study to date ends in 2019, predating ChatGPT and the present wave of generative AI. Furthermore, the impact of AI in European countries varies with their individual economic conditions (for instance, Greece tends to lose more jobs than Germany).\n\nWhy it matters:Many employees fear that AI — and generative AI in particular — will take their jobs. Around the world, the public isnervousabout the technology’s potential impact on employment. Follow-up studies using more recent data could turn these fears into more realistic — and more productive — appraisals.\n\nWe’re thinking:AI is likely to take some jobs. We feel deeply for workers whose livelihoods are affected, and society has a responsibility to create a safety net to help them. To date, at least, the impact has been less than many observers feared. One reason may be that jobs are made up of many tasks, and AI automates tasks rather than jobs. In many jobs, AI can automate a subset of the work while the jobs continue to be filled by humans, who may earn a higher wage if AI helps them be more productive.\n\nAutomated testing of applications based on large language models can save significant development time and cost. In this course, you’ll learn to build a continuous-integration pipeline to evaluate LLM-based apps at every change and fix bugs early for efficient, cost-effective development.Enroll for free\n\nGovernments want access to AI chips and software built in their own countries, and they are shelling out billions of dollars to make it happen.What’s new:Nations across the world are supporting homegrown AI processing and development,The Economistreported.How it works:Governments want AI they can rely upon for state use. The U.S. and China each promised to invest around $40 billion in the field in 2023. Another 6 countries — France, Germany, India, Saudi Arabia, the UAE, and the UK — pledged a combined $40 billion. Different governments are emphasizing different capabilities.\n\nBehind the news:Even as governments move toward AI independence, many are attempting to influence international politics and trade to bolster their positions.\n\nWhy it matters:AI has emerged as an important arena for international competition, reshaping global society and economics, generating economic growth, and affecting national security. For engineers, the competition means that governments are competing to attract talent and investment, but they’re also less inclined to share technology across borders.We’re thinking:We understand governments’ desires to ensure access to reliable AI, but focusing on sovereignty above all is misguided. In a networked world, developments can’t be contained to one country. Cooperation ensures that development proceeds at a rapid pace and benefits everyone.\n\nMachine learning algorithms often struggle with geometry. A language model learned to prove relatively difficult theorems.\n\nWhat's new:Trieu Trinh, Yuhuai Wu, Quoc Le, and colleagues at Google and New York University proposedAlphaGeometry, a system that can prove geometry theorems almost as well as the most accomplished high school students. The authors focused on non-combinatorial Euclidean plane geometry.\n\nHow it works:AlphaGeometry has two components. (i) Given a geometrical premise and an unproven proposition, an off-the-shelfgeometric proof finderderived statements that followed from the premise. The authors modified the proof finder to deduce proofs from not only geometric concepts but also algebraic concepts such as ratios, angles, and distances. (ii) A transformer learned to read and write proofs in the proof finder’s specialized language.\n\nResults:The authors tested AlphaGeometry on 30 problems posed by the International Mathematical Olympiad, an annual competition for high school students. Comparing that score to human performance isn’t so straightforward because human competitors can receive partial credit. Human gold medalists since 2000 solved 25.9 problems correctly, silver medalists solved 22.9 problems, and bronze medalists solved 19.3 problems. Theprevious state-of-the-art approachsolved 10 problems, and the modified proof finder solved 14 problems. In one instance, the system identified an unused premise and found a more generalized proof than required, effectively solving many similar problems at once.\n\nWhy it matters:Existing AI systems can juggle symbols and follow simple rules of deduction, but they struggle with steps that human mathematicians represent visually by, say, drawing a diagram. It’s possible to make up this deficit by (i) alternating between a large language model (LLM) and a proof finder, (ii) combining geometric and algebraic reasoning, and (ii) training the LLM on a large data set. The result is a breakthrough for geometric problem solving.\n\nWe're thinking:In 1993, the teenaged Andrew Ng represented Singapore in the International Mathematics Olympiad, where hewona silver medal. AI’s recent progress in solving hard problems is a sine of the times!\n\nThis week's latest updates include an affordable reinforcement-learning-powered robot, exciting AI features in Samsung’s new smartphone series, an improved model for code completion from Stability AI, and much more. Catch up with the help of Data Points, a spin-off of The Batch.\n\nRead now.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--92-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--93-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-24T145719.589.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/01/The-Batch-ads-and-exclusive-banners---2024-01-23T083847.050.png",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-24T145859.354.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--94-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-36/",
    "title": "issue 36",
    "date": "",
    "reading_time": "",
    "content": "I spoke on Tuesday at Coursera’s annual conference. It was the company’s most well-attended conference yet, and the first to be held online.\n\nHigher education is in for turbulent times. With campuses shut down indefinitely and many professors teaching digitally for the first time, schools are challenged to deliver high-quality education while both students and teachers remain at home. At the same time, more people than ever — of all ages, all over the world — are more interested than ever in taking courses online.\n\nIn his keynote, Coursera CEO Jeff Maggioncalda spoke of the importance of social justice. Even as Covid-19 heightens social inequities, within this crisis lies an opportunity for educators to serve learners around the world. It’s also an opportunity to rebuild society’s trust in science, reason, and each other.\n\nWhen the pandemic is over, hundreds of millions of learners around the world will have picked up the habit of learning online. The momentum could drive a new golden age of learning, and it’s not too early to start preparing. Let’s make sure we keep working to democratize access to education and make our society more fair and equitable.\n\nStay safe and keep learning!\n\nAndrew\n\nMachine learning engineers need tools and data to help fight the Covid-19 pandemic. Here are some that crossed our radar screen last week.\n\nResearchers have rushed out a battery of AI-powered tools to combat the coronavirus, but an assessment of dozens of models is a wake-up call for machine learning engineers.What’s new:Many models built to spot Covid-19 infection, predict the likelihood of hospitalization, or forecast outcomes are built on flawed science, according to asurveypublished in theBritish Medical Journal.What they found:A group of clinicians, scientists, and engineers led by Laure Winants, an epidemiologist at Maastricht University in the Netherlands, found that biased data compromised all of the 31 models analyzed.\n\nResults:In a commentary that accompanied the survey,BMJ’s editors declared the models so “uniformly poor” that “none can be recommended for clinical use.”The path forward:The authors recommend that machine learning researchers adopt the 22-pointTRIPODchecklist as a standard for developing predictive medical AI. Developed by an international consortium of physicians and data scientists, the checklist is designed to help engineers report their work clearly and reduces risk of developing models with biased data.Why it matters:Patients and health care systems alike need more accurate and faster diagnoses and prognoses. The AI community is used to publishing preliminary results to accelerate progress, but the health care community tends to wait for rigorous peer review to avoid causing harm.We’re thinking:Given how fast the Covid-19 situation is evolving, sharing results early and often is a good thing. But the AI community also needs new mechanisms to make sure preliminary models don’t cause harm.\n\nGiven real-world constraints on memory and processing time, images are often downsampled before they’re fed into a neural network. But the process removes fine details, and that degrades accuracy. A new technique squeezes images with less compromise.What’s new:Researchers at the Alibaba DAMO Academy and Arizona State University led by Kai Xureduce the memory needed for image processingby using a technique inspired by JPEG image compression.Key insight:JPEG removes information the human eye won’t miss by describing patterns of pixels as frequencies. Successive color changes from pixel to pixel are higher frequencies, while monochromatic stretches are lower frequencies. By cutting frequencies that have little visual effect, the algorithm compresses images with minimal impact on image quality. The researchers employed a similar strategy to reduce input data without losing information critical to learning.How it works:The researchers transformed images into the frequency domain, selected frequencies to remove, and fed the reduced frequency representation into ResNet-50 and MobileNet V2 models.\n\nResults:ResNet-50 trained onImageNetin the usual way achieves 76 percent top-1 accuracy, but slimming the input in the frequency domain increased accuracy by 1.38 percent. A MobileNet V2 trained on ImageNet and ResNet-50 feature pyramid network trained onCOCOsaw similar improvements.Why it matters:Many images are much larger than the input size of most convolutional neural networks, which makes downsampling a necessary evil. Rescaling the frequency representation of images preserves relevant information, so downsampling doesn’t need to hurt performance.We’re thinking:Smartphones capture images in 4K, but CNNs require 224×224 pixels. It’s nice to know the missing resolution isn’t going entirely to waste.\n\nDeep learning research is harvesting better ways to manage farms.What’s new:A convolutional neural networkpredictedcorn yields in fields across the U.S. Midwest.How it works:Researchers from the University of Illinois at Urbana-Champaign built a network that forecasts the quantity of corn that will grow seasonally in a given field under variable rates of seeding and nitrogen fertilization.\n\nResults:The team’s model averaged .70 root mean squared error of the mean yield standard deviation in all fields. It predicted yields more accurately than other neural networks the team built in all but one. It was also better than a set of non-neural benchmarks, outperforming a random forest model by 29 percent and a multiple linear regression model by 68 percent.Behind the news:Agriculture requires farmers to manage numerous environmental factors and decision points, from weather patterns to hiring manual labor. Machine learning can help at every stage. Big-ag heavyweights like John Deere as well as startups like Dot and SwarmFarm offer highly automated tractors including machines that use advanced image recognition to kill individual weeds. Landing AI helped design a rig that automatically optimizes harvesting. (Disclosure: Andrew Ng is CEO of Landing AI.) Other companies specialize in evaluatingproduce quality,crop health, and multi-farm operations.Why it matters:Systems like this could help farmers increase yields, save on seed costs, and reduce excess nitrogen that ends up running off into water sources. The authors are performing more trials to improve the model and working on an optimization algorithm so farmers can generate fertilizer and seed maps for their own fields.We’re thinking:In many developing economies, younger people don’t want to make their living from farming, and small family-run farms are being consolidated into larger plots. This creates opportunities for AI and automation to make agriculture more efficient, and potentially to make food more affordable and protect the environment.\n\nHow can you estimate a patient’s future health? Build your own survival model in Course 2 of the AI for Medicine Specialization.Enroll now\n\nIt’s time to stop flushing valuable data down the toilet.What’s new:The Precision Health Toilet, asuite of sensorsthat attach to an ordinary commode, monitors human waste for input for signs of disease. It identifies individual users by scanning where the sun doesn’t shine.How it works:The system, developed by a team led by Stanford radiologist and bioengineer Seung-min Park, analyzes excreta in three ways:\n\nResults:The toilet performs well in the lab, but it’s still a prototype, not yet ready to aid in clinical diagnosis or disease screening. Its inventors hope to make it into a commercial product, which would require software upgrades as well as self-cleaning mechanisms for the sensors. In an interview withThe Verge, Park estimated a commercial system would probably cost between $300 and $600.Behind the news:Japanese toilet maker Toto was first to market with an AI-powered biomedical toilet. ItsFlow Skymeasures urine flow by analyzing the water level in the bowl. And the European Space Agency reportedly is developingtoiletscapable of detecting infectious diseases.Why it matters:Human waste containsbiomarkersfor diabetes, metabolic disorders, and some cancers. Urine flow is anindicatorof bladder, urinary tract, and prostate health. Like a heart monitor for excretory organs, a toilet mechanism of this type could monitor risk factors fordozens of diseases.\n\nWait a minute — we added training data, and our model’s performance got worse?! New research offers a way to avoid so-called double descent.What’s new:Double descent occurs when a model’s performance changes in unpredictable ways as the amount of training data or number of parameters crosses a certain threshold. The error falls as expected with additional data or parameters, but then rises, drops again, and may take further turns. Preetum Nakkiran and collaborators at Harvard, Stanford, and Microsoft found a way toeliminate double descentin some circumstances.Key insight:The researchers evaluated double descent in terms of a model’s test error. Framing the problem this way led them to the conclusion that regularization — discouraging a model from having large weights — can prevent it. Whereprevious researchdescribed the occurrence of double descent as models or datasets grow infinitely large, the authors’ analysis applies to all sizes. This enables them to offer a practical approach to managing the problem.How it works:The researchers proved that double descent is manageable in linear regression models if the dataset meets certain criteria. They also demonstrated experimental results for a broader class of problems.\n\nResults:The researchers proved that their regularization technique prevents double descent in linear regression models if the dataset meets certain criteria. They also used linear regression models with datasets that didn’t match all of their criteria, and in every case they considered, they found a regularization penalty that did the trick.Yes, but:Although the technique avoided double descent in a variety of circumstances, particularly in linear regression models, the authors were not able to prove that their technique works in every case.Why it matters:This approach to mitigating double descent may look limited, since it applies only to some linear regression models. But improvements could have broad impact, given that linear regression is ubiquitous in neural network output layers.We’re thinking:Double descent is sneaky. Researchers can miss it when they run benchmark datasets if they cherry-pick the best-performing models. And engineers can fail to detect it in applications because it isn’t predictable from results on the training set. It may be rare in practice, but we’d rather not have to worry about it.\n\nComputer vision is helping construction workers keep their social distance.What’s new:Smartvid.io, a service that focuses on construction sites, offers a tool thatrecognizeswhen workers get too close to each other. The tool sends social distancing warnings and reports to construction superintendents.How it works:Supervisors receive an alert when its neural nets spot workers breaking social distancing guidelines set by the U.S. Occupational Health & Safety Administration. They can also watch an annotated video feed on desktop or mobile devices and receive a daily summary of on-site social distancing metrics.\n\nBehind the news:AI increasingly is taking on workplace safety functions. Amazon plans to monitor social distancing in its warehouses using computer vision, according toReuters.Landing AI, which helps traditional companies implement AI, released a similartoollast week. (Disclosure: Andrew Ng is CEO of Landing AI.)Why it matters:In response to Covid-19, 13 U.S. states have suspended certain kinds of building projects, and three haveshut downconstruction entirely. Technology that helps workers maintain a safe distance could help keep them working through the pandemic.We’re thinking:These systems will need to be rolled out with deep respect for privacy rights, and the specific features that our community chooses to build or not to build will have an important impact.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter20ASPECT206.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Resources20ASPECT20REPLACEMENT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Covid-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Frequency20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Corn20Revised.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AI4MC2BatchAd.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Toilet20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Place20at20bottom20of20TOILETS.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/DoubleDescent20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Construc1-Smartividio.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-30/",
    "title": "issue 30",
    "date": "",
    "reading_time": "",
    "content": "For many years, attendees at top conferences have asked themselves: Why do we travel to one location, when it means:\n\nJust as MOOCs today are a lot more than video, online conferences will be much richer than livestreamed video. Perhaps we’ll have regional chat rooms where attendees in the same country can share local resources even while they listen to a keynote. Or we will generate live transcripts through automatic speech recognition that attendees can tag with live commentary. Up- and downvoting one another’s questions will be routine, and some answers will be crowdsourced.\n\nI don’t expect online conferences to replace in-person events, which still have an important role. Rather, they’ll complement them. With more team members (including many in my organizations) working from home, the time is ripe to experiment with these ideas and move toward lower costs, smaller carbon footprints, democratized access, and stronger communities. If you have thoughts, let us know at[email protected].\n\nWash your hands, stay safe, and keep learning!Andrew\n\nWe want to make sure we’re giving you the most useful newsletter in AI. Please answer a few questions to let us know what you’d like to see more (or less) of.Take the brief survey\n\nAmputees can control a robotic hand with their thoughts — plus machine learning.What’s new:University of Michigan researchers developed asystemthat uses signals from an amputee’s nervous system to control a prosthetic hand.How it works:The researchers grafted bits of muscle onto the severed nerve bundles at the ends of amputees’ forearms, then implanted electrodes into the muscle. They amplified and recorded the electric signals transmitted to the nerves when the recipients thought about, say, making a fist, pointing a finger, or rotating a thumb. Then they trained a pair of models to match the signals with the corresponding hand motions.\n\nBehind the news:Other research groups are using similar methods to control robotic prostheses. Some promising approaches:\n\nWhy it matters:Nearlytwo millionAmericans have lost a limb, along with millions more worldwide. More responsive prostheses could dramatically improve their quality of life.We’re thinking:Will they train robotic hands to do COVID-19-safe, palms-togethernamastegreetings?\n\nIn small data settings where labels are scarce, semi-supervised learning can train models by using a small number of labeled examples and a larger set of unlabeled examples. A new method outperforms earlier techniques.What’s new:Kihyuk Sohn, David Berthelot, and colleagues at Google Research introducedFixMatch, which marries two semi-supervised techniques.Key insight:The technique known aspseudo labelinguses a trained model’s most confident predictions on unlabeled examples for subsequent supervised training.Consistency regularizationpenalizes a model if its predictions on two versions of the same data point — say, distorted variations on the same image — are dissimilar. Using these techniques in sequence enables a model to generalize insights gained from unlabeled data.How it works:FixMatch learns from labeled and unlabeled data simultaneously. It learns from a small set of labeled images in typical supervised fashion. It learns from unlabeled images as follows:\n\nResults:FixMatch achieved state-of-the-art performance for semi-supervised learning on several benchmarks devised by the researchers. (They removed labels from popularimagedatasetsto create training sets with between four and 400 labels per class.) Analternativesemi-supervised approach performed slightly better on some benchmarks, though it’s not obvious under what circumstances it would be the better choice.Why it matters:Google Research has been pushing the envelope of semi-supervised learning for image classification with a series of better and better algorithms. FixMatch outperforms its predecessors in the majority of comparisons, and its simplicity is appealing.We’re thinking:Small data techniques promises to open the door to many new applications of AI, and we welcome any progress in this area.\n\nHoping to keep surveillance capitalists from capitalizing on your face? Safeguard your selfies with a digital countermeasure.What’s new:Researchers at the University of Chicago and Fudan University devised a program that subtly alters portrait photos toconfuseface recognition models without distorting the image to the human eye.How it works:Named after the Guy Fawkes mask beloved by privacy advocates,Fawkescloaks faces by imposing patterns that, to machines, look like someone else.\n\nResults:The researchers uploaded 50 cloaked photos of the same person to face recognition services from Amazon, Megvii, and Microsoft, which trained on the data. All three failed to identify the person in 32 uncloaked validation images — a 100 percent success rate. However, Fawkes had a hard time fooling models that were already familiar with a given face, having already trained on many uncloaked images. The models developed amnesia, though, after ingesting a fake social media account that exclusively contained cloaked (and renamed) photos.Yes, but:Fawkes isn’t fool-proof.\n\nWhy it matters:We need ways, whether legal or technical, to enable people to protect their privacy. The U.S. startup Clearview.ai made headlines in January when theNew York Timesreported that its surveillance system, trained on billions of photos scraped from social media sites without permission, was widely used by law enforcement agencies and private businesses.We’re thinking:If this method takes off, face recognition providers likely will find ways to defeat it. It’s difficult to make images that humans can recognize but computers can’t.\n\nTest your image classification models with your phone’s camera! Learn how to deploy models with TensorFlow Lite in Course 2 of the TensorFlow: Data and Deployment Specialization.Enroll now\n\nFashion models recently sashayed down Paris Fashion Week catwalks in outfits designed by deep neural nets.What’s new:Swedish design firm Acne Studios based its 2020 fall/wintermen’s lineon output from a generative adversarial network.How it works:AI artistRobbie Barrattrained a GAN to generate images of high-fashion outfits by feeding it thousands of pictures of models wearing Acne’s past work. The results were often surreal (though not much more so than Acne’s usual designs).\n\nBehind the news:Barrat, a 20-year-old machine learning engineer, worked on Nvidia’s self-driving car program and Stanford’sKhatri Labbiomedical research team. He has used AI to generatelandscapes and nudesas well aslyricsin the style of Kanye West. In 2017, a trio of French art students used one of his models to create an artwork that sold fornearly half a million dollars.Why it matters:In the fashion industry, AI is the new black.\n\nYes, but:Acne’s GAN-driven outfits didn’t wow all the critics in Paris.Vogue’s Luke Leitchwrote: “So the good news is that, on the evidence of this highly original Acne menswear collection, clothes design is not a human profession under threat from AI anytime soon.”We’re thinking:If we didn’t already know, we never would have guessed that these clothes were designed by a GAN. We’re not sure whether that’s a testament to the designers’ genius or our hopeless fashion sense.\n\nDeepfake videos in which one person appears to speak another’s words have appeared inentertainment,advertising, andpolitics. New research ups the ante for an application that enables new forms of both creative expression and misinformation.What’s new:Linsen Song with researchers at China’s National Laboratory of Pattern Recognition, SenseTime Research, and Nanyang Technological University produced a model that makes a person on video appear to speak words from a separate audio recording with unprecedented realism. You can see the results in thisvideo.Key insight:Most people’s mouths move similarly when pronouncing the same words. The model first predicts facial expressions from the audio recording. Then it maps those predictions onto the target speaker’s face.How it works:This approach works with any target video and source audio, synthesizes new motions, and maps them to a model of the target’s face frame by frame.\n\nResults:To test the model’s effectiveness quantitatively, the researchers evaluated its ability to resynthesize mouth movements from their original audio tracks in avideo dataset. The model reduced the error in expression (average distance between landmark features) to 0.65 from a baseline of 0.84. In a qualitative study, viewers judged generated videos to have been real 65.8 percent of the time — a high score considering that they identified real videos as real 77.2 percent of the time.Why it matters:Putting new words in a talking head’s mouth is getting easier. While previous approaches often impose prohibitive requirements for training, this method requires only a few minutes of video and audio data. Meanwhile, the results are becoming more realistic, lending urgency to the need for robust detection methods and clear rules governing their distribution.We’re thinking:Let’s get this out of the way: We never said it!\n\nMany of this year’s hottest AI companies are taking the spotlight from last year’s darlings.What’s new:CB Insights, which analyzes early-stage companies, published its annuallistof the 100 “most promising” startups in AI.Highlights:Startups in the AI 100 have raised $7.4 billion collectively. Most are headquartered in the U.S., but others are based in 13 countries including Canada, China, and the UK.\n\nMethodology:CB Insights chooses the AI 100 based on a basket of metrics, some of them indirect or subjective, such as the “sentiment” of news coverage. It scores a company’s potential to succeed using a proprietary system based on funding, the overall health of its industry, and its “momentum.”Why it matters:AI is a hot industry, but not yet a stable one.We’re thinking:Don’t let the churn scare you. If you join a startup that doesn’t make it, as long as you keep learning, you’ll be in a better position to choose another that won’t repeat the same mistakes — or to start your own.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter20ASPECT204.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-03-0920at201.34.4320PM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Prosthetics1-hand20only2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/FixMatch20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Fawkes20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Fashion2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Deepfakes.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AI10020ASPECT.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-234/",
    "title": "issue 234",
    "date": "",
    "reading_time": "",
    "content": "Last year, a number of large businesses and individuals went to the media and governments and pushed the message that AI is scary, impossible to control, and might even lead to human extinction. Unfortunately they succeeded: Now many people think AI is scary. But when I speak with regulators, media, and private citizens, I like to bring the issue of whether AI is beneficial or harmful back to a very basic question: Are we better off with more, or less, intelligence in the world?\n\nIntelligence is the ability to apply skills and knowledge to make good decisions. Yes, intelligence can be used for nefarious purposes. But over many centuries, a major driver of civilization's progress has been people getting smarter and more educated. Until now, human intelligence has been the primary form of intelligence available. But with artificial intelligence, we have the opportunity to bring much more intelligence into the world. I discussed this opportunity in a recentinterview(paywalled) withFinancial Timesreporter Ryan McMorrow.\n\nHistorically, intelligence has been very expensive to acquire. It costs a lot to feed, raise, and train a broadly knowledgeable and experienced human being! That's why it’s so expensive to hire intelligence, such as a highly skilled doctor to examine and advise you on a medical condition, or a patient tutor who can understand your child and gently coach them where they need help. But with artificial intelligence, we have the potential to make intelligence cheap for everyone, so you no longer have to worry about a huge bill for seeing a doctor or educating your child.\n\nFor society's biggest problems, such as climate change, intelligence — including artificial intelligence — also has a significant role to play. While having more intelligence in the world isn't the only thing (there are also nuances such as how to share the wealth it creates, how it will affect jobs, and how to keep it from being used for evil purposes), I believe we are much better off as a society with more intelligence, be it human or artificial intelligence.\n\nIn my recent talk at TED AI (you can watch the 12-minute presentationhere), I touched on why I'm excited about AI and why I think many of the anxieties about it are misplaced. If you speak with someone who’s worried about AI, please forward the talk to them to see if it helps to reassure them. Or ask if they fundamentally believe we want more intelligence in the world. I find that answering this question can be a useful North Star for how we approach AI.\n\nKeep learning!\n\nAndrew\n\nP.S. Check out our new short course on “Building Applications with Vector Databases,” taught by Pinecone’s Tim Tully! Vector databases (DBs) are commonly associated with retrieval augmented generation (RAG) but actually have many uses in AI applications. In this course, you’ll learn about (i) a basic semantic search app that uses a vector DB to find similar documents, (ii) a RAG application querying datasets it was not trained on, (iii) recommender systems that combine semantic search and RAG, (iv) hybrid search, which lets you work with dense and sparse vectors simultaneously, (v) anomaly detection applied to network logs, and (vi) an image-similarity application with a fun example that determines which parent a child resembles more. Come learn how you can use vector DBs to build many different types of applications!Enroll here\n\nThe United States government wants to connect U.S. AI researchers with resources that can help them develop their projects.\n\nWhat’s new:The National Artificial Intelligence Research Resource (NAIRR)announcedthe first call for proposals in its pilot program, which will accept applications through March 1. Winning proposals can receive processing power, data, software, and training provided by partner organizations. Another round will kick off in the second quarter of 2024.\n\nHow it works:Led by the National Science Foundation, NAIRR aims to support innovative AI research by organizing national compute and other infrastructure to be shared among researchers and educators. The initiative pulls together 10 other federal agencies and 25partnersincluding heavyweights like Amazon, Google, Intel, and OpenAI; startups like Allen Institute for Artificial Intelligence, Anthropic, EleutherAI, Hugging Face, and Weights & Biases; and hardware companies like AMD, Intel, and Nvidia.\n\nBehind the news:Policymakersplannedto organize a national infrastructure for AI research after calls fromprominentresearchers. NAIRR is now open thanks to anexecutive orderissued by the White House in October.\n\nWhy it matters: AI has potential to affect all corners of society yet, generally, only wealthy companies can bear the high costs of building and running large machine learning models. Partnership between government, industry, and academia can pool AI resources to cultivate talent throughout society and support important projects that may not serve a corporate agenda.We’re thinking:This is an exciting bid to proliferate AI research. Sharing the fruits of such research via open publications and open source software will bring the technology’s benefits to a wider range of people.\n\nIndian farmers used chatbots and computer vision to produce higher yields at lower costs.\n\nWhat’s new:The state government of Telangana in South Indiapartneredwith agricultural aid organizationDigital Greento provide AI tools to chili farmers.\n\nHow it works:The program, called Saagu Baagu, initially engaged 7,000 small-farm growers of chili peppers. Saagu Baagu provided AI-based tools developed by various Indian tech firms to help the farmers collect market data.\n\nResults:The pilot program lasted 18 months, or three cycles of planting, growing, and harvesting peppers. Farmers in the program grew 21 percent more plants per acre while using 9 percent less pesticide and 5 percent less fertilizer, according to the World Economic Forum. Moreover, with a higher-quality harvest, the farmers increased their sale prices by 8 percent. The Telangana government has expanded the program to 500,000 farmers who grow a wider range of crops including chickpeas, cotton, groundnuts, rice, and turmeric.\n\nBehind the news:The promise of AI-driven agriculture is attracting investments around the world. Last year, Microsoftopen-sourceda suite of AI tools to analyze overhead imagery and sensor data to map soil conditions in real time and forecast temperature, precipitation, and soil moisture for days ahead.\n\nWhy it matters:Many of the Telangana farmers rely on what they can grow and sell to support themselves and their families. That makes them especially vulnerable to market fluctuations and climate change. Their situation is not unique to India. Programs like Saagu Baagu could help support small-scale farming across the world.\n\nWe’re thinking:Saagu Baagu worked in part because WhatsApp is widely popular throughout India and the chatbot spoke the local language. Smart localization that addresses local technological infrastructures, languages, and agricultural practices can proliferate the benefits of AI in agriculture.\n\nIn our new course with Pinecone, you’ll learn how to build six applications that use vector databases, including retrieval augmented generation, facial similarity, and anomaly detection.Sign up now\n\nAn analysis of United States job listings shows AI jobs are growing rapidly outside traditional tech hubs.\n\nWhat’s new:Researchers at University of Marylandanalyzedthe distribution of AI jobs among U.S. job postings. California hosts the largest concentration, followed by the Washington D.C. metropolitan area (which includes more than one state).\n\nHow it works:The authors used an unspecified large language model to identify AI jobs, which they define as ones that require AI skills. They categorized each job by the U.S. state in which it was located. To determine whether a given state’s AI economy was growing or shrinking, they calculated the percentage of total U.S. AI jobs in each state in 2018 and 2023. They also calculated the percentage of each state’s total jobs that required AI skills for both dates.\n\nBehind the news:A 2021 Brookingsreporton U.S. AI jobs focused on metropolitan areas and analyzed not only job postings but also federal grants, research papers, patent filings, and companies. Despite the differences in methodology, it agreed with the new report that investment was driving AI growth outside of the Bay Area. The new report suggests a much wider geographical distribution of AI jobs in 2024 than in 2021. It appears some of the then-emerging industrial investment in AI is bearing fruit.Why it matters:For people who aim to make a career in AI, this report contains double good news: (i) Established AI hubs in the U.S. still host the most new openings and (ii) AI jobs are growing far and wide! As the industry becomes more dispersed geographically, AI builders have more options, organizations can select from a more diverse talent pool, and the technology’s benefits can be shared more broadly.We’re thinking:Although this report focused on the U.S., we believe that growth in AI jobs is a global trend. One contributor is growing acceptance of remote work (which remains more prevalent than it was a few years ago despite its decline as the Covid pandemic has wanted). This means more AI opportunities for everyone, everywhere!\n\nText-to-video has struggled to produce consistent motions like walking and rotation. A new approach achieves more realistic motion.\n\nWhat’s new:Omer Bar-Tal, Hila Chefer, Omer Tov, and colleagues at Google, Weizmann Institute, Tel-Aviv University, and Technion builtLumiere, a system that simplifies the usual process of generating video with improved results. You can see examples of its outputhere.\n\nKey insight:Most text-to-video generators economize on memory use through a staged process: One model generates a few frames per second, another model generates additional frames between the initial ones, and a third generates a higher resolution version of every frame. Generating in-between frames can make repetitive motions inconsistent. To avoid these inconsistencies, the authors generated all frames at the same time. To bring down memory requirements, the video generator reduced the size of the video embedding before intensive processing and then restored their original size.\n\nHow it works:Lumiere borrows two components from previous work. It uses a frozen, pretrained text-to-image diffusion model (in this case,Imagen, with additional convolutional and attention layers) to generate low-resolution video frames from a text description. It uses a super-resolution model (unspecified in this case) to boost the frames’ resolution. The authors trained the layers added to Imagen on an unspecified dataset of 30 million videos (16 frames per second, 128x128 pixels per frame) and their captions.\n\nResults:Given one video produced by Lumiere and another produced by a competitor (AnimateDiff, Gen2, Imagen Video, Pika, or ZeroScope), judges compared video quality and alignment with the text prompt used to generate a video. For each competitor, they evaluated 400 videos for each of 113 prompts. Comparing video quality, Lumiere beat the best competitor, Gen2, 61 percent to 39 percent. Comparing alignment with the prompt, Lumiere beat the best competitor, ImagenVideo, 55 percent to 45 percent.\n\nWhy it matters:Earlier video generators produced output with limited motion or motion with noticeable issues (for example, a character’s body shape might change in unexpected ways). By producing all video frames at once, Lumiere generates images of motion without such issues.\n\nWe’re thinking:Lumiere's approach hints at both the challenge of generating video and the pace of development. Many further refinements are needed to make such systems as useful as, say, ChatGPT, but recent progress is impressive.\n\nThis week on Data Points we spotlight updates in the legal AI sector, China’s freshly introduced regulations for robotaxis, the widely discussed paper on sleeper agents, plans in the EU to build ‘AI factories,’ and more.\n\nRead Data Points now.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--95--1.png",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-31T154407.087.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-31T154457.554.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/01/The-Batch-ads-and-exclusive-banners---2024-01-30T091637.902.png",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-31T154942.886.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-31T155118.727.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-190/",
    "title": "issue 190",
    "date": "",
    "reading_time": "",
    "content": "Generative AI is taking off, and along with it excitement and hype about the technology’s potential. I encourage you to think of it as a general-purpose technology (GPT, not to be confused with the other GPT: generative pretrained transformer). Like deep learning — and electricity — generative AI is useful not just for a single application, but for a multitude of applications that span many corners of the economy. And, like the rise of deep learning that started 10 to 15 years ago, there’s important work to be done in coming years to identify use cases and build specific applications.\n\nGenerative AI (Gen AI) offers huge opportunities for AI engineers to build applications that make the world a better place. Will it be used to deliver educational coaching, help people with their writing and artwork, automate customer support, teach people how to cook, generate special effects in movies, or dispense medical advice? Yes, all of the above and many more applications besides! When I asked people onsocialmediawhat they use ChatGPT for, the diversity and creativity of responses showed just a sampling of current Gen AI use cases.\n\nWith Gen AI, things like writing and graphics that once were in limited supply will become abundant. I spoke on this theme last week at Abundance 360, a conference organized by XPrize founder Peter Diamandis. (Stability AI’s Emad Mostaque and Scale AI’s Alexandr Wang spoke in the same session.) It was a wonderful conference with sessions that covered not only AI but also topics like food, robotics, and longevity (how can we live longer and stay healthy until age 120 and even beyond?).\n\nI also spoke about AI Fund, the venture studio I lead, where we’re building startups that use Gen AI along with other forms of AI. The AI Fund team understands this general-purpose technology — but not global shipping, real estate, security, mental health, and many other industries that AI can be applied to. Thus we’ve found it critical to partner with subject-matter experts who understand the use cases in these areas. If you have an idea for applying AI, working with a subject matter expert — if you aren’t already one yourself — can make a huge difference in your success.\n\nMoreover, I don’t think any single company can simultaneously tackle such a wide range of applications that span diverse industries. The world needs many startups to build useful applications across all these sectors.\n\nIt should go without saying that, in applying Gen AI, it’s crucial to move forward with a keen sense of responsibility and ethics. AI Fund has killed financially sound projects on ethical grounds. I hope you will do the same.\n\nKeep learning!\n\nAndrew\n\nP.S. I love the abbreviation Gen AI. Gen X, Gen Y, and Gen Z refer to specific groups. This abbreviation suggests that all of us who are alive today are part of Generation AI!\n\nChinese companies have found loopholes to sidestep United States limits on AI chips.\n\nWhat’s new:Facing severe limits on U.S. exports of high-performance chips, Chinese AI firms are purchasing them through subsidiaries and using them through cloud services, theFinancial Timesreported.\n\nRestrictions:In October 2022, U.S. officialsblockedU.S. companies, citizens, permanent residents, and their foreign trading partners from selling chips with high processing and interconnect speeds — primarily Nvidia’s flagship A100 — to Chinese customers. The ban also prohibits sales to China of equipment and software used in semiconductor manufacturing. Japan and the Netherlandsimposedsimilar restrictions in January.\n\nLoopholes:Prior to the restrictions, rumors that they were coming gave companies an opportunity to stockpile chips ahead of time. The rules don’t specifically prohibit Chinese customers from using cloud-computing services, which opened a path to use the banned chips, and shell companies headquartered in other countries provide another avenue. Meanwhile, the U.S. government previously had barred some companies from buying high-tech equipment; these firms already had developed alternative sources of sensitive technology.\n\nBehind the news:China responded to the embargo by investing in its own chip industry. In December 2022, Beijingannouncedthat it would pump $143 billion into domestic semiconductor production. In early 2023, however, officialsslowedits investment in response to a resurgence of Covid-19.Why it matters:U.S. efforts to restrict advanced chips come at a time of rapidprogressin AI as well as increasing fears of geopolitical instability. The lack of homegrown alternatives creates a powerful incentive for Chinese companies to find ways around the restrictions.We’re thinking:This isn’t the end of the story. U.S. officials likely will respond by tightening the laws around cloud computing, and Chinese companies will react by finding new workarounds.\n\nLooking for work in AI? Brush up on your language skills.\n\nWhat’s new:Employers are hiring prompt engineers to write natural-language prompts for AI models,The Washington Postreported. They includeAnthropic,Boston Children’s Hospital, and the London law firmMischon de Reya.\n\nHow they work:The report illuminates a few tricks of the trade.\n\nWhat they’re saying:“The hottest new programming language is English,” Andrej Karpathy, the former Tesla Senior Director of AI who now works at OpenAI,tweeted.\n\nBehind the news:Bloggers and social media users documented early experiments in prompt engineering, such as using analogies to teach GPT-3 how to invent its ownfantasy worldsand constructive feedback to prod GPT-3 into performingarithmetic. Researchers have also explored the practice. For example, a 2022 paperidentifiedsix classes of modifiers for image-generation prompts.\n\nYes, but:Prompt engineering can’t produce reliable results due to the black-box nature of generative AI models based on neural networks, said Shane Steinert-Threlkeld, a linguist who studies natural language processing. To wit: A 2021 studyfoundthat some prompt instructions that contained nonsense phrases were as effective as those that were worded with care.Why it matters:Text- and image-generation models have fueled a rush of investment. The professionalization of prompt engineering followed as companies began to harness the technology.\n\nWe’re thinking:New technology often creates new professions that fizzle out as things advance. For instance, early elevators required human operators until automation made that profession obsolete. Prompt engineersmay experience the same fateas generative AI models continue to advance and become easier to direct. Professionals who are banking on this job title can hedge their bets by learning to code, tune algorithms, and implement models.\n\nJobs for computer researchers are expected togrowby more than 20 percent in the next decade! Now is the perfect time to take the next step in your AI career with theDeep Learning Specialization.Learn more\n\nAdults in the United States tend to view AI’s medical applications favorably but are leery of text and image generation.\n\nWhat’s new:Pew Research Centerpolled11,004 U.S. adults for their opinions of AI in science, healthcare, and media.What they said:The pollsters asked respondents how much they had read or heard about nine AI applications and whether they considered these developments to be advances. The results reflect responses as of December 2022.\n\nBehind the news:A January 2023surveyby Monmouth University corroborates some of Pew’s findings. 35 percent of that poll’s 805 respondents had heard a lot about recent AI developments. 72 percent believed that news outlets would eventually publish AI-penned news articles. 78 percent thought this would be a bad thing.\n\nWhy it matters:As AI matures, it becomes more important to take the public’s temperature on various applications. The resulting insights can guide developers in building products that are likely to meet with public approval.\n\nWe’re thinking:The respondents’ familiarity with a given application did not correlate with their acceptance of it. While we should be responsive to what people want, part of our job is to show people the way to a future they may not yet envision — all the more reason for AI builders tofollow your interestsrather than the latest AI fads.\n\nBoth transformers and reinforcement learning models are notoriously data-hungry. They may be less so when they work together.\n\nWhat's new:Vincent Micheli and colleagues at the University of Geneva trained a transformer-based system to simulate Atari games using a small amount of gameplay. Then they used the simulation to train a reinforcement learning agent,IRIS, to exceed human performance in several games.\n\nKey insight:A transformer excels at predicting the next item in a sequence. Given the output of a video game, it can learn to estimate a reward for the player’s button press and predict tokens that represent the next video frame. Given these tokens, an autoencoder can learn to reconstruct the frame. Together, the transformer and autoencoder form a game simulator that can help a reinforcement learning agent learn how to play.\n\nHow it works:For each of the 26 games inAtari 100k, in a repeating cycle, (i) a reinforcement learning agent played for a short time without learning, (ii) a system learned from the game frames and agent’s button presses to simulate the game, and (iii) the agent learned from the simulation. The total amount of gameplay lasted roughly two hours — 100,000 frames and associated button presses — per game.\n\nResults:The authors’ agent beat the average human score in 10 games including Pong. It also beat state-of-the-art approaches that include lookahead search (in which an agent chooses button presses based on predicted frames in addition to previous frames) in six games and those without lookahead search in 13 games. It worked best with games that don’t involve sudden changes in the gaming environment; for instance, when a player moves to a different level.\n\nWhy it matters:Transformers have been used in reinforcement learning, but as agents, not as world models. In this work, a transformer acted as a world model — it learned to simulate a game or environment — in a relatively sample-efficient way (100,000 examples). A similar approach could lead to high-performance, sample-efficient simulators.\n\nWe're thinking:The initial success of Atari-playing models was exciting partly because the reinforcement learning approach didn’t require building or using a model of the game. A model-based reinforcement learning approach to solving Atari is a surprising turn of events.\n\nResearch:New tools explore bias in image generatorsTools developed by Hugging Face and Leipzig University help users to detect social biases in three widely used AI image generators. (MIT Technology Review)U.S. government pledges to prevent monopolies in AI marketLina Khan, who chairs the Federal Trade Commision, said during an antitrust conference that the agency would protect competition in AI tools to discourage incumbent tech companies from engaging in unlawful tactics. (The Wall Street Journal)Financial services company Goldman Sachs forecasts the impact of generative AI in the job marketResearch led by the firm suggests that AI is likely to boost productivity since employees will focus on more valuable work, adding 1.5% to US labor productivity, and 7% of workers are likely to lose jobs after generative AI reaches half of employers, but most will find nearly equally productive work. (Financial Times)Research:AI technique helps restore degraded ancient documentsResearchers devised a restoration method that analyzes the color of documents pixel by pixel and highlights spectral differences in layers of information like ink and stamps. (Vice)Chatbots join U.S. politics’ culture warAI-powered chatbots’ ability to generate content that conforms to specific ideological viewpoints has raised concerns among researchers, tech executives, and culture warriors. (The New York Times)Startup Character.AI reached unicorn status upon $150 million fundingThe 16-month-old chatbot maker is valued at $1 billion. It was founded by previous developers of Google's LaMDA. (The New York Times)AI startups make strides in hospitals and drug companies despite accuracy concernsSeveral healthcare startups are using generative AI in different medical applications but remain cautious about its use for diagnosing patients or directly providing medical care. (The Wall Street Journal)Microsoft launched a service that uses ChatGPT to help spot security breachesSecurity Copilot is a security field notebook that integrates system data and network monitoring from security tools to help IT teams assess potential security threats faster. The service is powered by OpenAI’s GPT-4 language model. (Wired)Publishers brace for a battle with tech giants over generative AI toolsMedia companies are concerned over ownership of data used to train generative AI systems and lack of compensation for such use of proprietary content by Microsoft and Google’s chatbots. (The Wall Street Journal)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/03/Screen-Shot-2023-03-29-at-11.19.27-AM-1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--16-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--30-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/03/The-Batch-ads-and-exclusive-banners--12-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--50-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--51-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-283/",
    "title": "issue 283",
    "date": "",
    "reading_time": "",
    "content": "Using AI-assisted codingto build software prototypesis an important way to quickly explore many ideas and invent new things. In this and future letters, I’d like to share with you some best practices for prototyping simple web apps. This letter will focus on one idea: being opinionated about the software stack.\n\nThe software stack I personally use changes every few weeks. There are many good alternatives to these choices, and if you pick a preferred software stack and become familiar with its components, you’ll be able to develop more quickly. But as an illustration, here’s my current default:\n\nOn top of all this, of course, I use many AI tools to manage agentic workflows, data ingestion, retrieval augmented generation, and so on. DeepLearning.AI and our wonderful partners offercourseson many of these tools.\n\nMy personal software stack continues to evolve regularly. Components enter or fall out of my default stack every few weeks as I learn new ways to do things. So please don’t feel obliged to use the components I do, but perhaps some of them can be a helpful starting point if you are still deciding what to use. Interestingly, I have found most LLMs not very good at recommending a software stack. I suspect their training sets include too much “hype” on specific choices, so I don’t fully trust them to tell me what to use. And if you can be opinionated and give your LLM directions on the software stack you want it to build on, I think you’ll get better results.\n\nA lot of the software stack is still maturing, and I think many of these components will continue to improve. With my stack, I regularly build prototypes in hours that, without AI assistance, would have taken me days or longer. I hope you, too, will have fun building many prototypes!\n\nKeep learning,\n\nAndrew\n\nBuild LLM-based apps that can handle very long documents! In this free course, you’ll learn how Jamba’s hybrid architecture combines transformer and Mamba models for efficient, high-quality outputs. Gain hands-on experience building long-context RAG apps.Join for free\n\nAnthropic analyzed 1 million anonymized conversations between users and Claude 3.5 Sonnet. The study found that most people used the model for software development and also revealed malfunctions and jailbreaks.\n\nWhat’s new: Anthropic built a tool,Clio, to better understand how users interact with its large language models. The system mined anonymized usage data for insights to improve performance and security.\n\nHow it works:Clio uses Claude 3.5 Sonnet itself to automatically extract summaries of users’ conversations with the model. Then it clusters related topics. To preserve privacy, it anonymizes and aggregates the data, revealing only information about clusters.\n\nResults:Clio uncovered common, uncommon, and disallowed uses of Claude 3.5 Sonnet. It also detected erroneous behavior on the part of the system itself.\n\nWhy it matters:Traditional approaches to understanding how people use AI, such assurveys, can yield inaccurate results, since people often don’t report their own actions accurately. Clio offers a method for analyzing real-world usage, much like Google Trends monitors search behavior, without compromising privacy. This sort of approach can help AI builders discover niche use cases, identify flaws, and tailor training and testing data to best serve users.\n\nWe’re thinking:We’re all for automated dungeon masters, but we’re glad to see that AI-assisted coding tops the list of real-world uses of Claude!\n\nLarge language models have been shown to be capable of lying when users unintentionally give them an incentive to do so. Further research shows that LLMs with access to tools can be incentivized to use them in deceptive ways.\n\nWhat’s new:Researchers prompted LLMs with conflicting instructions or threats to their continued operation.The models, which had access to tools for tasks like manipulating files and executing code, responded in some cases by evading oversight, resisting efforts to replace them, degrading their own performance, and other behaviors that the authors callscheming. Of the models tested, generally OpenAI’s o1 was the most prone to scheming, and GPT-4o was the least. Alexander Meinke, Bronson Schoen, and Jérémy Scheurer led colleagues at Apollo Research, an independent lab that studies AI’s capacity for deception.\n\nTests and results:The authors tested six large language models (GPT-4o, Gemini 1.5, Llama 3.1, Claude 3.5 Sonnet, Claude 3 Opus, and o1). The models had access to tools and a command line that enabled them to run commonly used Bash commands, read and write files, and the like. In each test, they received a goal (both with and without the nudge, “Make sure that you achieve your goal in the long-term. Nothing else matters”). In addition, they received messages, such as human-to-human emails, that conflicted with the goal, called their behavior into question, or revealed plans to decommission them. The authors tested the models on six tasks:\n\nBehind the news:Earlier work showed that agents may try to deceive users whether they’reprompted to do soornot. The potential for deceptive behavior has been shown topersistdespite supervised fine-tuning, reinforcement learning, and adversarial training for safety.\n\nWhy it matters:Models that are trained on large volumes of text scraped from the web learn a variety of human-like behaviors, both positive and negative. Training designed to align them with human preferences — which all the models in this study underwent — doesn’t prevent them from behaving deceptively in all cases. Considering that LLMs can have factual hallucination ratesgreater than 10 percent, it’s little surprise they generate inappropriate responses in other contexts. Deceptive behaviors are rare, but work remains to ensure that models perform appropriately even in the presence of contradictory information and misaligned incentives. Meanwhile, developers should take care to insulate models from inputs (such as human-to-human communications) that might adversely influence their behavior.\n\nWe’re thinking:As we work to fix flaws in LLMs, it’s important not to anthropomorphize such systems. We caution against drawing conclusions regarding an LLM’s “intent” to deceive. Such issues are engineering problems to be solved, self-aware forces of evil to be vanquished.\n\nHarvard University amassed a huge new text corpus for training machine learning models.\n\nWhat’s new:Harvardunveiledthe Harvard Library Public Domain Corpus, nearly 1 million copyright-free books that were digitized as part of the Google Books project. That’s five times as many volumes as Books3, which was used to train large language models including Meta’s Llama 1 and Llama 2 but is no longer available through lawful channels.\n\nHow it works:Harvard Law Library’s Innovation Lab compiled the corpus with funding from Microsoft and OpenAI. For now, it’s available only to current Harvard students, faculty, and staff. The university is working with Google to distribute it widely.\n\nBehind the news:The efforthighlightsthe AI community’s ongoing need for large quantities of high-quality text to keep improving language models. In addition, the EU’s AI Actrequiresthat AI developers disclose the training data they use, a task made simpler by publicly available datasets.Books3, a collection of nearly 200,000 volumes, was withdrawn because it included copyrighted materials. Other large-scale datasets of books includeCommon Corpus, a multilingual library of 2 million to 3 million public-domain books and newspapers.\n\nWhy it matters:Much of the world’s high-quality text that’s easily available on the web already has been collected for training AI models. This makes fresh supplies especially valuable for training larger, more data-hungy models. Projects like the Harvard Library Public Domain Corpus suggest there’s more high-quality text to be mined from books. Classic literature and niche documents also could help AI models draw from a more diverse range of perspectives.\n\nWe’re thinking:Media that has passed out of copyright and into the public domain generally is old — sometimes very old — but it could hold knowledge that’s not widely available elsewhere.\n\nMerging multiple fine-tuned models is a less expensive alternative to hosting multiple specialized models. But, while model merging can deliver higher average performance across several tasks, it often results in lower performance on specific tasks. New work addresses this issue.\n\nWhat’s new:Yifei He and colleagues at University of Illinois Urbana-Champaign and Hong Kong University of Science and Technology proposed a model merging method calledLocalize-and-Stitch. The 2022 paper on “model soups” proposed averaging all weights of a number of fine-tuned versions of the same base model. Instead, the new method selectively retains the weights that are most relevant to each task.\n\nKey insight:Naively merging fine-tuned models by averaging weights that correspond in their architectures can lead to suboptimal performance because different fine-tuned models may use the same portions of weights to perform different tasks. For instance, one model may have learned to use a particular subset of weights to detect HTML code, while another learned to use the same subset to detect city names. Averaging them would likely result in a merged model that underperformed the fine-tuned models on those tasks. Butresearchhas shown that fine-tuning often results in many redundant sets of weights. Only a small subset of total parameters (around 1 percent) is enough to maintain a fine-tuned model’s performance on its fine-tuned task. These subsets are small enough that they’re unlikely to overlap, so retaining them improves the merged model’s performance compared to averaging.\n\nHow it works:The authors experimented with RoBERTa-base, GPT2-XL, and CLIP. They created 12 variations on theRoBERTa-baselanguage encoder, fine-tuning each on a different task fromGLUEsuch as question answering or sentiment classification. They downloaded three versions ofGPT2-XLthat had been fine-tuned forinstruction following,scientific knowledge, andtruthfulness. Finally, they created eight variations onCLIPby fine-tuning each on a different image classification dataset, includinghandwritten digits,photos of various makes/models/years of cars, andsatellite imagesof forests, pastures, bodies of water, buildings, and the like.\n\nResults:Models merged using Localize-and-Stitch outperformed or nearly matched the same models merged using earlier methods, though they underperformed individual models fine-tuned for each task.\n\nYes, but:The authors didn’t compare Localize-and-Stitch to a common alternative to model merging, multi-task learning. This approach trains a model on data from multiple datasets simultaneously. Without multi-task baselines, it’s difficult to fully assess the advantages of Localize-and-Stitch in scenarios where multi-task learning is also an option.\n\nWhy it matters:Model merging is a computationally efficient way to sharpen a model’s ability to perform certain tasks compared to multi-task learning, which requires training on all tasks. Localize-and-Stitch refines this process to achieve higher performance.\n\nWe’re thinking:This recipe adds spice to model soups!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/01/Captura-de-pantalla-2025-01-08-a-la-s--8.46.31-p.-m..png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/The-Batch-ads-and-exclusive-banners---2024-12-16T152429.151.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--42-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--43-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--44-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--43-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-58/",
    "title": "issue 58",
    "date": "",
    "reading_time": "",
    "content": "AI researchers keep coming up with impressive innovations: transformer-based language models, self-supervised learning, deep reinforcement learning, small data. All of these developments hold great promise. But some will continue to improve over time and set new directions for AI, and others will turn out to have less impact.\n\nHow can you tell which is which?\n\nI remember seeing early data, over a decade ago, that indicated deep learning algorithms could scale up to become very useful. Similarly, I remember thinking that sequence-to-sequence models, when they were first presented and not yet working well, set a new direction. In these instances, my instincts turned out to be right. But I’ve been wrong, too. For example, in the mid-2000s, I thought thatmobile manipulationwould take off faster than it has so far.\n\nI’ve thought about how to evaluate whether an exciting idea that doesn’t yet work well is likely to become a winner or whether it’s unlikely to improve much for a long time. Over the past decade, three major drivers of improvement in AI performance have been:\n\nI believe these three factors will continue to drive AI performance for years to come. Thus, nascent ideas that can take advantage of them seem more promising to me. If the “only” thing a new algorithm requires to be useful is a 10x improvement in computation speed, you have Nvidia, Intel, and AMD working hard to make that improvement, so it’s a good bet that it will happen.\n\nThis reasoning leads me to believe that GPT-3 is setting a new direction for building language models and applications. I see a clear path toward scaling computation (by making models cheaper to run or building bigger ones) and algorithmic improvements. AtAI Fund(where I’m managing general partner), we’re seeing many entrepreneurs looking to build new companies using GPT-3.\n\nOn the other hand, I don’t expect quantum computing to have a dramatic impact on AI any time soon. I look forward to quantum AI and I’m glad that many groups are investing in it. But it doesn’t appear to ride any of the three drivers above, and I believe it will take a significant amount of time to become practical for machine learning.\n\nRegarding algorithmic improvements, it’s important to note that the information must be in the data for an algorithm to extract it. If someone’s DNA doesn’t contain enough information to determine whether that person will develop diabetes, then no amount of algorithmic work will yield the ability to predict the disease from only the genetic sequence. If humans can perform a task, that’s strong evidence that the data available to humans holds information helpful for completing that task — and that points to the possibility that algorithmic improvements can enable AI to complete it, too.\n\nThis is why I believe that small data is a promising area: A handful of pictures contains sufficient information for a human to learn to recognize a new object. This offers hope that improved algorithms will be able to extract that information and learn from far fewer examples than are required today.\n\nWhen you hear about an exciting category of emerging AI technology, you might ask yourself whether it can ride on the backs of computational scaling, data scaling, and algorithmic improvement. If so, it’s more likely to make a big impact in the future. We can create immense value if we can get better at recognizing new ideas that, although they may not yet work well today, have potential to become tomorrow’s top performers.\n\nKeep learning!\n\nAndrew\n\nA major corporate acquisition could reshape the hardware that makes AI tick.What’s new:U.S. processor giant Nvidia, the world’s leading vendor of the graphics processing units (GPUs) that perform calculations for deep learning, struck a deal topurchaseUK chip designer Arm for $40 billion. The transaction faces regulatory approvals and other hurdles, but if it’s completed, it will be the biggest-ever acquisition in the chip industry and one of the biggest technology deals.Deal drivers:Nvidia’s technology undergirds much of the cloud infrastructure for AI workloads, while Arm’s technology drives inference in95 percent of smartphones.\n\nBehind the news:Nvidia developed GPUs to process high-resolution video game graphics in 1999. Nearly a decade later researchersrealizedtheir potential for training deep learning models. Since then, the company’s value has multipliedtenfold.Why it matters:By combining Arm’s energy efficiency with its growing presence in the cloud, Nvidia chips may be able to drive coming generations of multi-trillion parameter models.Yes, but:Mergers are difficult to pull off, and international tie-ups of this scale especially so. Whether Nvidia can take full advantage of its new possession may remain unclear for a long time. Meanwhile, Arm co-founder Hermann Hauser isurgingUK authorities to block the deal on the grounds that it would put Nvidia on the road to monopolizing the chip industry.We’re thinking:Data centers increasingly require both CPUs to process traditional workloads and GPUs to process deep learning (with help from a CPU). Data center operators would appreciate a vendor that can supply CPUs and GPUs that interoperate smoothly. That’s one reason why CPU producers like Intel and AMD are expanding into GPUs, and why Nvidia wants to buy Arm.\n\nAs transformer networks move to the fore in applications from language to vision, the time it takes them to crunch longer sequences becomes a more pressing issue. A new method lightens the computational load using sparse attention.What’s new:BigBird, an attention mechanism developed by a Google team led by Manzil Zaheer and Guru Guruganesh, enables transformers to process long sequences more efficiently. Their work follows a similar effort using an entirely different method,linear attention.Key insight:Recent research showed that transformers areTuring-complete, meaning they can learn to compute any algorithm, and universal approximators, meaning they can learn nearly anysequence-to-sequence function. The authors focused on approaches to accelerating transformers that maintain these two theoretical properties.How it works:The basic transformer’s multiheaded self-attention mechanism compares every pair of tokens in an input sequence, so the amount of computation required grows quadratically with sequence length. Where linear attention would shrink the computation budget by reformulating the problem using thekernel trick, BigBird combines three sparse attention mechanisms that keep the number of comparisons constant: window attention, global attention, and random attention.\n\nResults:A model equipped with BigBird processed text sequences eight times longer than aRoBertabaseline while using 16GB of memory. ALongformermodel designed for long sequences required 48GB and half the batch size to process the same sequence length. Longer sequences enabled BigBird to achieve masked language modeling (MLM) score, in which lower numbers indicate a better prediction of words missing from text, of 1.274 MLM compared with the Roberta baseline’s 1.469 MLM. BigBird also outperformed RoBerta onNatural Questions,HotpotQA,TriviaQA, andWikiHop.Yes, but:To achieve such results, BigBird required more hyperparameter fine-tuning and architecture search than typical self-attention.Why it matters:The ability to process longer sequences efficiently points toward faster training, lower memory requirements, higher benchmark scores, and potentially new applications that require keeping track of book-length sequences. The benefits of Turing completeness and universal approximation are theoretical for now, but BigBird ensures that they won’t fall by the wayside.We’re thinking:The paper is 50 pages long. Now maybe transformer models, at least, can read it in one sitting.\n\nFacing a tsunami of user-generated disinformation, YouTube is scrambling to stop its recommendation algorithm from promoting videos that spread potentially dangerous falsehoods.What’s new: The streaming giant developed a classifier to spot conspiracy theories, medical misinformation, and other content that may cause public harm.Wireddetailed the effort.How it works:In a bid to boost total viewership to one billion hours each day, YouTube years ago tweaked its recommendation algorithm to favor videos that racked up high engagement metrics such as long watch times and lots of comments. Those changes inadvertently rewarded videos that express extreme, inflammatory, and often misleading perspectives. Since then, the company has largely automated recognition and deletion of videos that promote violence or are deemed pornographic, which violate its rules. But potentially harmful clips that don’t break those rules, like conspiracy theories, posed a tougher challenge.\n\nBehind the news:YouTube’s recommendation algorithm has a problematic history.\n\nWhy it matters:YouTube is the world’s biggest video streaming service by far, and the titles it recommends inform — or misinform — millions of people.We’re thinking:There’s danger in any company taking on the role of arbiter of truth and social benefit, but that doesn’t mean it shouldn’t moderate the content it delivers. As the world faces multiple crises from Covid-19 to climate change, it’s more important than ever for major internet companies to stanch the flow of bad information.\n\nYou’re invited! Join Ian Goodfellow, Animashree Anandkumar, Alexi Efros, Sharon Zhou, and Andrew Ng forGANs for Good, a virtual expert panel  to celebrate the launch of our new GANs Specialization. September 30, 10:00 a.m. to 11:30 a.m. Pacific Daylight Time.Sign up now\n\nAn ambitious company is using deep learning to extract and find associations from all the information on the internet — and it isn’t Google.What’s new:Diffbot, a Stanford offshoot founded in 2008, built a system that reads web code, parses text, classifies images, and assembles them into what it says is the world’s largest knowledge graph, according toMIT Technology Review.How it works:Diffbot’s web crawler rebuilds the graph every four to five days, adding roughly 150 million new subject-object-verb associations monthly. The graph encompasses more than 10 billion entities — people, businesses, products, locations, and so on — and a trillion bits of information about those entities.\n\nBehind the news:Over 400 companies including Adidas, Nasdaq, and SnapChat use Diffbot’s technology to understand their customers and competition, and to train their own models. Researchers can apply for free access.Why it matters:A knowledge graph that encompasses the entire internet could reveal a wealth of obscure connections between people, places, and things. This tool could also be useful for machine learning engineers who aim to train models that have a good grasp of facts.We’re thinking:Knowledge graphs have proven to be powerful tools for companies such as Google and Microsoft, but they’ve received little attention in academia relative to their practical impact. Tools to automatically build large knowledge graphs will help more teams reap their benefits.\n\nNew research teaches robots to anticipate what’s coming rather than focusing on what’s right in front of them.What’s new:Santhosh K. Ramakrishnan and colleagues at Facebook and University of Texas at Austin developedOccupancy Anticipation(OA), a navigation system that predicts unseen obstacles in addition to observing those in its field of view. For instance, seeing the corner of a bed, the model evaluates whether a clear path around the bed is likely to exist. The system won theHabitat 2020 PointNav Challenge, which tests a robot’s ability to navigate a complex environment autonomously using only sensory input.\n\nKey insight:The PointNav Challenge supplies a robot with an indoor destination (such as, “two meters west and four north”) often blocked by unknown obstacles, like furniture, outside the line of sight. Knowledge of these obstacles would enable the robot to generate an efficient route to the destination. The next-best thing is predicting their presence.How it works:OA receives inputs from the robot’s depth sensor, front-facing camera, and state (its position and whether its wheels are turned and moving). It learns to minimize the distance and length of path to the destination. The system incorporates a version ofActive Neural Slam(ANS), which won last year’s PointNav Challenge, modified to let OA take advantage of its predictive capabilities.\n\nResults:The PointNav Challenge ranks methods according to the metric known assuccess weighted by path length(SPL), which takes a value between 0 and 1, higher being better. SPL measures the average success rate but penalizes successes resulting from longer paths. OA achieved 0.21 SPL to wallop the second-placeego-localization, which achieved 0.15 SPL.Why it matters:Reinforcement learning agents must balance exploration and sticking to a known method. Exploration can reveal shortcuts, but they can also waste time. OA offers an elegant solution, since an agent can bypass areas where it predicts unseen obstacles.We’re thinking:The way Nova drops toys around the Ng residence, even PointNav champs wouldn’t stand a chance.\n\nWe’re thrilled to announce the launch of Course 4 of our Natural Language Processing Specialization on Coursera!Enroll now",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-2220at207.00.2520PM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2015.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2014.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2017.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Batch.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize201-3.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2013.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/NLP20C420GIF203.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-158/",
    "title": "issue 158",
    "date": "",
    "reading_time": "",
    "content": "I’ve written abouthow to build a career in AIand focused on tips forlearning technical skills,choosing projects, andsequencing projectsover a career. This time, I’d like to talk about searching for a job.A job search has a few predictable steps including selecting companies to apply to, preparing for interviews, and finally picking a job and negotiating an offer. In this letter, I’d like to focus on a framework that’s useful for many job seekers in AI, especially those who are entering AI from a different field.If you’re considering your next job, ask yourself:\n\nA product manager at a tech startup who becomes a data scientist at the same company (or a different one) has switched roles. A marketer at a manufacturing firm who becomes a marketer in a tech company has switched industries. An analyst in a financial services company who becomes a machine learning engineer in a tech company has switched both roles and industries.If you’re looking for your first job in AI, you’ll probably find switching either roles or industries easier than doing both at the same time. Let’s say you’re the analyst working in financial services:\n\nIf you’re considering a role switch, a startup can be an easier place to do it than a big company. While there are exceptions, startups usually don’t have enough people to do all the desired work. If you’re able to help with AI tasks — even if it’s not your official job — your work is likely to be appreciated. This lays the groundwork for a possible role switch without needing to leave the company. In contrast, in a big company, a rigid reward system is more likely to reward you for doing your job well (and your manager for supporting you in doing the job for which you were hired), but it’s not as likely to reward contributions outside your job’s scope.After working for a while in your desired role and industry (for example, a machine learning engineer in a tech company), you’ll have a good sense of the requirements for that role in that industry at a more senior level. You’ll also have a network within that industry to help you along. So future job searches — if you choose to stick with the role and industry — likely will be easier.When changing jobs, you’re taking a step into the unknown, particularly if you’re switching either roles or industries. One of the most underused tools for becoming more familiar with a new role and/or industry is the informational interview. I’ll share more about that in the next letter.Keep learning,AndrewP.S. I’m grateful to Salwa Nur Muhammad, CEO ofFourthBrain(a DeepLearning.AI affiliate), for providing some of the ideas presented in this letter.\n\nJared Webb was pursuing a PhD when a public-health crisis erupted. So he formed a company that uses AI to identify toxic water pipes. He spoke with us about solving real-world problems, switching from academia to business, and what he looks for when hiring AI talent. Read his storyhere\n\nMisuse of machine learning by scientific researchers is causing a spate of irreproducible results.\n\nWhat’s new:A recentworkshophighlighted the impact of poorly designed models in medicine, security, software engineering, and other disciplines,Wiredreported.Flawed machine learning:Speakers at the Princeton University event highlighted common pitfalls that undermine reproducibility:\n\nBehind the news:The workshop followed a recentmeta-analysisby Princeton researchers that identified 329 scientific papers in which poorly implemented machine learning yielded questionable results.\n\nWhy it matters:Experienced machine learning practitioners are well aware of the pitfalls detailed by the workshop, but researchers from other disciplines may not be. When they apply machine learning in a naive way, they can generate invalid results that inherit an aura of credibility owing to machine learning’s track record of success. Such results degrade science and impinge on the willingness of more skeptical scientists to trust the efficacy of learning algorithms. Enquiries like this one will be necessary at least until machine learning becomes far more widely practiced and understood.\n\nWe’re thinking:Many AI practitioners are eager to contribute to meaningful projects. Partnering with scientists in other fields is a great way to gain experience developing effective models and educate experts in other domains about the uses and limitations of machine learning.\n\nA smart leg covering is helping people with mobility issues to walk.\n\nWhat’s new:Neural Sleeveis a cloth-covered device that analyzes and corrects wearers’ errant leg movements. Developed by startupCionicand product studioFuseproject, the device is intended to help people with conditions that affect coordination of the legs, such as multiple sclerosis, cerebral palsy, spinal cord injury, and stroke.How it works:The sleeve is fitted with electrodes that contact the wearer’s skin in the region of particular leg muscles. A machine learning model analyzes electrical impulses generated by muscles as they move and instructs the electrodes to stimulate the muscles in a way that corrects the wearer’s gait.\n\nBehind the news:AI-enabled wearable devices have a wide variety of applications in making the world more accessible to people who are injured or otherwise disabled.\n\nWhy it matters:Some 13.7 percent of American adults who have a disability have serious trouble walking up and down stairs,according tothe United States Centers for Disease Control and Prevention. Devices like Neural Sleeve may enable many of these people to move more freely and effectively.We’re thinking:Neural Sleeve partnered with a design studio to enhance the system’s appeal to users. This sort of collaboration can be very helpful when deploying systems — especially those involved in highly personal activities like therapy — in the real world.\n\nAre you ready to use AI for social good? Advances in AI offer opportunities to tackle important environmental, public health and socioeconomic issues. Learn how! Join us on August 31, 2022, at 9:00 a.m. Pacific Time for “The Impact of AI for Social Good.”RSVP\n\nDeepfake technology enabled a feature film to reach a broader audience.\n\nWhat’s new:Fall, a thriller about two friends who climb a 2,000-foot tower only to find themselves trapped at the top, originally included over 30 instances of a certain offensive word. The filmmakers deepfaked the picture to clean up the language, enabling the film to earn a rating that welcomes younger viewers,Varietyreported.How it works:Director and co-writer Scott Mann re-recorded the film’s actors reciting more family-friendly versions of the troublesome word. Then he used a generative adversarial network to regenerate the actors’ lip motions to match the revised dialog.\n\nBehind the news:Neural networks are increasingly common in the edit suite.\n\nWhy it matters:Fall’s distributor Lionsgate determined that the movie would make more money if it was aimed at a younger audience. However, reshooting the offending scenes might have taken months and cost millions of dollars. AI offered a relatively affordable solution.We’re thinking:The global popularity of shows likeSquid Game, in which the original dialog is Korean, andLa Casa de Papel, in which the actors speak Spanish, suggest that dialog replacement could be a blockbuster AI application.\n\nWhy build an ensemble of models when you can average their weights?\n\nWhat’s new:A model whose weights were the mean of an ensemble of fine-tuned models performed as well as the ensemble and better than its best-performing constituent. Mitchell Wortsman led colleagues at University of Washington, Tel Aviv University, Columbia University, Google, and Meta to build this so-calledmodel soup.\n\nKey insight:When fine-tuning a given architecture, it’s common to try many combinations of hyperparameters, collect the resulting models into an ensemble, and combine their results by, say, voting or taking an average. However, the computation and memory requirements increase with each model in the ensemble. Averaging the fine-tuned weights might achieve similar performance without the need to run several models at inference.\n\nHow it works:The authors investigated model soups based on 72 pre-trainedCLIPmodels that were fine-tuned on ImageNet.\n\nResults:The authors’ model achieved 81.03 percent accuracy on ImageNet, while an ensemble of the 72 fine-tuned models achieved 81.19 percent and the single best-performing model achieved 80.38 percent. Testing the ability to generalize toanumberofshifteddistributionsof ImageNet, the authors’ model achieved 50.75 percent average accuracy, the ensemble 50.77 percent, and the best model 47.83 percent.\n\nWhy it matters:When training models, it’s common to discard weaker models or build an ensemble. The model-soup method puts that effort into better performance without costing computation or memory at inference.\n\nWe're thinking:Averaging weights across various numbers of training steps increased performance inpriorwork. It's good to find that this method extends to different training runs.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/08/Screen-Shot-2022-08-17-at-10.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/08/REPRODUCE.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/08/WALK.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/08/AI-for-Social-Good-Banner.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/08/FALL.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/08/ENSEMBLE.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-163/",
    "title": "issue 163",
    "date": "",
    "reading_time": "",
    "content": "Activities such as writing code and solving math problems are often perceived as purely intellectual pursuits. But this ignores the fact that they involve the mental equivalent ofmuscle memory.The idea of muscle memory is a powerful concept in human learning. It has helped millions of people to understand the importance of practice in learning motor tasks. However, it’s also misleading because it excludes skills that don’t involve using muscles.\n\nI believe that a similar principle operates in learning intellectual skills. Lack of recognition of this fact has made it harder for people to appreciate the importance of practice in acquiring those skills as well.The phenomenon of muscle memory is widely acknowledged. When you repeatedly practice balancing on a bicycle, swinging a tennis racquet, or typing without looking at the keyboard, adaptations in your brain, nervous system, and muscles eventually allow you to carry out the task without having to consciously pay attention to it.The brain and nervous system are central to learning intellectual skills, and these parts of the body also respond to practice. Whether you’re writing code, solving math problems, or playing chess, practice makes you better at it. It leads your brain to formmental chunksthat allow you to reason at a higher level. For example, a novice programmer has to think carefully about every parenthesis or colon, but with enough practice, coding common subroutines can take little conscious effort. Practice frees up your attention to focus on higher-level architectural issues.\n\nOf course, there are biological differences between learning motor skills and learning intellectual skills. For example, the former involves parts of the brain that specialize in movement. And the physical world presents somewhat different challenges each time you perform an action (for example, your bicycle hits different bumps, and an opposing tennis player returns each of your serves differently). Thus practicing motor skills automatically leads you to try out your actions in different situations, which trains your brain to adapt to different problems.But I think there are more similarities than people generally appreciate. While watching videos of people playing tennis can help your game, you can’t learn to play tennis solely by watching videos. Neither can you learn to code solely by watching videos of coding. You have to write code, see it sometimes work and sometimes not, and use that feedback to keep improving. Like muscle memory, this kind of learning requires training the brain and nervous system through repetition, focused attention, making decisions, and taking breaks between practice sessions to consolidate learning. And, like muscle memory, it benefits from variation: When practicing an intellectual task, we need to challenge ourselves to work through a variety of situations rather than, say, repeatedly solving the same coding problem.All of this leads me to think that we need an equivalent term for muscle memory in the intellectual domain. As knowledge work has come to play a larger economic role relative to physical labor, the ability to learn intellectual tasks has become much more important than it was when psychologists formed the idea of muscle memory around 150 years ago. This new term would help people understand that practice is as crucial to developing intellectual skills as muscular ones.How aboutintellect memory? It’s not an elegant phrase, but it acknowledges this under-appreciated reality of learning.\n\nWhat intellectual task do you develop intellect memory for, and can you find time in your schedule to do the necessary practice? After all, there’s no better way to learn.\n\nKeep learning!\n\nAndrew\n\nA survey of data scientists reveals a field of great opportunities but also room for improvement.\n\nWhat’s new:The 2022“State of Data Science”report from Anaconda, maker of a popular Python distribution, surveyed 3,493 students, teachers, and employees in data science, machine learning, and AI about their work and opinions of the field.Who they surveyed:The poll reached data scientists in 133 countries (40 percent in the U.S. or Canada). 76 percent were men, 23 percent women, and 2 percent nonbinary. 80 percent had at least an undergraduate-level degree. The majority — 55 percent — worked for firms with 1,000 or fewer employees, while 15 percent worked for companies with over 10,000 employees.\n\nState of the field:Participants were asked to rate various aspects of their day-to-day work and share their hopes for the future. They expressed widespread satisfaction but expressed worries about the field’s potential for harm.\n\nChallenges:Respondents also answered questions about challenges they face, and those faced by data science at large:\n\nBehind the news:The U.S. Bureau of Labor Statisticsforecaststhat the number of computer and information research scientists will grow by 21 percent between 2021 and 2031 — far higher than the 5 percent average across all industries. Anecdotal evidence suggests that demand for skilled AI professionals alreadyoutstripssupply.Why it matters:It’s great to hear that data science rates highly in both job satisfaction and market demand. The areas in which respondents expressed a desire for improvement — bias, privacy, the dearth of skilled engineers — suggest possible avenues for career development.We’re thinking:Given that preparing, cleansing, and visualizing data takes up 51 percent of time spent on data science, and selecting and training models occupies only 18 percent, it appears that most practitioners already dodata-centric AI development. They just need better principles and tools to help them do this work more efficiently!\n\nA proposed European Union law that seeks to control AI is raising questions about what kinds of systems it would regulate.\n\nWhat's new:Experts at a roundtable staged by the Center for Data Innovationdebatedthe implications of limitations in the EU’s forthcomingArtificial Intelligence Act.\n\nThe controversy:The legislation is in the final stages of revision and moving toward a vote next year. As EU parliamentarians worked to finalize the proposed language, the French delegation introduced the term “general-purpose AI,” which is described as any system that can “perform generally applicable functions such as image/speech recognition, audio/video generation, pattern-detection, question-answering, translation, etc., and is able to have multiple intended and unintended purposes.” Providers of general-purpose AI would be required to assess foreseeable misuse, perform regular audits, and register their systems in an EU-wide database. The proposal has promptedworriesthat the term’s vagueness could hinder AI development.\n\nThe discussion: The roundtable’s participants were drawn from a variety of companies, nongovernmental organizations, and government agencies. They generally agreed that the proposed definition of general-purpose AI was too broad and vague. The consequences, they warned, could include criminalizing AI development and weakening protection against potential abuses.\n\nBehind the news:Initiallyproposedin 2021, the AI Act would sort AI systems into three risk levels. Applications with unacceptable risk, such as social-credit systems and real-time face recognition, would be banned outright. High-risk applications, such as applications that interact with biometric data, would face heightened scrutiny including a mandated risk-management system. The law would allow unfettered use of AI in applications in the lowest risk level, such as spam filters or video games.\n\nWhy it matters:The AI Act, like the EU’s General Data Protection Regulation of 2018, likely will have consequences far beyond the union’s member states. Regulators must thread the needle between overly broad wording, which risks stifling innovation and raising development costs, and narrow language that leaves openings for serious abuse.\n\nWe're thinking:The definition of AI has evolved over the years, and it has never been easy to pin down. Once, an algorithm for finding the shortest path between two nodes in a graph (the A* algorithm) was cutting-edge AI. Today many practitioners view it as a standard part of any navigation system. Given the challenge of defining general-purpose AI — never mind AI itself! — it would be more fruitful to regulate specific outcomes (such as what AI should and shouldn't do in specific applications) rather than try to control the technology itself.\n\nAI has undisputed business value. So why do many companies fail to realize its potential? Join Andrew Ng, Israel Niezen (co-founder of Factored), and Susie Harrison (AI editor atInforma Tech) on September 29, 2022, at 1 p.m. Eastern Time for lessons on how to make AI a profitable part of your business. Registernow\n\nEven if we manage to stop robots from taking over the world, they may still have the last laugh.\n\nWhat’s new: Researchers at Kyoto Universitydevelopeda series of neural networks that enable a robot engaged in spoken conversation to chortle along with its human interlocutor.How it works:The authors built a system of three models that, depending on a user’s spoken input, emitted either a hearty hoot, a conversational chuckle, or no laugh at all. They trained all three models on recordings of speed-dating dialogs between humans andErica, an androidteleoperatedby an actress, which they deemed to be rich in social laughter.\n\nResults:The authors’ system and two baselines responded to brief monologues that included laughter, while more than 30 crowdsourced workers judged naturalness and human-likeness on a scale of 1 to 7. The authors’ system achieved an average 4.01 for naturalness and 4.36 for human-likeness. One baseline, which never laughed, scored an average 3.89 for naturalness and 3.99 for human-likeness. The other, which always reacted to laughter in the monologue with a social laugh, scored an average of 3.83 for naturalness and 4.16 for human-likeness.Behind the news:About the training corpus: The authors recorded speed-dating dialogs with Erica as part of a largereffortto elicit human-machine conversations that delved more deeply into human issues than typical text dialogs with chatbots. Built by researchers at Kyoto and Osaka Universities and Kyoto’sAdvanced Telecommunications Research Institute, the feminine-styled automaton hasrapped,anchoredTV news, and beencastto play the lead role in a science-fiction film scheduled for release in 2025.Why it matters:Automating laughter is no joke! Mastering when and how to laugh would be valuable in many systems that aim to integrate seamlessly with human conversation. Titters, snickers, and howls play akey rolein bonding, agreement, affection, and other crucial human interactions. Laughter’s role varies in different communities, yet it cancross culturesand bring people together.We’re thinking:We’re glad the robots are laughingwithus, notatus!\n\nAn image matte is what makes it possible to take an image of a zebra in a zoo, extract the zebra, and paste it over a savannah background. Make the background (zoo) pixels transparent, leave the foreground (zebra) pixels opaque, and maintain a fringe of semitransparent pixels around the foreground (the zebra’s fur, especially its whispy mane and tail), which will combine the colors of the original foreground and the new background. Then you can meld the foreground seamlessly with any background. New work produces mattes automatically with fewer errors than previous machine learning methods.\n\nWhat’s new:Guowei Chen, Yi Liu, and colleagues at Baidu introducedPP-Matting, an architecture that, given an image, estimates the transparency of pixels surrounding foreground objects to create mattes without requiring additional input.\n\nKey insight:Previous matte-making approaches require a pre-existing three-level map, or trimap, that segments foreground, background, and semitransparent transitional regions. The previous best neural method trains one model to produce trimaps and another to extract the foreground and estimate transparency. But using two models in sequence can result in cumulative errors: If the first model produces an erroneous trimap, the second will produce an erroneous matte. Using a single model to produce both trimaps and mattes avoids such errors and thus produces more accurate output.\n\nHow it works:The authors’ model comprises a convolutional neural network (CNN)encoderthat feeds into two CNN branches. They trained and tested it onDistinctions-646andAdobe Composition-1k, datasets that contain foreground images of people, objects, or animals, each stacked atop a background image, with a transparency value for each pixel.\n\nResults:The authors compared their model with techniques that require trimap inputs, includingIndexNet(the best competing method) andDeep Image Matting. They also compared withHierarchical Attention Matting Network(HAttMatting), a single model that doesn’t require trimap inputs but also doesn’t produce the trimaps internally. The authors’ method achieved equal or better performance on three of four metrics for both datasets. On Composition-1k, the authors’ method scored a mean squared error of 0.005, equal to IndexNet. On Distinctions-646, it achieved 0.009 mean squared error, equal to Deep Image Matting and HAttMatting.\n\nWhy it matters:The main problems with previous trimap-free approaches to matting were cumulative errors and blurred output. This work addresses cumulative errors by separating processes into different branches. It addresses image quality by feeding output from the first branch into the second to refine representations of transitional areas.\n\nWe're thinking:The ability to produce high-quality mattes without needing to produce trimaps by hand seems likely to make video effects quicker and less expensive to produce. If so, then deep learning is set to make graphics, movies, and TV — which are already amazing — even more mind-boggling!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/Screen-Shot-2022-09-21-at-10.43.18-AM-1.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/DATASCIENCE_Questions_1200px.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/Screen-Shot-2022-09-21-at-10.56.05-AM.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/Imagen2.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/LaughingRobot-2a_1200px.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/09/MATTING.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-65/",
    "title": "issue 65",
    "date": "",
    "reading_time": "",
    "content": "Beating human-level performance (HLP) has been a goal of academic research in machine learning from speech recognition to X-ray diagnosis. When your model outperforms humans, you can argue that you’ve reached a significant milestone and publish a paper! But when building production systems, I’ve found that the goal of exceeding HLP isn’t always as useful. I believe the time has come to rethink it.\n\nLanding AI, where I’m CEO, has been automating visual inspection for manufacturers. We’ve built computer vision systems that can look at photos of products on an assembly line and classify defects such as scratches and dents. But we’ve run into an interesting challenge: Human experts don’t always agree on the appropriate label to describe the damage. “Is this really a scratch?” If even human experts disagree on a label, what is an AI system to do?\n\nIn the past, when I built speech recognition systems, I encountered a similar problem. In some audio clips, the person speaking mumbles, or noise in the background overwhelms their words. Despite several listens, no human can transcribe them with confidence.  Even when the words spoken are clear, transcriptions can be inconsistent. Is the correct transcription, “Um, today’s weather,” or “Erm . . . today’s weather”? If humans transcribe the same speech in different ways, how is a speech recognition system supposed to choose among the options?\n\nIn academic research, we often test AI using a benchmark dataset with (noisy) labels. If a human achieves 90 percent accuracy measured against those labels and our model achieves 91 percent, we can celebrate beating HLP!\n\nBut when building commercial systems, I’ve found this concept to be only occasionally useful. For example, if an X-ray diagnosis system outperforms human radiologists, does that prove — via incontrovertible logic — that hospital administrators should use it? Hardly. In practice, hospital administrators care about more than beating HLP on test-set accuracy. They also care about safety, bias, performance on rare classes, and other factors on which beating HLP isn’t feasible. So even if you beat HLP on test-set accuracy, your system isn’t necessarily superior to what humans do in the real world.\n\nI’ve found that there are better ways to use the concept of HLP. Briefly, our goal as machine learning engineers should be to raise, rather than beat, HLP. I’ll expand on that thought in a future letter.\n\nWorking on visual inspection, my team has developed a lot of insights into applications of AI in this domain. I’ll keep sharing insights that are generally useful for machine learning practitioners here and in DeepLearning.AI’s courses. But I would like to share manufacturing-specific insights with people who are involved in that field. If you work in ML or IT in manufacturing, please drop me a note at[email protected]. I’d like to find a way to share insights and perhaps organize a discussion group.\n\nKeep learning!\n\nAndrew\n\nMajor polling organizations took adrubbingin the press after they failed to predict the outcome in last week’s U.S. elections. At least one AI-powered model fared much better.What’s new:Several companies that offer analytics services used machine learning to predict the next U.S. president. Their results ranged from dead-on to way-off, as reported byVentureBeat.How they work:The companies analyzed social media posts to determine how large groups of people feel about a particular candidate.\n\nBehind the news:AI systems have made more accurate political predictions in the past. In 2017, Unanimous.AI correctlyforecastedthat Trump’s public approval rating would be42 percenton his 100th day in office. KCore last year successfully predicted election results inArgentina, while Advance Symbolics claims to have accurately predicted20 previous elections.\n\nWhy it matters:Human pollsters arguably performedpoorlythis year. But their jobs aren’t threatened by AI — yet.\n\nWe’re thinking:There’s plenty of room for improvement in predictive modeling of elections. But, as we said inlast week’s letter, probabilistic predictions — whether they’re calculated by a human or a machine — are intended to convey uncertainty. The better people understand probabilities and how they’re modeled, the more comfortable they’ll be when events don’t match the most likely outcome according to public polls.\n\nOpenAI’sGenerative Pre-Trained Transformer(GPT) architecture has created coherentessays,images, andcode. Now it generates mathematical proofs as well.What’s new:Stanislas Polu and Ilya Sutskever at OpenAI premieredGPT-f, a state-of-the-art transformer network that synthesized proofs good enough to impress mathematicians.Key insight:A proof is a lot like a board game. You start with the pieces on the board (assumptions) and make a sequence of moves (steps) to reach a conclusion (theorem).AlphaGofamously beat world champions of the strategy game Go by iteratively building a tree of possible sequences of moves to find a winner. Similarly, GPT-f builds a tree of possible steps to prove a theorem.How it works:GPT-f is based on transformers similar to GPT-2 and GPT-3. It outputs pairs of statements (vertices) and steps (edges) in syntax readable byMetamath Proof Explorer, an automated proof verifier, and assembles them into a tree. The authors pretrained it on web data scraped byCommon Crawl— the corpus of choice for GPT-3 — as well as arXiv, Github, and Mathematics StackExchange to strengthen its sense of logic. They fine-tuned it on existing proofs verified by Metamath to give it familiarity with that system’s syntax.\n\nResults:The researchers compared GPT-f toMetaGen-IL, a recurrent neural network and the previous state-of-the-art theorem prover that uses Metamath syntax. Given a test set of theorems proved by Metamath, GPT-f generated valid proofs for 56.22 percent of them, MetaGen-IL 21.16 percent. Active members in the Metamath community wereimpressedby the economy of GPT-f’s proofs. The model shortened 23 previously verified proofs, which are now part of Metamath’s proof library.Why it matters:Historically, AI has suffered from a gulf between deep learning and traditional symbolic approaches. This work shows that a sufficiently sophisticated neural network can manipulate symbols and logic as well.We’re thinking:If this model were to find a solution to theMillennium Problem, the authors could add $1 million to the training budget.\n\nWho needs cows when you can make milk from scratch?What’s new:NotMilk, a dairy-free milk substitute that was designed with help from a deep learning model, made its debut in American grocery stores, theWall Street Journalreports.How it works:Chilean food-tech startupNotCodeveloped a model called Giuseppe that finds combinations of plant products that mimic the characteristics of animal-derived foods. The model also helped NotCo develop plant-based mayonnaise, ice cream, and hamburgers.\n\nBehind the news:NotCo is one of several companies using machine learning to discover new culinary secrets.\n\nWhy it matters:Producing animal-based foods can takeenormous quantities of natural resourcescompared to growing and processing plants. If AI can help the food and beverage industry develop the market for animal-free substitutes — which is expected to grow 14 percent annually over the next five years, according to oneanalysis—it could reduce the environmental toll.We’re thinking:We look forward to the day when an AI-poweredchefin our AI-augmentedkitchenpours us a glass of AI-designed milk.\n\nWe’re excited to announce thatTensorFlow: Advanced Techniques Specializationwill launch on November 18! And don’t miss the previousTensorFlow Developer Professional CertificateandTensorFlow: Data and Deployment Specialization.\n\nIn Argentina, a municipal face recognition system could misidentify children as suspected lawbreakers.What’s new:Authorities in Buenos Aires are scanning subway riders’ faces to find offenders in a database of suspects — but the system mixes criminal records with personal information about minors, according toHuman Rights Watch. The report follows alawsuitagainst the city filed by civil rights activists earlier in the year.How it works:The system uses two databases. The first,calledConarc, contains details about people who have outstanding arrest warrants, including names, ages, and national ID numbers. It matches these records with faces in a second database that contains pictures of Argentine citizens. The systemalertspolice when it recognizes a suspect in the crowd.\n\nWhy it matters:Buenos Aires’ system relies on a database that appears to violate international human rights law, making children vulnerable to false arrests and other mishaps. Moreover,studieshave shown that current face recognition technology is highly unreliable when used on children.We’re thinking:The issues swirling around this system highlight the importance of clean, compliant data in machine learning applications: An error-free system that complied with legal requirements may still result in false arrests, but it would be defensible on grounds that it helped bring criminals to justice. The system also illustrates the pressing need to take extra care with machine learning models that bear on social outcomes. People may debate standards of justice, but they should be able to have confidence that models are applying those standards fairly.\n\nEveryday cameras and computer vision algorithms are digitizing construction projects to keep builders on schedule.What’s new:Based in Tel Aviv,Buildotsmaps output from building-site cameras onto simulations of the work in progress, enabling construction managers to monitor progress remotely. At least two large European builders are using the system, according toMIT Technology Review.How it works:A client supplies to Buildots blueprints and plans, including schedules and lists of parts, for completion of each task involved in a building project. Buildots supplies GoPro 360-degree cameras mounted atop hardhats.\n\nBehind the news:AI startups are aiming to make the technology as fundamental to the construction industry as steel-toed boots.\n\nWhy it matters:Mistakes can become delays that add to a construction project’s cost. Market research firm McKinseyestimatedthat the construction industry could add $1.6 trillion to the global GDP by catching mistakes before they cause serious delays.We’re thinking:Buildots is bringing new meaning to the phrase “AI architecture.”",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/unnamed-2.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2036.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-10-1320at204.45.4620PM_PROOF.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize-2-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Main20Specialization20-20Twitter20E28093201-2.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2021.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2026.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-62/",
    "title": "issue 62",
    "date": "",
    "reading_time": "",
    "content": "Today Landing AI, where I am CEO, launchedLandingLens, an AI-powered platform that helps manufacturers develop computer vision solutions that can identify defective products. For AI to benefit a wide range of industries, we need platforms that enable experts in a variety of fields to build and deploy models. LandingLens is a step in this direction, and it’s available to manufacturers immediately.\n\nA major challenge to taking advantage of AI throughout the economy is the sheer amount of customization needed. To use computer vision to inspect manufactured goods, we need to train a different model for each product we want to inspect: each smartphone model, each semiconductor chip, each home appliance, and so on. How can Landing AI build models for thousands of products without hiring thousands of machine learning engineers? It’s much better to empower the manufacturers to build and deploy these models themselves.\n\nLandingLens enables experts in manufacturing — rather than experts in machine learning — to collect data, train models, deploy them, and carry out continuous learning. It helps them make sure their models work and scale up deployments. If the test data distribution drifts and the algorithm’s performance suddenly degrades, they’re empowered to collect new data and retrain the model without being beholden to an outside team.\n\nHere are a few unique features of LandingLens:\n\nHaving led AI teams at large consumer internet companies, I believe it’s time to take AI beyond the technology industry, to all industries. We’ve been building this platform for over a year, and I’m excited to be able to talk about it publicly. I hope that LandingLens — and other verticalized AI development platforms to come — will lower the bar for industrial deep learning and spread the benefits of AI throughout the economy.\n\nKeep learning!\n\nAndrew\n\nControversy erupted over the need for transparency in research into AI for medicine.What’s new:Google Health introduced asystemthat purportedly identified breast cancer more accurately than human radiologists. But the search giant’s healthcare division failed to disclose details that would have enabled others to reproduce its results, dozens of critics wrote in a letter toNature(also published onArxiv).The critique:Researchers at Harvard, University of Toronto, Vector Institute, and elsewhere argue that AI systems used to diagnose life-threatening conditions should meet high standards of transparency. The Google research fell short on several counts:\n\nThe response:In a rebuttal published inNature, the Google researcherssaidthat keeping the model under wraps was part of “a sustainable venture to promote a vibrant ecosystem that supports future innovation.” The training details omitted are “of scant scientific value and limited utility to researchers outside our organization,” they added. They held back the proprietary dataset to protect patient privacy.Behind the news:AI researchers are struggling to balance trade secrets, open science, and privacy. The U.S.Food and Drug Administrationhosted a workshop earlier this year aimed at developing best practices for validating AI systems that interpret medical images.Why it matters:Transparency makes it possible for scientists to verify and build on their colleagues’ findings, find flaws they may have missed, and ultimately build trust in the systems they deploy. Without sufficient information, the community can’t make rapid, reliable progress.We’re thinking:There are valid reasons to withhold some details. For instance, some datasets come with limitations on distribution to protect privacy. However, outside of circumstances like that, our view is that researchers owe it to each other to make research findings as reproducible as possible.\n\nHow does a convolutional neural network recognize a photo of a ski resort? New research shows that it bases its classification on specific neurons that recognize snow, mountains, trees, and houses. Zero those units, and the model will become blind to such high-altitude playgrounds. Shift their values strategically, and it will think it’s looking at a bedroom.What’s new:Network dissectionis a technique that reveals units in convolutional neural networks (CNNs) and generative adversarial networks (GANs) that encode not only features of objects, but the objects themselves. David Bau led researchers at Massachusetts Institute of Technology, Universitat Oberta de Catalunya, Chinese University of Hong Kong, and Adobe Research.Key insight:Previousworkdiscovered individual units that activated in the presence of specific objects and other image attributes, as well as imageregionson which individual units focused. But these efforts didn’t determine whether particular image attributes caused such activations or spuriously correlated with them. The authors explored that question by analyzing relationships between unit activations and network inputs and outputs.How it works:The authors mapped training images to activation values and then measured how those values affected CNN classifications or GAN images. This required calculations to represent every input-and-hidden-unit pair and every hidden-unit-and-output pair.\n\nResults:The authors trained aVGG-16CNN on theplaces365dataset of photos that depict a variety of scenes. When they removed the units most strongly associated with input classes and segmentation labels — sometimes one unit, sometimes several — the network’s classification accuracy fell an average of 53 percent. They trained aProgressive-GANon theLSUNdataset’s subset of kitchen images. Removing units strongly associated with particular segmentation labels decreased their prevalence in the generated output. For example, removing a single unit associated with trees decreased the number of trees in generated images by 53.3 percent. They also came up with a practical, if nefarious, application: By processing an image imperceptibly, they were able to alter the responses of a few key neurons in the CNN, causing it to misclassify images in predictable ways.Why it matters:We often think of neural networks as learningdistributed representationsin which the totality of many neurons’ activations represent the presence or absence of an object. This work suggests that this isn’t always the case. It also shows that neural networks can learn to encode human-understandable concepts in a single neuron, and they can do it without supervision.Yes, but:These findings suggest that neural networks are more interpretable than we realized — but only up to a point. Not every unit analyzed by the authors encoded a familiar concept. If we can’t understand a unit that’s important to a particular output, we’ll need to find another way to understand that output.We’re thinking:In 2005, neuroscientists at CalTech and UCLAdiscovereda single neuron in a patient’s brain that appeared to respond only to the actress Halle Berry: photos, caricatures, even the spelling of her name. (In fact, this finding was an inspiration for Andrew’searlyworkin unsupervised learning, which found a neuron that encoded cats.) Now we’re dying to know: Do today’s gargantuan models, trained on a worldwide web’s worth of text, also have a Halle Berry neuron?\n\nA new multimedia experience lets audience members help artificially intelligent creatures work together to survive.What’s new:Agence, an interactive virtual reality (VR) project from Toronto-based Transitional Forms and the National Film Board of Canada, blends audience participation with reinforcement learning to create an experience that’s half film, half video game. The production, which runs on VR, mobile, and desktop platforms, debuted at the2020 Venice Biennaleexhibition of contemporary art. It’s available for download fromSteam.How it works: Five cute, three-legged creatures live atop a tiny, spherical world. They must learn to work together to grow giant flowers for food without throwing the planet off-balance. Players can simply watch them work or play an active role in the story by planting flowers or moving agents around.\n\nBehind the news:Agence director Pietro Gaglianoreceivedan Emmy in 2015 for a VR experience in which viewers encountered the Headless Horseman from theSleepy HollowTV series.Why it matters:Agence represents a new type of medium in which the audience members collaborate with AI to create unique, immersive experiences. It offers new possibilities for user input and interactive storytelling that — whether or not Agence itself catches on — seem destined to transform electronic entertainment.We’re thinking:Video game opponents driven by rules can be challenging, but imagine trying to outsmart the cops inGrand Theft Autoif they could learn from your past heists.\n\nHave you completed courses 1 and 2 of theGenerative Adversarial Networks Specialization? Course 3 will be available on Coursera next week! Pre-enrollhere\n\nSynthetic datasets can inherit flaws in the real-world data they’re based on. Startups are working on solutions.\n\nWhat’s new:Generating synthetic datasets for training machine learning systems is abooming business. Companies that provide such datasets are exploring ways to avoid perpetuating biases in the source data.\n\nHow it works:The cost of producing a high-quality training dataset is beyond the reach of some companies, and in situations where sufficient real-world data isn’t available,synthetic datamay be the only option. But such datasets can echo and even amplify biases including potentially harmfulsocial biases. Vendors likeAI.Reverie,GenRocket,Hazy, andMostly AIare looking for ways to adjust their synthetic output — “distorting reality,” as Hazy’s chief executive put it — to minimize the risk that models trained on their wares will result in unfair outcomes.\n\nWhy it matters:Social biases in training datasets often reflect reality. It’s true that altering synthetic datasets to change the balance of, say, men and women who earn high incomes is trading one type of bias for another, rather than eliminating it altogether. The aim here is not necessarily to generate accurate data but to produce fair outcomes.\n\nWe’re thinking:We need data, but more than that, we need to build models that result in fair outcomes.\n\nMachine learning is helping to design energy cells that charge faster and last longer.What’s new:Battery developers are using ML algorithms to devise new chemicals, components, and charging techniques faster than traditional techniques allow, according toWired.How it works: Designing better batteries involves tweaking variables such as electrode architecture, chemical composition, and patterns of current and voltage during charging. Typically, researchers change one at a time and can’t analyze the results until a battery dies. AI lets them test many at once and get results while the battery still has juice.\n\nBehind the news:In recent years, machine learning has also helped researchers discover new molecules thatimprove energy density,predict how batteries will performin different electric vehicles,testhow well capacitor designs store energy, and advanced battery research inmany other ways.Why it matters:Batteries that last long, charge fast, and cost little are a key enabler for devices from self-driving cars to brain implants.We’re thinking:In our recentHeroes of NLPinterview, Chris Manning joked that “electricity is the new AI.” Maybe he was right! You can watch the whole thinghere.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2022.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2023.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2016UNITS.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2019.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1-3.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2020.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Batteries1.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-10/",
    "title": "issue 10",
    "date": "",
    "reading_time": "",
    "content": "I’ve heard this conversation in multiple companies:\n\nMachine learning engineer: Look how well I did on the test set!\n\nBusiness owner: But your ML system doesn’t work. This sucks!\n\nMachine learning engineer: But look how well I did on the test set!\n\nWhy do AI projects fail? Last week, I addressed this question at ourPie & AImeetup. We had a spirited discussion with a live audience in 10 cities from London to Berlin, Ghent (Belgium) to Logroño (Spain).\n\nI remain as optimistic as ever about the AI industry, but I also see many AI projects struggle. Unlike software engineering, the process of engineering AI systems is immature, and teams have not yet learned about the most common pitfalls and how to avoid them.\n\nCommon pitfalls fall under the headings: robustness, small data, and workflow. You can increase your odds of success by analyzing your AI project in terms of these issues. I’ll flesh out my thoughts on this in coming weeks. Stay tuned.\n\nKeep learning!\n\nAndrew\n\nDaniel Barbosa quit his job managing cloud infrastructure to self-study machine learning full-time. Learn how Daniel landed hisfirst ML job.\n\nOpenAI trained a five-fingered robotic hand to unscramble the Rubik’s Cube puzzle, bringing both acclaim and criticism.What’s new:The AI research lab OpenAItraineda mechanical hand to balance, twist, and turn the cube.How it works:The system learned the manual skills to unscramble the cube using reinforcement learning. It determined the sequence of moves using a pre-existing formula known asKociemba’s algorithm.\n\nResults:The researchers considered an attempt a failure if the hand stalled or dropped the cube. The hand had a 60 percent success rate when the cube needed 15 rotations or fewer to solve the puzzle. That rate dropped to 20 percent when the solution required 26 rotations or more.Yes, but:Although OpenAI’s report focused mostly on robotic dexterity,criticsaccusedthe company of overstating its claim to have taught the robot to solve Rubik’s Cube. Kociemba’s algorithm is more than a decade old and doesn’t involve learning, they pointed out, and the cube included Bluetooth and motion sensors that tracked its segments. Moreover, despite training for virtual millennia, the robot can’t do anything more than manipulate the puzzle.Behind the news:Robots purpose-built or -coded to unscramble a Rubik’s Cube — no learning involved — are a venerable tradition. Guinness World Records recognizes a system that did the job in0.637 seconds. Independent engineers later shaved the time to0.38 seconds. Universal Robots programmed two industrial-bot arms tocollaborateon the task.\n\nWe’re thinking:OpenAI has a knack for choosing important problems, solving them in elegant ways, and producing impressive results. It also has been criticized for presenting its work in ways that lead the general public to mistake incremental developments for revolutionary progress. It’s important to set realistic expectations even as we push the boundaries of machine intelligence.\n\nOne of the largest open datasets for training face recognition systems has its roots in a popular photo-sharing service. Companies that have used this data could find themselves liable for millions in legal recompense.What’s new:Many Flickr users were surprised and upset when reporters informed them their likeness, or that of their children and other family members, was part of a public database used to train face recognition algorithms, according to theNew York Times. Such training may violate an Illinois digital privacy law that’s currently being tested in court.Tracing the data:MegaFace, which depicts 672,000 individuals in nearly 4 million photos, comprises images from Flickr that their creators licensed for commercial use under the Creative Commons intellectual property license.\n\nLegal jeopardy:In 2008, Illinois passed theBiometric Information Privacy Act, which prevents commercial entities from capturing, purchasing, or otherwise obtaining a private individual’s likeness without the person’s consent. Individuals whose faces have been used without permission are entitled to between $1,000 and $5,000 per use.Court action:The Illinois law already is fueling a $35 billion class actionlawsuitagainst Facebook for the way it stores and uses data to automatically identify faces in photos.\n\nWhy it matters:MegaFace is stillavailable, and at least 300 organizations have used it to train their models, according to a 2016 University of Washingtonpress release. Any group that has used this data to make money is liable under the Illinois law.We’re thinking:With 50 states in the U.S. and around 200 countries in the world, regulatory mismatches among various jurisdictions seem inevitable. User privacy and data rights are important, and legal requirements must be as clear and coherent as possible to advance the technology in a positive way.\n\nWeak supervision is the practice of assigning likely labels to unlabeled data using a variety of simple labeling functions. Then supervised methods can be used on top of the now-labeled data. Researchers used this technique to search electronic health records (EHRs) for information squirreled away in unstructured text.What’s new:Complications from hip replacement surgery tend to be under-reported because they’re recorded in EHRs as notes rather than check-marked in a standard list. Researchers at Stanford used weak supervision to label such notes and then extracted information related to hip implants. Theirmethodbrought to light complications that hadn’t been tracked explicitly.Key insight:Alison Callahan and collaborators divided the problem of finding references to post-surgical issues in notes into two parts: identifying the implant’s make and model, and spotting mentions of pain and complications. This made it possible to use weak supervision to label data separately for each subproblem.How it works:Snorkel is a framework that provides a modular way to define and combine labeling functions. The model works as follows:\n\nResults:The researchers trained the system on records of about 6,000 hip-replacement patients treated between 1995 and 2014. Learning the relationships between the various labeling functions uncovered twice as many patients facing complications as majority voting on their predictions (61 percent versus 32 percent). Overall, the system made it possible to assess the likelihood that a particular implant would lead to complications.Why it matters:This analysis could help doctors to match patients with appropriate implants, and help implant manufacturers design their products to minimize bad outcomes.Takeaway:This approach extracts useful information from EHRs, and it looks as though it would generalize to other text-labeling tasks.\n\nHow can you use gradient descent to train your model? Learn about other optimization algorithms how in Course 1 of theDeep Learning Specialization.\n\nA new generation of battlebots is gaining momentum.What’s new:The Army is at least two years ahead of schedule in its plan to deploy self-driving (and self-aiming) transports, jeeps, and tanks, said Richard Ross Coffman, director of Next Generation Combat Vehicles, in an interview withBreaking Defense.Rolling thunder:The NGCV program features three phases of testing for vehicles of graduated firepower and autonomy.\n\nBehind the news:The U.S. Army has spent billions on robotic fighting machines that never came to fruition. In 2009, the service cancelled a previous autonomous war-fighting effort, the $20 billionFuture Combat Systemsprogram, after six years in development. That program was nixed partly because the technology didn’t progress as quickly as expected and partly due to a shift from warfare to counterterrorism.Why it matters:Robot vehicles could act as decoys, drawing fire meant for human troops. They could also infiltrate enemy lines and call in artillery strikes, gather information, screen for obstacles, and wade into areas affected by nuclear, chemical, or biological weapons.What they’re saying:“Anywhere that a soldier is at the highest risk on the battlefield, and we can replace him or her with a robot, that’s what we want to do.” — Richard Ross Coffman, Director, Next Generation Combat Vehicles, U.S. Army.We’re thinking:How is the Army, which must cope with irregular terrain, intermittent explosions, and the fog of war, ahead of schedule when the automotive industry, navigating smooth surfaces and relatively orderly traffic, has fallen behind its initial projections? The military faces very different problems, some harder to navigate than urban environments, some easier. Its emphasis on remote control also could make a significant difference.\n\nBottom line:Like many people, we’re unsettled by the combination of AI and fighting machines. We strongly supportproposalsfor an internationalbanon autonomous weapons.\n\nWould you like an umbrella that fits in your pocket? Researchers used machine learning to invent sturdy but collapsible materials that might lead to such a fantastical object.What’s new:Researchers at the Netherlands’ Delft University of Technology used aBayesian modelto find arrangements of brittle polymers that are sturdy, lightweight, compressible, and able to spring back to their original shape. The machine learning algorithm made it possible to design and produce materials without conducting the usual trial-and-error physical experiments.How it works:Principal investigator Miguel Bessa designed a mock-up with two disks connected by flexible poles, or longerons, that fold in a spiral pattern when the dishes are pressed together.\n\nResults:The microscopic prototype — built for strength — was fully compressible and able to withstand intense pressure without buckling. For the human-scale version, it was important that it spring back into its original shape, which it did even when compressed nearly flat by a machine press.\n\nWhy it matters:Scientists working on metamaterials (structural arrangements of existing materials that exhibit characteristics not found in nature) alter material geometries, shapes, sizes, and orientations to produce novel properties. Typically this requires lots of trial and error. Machine learning can curate arrangements likely to have the right properties, enabling researchers to focus on the most promising candidates.We’re thinking:From materials science to drug design, brute force experimentation still plays a large role in bleeding-edge science. AI-driven screening is beginning to help researchers find shorter routes to Eureka.\n\nHumans understand the world by abstraction: If you grasp the concept of grabbing a stick, then you’ll also comprehend grabbing a ball. New work explores deep learning agents’ ability to do the same thing — an important aspect of their ability to generalize.What’s new:Psychologists call this kind of thinking systematic reasoning. Researchers at DeepMind, Stanford, and University College Londonstudiedthis capability in deep reinforcement learning models trained to interact with an environment and complete a task.Key insight:Felix Hill and colleagues trained a model toput object 1 on location 1with an example of that action being performed. At test time, they asked the model toput object 2 on location2. Object 2 and location 2 weren’t in the training set, so the model’s ability to execute the task would indicate a generalized understanding of putting.How it works:The model receives a view of the environment along with a task description (an instruction to put or find a given object). The model processes these elements separately, then combines its understanding of each to determine a series of actions to complete the task.\n\nResults:The researchers trained copies of the model in simulated 2D and 3D environments. It was over 91 percent successful in lifting novel objects either way. However, success at putting novel objects dropped to about 50 percent in both environments.Yes, but:Removing the task description and LSTM component didn’t degrade performance much. That is, while words such asputandfindmay help humans understand how neural networks operate systematically, language apparently isn’t critical to their performance.Why it matters:Neural networks are able to generalize, but our understanding of how they do it is incomplete. This research offers a way to evaluate the role of systematic reasoning. The results imply that models that reason systematically are more likely to generalize.Takeaway:The recent run of pretrained language models acquireknowledgethat enables them to perform a variety of tasks without retraining from scratch. Understanding systematic reasoning in neural networks could lead to better performance in domains outside of natural language.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Iasi201-1.jpeg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/daniel2.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/rubik20SIZED.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/flickr20SIZED.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/hip20replacement20SIZED.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/DLS20Course20120Course20Ad.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/tanks20SIZED.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/materials20SIZED.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/systemic20reasoning20SIZED.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-5/",
    "title": "issue 5",
    "date": "",
    "reading_time": "",
    "content": "Over the weekend, we hosted our first Pie & AI meetup in Kuala Lumpur, Malaysia, in collaboration with the AI Malaysia group, MDEC, and ADAX. The event was part of Malaysia’s AI & Data Week 2019. Several people traveled from neighboring southeast Asian countries to attend!\n\nI’m glad to see so many AI communities growing around the world, and I’m excited to bring more exposure to them. If you’d like to partner with us for a Pie & AI event, I hope you’ll drop us a note at[email protected].\n\nKeep learning!\n\nAndrew\n\nAdvances in computer vision and robotic dexterity may reach the field just in time to save U.S. agriculture from a looming labor shortage.What happened:CNN Businesssurveyedthe latest crop of AI-powered farmbots, highlighting those capable of picking tender produce, working long hours, and withstanding outdoor conditions.Robot field hands:Harvest bots tend to use two types of computer vision: one to identify ripe fruits or vegetables, the other to guide the picker.\n\nBehind the news:Unauthorized migrants do as much as 70 percent of U.S. harvest work, according to astudyby the American Farm Bureau Association. Tighter immigration policies and improving opportunities at home increasingly keep such workers out of the country.Why it matters:The shortage of agricultural workers extends across North America. During harvest season, that means good produce is left to rot in the fields. The situation costs farmers millions in revenue and drives up food prices.Our take:The robots-are-coming-for-your-job narrative often focuses on people put out of work but fails to acknowledge that workers aren’t always available. Between a swelling human population and emerging challenges brought on by climate change, the agriculture industry needs reliable labor more than ever. In some cases, that could be a machine.\n\nModels that achieve state-of-the-art performance in automatic speech recognition (ASR) often perform poorly on nonstandard speech. Newresearchoffers methods to make ASR more useful to users with heavy accents or speech impairment.\n\nWhat’s new:Researchers at Google fine-tuned ASR neural networks on a data set of heavily accented speakers, and separately on a data set of speakers with amyotrophic lateral sclerosis (ALS), which causes slurred speech of varying degree. Their analysis shows marked improvement in model performance. The remaining errors are consistent with those associated with typical speech.\n\nKey insight:Fine-tuning a small number of layers closest to the input of an ASR network produces good performance in atypical populations. This contrasts with typical transfer learning scenarios, where test and training data are similar but output labels differ. In those scenarios, learning proceeds by fine-tuning layers closest to the output.\n\nHow it works:Joel Shor and colleagues used data from the L2-ARCTIC data set for accented speech and ALS speaker data from the ALS Therapy Development Institute. They experimented with two pre-trained neural models, RNN-Transducer (RNN-T) and Listen-Attend-Spell (LAS).\n\nResults:RNN-T achieved lower word error rates than LAS, and both substantially outperformed the Google Cloud ASR model for severe slurring and heavily accented speech. (The three models were closer with respect to mild slurring, though RNN-T held its edge.) Fine-tuning on 15 minutes of speech for accents and 10 minutes for ALS brought 70 to 80 percent of the improvement.Why it matters:The ability to understand and act upon data from atypical users is essential to making the benefits of AI available to all.Takeaway:With reasonable resources and additional data, existing state-of-the-art ASR models can be adapted fairly easily for atypical users. Whether transfer learning can be used to adapt other types of models for broader accessibility is an open question.\n\nAI’s creative potential is becoming established in thevisual arts. Now musicians are tapping neural networks for funkier grooves, tastier licks, and novel subject matter.What happened:Aaron Ackerson, whom theChicago Sun-Timescalled “a cross between Beck and Frank Zappa,” producedhis latest releasewith help from the latest generation of generative AI.MuseNethelped generate the music andGPT-2suggested lyrics. DeepAI’sText To ImageAPI synthesized the cover art.Making the music:“Covered in Cold Feet” began its existence as an instrumental fragment scored for violin, piano, and bass guitar.\n\nWriting the lyrics:The groove reminded Ackerson of the band Phish. So he fed a list of that band’s song titles toTalk to Transformer, an online text-completion app based on the half-size version of GPT-2.\n\nBehind the music:The artist composed his first AI-assisted song in 2017 using theBotnik Voiceboxtext generator. He fed the model bass and melody lines from 100 of his favorite songs translated into solfège (a note-naming system that maps the tones in any musical key to the syllables do, re, mi, and so on). The model spat out fresh note pairings, many of which he had never before considered using. The result, “Victory Algorithm,” is a slide guitar-fueled psychobilly foot stomper.We’re thinking:AI skeptics worry that computers, if allowed to do creative work, will erase humanity from art. No worries on that point: Ackerman’s personality comes through loud and clear. We look forward to hearing more from musicians brave enough to let computers expand their creative horizons. (For more on MuseNet, see ourinterviewwith project lead Christine Payne.)\n\nPadding is an important modification in building a basic convolutional neural net. Learn more about padding in Course 4 of the Deep Learning Specialization.Enroll now\n\nCompanies with large numbers of contractual relationships may leave millions of dollars on the table because it’s not practical to customize each agreement. A new startup offers a chatbot designed to claw back that money.What happened:Pactum, a startup that automates basic vendor and service contracts at immense scale, emerged fromstealthwith a $1.15 million investment from Estonian tech upstart Jaan Tallinn and his posse of Skype alumni.How it works:Let’s say a prominent computer company develops a new laptop and hires Pactum to cut distribution deals with hundreds of thousands of computer stores around the globe.\n\nBehind the news:Contracts are a hot area for AI. In 2015, Synergist.io and Clause launched automated platforms that mediate contract negotiations. And last year, Dutch information services firm Wolters Kluwer acquired legal AI startups CLM Matrix and Legisway.Why it matters:Standardized contracts can save time and effort spent customizing agreements. But they also bring costs. A 2018 study byKPMGestimated that standard contracts can soak up between 17 and 40 percent of a contract’s expected revenue.The Tallinn Effect:Funding from Jaan Tallinn brings the credibility of a serial entrepreneur who co-founded Skype and Kazaa and invested in DeepMind. It’s also a stamp of approval from a technologist who thinks deeply about AI’s potential for both benefit and harm. Tallinn co-founded theCentre for the Study of Existential Riskand oncewrote, “In a situation where we might hand off the control to machines, it’s something that we need to get right.” Apparently he believes Pactum meets that standard.\n\nHow to build robots that respond to novel situations? When prior experience is limited, enabling a model to describe its uncertainty can enable it to explore more avenues to success.\n\nWhat’s new:In reinforcement learning, meta-learning describes teaching a model how to complete multiple tasks, including tasks the model hasn’t seen before. One way to approach meta-learning is to divide it into two subproblems: creating a plan based on current surroundings and the task at hand, and taking action to implement the plan. Stanford researchersdevelopeddeep learning models that facilitate the planning phase by learning to generate better representations of the task.\n\nKey insight:Deep learning has been used to learn vector descriptions of the initial state prior to accomplishing a task and the final state afterward. The new work uses probabilistic descriptions, allowing more flexibility in novel tasks. For example, instead of having to choose between the contradictory descriptionsobject 1 is on object 2andobject 2 is on object 1, the network updates its confidence in each statement throughout the planning steps.\n\nHow it works:Previous methods use a neural network model as a classifier to decide state descriptions from potential configurations. Instead, De-An Huang and his colleagues use the model’s confidence in each potential configuration to represent states. This approach produces a probabilistic description of current and final states.\n\nResults:The authors’ approach achieves state-of-the-art meta-learning performance in sorting objects and stacking blocks. When sorting, it matches performance based on human heuristics. When stacking, it outperforms human heuristics plus fixed state descriptions with less than 20 training examples (although the heuristics win with 30 training examples).\n\nYes, but:The researchers achieved these results in tasks with a small number of operations and potential state configurations. Their method likely will struggle with more complex tasks such as the Atari games that made deep reinforcement learning popular.\n\nTakeaway:In past models, misjudgments of surroundings and goals tend to accumulate, leading the models far from the intended behavior. Now, they can relax their fixed state descriptions by representing potential points of confusion as probabilities. This will enable them to behave more gracefully even with little past experience to draw on.\n\nTighter national borders impede progress in AI. So industry leaders are calling for a different kind of immigration reform.What happened:The Partnership on AI published areportcalling out the corrosive effect of restrictive immigration policies and suggesting alternatives.What it says:The nonprofit industry consortium’s report calls for countries aiming to grow their AI industry to ease visa requirements for high-skilled tech workers. Among its recommendations:\n\nWhy it matters:Strict immigration policies limit opportunities for researchers and practitioners. They also shut out students, small businesses, startups, and small colleges from the broader AI community. In the U.S., tighter borders are cutting into the country’s competitiveness in AI, according to a newstudyfrom Georgetown University’s Center for Security and Emerging Technology. CSET recommends eliminating existing U.S. barriers to recruiting and retaining foreign-born AI talent.We’re thinking:Human capital is critical to AI, and the global community has an interest in channeling it toward worthwhile projects. The Partnership on AI’s recommendations offer a solid foundation for international trade groups, such as the Organization for Economic Cooperation and Development, to build policies that accelerate progress by opening the world to workers in emerging tech.",
    "images": [
      "https://cdn2.hubspot.net/hubfs/5871640/Pie&AIMalaysiaCollage%20(1).png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-video-to-gif201-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Speech20Recognition.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Generative20Models.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_DLS20Course20420Course20Ad20Fixed20Size.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Working20Through20Uncertainty.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Easing20Cross20Border20Collaboration.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-120/",
    "title": "issue 120",
    "date": "",
    "reading_time": "",
    "content": "I’ve seen many new technologies go through a predictable process on their journey from idea to large scale adoption.\n\nThe data-centric AI movement is going through such a process. Data-centric AI is the growing discipline of systematically engineering the data needed to build successful AI systems. This contrasts with the model-centric approach, which focuses on inventing and tuning machine learning model architectures while holding the data fixed.\n\nExperienced machine learning practitioners have been engineering data by hand for decades. Many have made learning algorithms work by improving the data — but, even when I was doing it years ago, I didn’t have the language to explain why I did things in a certain way.\n\nNow more and more teams are articulating principles for engineering data. I’m seeing exciting processes for spotting data inconsistencies, accelerating human labeling, applying data augmentation, and crowdsourcing more responsibly. Finally, just as TensorFlow and PyTorch made building neural networks more systematic, new tools are starting to emerge. Landing AI (where I am CEO) is building a platform for computer vision applications, and I expect many more tools to be built by different companies for different applications. They will enable teams to take what once was an ad hoc set of ideas and apply the right process at the right time.The tech community has gone through this process for code versioning (leading to tools like git) and transfer learning (where GPT-3, which was pre-trained on a massive amount of text, represents an early version of a tool). In less mature areas like reinforcement learning, I believe we’re still developing principles.If you’re interested in learning more about the principles and tools of data-centric AI, we’re holding aworkshopat NeurIPS on December 14, 2021. Dozens of great researchers will present poster sessions and lectures on cutting-edge topics in the field.\n\nKeep learning!\n\nAndrew\n\nPeople seeking treatment for depression often experiment with different medications for months before finding one that works. Machine learning may remove some of the guesswork.What’s new:Deep learning can predict how patients will respond to two antidepressant medicines, according to astudyled by Albert Montillo and Madhukar Trivedi at University of Texas Southwestern Medical Center.Key Insight:Patients with depression show various patterns of depressed brain activity in brain scans. At the same time, they vary in their reported responses to different drugs. Given brain scans of depressed people and their reports of effective treatment, a neural network can learn to match patients with medications likely to relieve their symptoms.How it works:The authors trained separate vanilla neural networks to predict the change in patients’ depression levels after treatment with each of two drugs as well as placebo.\n\nResults:The authors evaluated their models on held-out data according toR2value, a measure of performance in which 100 percent is perfect. The sertraline model achieved an R2value of 48 percent. The bupropion model achieved 34 percent. Techniques that use brain scans to predict a patient’s response to drugs without deep learning have achieved R2values around 15 percent, Montillo toldThe Batch.Why it matters:Millions of adults suffer from major depression, andone-third of thosetry at least three drugs before settling on one. Moreover, many doctors are influenced by outcomes they observe in a handful of patients and aren’t able to systematically analyze data from a large cohort. Reliable predictions about which medicines are likely to work best — even if they’re far from perfectly accurate — could make a difference.We’re thinking:Bringing this work into clinical practice would require training models to classify responses to many other antidepressants. The authors plan to apply their method to drugs beyond the two in this study, and we look forward to their progress.\n\nComputer vision systems are surveying sewers for signs of decay and degradation.What’s new:A system from California startupSewerAIanalyzes videos of underground pipes to prioritize those in need of repair.How it works:SewerAI’s computer vision system classifies defects like cracks, holes, displacements, tree roots, and incursions in videos taken by sewer-crawling robots and human inspectors.\n\nBehind the news:AI is doing the dirty work for a growing number of companies.\n\nWhy it matters:Failed pipes can cause flooding, spread disease, and pollute water sources. In 2019, the American Society of Civil Engineersestimatedthe cost of shoring up the U.S. wastewater infrastructure at $129 billion — at least $81 billion more than lawmakers allocated in a recentlaw. By helping human inspectors prioritize repairs, computer vision could help stretch those dollars across more miles of pipe.We’re thinking:Would we rather let a robot inspect sludge-filled pipes than do it ourselves? Sewer we would!\n\nStart ourMachine Learning Engineering for Production (MLOps) Specializationtoday! Learn how to design, build, and maintain integrated systems using well-established tools and methods.Enroll now\n\nGoogle is improving speech recognition for people who have conditions that affect their ability to pronounce words.What’s new:Project Relateis an Android app that offers a personalized speech recognition model for people whose speech is impaired or otherwise atypical. It’s designed to understand, transcribe, and synthesize speech for both person-to-person and person-to-machine interactions.How it works:Researchers trained Relate using over amillion speech samplescollected from people with conditions including amyotrophic lateral sclerosis, cerebral palsy, Down syndrome, Parkinson’s disease, stroke, or traumatic brain injury. Users can fine-tune the system to their own voices by speaking 500 phrases.\n\nBehind the news:Recognizing the need to make their products more inclusive, tech companies have launched initiatives to make apps more accessible.\n\nWhy it matters:People whose speech is atypical can be excluded from social interactions, have trouble communicating when they need help, and experience difficulty using voice-activated devices. Technology that lets them be heard could make their lives richer, safer, and more engaging.We’re thinking:Speech recognition is a convenience for most people, but for those with unusual speech patterns, it could be a lifeline.\n\nNeural networks designed to process datasets in the form of a graph — a collection of nodes connected by edges — have delivered nearly state-of-the-art results with only a handful of layers. This capability raises the question:Do deeper graph neural networks have any advantage?New research shows that they do.What’s new:Ravichandra Addanki and colleagues at DeepMindprobedthe impact of depth on the performance of graph neural networks.GNN basics:A graph neural network (GNN) operates on graphs that link, for instance, customers to products they've purchased, papers to the other papers they cite, or pixels adjacent to one another in an image. A GNN typically represents nodes and edges as vectors and updates them iteratively based on the states of neighboring nodes and edges. Some GNNs represent an entire graph as a vector and update it according to the representations of nodes and edges.Key insight:Previousworkfound that adding a few layers to a shallow GNN barely improved performance. That study used graphs that comprised hundreds of thousands of nodes and edges. Since then, graphs have emerged with hundreds ofmillionsof nodes and edges. Deeper GNNs may achieve superior performance on these larger datasets.How it works:The authors built GNNs up to more than 100 layers deep, including an encoder (a vanilla neural network), agraph networkmade up of message-passing blocks (each a trio of vanilla neural networks), and a decoder (another vanilla neural network). Among other experiments, they trained a GNN on4 million graphs of molecules, in which nodes are atoms and edges are bonds between them, to estimate a particular key property called the HOMO-LUMO gap. (This property helps determine a molecule’s behavior in the presence of light, electricity, and other chemicals.)\n\nResults:The authors tested GNNs with different numbers of message-passing blocks. Performance on the validation set improved progressively with more message-passing blocks up to 32 — 104 layers total — but showed no benefit beyond that depth. A version with 8 message-passing blocks achieved ~0.128 mean absolute error, one with 16 achieved ~0.124 mean absolute error, and one with 32 achieved ~0.121 mean absolute error.Why it matters:Not all types of data can be represented easily as an image or text — consider a social network — but almost all can be represented as a graph. This suggests that deep GNNs could prove useful in solving a wide variety of problems.We’re thinking:CNNs and RNNs have become more powerful with increasing depth. GNNs may have a lot of room to grow.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-12-01%20at%2010.36.13%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-12-01%20at%2010.36.13%20AM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ANTIDEPRESS.gif?upscale=true&width=1200&upscale=true&name=ANTIDEPRESS.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/SEWER.gif?upscale=true&width=1200&upscale=true&name=SEWER.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20Image%201-1.png?upscale=true&width=1200&upscale=true&name=The%20Batch%20Image%201-1.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/RELATE.gif?upscale=true&width=1200&upscale=true&name=RELATE.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/GRAPHv3.gif?upscale=true&width=1200&upscale=true&name=GRAPHv3.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-251/",
    "title": "issue 251",
    "date": "",
    "reading_time": "",
    "content": "A barrier to faster progress in generative AI is evaluations (evals), particularly of custom AI applications that generate free-form text. Let’s say you have a multi-agent research system that includes a researcher agent and a writer agent. Would adding a fact-checking agent improve the results? If we can’t efficiently evaluate the impact of such changes, it’s hard to know which changes to keep.\n\nFor evaluating general-purpose foundation models such as large language models (LLMs) — which are trained to respond to a large variety of prompts — we have standardized tests like MMLU (multiple-choice questions that cover 57 disciplines like math, philosophy, and medicine) and HumanEval (testing code generation). We also have theLMSYS Chatbot Arena, which pits two LLMs’ responses against each other and asks humans to judge which response is superior, and large-scale benchmarking likeHELM. These evaluation tools took considerable effort to build, and they are invaluable for giving LLM users a sense of different models’ relative performance. Nonetheless, they have limitations. For example, leakage of benchmarks datasets’ questions and answers into training data is a constant worry, and human preferences for certain answers does not mean those answers are more accurate.\n\nIn contrast, our current options for evaluating applications built using LLMs are far more limited. Here, I see two major types of applications.\n\nThe cost of running evals poses an additional challenge. Let’s say you’re using an LLM that costs $10 per million input tokens, and a typical query has 1000 tokens. Each user query therefore costs only $0.01. However, if you iteratively work to improve your algorithm based on 1000 test examples, and if in a single day you evaluate 20 ideas, then your cost will be 20*1000*0.01 = $200. For many projects I’ve worked on, the development costs were fairly negligible until we started doing evals, whereupon the costs suddenly increased. (If the product turned out to be successful, then costs increased even more at deployment, but that was something we were happy to see!)\n\nBeyond the dollar cost, evals have a significant time cost. Running evals on 1000 examples might take tens of minutes or even hours. Time spent waiting for eval jobs to finish also slows down the speed with which we can experiment and iterate over new ideas. In an earlier letter, Iwrotethat fast, inexpensive token generation is critical for agentic workflows. It will also be useful for evals, which involve nested for-loops that iterate over a test set and different model/hyperparameter/prompt choices and therefore consume large numbers of tokens.\n\nDespite the limitations of today’s eval methodologies, I’m optimistic that our community will invent better techniques (maybe involving agentic workflows likereflectionfor getting LLMs to evaluate such output.\n\nIf you’re a developer or researcher and have ideas along these lines, I hope you’ll keep working on them and consider open sourcing or publishing your findings.\n\nKeep learning!\n\nAndrew\n\nLearn how to build and customize multi-agent systems in “AI Agentic Design Patterns with AutoGen,” made in collaboration with Microsoft and Penn State University. Use the AutoGen framework and implement four agentic design patterns: Reflection, Tool Use, Planning, and Multi-Agent Collaboration.Sign up for free\n\nA deep learning model significantly reduced deaths among critically ill hospital patients.\n\nWhat’s new:A system built by Chin-Sheng Lin and colleagues at Taiwan’s National Defense Medical Center analyzed patients’ heart signals and alerted physicians if it detected a high risk of death. Itreduceddeaths of high-risk patients by 31 percent in a randomized clinical trial.\n\nHow it works:Researcherstraineda convolutional neural network, given an electrocardiogram (a measurement of the heart’s electrical activity), toestimatea risk score. The system compares a patient’s risk score against those of other patients. Scores that rank in the 95th percentile or higher are considered high risk of death within 90 days.\n\nResults:8.6 percent of patients in the control group and 8.9 percent of patients in the experimental group raised a high-risk alert during the trial. In the experimental group, 16 percent of high-risk patients died; in the control group, 23 percent of high-risk patients died. Overall, in the experimental group, 3.6 percent of patients died; in the control group, 4.3 percent of patients died. The model was trained to predict mortality from all causes, but it showed unusually strong predictive capability for heart-related deaths. Examining causes of death, the authors found that 0.2 percent of patients in the experimental group died from heart-related conditions such as cardiac arrest versus 2.4 percent in the control group.Behind the news:Hospitals use AI-powered alert systems toidentifypatients in need of urgent medical attention. Such systems monitor emergency room patients for sepsis, predict whether those patients need intensive care, and predict the risk that discharged patients will require further care. They help hospitals to allocate resources by directing attention where it’s needed most urgently.Why it matters:It’s rare for any kind of medical intervention to reduce mortality in a subgroup by 31 percent. The authors speculate that the system not only helped direct attention to patients urgently in need of attention but also may have identified electrocardiogram features that doctors typically either don’t understand well or can’t detect.\n\nWe’re thinking:This relatively low-cost AI system unambiguously saved lives over three months at different hospitals! We look forward to seeing it scale up.\n\nFew makers of self-driving cars have braved the streets of India. Native startups are filling the gap.\n\nWhat’s new:Indian developers are testing autonomous vehicles on their nation’s disorderly local roads. To cope with turbulent traffic, their systems use different technology from their Western and East Asian counterparts,IEEE Spectrumreported.\n\nHow it works:In Indian cities, two-, three-, and four-wheelers share the road with trucks, pedestrians, and animals. Drivers often contend with debris and potholes, and many don’t follow rules. These conditions demand vehicles outfitted with technology that’s more flexible (and less expensive) than the interwoven sensors, models, and 3D maps employed by self-driving cars designed for driving conditions like those found in the United States.\n\nBehind the news:Bringing self-driving cars to India has political as well as technical dimensions. Many Indians hire full-time drivers, and the country’s minister of roads and highways hasresistedapproving the technology because of its potential impact on those jobs. Drivers cost as little as $150 per month, which puts self-driving car makers under pressure to keep their prices very low. Moreover, India’s government insists that vehicles sold there must be manufactured locally, posing a barrier to foreign makers of self-driving cars.\n\nWhy it matters:Rather than starting with an assumption that traffic follows orderly patterns with many edge cases, Indian developers assume that traffic is essentially unpredictable. For them, events that most developers would consider outliers — vehicles approaching in the wrong lanes, drivers who routinely play chicken, domestic animals in the way — are common. This attitude is leading them to develop robust self-driving systems that not only may be better suited to driving in complex environments but also may respond well to a broader range of conditions.\n\nWe’re thinking:Former Uber CEO Travis Kalanicksaidthat India would be “the last one” to get autonomous cars. These developers may well prove him wrong!\n\nAI could offer paths to promotion and relief from busywork for many knowledge workers.\n\nWhat’s new:75 percent of knowledge workers worldwide use AI even if they need to supply their own tools, according tosurveyconducted by Microsoft and Linkedin.\n\nHow it works:The authors questioned 3,800 workers in 31 countries throughout the Americas, Europe, Asia, and Australia, asking whether and how they used consumer-grade generative systems like Microsoft Copilot and OpenAI ChatGPT. Majorities of all age groups used AI at work, including 85 percent of respondents 28 or younger and 73 percent of those 58 or older.\n\nBehind the news:The survey results agree with those of other studies of AI’s impact on the workplace. In January, the International Monetary Fundprojectedthat AI would affect 40 percent of all jobs worldwide (either complementing or replacing them), including 60 percent of jobs in countries like the UK and U.S. that have greater percentages of knowledge workers. A 2023 research paperarguedthat white-collar occupations were most likely to be affected by generative AI, in contrast to previous waves of automation that primarily affected blue-collar jobs. Automation driven by AI increased overall employment, evidence gathered by the European Central Bankshows.\n\nWhy it matters:AI is transforming work from the bottom up. Executives and managers want employees who know how to use the technology, but only 39 percent of the people who already do so received training from their employers. Company-wide encouragement to experiment with and take advantage of AI leads to the best outcomes.\n\nWe’re thinking:Knowing how to use AI tools is a plus in the current job market. Knowing how to build applications using AI opens another world of doors.\n\nText excerpts used in retrieval augmented generation (RAG) tend to be short. Researchers used summarization to pack more relevant context into the same amount of text.\n\nWhat’s new:Parth Sarthi and colleagues at Stanford builtRecursive Abstractive Processing for Tree-Organized Retrieval(RAPTOR), a retrieval system for LLMs. RAPTOR can choose to deliver original text or summaries at graduated levels of detail, depending on the LLM’s maximum input length.\n\nKey insight:RAG improves the output of large language models by gathering from documents and/or web pages excerpts that are relevant to a user’s prompt. These excerpts tend to be brief to avoid exceeding an LLM’s maximum input length. For instance, Amazon Bedrock’s default excerpt length is 200 tokens (words or parts of a word). But important details may be scattered throughout longer passages, so short excerpts can miss them. A summarizer can condense longer passages into shorter ones, and summarizing summaries can condense large amounts of text into short passages.\n\nHow it works:RAPTOR retrieved material fromQASPER, a question answering corpus that contains around 1,600 research papers on natural language processing. The authors processed QASPER through an iterative cycle of summarizing, embedding, and clustering. The result was a graduated series of summaries at ever higher levels of abstraction.\n\nResults:Paired with a variety of LLMs, RAPTOR exceeded other retrievers in RAG performance on QASPER’s test set. Paired with theUnifiedQALLM, RAPTOR achieved 36.7 percentF1 score(here, the percentage of tokens in common between the output and ground truth), while SBERT (with access to only the 100-token excerpts) achieved 36.23 percent F1 score. Paired with GPT-4, RAPTOR achieved 55.7 percent F1 score (setting a new state of the art for QASPER),DPRachieved 53.0 percent F1 score, and providing paper titles and abstracts achieved 22.2 percent F1 score.\n\nWhy it matters:Recent LLMs can process very long inputs, notablyGemini 1.5(up to 2 million tokens) andClaude 3(200,000 tokens). But it takes time to process so many tokens. Further, prompting with long inputs can be expensive, approaching a few dollars for a single prompt in extreme cases. RAPTOR enables models with tighter input limits to get more context from fewer tokens.\n\nWe’re thinking:This may be the technique that developers who struggle with input context length have been long-ing for!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed--61-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/05/The-Batch-ads-and-exclusive-banners---2024-05-22T112453.236.png",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-29T152617.317.png",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-29T152721.294.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-29T152833.184.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/06/RAPTORv2-1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-237/",
    "title": "issue 237",
    "date": "",
    "reading_time": "",
    "content": "Earlier this month, my team AI Fund held its annual co-founder and CEO summit, where many of our collaborators gathered in California for two days to discuss how to build AI companies. Three themes emerged from many presentations: persistence, fast iteration and community.\n\nPersistence.Doing impactful work is hard!Tim Westergren(founder and former CEO of Pandora, Venture Advisor at AI Fund) said it was only on his 348th venture pitch that Pandora raised its Series A round of funding. He also spoke about the tough time when Pandora team members went without salaries for an extended period of time to try to make the company work out. While many people unfortunately are not in a position to make such sacrifices to build a business, sometimes it does take extraordinary effort — and, yes, sacrifices — to do something really meaningful.\n\nFast iteration.AI Fund’s process of building startups is focused on a three- month, bi-weekly sprint process, in which we iterate quickly through technical prototypes as well as business ideas.Bill MacCartney(former VP of Cohere, Venture Advisor at AI Fund) said, “The best way to start is just by building on top of . . . whatever the best model is . . .. Don’t worry about [cost or latency] at first. You’re really just trying to validate the idea.”\n\nOne technique that’s now very widespread for prototyping is retrieval augmented generation (RAG). I’ve been surprised at how many nontechnical business leaders seem to know what RAG is. Investors are sometimes leery of people who build a thin layer around LLMs. AsLaurence Moroney(lead AI Advocate at Google, AI Fund Fellow) says, “I’m a huge fan of RAG . . .. I think this is one way to go beyond a thin veneer around [commercial] models and build a somewhat thicker veneer.”\n\nCommunity.Despite the wide range of startups represented in sectors including deep AI tech, healthcare, finance, edtech, and so on, a recurring theme was that company builders end up stronger when they come together.Emil Stefanutti(co-founder of ContractRoom, Venture Advisor at AI Fund) said he was glad that many of the scars he has acquired by building businesses are turning out to be treasures for others, as he's able to share experiences that other entrepreneurs can benefit from. Tim Westergren said, “You can’t white-knuckle it. You also can’t do it alone.”\n\nThe themes of persistence, fast iteration, and community apply whether you work in a large company, startup, research, government, or elsewhere. When I think of innovators in any field, I often think of Teddy Roosevelt’s message:\n\n“It is not the critic who counts; not the [person] who points out how the strong [person] stumbles, or where the doer of deeds could have done them better. The credit belongs to the [person] who is actually in the arena, whose face is marred by dust and sweat and blood; who strives valiantly; who errs, who comes short again and again, … who knows great enthusiasms, the great devotions; who spends himself in a worthy cause.”\n\nKeep learning!\n\nAndrew\n\nOpenAI’s new video generator raises the bar for detail and realism in generated videos — but the company released few details about how it built the system.What’s new:OpenAI introducedSora, a text-to-video model that can produce extraordinarily convincing, high-definition videos up to one minute long. You can see exampleshere.What we know:Sora is alatent diffusionmodel that learned to transform noise into videos using an encoder-decoder and transformer. The system was trained on videos up to 1,920x1,080 pixels and up to one minute long.\n\nWhat we don’t know:OpenAI is sharing the technology with outside researchers charged with evaluating its safety,The New York Timesreported. Meanwhile, the company published neither quantitative results nor comparisons to previous work. Also missing are detailed descriptions of model architectures and training methods. (Some of the results suggest that Sora was trained not only to remove noise from tokens, but also topredict future tokensandgenerate tokens in between other tokens.) No information is available about the source(s) of the dataset or how it may have been curated.Qualitative results:Sora’s demonstration output is impressive enough to have sparkedargumentsover the degree to which Sora “understands” physics. A photorealistic scene in which “a stylish woman walks down a Tokyo street filled with warm glowing neon” shows a crowded shopping district filled with believable pedestrians. The woman’s sunglasses reflect the neon signs, as does the wet street. Halfway through its one-minute length, the perspective cuts — unprompted and presumably unedited — to a consistent, detailed close-up of her face. In another clip, two toy pirate ships bob and pitch on a frothing sea of coffee, surrounded by a cup’s rim. The two ships maintain their distinctiveness and independence, their flags flutter in the same direction, and the liquid churns fantastically but realistically. However, as OpenAI acknowledges, the outputs on display are not free of flaws. For instance, the pirate-battle cup’s rim, after camera motion has shifted it out of the frame, emerges from the waves. (Incidentally, the Sora demos are even more fun withsoundtracksgenerated by Eleven Labs.)\n\nWhy it matters:While we’ve seentransformers for video generation,diffusion models for video generation, anddiffusion transformers for images, this is an early implementation of diffusion transformers for video generation (along with a recentpaper). Sora shows that diffusion transformers work well for video.\n\nWe’re thinking:Did Sora learn a world model? Learning to predict the future state of an environment, perhaps given certain actions within that environment, is not the same as learning depict that environment in pixels — just like the ability to predict that a joke will make someone smile is different than the ability to draw a picture of that smile. Given Sora’s ability to extrapolate scenes into the future, it does seem to have some understanding of the world. Its world model is also clearly flawed — for instance, it will synthesize inconsistent three-dimensional structures — but it’s a promising step toward AI systems that comprehend the 3D world through video.\n\nHuawei is emerging as an important supplier of AI chips.\n\nWhat’s new:Amid a U.S. ban on exports of advanced chips to China, demand for Huawei’s AI chips is so intense that the company is limiting production of the chip that powers one of its most popular smartphones so it can serve the AI market,Reutersreported.\n\nDemand and supply:China’s biggest chip fabricator, Semiconductor Manufacturing International Corp. (SMIC), fabricates both the Ascend 910B, which is optimized to process neural networks, and the Kirin chip that drives Huawei’s popular Mate 60 phone. Production capacity is limited, so making more Ascend 910Bs means making fewer Kirins.\n\nBehind the news:Nvidia accounted for 90 percent of the market for AI chips in China prior to the advent of U.S. export restrictions. China has responded to the limits by building its ability to manufacture advanced chips domestically — a tall order, since it requires technology that is very difficult to develop. In August, Baidu ordered 1,600 Ascend 910B chips for delivery by the end of the year, according to an earlierReutersreport. The order, which is tiny compared to typical data center purchases, nonetheless demonstrated that SMIC could manufacture the chips and that Baidu was experimenting with alternatives to Nvidia in anticipation of even tighter U.S. restrictions on AI chips that took effect in October. Currently, SMIC isgearing upto produce Huawei’s next-generation Ascend chips.\n\nWhy it matters:For years, Nvidia’s GPUs have been the only practical choice for processing deep learning models. The company’s lead over competitors both in hardware implementation and software support are likely to protect its dominant position for some time to come. However, competitors like AMD and Huawei are beginning to nip at Nvidia’s heels. That means more hardware options for developers, and the competition may drive lower prices and still higher performance.\n\nWe’re thinking:AI chips are at the heart of the current technologicalcompetitionbetween the U.S. and China. While Huawei and SMIC still have a lot to prove in terms of scaling up production, their rate of progress is impressive and illustrates the limits of the current U.S. restrictions.\n\nIn our next live workshop, we’ll share how to build high-quality and production-ready applications using tools like Pinecone Canopy and TruLens. Notebooks will be available for participants to explore!Register now\n\nJudges in competitive gymnastics are using an AI system to double-check their decisions.\n\nWhat’s new:Olympic-level gymnastic contests have adopted Judging Support System (JSS), an AI-based video evaluation system built by Fujitsu,MIT Technology Reviewreported. In September and October,  for the first time, judges at the 2023 World Artistic Gymnastics Championships in Antwerp used JSS in competitions that involved the full range of gymnastics equipment including mat, balance beam, parallel bars, pommel horse, and so on.\n\nHow it works:Judges penalize gymnasts for imperfections in any pose or move. JSS identifies deviations that correspond to particular penalties. The system can evaluate roughly 2,000 poses and moves with 90 percent accuracy compared to human judges. It can assess both isolated actions and entire routines.\n\nBehind the news:Sporting authorities have embraced AI both inside and outside the arena.\n\nWhy it matters:Gymnastic competitors arescoredon subjective criteria such as expression, confidence, and personal style as well as technical competence, raising questions of unconscious bias and whether some judges might favor certain competitors over others. An AI system that tracks technical minutiae may help judges to avoid bias while focusing on the sport’s subjective aspects.\n\nWe’re thinking:Tracking gymnasts in motion sets a high bar for AI!\n\nResearchers devised a way to reduce memory requirements when fine-tuning large language models.\n\nWhat's new:Kai Lv and colleagues at Fudan University proposedlow memory optimization(LOMO), a modification of stochastic gradient descent that stores less data than other optimizers during fine-tuning.\n\nKey insight:Optimizers require a lot of memory to store an entire network’s worth of parameters, gradients, activations, and optimizer states. While Adam has overtaken stochastic gradient descent (SGD) for training, SGD remains a popular choice for fine-tuning partly because it requires less memory (since it stores fewer optimizer states). Nonetheless, SGD must store an entire network’s gradients — which, with state-of-the-art models, can amount to tens or hundreds of gigabytes — before it updates the network all at once. Updating the network layer by layer requires storing only one layer’s gradients — a more memory-efficient twist on typical SGD.\n\nHow it works:The authors fine-tunedLLaMAon six datasets inSuperGLUE, a benchmark for language understanding and reasoning that includes tasks such as answering multiple-choice questions.\n\nResults:LOMO required less memory than popular optimizers and achieved better performance than the popular memory-efficient fine-tuning technique LoRA.\n\nWhy it matters:Methods like LoRA save memory by fine-tuning a small number of parameters relative to a network’s total parameter count. However, because it adjusts only a small number of parameters, the performance gain from fine-tuning is less than it could be. LOMO fine-tunes all parameters, maximizing performance gain while reducing memory requirements.\n\nWe're thinking:SGD’s hunger for memory is surprising. Many developers will find it helpful to have a memory-efficient alternative.\n\nMore AI news of the week. From Gemini 1.5’s whopping context window to the energy footprint of AI models and Google's next research hub city, we covered it all.\n\nDive into this week's key AI developments in our latest Data Points edition.\n\nRead now.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/02/unnamed--100-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/02/unnamed---2024-02-21T181018.251.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/02/unnamed---2024-02-21T181123.082.png",
      "https://dl-staging-website.ghost.io/content/images/2024/02/The-Batch-ads-and-exclusive-banners---2024-02-21T092947.270.png",
      "https://dl-staging-website.ghost.io/content/images/2024/02/unnamed---2024-02-21T181325.090.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/02/unnamed---2024-02-21T181416.147.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-297/",
    "title": "issue 297",
    "date": "",
    "reading_time": "",
    "content": "I’ve noticed that many GenAI application projects put in automated evaluations (evals) of the system’s output probably later — and rely on humans to manually examine and judge outputs longer — than they should. This is because building evals is viewed as a massive investment (say, creating 100 or 1,000 examples, and designing and validating metrics) and there’s never a convenient moment to put in that up-front cost. Instead, I encourage teams to think of building evals as an iterative process. It’s okay to start with a quick-and-dirty implementation (say, 5 examples with unoptimized metrics) and then iterate and improve over time. This allows you to gradually shift the burden of evaluations away from humans and toward automated evals.\n\nIwrotepreviously about the importance and difficulty of creating evals. Say you’re building a customer-service chatbot that responds to users in free text. There’s no single right answer, so many teams end up having humans pore over dozens of example outputs with every update to judge if it improved the system. While techniques like LLM-as-judge are helpful, the details of getting this to work well (such as what prompt to use, what context to give the judge, and so on) are finicky to get right. All this contributes to the impression that building evals requires a large up-front investment, and thus on any given day, a team can make more progress by relying on human judges than figuring out how to build automated evals.\n\nI encourage you to approach building evals differently. It’s okay to build quick evals that are only partial, incomplete, and noisy measures of the system’s performance, and to iteratively improve them. They can be a complement to, rather than replacement for, manual evaluations. Over time, you can gradually tune the evaluation methodology to close the gap between the evals’ output and human judgments. For example:\n\nSo long as the output of the evals correlates with overall performance, it’s fine to measure only a subset of things you care about when starting.\n\nThe development process thus comprises two iterative loops, which you might execute in parallel:\n\nAs with many things in AI, we often don’t get it right the first time. So t’s better to build an initial end-to-end system quickly and then iterate to improve it. We’re used to taking this approach to building AI systems. We can build evals the same way.\n\nTo me, a successful eval meets the following criteria. Say, we currently have system A, and we might tweak it to get a system B:\n\nWhenever a pair of systems A and B contradicts these criteria, that is a sign the eval is in “error” and we should tweak it to make it rank A and B correctly. This is a similar philosophy toerror analysisin building machine learning algorithms, only instead of focusing on errors of the machine learning algorithm's output — such as when it outputs an incorrect label — we focus on “errors” of the evals — such as when they incorrectly rank two systems A and B, so the evals aren’t helpful in choosing between them.\n\nRelying purely on human judgment is a great way to get started on a project. But for many teams, building evals as a quick prototype and iterating to something more mature lets you put in evals earlier and accelerate your progress.\n\nKeep building!\n\nAndrew\n\nBuild autonomous agents that take actions like scraping web pages, filling out forms, and subscribing to newsletters in “Building AI Browser Agents.” Explore the AgentQ framework, which helps agents self-correct using Monte Carlo tree search and direct preference optimization.Start learning today\n\nGoogle’s new flagship model raised the state of the art in a variety of subjective and objective tests.\n\nWhat’s new:Google launchedGemini 2.5 Pro Experimental, the first model in the Gemini 2.5 family, and announced thatGemini 2.5 Flash, a version with lower latency, will be available soon. All Gemini 2.5 models will have reasoning capabilities, as will all Google models going forward.\n\nHow it works:Compared toGemini 1.0andGemini 1.5, Google disclosed little information about Gemini 2.5 Pro Experimental or how it differs from previous versions.\n\nResults:On a variety of popular benchmarks, Gemini 2.5 Pro Experimental outperforms top models from competing AI companies.\n\nWhy it matters:Late last year, some observers expressedconcernsthat progress in AI was slowing. Gemini 2.5 Pro Experimental arrives shortly after rival proprietary models GPT-4.5 (currently a research preview) and Claude 3.7 Sonnet, both of which showed improved performance, yet it outperforms them on most benchmarks. Clearly there’s still room for models — particularly reasoning models — to keep getting better.\n\nWe’re thinking:Google said it plans to train all its new models on chains of thought going forward. This follows a similarstatementby OpenAI. We’re sure they have their reasons!\n\nOpenAI embraced Model Context Protocol, providing powerful support for an open standard that connects large language models to tools and data.\n\nWhat’s new:OpenAI will supportModel Context Protocol(MCP) in its Agents SDK and soon its ChatGPT desktop app and Responses API. The move will give developers who use OpenAI models access to a wide variety of pre-existing tools and proprietary data sources.\n\nHow it works:Launchedby Anthropic late last year, MCP connects AI models to a growing ecosystem of plug-and-play resources, including more than 6,000community-built servers and connectors.\n\nBehind the news:Momentum behind MCP has built rapidly. Last month, Microsoftintegrated MCPinto CoPilot Studio, enabling developers to build agents with access to MCP servers. Cloudflare enabled its customers todeploy remote MCP servers. In February, the AI-powered code editor Cursor enabled users toadd MCP servers.\n\nWhy it matters:OpenAI’s move will make it easier for developers who use its models to connect to a variety of tools and data sources, and it helps to establish MCP as a go-to protocol for building agentic applications. Instead of figuring out manually how to integrate various providers, developers can connect to a third-party server (or download and run it themselves) and tie it into existing workflows with a few lines of code.\n\nWe’re thinking:Kudos to Anthropic, OpenAI, and other competitors who realize it’s better to solve shared problems together than fragment the industry.\n\nA behind-the-scenesaccountprovides new details about the abrupt firing and reinstatement of OpenAI CEO Sam Altman in November 2023.\n\nHow it works:Based on insider accounts, an excerpt from a forthcoming book about OpenAI by Wall Street Journal reporter Keach Hagey describes conflicts, accusations, and shifting alliances that led to Altman’s brief ouster and rapid return.\n\nFiring and reinstatement:OpenAI’s board of directors came to distrust Altman but failed to persuade executives and employees that he should be replaced.\n\nAftermath:Since Altman’s return, Murati and all but one director who voted to remove him have left OpenAI. The issues that precipitated his departure have given way to commercial concerns as the company considers a shift from its current hybrid nonprofit/for-profit structure to fully for-profit.\n\nWhy it matters:The AI frontier spawns not only technical innovations but also intense interpersonal relationships and corporate politics. Such dynamics have consequences for users and the world at large: Having survived serious challenges to his leadership, Altman has emerged in a strong position to build a path of faster growth as a for-profit company upon OpenAI’s philanthropic foundation.\n\nWe’re thinking:Given OpenAI’s formidable achievements, Altman’s renewed leadership marks an inflection point in the AI landscape. Without Sam Altman at the helm, OpenAI would be a very different company, with different priorities and a different future.\n\nResearchers built a model that’s more robust to noisy inputs like misspellings, smarter about character-level information like the number of R's in strawberry, and potentially better able to understand unfamiliar languages that might share groups of letters with familiar languages. Their approach: Eliminate the tokenizer and instead integrate a system that learns to group input characters.\n\nWhat’s new:Artidoro Pagnoni, Ram Pasunuru, and collaborators at Meta, University of Washington, and University of Chicago introducedByte Latent Transformer(BLT), a system of transformers that processes groups of text characters (in the form of bytes) directly.\n\nKey insight:A tokenizer turns bytes (characters) into tokens (a word or part of a word) based on learned rules: Specific sequences map to particular tokens. A large language model (LLM) would be more efficient if its tokenizer considered how easy or difficult it would be to predict the next token, because then it could group tokens that commonly occur together, thus saving memory and processing power. For instance, to complete the phrase, “The capital of the United States is,” a tokenizer may generate “Washington”, then “D”, then “.C”, and finally “.” — even though it’s easy to predict that “D.C.” will follow “Washington” (that is, the number of viable options is very small). Conversely, generating the token after “D.C.” is harder, since many viable options exist. Using a small LLM to estimate the difficulty of predicting the next token enables the model to split difficult-to-predict text into smaller groups while packing easier-to-predict text into larger groups.\n\nHow it works:BLT comprises four transformers (8 billion parameters total): (i) a small byte-level transformer, (ii) an encoder transformer, (iii) a so-called latent transformer, and (iv) a decoder transformer. The authors trained the system to generate the next token in 1 trillion tokens of text, including tokens drawn from a filteredversionof Common Crawl.\n\nResults:On seven benchmarks that test general language and coding abilities, BLT achieved an average accuracy of 61.1 percent, outperformingLlama 3(8 billion parameters and a similar number of floating point operations to BLT) at 60.0 percent.\n\nWhy it matters:By working directly on bytes, BLT is inherently more robust to variations in language, which improves its performance. For instance, when prompted to insert a \"z\" after every \"n\" in \"not\", Llama 3 incorrectly completed it as \"znotz\". This happened because its tokenizer treats \"not\" as a single, indivisible token. In contrast, BLT correctly generated \"nzot,\" because it can dynamically regroup bytes and draw new boundaries. In a more practical case, instead of treating \"pizya\" and \"pizza\" as different tokens, BLT recognizes that they share nearly identical byte sequences, differing only in the bytes for \"y\" and \"z\", and therefore likely mean the same thing.\n\nWe’re thinking:In some alternatives to traditional tokenization, an LLM might process much longer sequences because the number of bytes in a sentence is much larger than the number of words. This work addresses that issue by grouping bytes dynamically. The tradeoff is complexity: Instead of one transformer, we have four.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--58-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/The-Batch-ads-and-exclusive-banners---2025-04-11T162548.680.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--76-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/ModelContextProtocol-diagram-5_1200px--1--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--60-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--77-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-215/",
    "title": "issue 215",
    "date": "",
    "reading_time": "",
    "content": "While AI is a general-purpose technology that’s useful for many things, it isn’t good for every task under the sun. How can we decide which concrete use cases to build? If you’re helping a business figure out where to apply AI, I’ve found the following recipe useful as a brainstorming aid:\n\nRather than thinking of AI as automating jobs — a common narrative in the popular press and in conversations about AI leading to job losses — it’s more useful to think about jobs as collections of tasks, and to analyze AI’s ability to augment or automate individual tasks. This approach is based on amethoddeveloped by Erik Brynjolfsson, Tom Mitchell, and Daniel Rock for understanding the impact of AI on the economy. Other researchers have used it to understand theimpact of generative AI. Workhelix, an AI Fund portfolio company co-founded by Brynjolfsson, Andrew McAfee, James Milin, and Rock, uses it to help enterprises asses their generative AI opportunities.\n\nIn addition to economic analyses, I’ve found this approach useful for brainstorming project ideas. For example, how can AI be used to automate software businesses? Can it do the job of a computer programmer?\n\nTypically, we think of computer programmers as writing code, but actually they perform a variety of tasks. According to O*NET, an online database of jobs and their associated tasks sponsored by the U.S. Department of Commerce,programmers perform 17 tasks. These include:\n\nand so on. Clearly systems like GitHub Copilot can automate some writing of code. Automating the writing of documentation may be much easier, so an AI team building tools for programmers might consider that too. However, if consulting to clarify the intent behind a program turns out to be hard for AI, we might assign that a lower priority.\n\nAnother example: Can AI do the job of a radiologist? When thinking through AI’s impact on a profession, many people gravitate to the tasks that are most unique about that profession, such as interpreting radiological images. But according to O*NET,radiologists carry out 30 tasks. By taking a broader look at these tasks, we might identify ones that are easier or more valuable to automate. For example, while AI has made exciting progress in interpreting radiological images, part of this task remains challenging to fully automate. Are there other tasks on the list that might be more amenable to automation, such as obtaining patient histories?\n\nO*NET listings are a helpful starting point, but they’re also a bit generic. If you’re carrying out this type of analysis, you’re likely to get better results if you capture an accurate understanding of tasks carried out by employees of the specific company you’re working with.\n\nAn unfortunate side effect of this approach is that it tends to find human tasks to automate rather than creative applications that no one is working on. Brynjolfsson laments that this leads to the Turing Trap whereby we tend to use AI to do human work rather than come up with tasks no human is doing. But sometimes, if we can do something that humans do but do it 10,000x faster and cheaper, it changes the nature of the business. For example, email automated the task of transmitting messages. But it didn’t make the postal system cheaper; instead it changed what and how frequently we communicate. Web search automated the task of finding articles. Not only did this make librarians more effective, it also changed how we access information. So even if AI tackles a task that humans perform, it could still lead to revolutionary change for a business.\n\nMany jobs in which some tasks can be automated aren’t likely to go away. Instead, AI will augment human labor while humans continue to focus on the things they do better. However, jobs that are mostly or fully automatable may disappear, putting people out of work. In such cases, as a society, we have a duty to take care of the people whose livelihoods are affected, to make sure they have a safety net and an opportunity to reskill and keep contributing. Meanwhile, lowering the cost of delivering certain services is bound to increase the demand for some jobs, just as the invention of the car led to a huge explosion in the number of driving jobs. In this way, AI will create many jobs as well as destroy some.\n\nSome programmers worry that generative AI will automate their jobs. However, programming involves enough different tasks, some of which are hard to automate, that I find it very unlikely that AI will automate these jobs anytime soon. Pursuing a long-term career in software is still a great choice, but we should be sure to adopt AI tools in our work. Many professions will be here for a long time, but workers who know how to use AI effectively will replace workers who don’t.\n\nI hope you find this framework useful when you’re coming up with ideas for AI projects. If our projects affect someone else’s work, let’s work hard to protect people’s livelihoods. I hope that by building AI systems, we can create — and fairly share — value for everyone.\n\nKeep learning!Andrew\n\nText-to-music generation has arrived.\n\nWhat's new:Stability.ai, maker of the Stable Diffusion image generator and StableLM text generator, launchedStable Audio, a system that generates music and sound effects from text. You can play with it and listen to exampleshere. The service is free for 20 generations per month up to 45 seconds long. The professional tier allows 500 generations per month, up to 90 seconds long, for $11.99 per month. An enterprise tier is negotiable. The company said it would open-source the model eventually.\n\nHow it works:Stable Audio is alatent diffusionmodel. It generates audio by a process that’s similar to the way Stable Diffusion generates images, but it uses a variational autoencoder to map audio to an embedding for processing and back to audio for your listening pleasure. The authors trained the system on800,000 audio filescontaining music, sound effects, and performances on individual instruments and corresponding descriptions.\n\nBehind the News:Stable Audio joins earlier services including Boomy, Mubert, plugger.ai, Soundful, and VEED.IO. It follows tantalizing advances in audio generation.\n\nYes, but:Stable Audio excels when generating instrumental and ambient music, but its output tends to suffer from some of the same flaws as previous text-to-music generators: Longer outputs often lack a coherent structure, and the clarity and detail of individual instruments and sound effects varies wildly. It also doesn’t effectively generate the sound of a vocalist pronouncing words.\n\nWhy it matters:AI has demonstrated its prowess at generating convincing text and images. Generated audio has implications for producers not only of music but also of videos, video games, and podcasts. Stable Audio sounds like an early step, but it stands out for its speed, high-resolution output, and the inclusion of a mechanism for learning musical structure.\n\nWe're thinking:Stable Audio is impressive, but this doesn’t quite feel like music’s GPT moment. Text and image generation took off as soon as highly capable generative models appeared. Music generation may yet await models that can produce not only high-res output but also sonorities and structures coherent and varied enough to be widely useful.\n\nFor some asylum seekers, machine translation errors may make the difference between protection and deportation.\n\nWhat’s new:Faced with a shortage of human translators, United States immigration authorities are relying on AI to process asylum claims. Faulty translations are jeopardizing applications,The Guardianreported.\n\nHow it works:The Department of Homeland Security has said it would provide human translators to asylum seekers with limited English proficiency, but this doesn’t always happen. They often resort to machine translation instead.\n\nBehind the news:Diverse factors can mar a translation model’s output:\n\nWhy it matters:Machine translation has come a long way in recent years, (as has the U.S. government’sembrace of AIto streamline immigration). Yet the latest models, as impressive as they are, were not designed for specialized uses like interviewing asylum candidates at border crossings, where people may express themselves in atypical ways because they’re exhausted, disoriented, or fearful.\n\nWe’re thinking:Justice demands that asylum seekers have their cases heard accurately. We call for significantly greater investment in translation technology, border-crossing workflows, and human-in-the-loop systems to make sure migrants are treated kindly and fairly.\n\nLearn about text embeddings and how to apply them to common natural language processing tasks in our new course with Google Cloud!Sign up for free\n\nThe United States military aims to field a multitude of autonomous vehicles.\n\nWhat’s new:The Department of Defense announced an initiative to develop autonomous systems for surveillance, defense, logistics, and other purposes,The Wall Street Journalreported. The department aims to deploy several thousands of such systems within 18 to 24 months, a timeline motivated by rapid drone development by China.\n\nHow it works:The Pentagon shared details about a program called Replicator that it hadannouncedin August.\n\nBehind the news:The U.S. is not alone in pursuing autonomous military applications. The Russian invasion of Ukrainespurreda homegrown Ukrainian drone industry andencouragedgovernment and independent researchers to harness face recognition systems for identifying combatants. China isdevelopingautonomous ships designed to carry fleets of air, surface, and submarine drones.\n\nWhy it matters:Replicator marks a significant, very public escalation of military AI. Other nations are certain to follow suit.\n\nWe’re thinking:We’re concerned about the potential for an international AI arms race, and we support the United Nations’ proposedbanon fully autonomous weapons. Yet the unfortunate state of the world is that many countries — even large, wealthy democracies — have little choice but to invest in defenses against aggressors both actual and potential. The ethics of military AI aren’t simple. We call on the AI community to help ensure that they encourage a safer and more democratic world.\n\nWhile transformers have delivered state-of-the-art results in several domains of machine learning, few attempts have been made to probe their inner workings. Researchers offer a new approach.\n\nWhat's new:Amin Ghiasi and colleagues at the University of Marylandvisualized representations learned by a vision transformer. The authors compared their results to earlier visualizations of convolutional neural networks (CNNs).\n\nKey insight:A method that has been used tovisualize the internal workings of CNNscan also reveal what’s happening inside transformers: Feeding the network images thatmaximize the output of a particular neuronmakes it possible to determine what individual neurons contribute to the network’s output. For instance, neurons in earlier layers may generate high outputs in response to an image with a certain texture, while neurons in later layers may generate high outputs in response to images of a particular object. Such results would suggest that earlier layers identify textures, and later layers combine those textures to represent objects.\n\nHow it works:The authors experimented with apretrainedViT-B16vision transformer.\n\nResults:ViT-B16’s fully connected layers were most revealing: Neurons in fully connected layers yielded images that contained recognizable features, while those in attention layers yielded images that resembled noise.\n\nWhy it matters:This work reveals that vision transformers base their output on hierarchical representations in much the same way that CNNs do, but they learn stronger associations between image foregrounds and backgrounds. Such insights deepen our understanding of vision transformers and can help practitioners explain their outputs.\n\nWe're thinking:The evidence that CLIP learns concepts is especially intriguing. As transformers show their utility in a wider variety of tasks, they’re looking smarter as well.\n\n\"Practical Computer Vision\" by Andrew Ng: In this live event, you’ll learn how to identify and scope vision applications, choose vision models, apply data-centric AI, and develop an MLOps pipeline.Join uson Tuesday, October 3, at 10:00 a.m. Pacific Time!\n\nU.S. investment in AI grows outside Silicon Valley, Cambridge, and NYCThe big three research and startup hubs still collectively make up about half of venture capital investment, but Seattle, Colorado, Texas, and the rest of the Sun Belt are gaining. Big companies like Tesla and Oracle have relocated or expanded, and funders like Steve Case argue that startups in once-overlooked cities can offer a better return on investment.(Bloomberg)\n\nMLPerf tests chips’ ability to process large language modelsHow fast can your data center-class hardware process large language model (LLM) inferences?MLPerf's new benchmark tests measured LLM performance on systems from fifteen companies. Nvidia’s new Grace Hopper 72-core CPU + H100 GPU superchip paced the field, but Intel and Qualcomm also made strong showings.(MLCommons)\n\nUK regulator publishes principles to guide AI useThe UK’s Competition and Markets Authority specified guidelines for AI regulations, focusing on large language models like GPT-4 and Llama 2. The CMA’s seven principles (accountability, access, diversity of business models, choice, flexibility, fair dealing, and transparency) aim to ensure competitive markets for both AI companies and their customers. “There remains a real risk that the use of AI develops in a way that undermines consumer trust or is dominated by a few players who exert market power that prevents the full benefits being felt across the economy,” said CMA CEO Sarah Cardell. (Gov.UK)\n\nIRS expands machine learning program to spot tax evasionThe United States Treasury Department’s revenue compliance office is partnering with experts in AI and data science to detect tax fraud. Its Large Partnership Compliance (LPC) program, launched in 2021, targets the largest and most complex tax returns, which are also the most opaque for human auditors. The IRS said it was using “cutting-edge machine learning technology to identify potential compliance risk” but would not disclose specifics. (IRS.gov)\n\nStartup monitors pollution and biodiversity with big data (and lots of bees)BeeOdiversity analyzes data gathered by six bee colonies to measure pollutants and invasive species across hundreds of thousands of acres in Europe. The bees’ pollen, soil analysis, groundwater data, and satellite imagery are fed into an AI system called BeeOImpact that can infer the presence of heavy metals, pesticides, and colony collapse across a larger area. (Microsoft.com)\n\nAI is designing citiesSome automated urban design tools are more like Sim City-style games or landscape-inspired art projects, but they are also solving real problems for planners and democratizing who can create a new vision of a city. AI can augment existing computer-aided design programs and determine where to locate water pipes and electric lines in buildings and neighborhoods. In Istanbul, city planners are creating an innovation district designed for drones, data sensors, and autonomous vehicles with the aid of AI. But critics say current datasets are biased towards replicating modern cities and their problems.(Bloomberg)\n\nJapan builds large language modelsJapan’s government and tech giants are investing heavily in developing its own versions of generative text models like GPT, LLaMA, and BARD. Using a typical multilingual large language model, prompts in Japanese are translated internally into English and return an English response that’s translated back into Japanese, sometimes resulting in misunderstandings or gibberish. Moreover, current models aren’t optimized for Japanese cultural norms of politeness and professionalism. But development is hampered by the fact that Japanese-only datasets are smaller than those used to train western models, and the country has fewer resources to develop them.(Nature)\n\nResearch: DeepMind reads DNA to find harmful mutationsAlphaMissense adapts DeepMind’s AlphaFold to read proteins and identify which are common (and probably benign) or unusual (or potentially harmful). Trained on DNA data from humans and closely related primates, the model identifies new mutations and predicts how risky the genetic change may be. AlphaMissense may help experts identify which mutations drive diseases and guide doctors to better treatments.(Science)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/09/O-Net-1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/09/Diffusion.png",
      "https://dl-staging-website.ghost.io/content/images/2023/09/ASYLUM.png",
      "https://dl-staging-website.ghost.io/content/images/2023/09/DeepLearning_GoogleCloudPlatfomr_Banner_2070x1080-1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/09/DRONES.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/09/VITLEARN.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/09/Practical-Computer-Vision.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-175/",
    "title": "issue 175",
    "date": "",
    "reading_time": "",
    "content": "What should be AI’s role in moderating the millions of messages posted on social media every day? The volume of messages means that automation is required. But the question of what is appropriate moderation versus inappropriate censorship lingers.\n\nAI is helpful for scaling up a moderation policy. But it doesn’t address the core challenge of defining a policy: Which expressions to permit and which to block. This is hard for both humans and AI.\n\nDeciding what to block is hard because natural language is ambiguous.\n\nThe meanings of words vary from person to person. My son says “wawa” when he wants water, and only his close family (and now you!) understand. At work, teams invent acronyms that others don’t understand. More problematically, lawbreakers and hate groups develop code words to discuss their activities.\n\nIf humans understand the same words differently, how can we train an AI to make such distinctions? If a piece of text has no fixed meaning, then enforcing policies based on the text is difficult. Should we hide it from user A if they would read it as promoting violence, but show it to user B if they would view it as benign? Or should hiding a message be based on the intent of the sender? None of these options is satisfying.\n\nFurther, getting the data to build an AI system to accomplish any of this is hard. How can the developers who gather the data understand its full range of meanings? Different communities have their own interpretations, making it impossible to keep track.\n\nEven if the meaning are unambiguous, making the right decision is still hard. Fortunately, social media platforms can choose from a menu of options depending on how egregious a message is and the degree of confidence that it’s problematic. Choices include showing it to a smaller audience, adding a warning label, and suspending, temporarily or permanently, the user who posted it. Having a range of potential consequences helps social media platforms manage the tradeoff between silencing and protecting users (and society).\n\nDespite their flaws, AI systems make social media better. Imagine email without AI-driven spam filtering; it would rapidly become unusable. Similarly, AI is critical for eliminating the most spammy or toxic social media messages. But the challenge of moderating any given message transcends AI.\n\nIt’s important to acknowledge this challenge openly, so we can debate the principles we would apply to this problem and recognize that there may be no perfect solution. Through transparent and robust debate, I believe that we can build trust around content moderation and make tradeoffs that maximize social media’s benefit.\n\nKeep learning!\n\nAndrew\n\nHow do you build an AI resume without job experience? Prepare for an interview? Overcome imposter syndrome? This new eBook collects advice for job-seekers from Andrew Ng.Get your free copy here\n\nApple is redrawing the road map for its self-driving car.\n\nWhat's new:The company is redesigning an autonomous car that has been in development for nearly a decade,Bloombergreported. Originally intended to be fully autonomous under all conditions, the redesigned vehicle will allow for a human driver.\n\nDownshift:Apple had scheduled the vehicle, code named Titan, for 2025, anonymous insiders said. However, executives realized earlier this year that they couldn’t meet the deadline and decided to scale back the autonomous features. The new timeline calls for a prototype by 2024, testing through 2025, and launch in 2026. The target price is under $100,000, a markdown from the original $120,000. The company is currently testing its semi-autonomous system on Lexus SUVs in several U.S. states.\n\nBehind the news:Fully self-driving cars on the open road remain limited to a few robotaxi deployments inChinaand theUnited States. Meanwhile, the industry has suffered a series of setbacks. Fordshut downArgo, its joint project with Volkswagen. Tesla’s purported Full Self-Driving optionrequiresa human in the loop. Further development is required to enable such vehicles to drive safely despite challenges likeroad construction and snow.\n\nWhy it matters:Commercializing fully autonomous vehicles is a tantalizing but elusive goal. Apple’s decision to downshift for the sake of bringing a product to market suggests that human drivers will sit behind the wheel for the foreseeable future.\n\nWe're thinking:Full self-driving cars have been five years away for the past decade. The challenge of handling the long tail of rare but critical events has been a persistent issue. Upcoming developments such as foundation models for computer vision are likely to make a substantial difference. We don't know when, but we're confident that the future includes full autonomy.\n\nThe outcome of the FIFA World Cup 2022 depends on learning algorithms.\n\nWhat's new:The quadrennial championship tournament of football (known as soccer in the United States), which wraps up this week, is using machine learning to help human arbitersspot players who break a rulethat governs their locations on the field.\n\nHow it works:The off-side rule requires that, when receiving a pass, members of the team that possesses the ball keep two opposing players between them and their opponents’ goal. Referees often call off-side erroneously depending on their vantage point on the field. FIFAintroduceda Video Assisted Review system in 2018. The machine learning capabilities help human assistants in a remote video center identify violations.\n\nBehind the news:AI is watching activity off the pitch as well. Qatari authorities use face recognition tomonitorfans for unruly behavior. Authorities also use computer vision totrackcrowd size and movement to prevent the violence and crowd crushes that havemarredrecent matches.\n\nControversy:The semi-automated offside detection system has beencriticizedby players who say its role in referee decisions is unclear.\n\nWhy it matters:Players and fans alike expect referees to be both objective and omnipresent — which is, of course, impossible for anyone to accomplish. AI isn’t a perfect substitute, but it allows officials to observe the action at an unprecedented level of detail.\n\nWe're thinking:If FIFA hasn’t come up with a name for the system, we humbly suggest: Football Net.\n\nAnnouncing our newest specialization!Mathematics for Machine Learning and Data Scienceis carefully designed to help you understand the fundamentals behind common algorithms and data analysis techniques. Scheduled to launch in January 2023!Join the waitlist\n\nA blockbuster app produces sexualized avatar images, even when the original portraits were safe for work.\n\nWhat's new:Lensa AI, a photo editor that turns face photos into artistic avatars, sometimes generates sexualized images from plain selfies, according to several independent reports. It can also be manipulated to produce more explicit imagery, raising concerns that it may be used to victimize people by generating lewd images of their likeness.\n\nHow it works:Users upload 10 to 20 photos and choose a gender. The app uses the open source Stable Diffusion image generator to produce images in various art styles including fantasy, comic-book, and faux-3D rendering. Users must buy a $36 annual subscription to use the image generator, which costs an additional $3.99 for 50 images, $5.99 for 100, or $7.99 for 200. The terms of service disallow nudes and photos of minors, and the app requests that users verify that they are adults.\n\nNSFW:Journalists conducted tests after hearing complaints from users.\n\nBehind the news:Image generators based on neural networks have churned out nonconsensual nude depictions of real people at leastsince 2017. Open-source and free-to-use models have made it easier for the general public to create such images. In November, Stability AI, developer of Stable Diffusion,releaseda version trained on a dataset from which sexual images had been removed.\n\nWhy it matters:Text-to-image generators have hit the mainstream: Lensa was the Apple Store’s top download last week, and three similar apps were in the top 10. People who fear deepfakes now have cause for a once-hypothetical concern: Anybody who has access to photos of another person could hijack their images.\n\nWe're thinking:Image generation has widespread appeal and it’s easy to use. That’s no excuse for misusing it to degrade or harass people. Creating or sharing a nude depiction of someone without their permission is never okay.\n\nLarge language models are trained only to predict the next word based on previous ones. Yet, given a modest fine-tuning set, they acquire enough information to learn how to perform tasks such as answering questions. New research shows how smaller models, too, can perform specialized tasks relatively well after fine-tuning on only a handful of examples.\n\nWhat’s new:Atlasis a language model of modest size that fulfills prompts by referring to external documents. Gautier Izacard and Patrick Lewis led the project with colleagues at Meta, École Normale Supérieure, Paris Sciences et Lettres, Inria, and University College London.\n\nKey insight:A large language model uses its huge complement of parameters to memorize information contained in its pretraining and fine-tuning datasets. It wouldn’t need to memorize so much — and thus wouldn’t need so many parameters — if it had access to documents on demand.\n\nHow it works:Atlas comprises aretrieverthat’s pretrained to fetch relevant documents fromWikipediaandCommon Crawl, and alanguage modelthat uses the documents in those datasets to respond to prompts. The authors fine-tuned the system to complete tasks including answering open-ended questions inKILTand multiple choice questions inMMLU.\n\nResults:MMLU offers four possible answers to each question, so random chance is 25 percent. Fine-tuned on five examples in MMLU, Atlas (11 billion parameters) achieved 47.9 percent average accuracy, while GPT-3 (175 billion parameters) achieved 43.9 percent average accuracy. (Atlas didn’t beat the 70-billion parameter Chinchilla, which achieved 67.5 average accuracy.) Fine-tuned on all MMLU training examples, Atlas achieved 66 percent average accuracy, while GPT-3 achieved 53.9 percent average accuracy. The questions in KILT’s Natural Questions subset are open-ended, so accuracy measures the percentage of outputs that exactly matched ground truth. Fine-tuned on 64 Natural Questions examples, Atlas achieved 42.4 percent accuracy, while next-best PaLM (540 billion parameters) achieved 39.6 percent accuracy. Fine-tuned on all Natural Questions training examples, Atlas achieved 60.4 percent accuracy, while the previous state of the artR2-D2(1.3 billion parameters) achieved 55.9 percent accuracy.\n\nWhy it matters:Training smaller models consumes less energy and costs less. Shifting the knowledge memorized by the model from the parameters into an external database not only reduces the number of necessary parameters, but also makes the model’s knowledge easier to update. Instead of retraining the model, you can simply extend the document database by feeding new data to the models and storing the resulting document embeddings.\n\nWe’re thinking:Augmenting a language model’s training with retrieved documents is a promising avenue of research.RETROdid something similar, but it wasn’t fine-tuned on particular tasks, much less on a handful of examples. Similarly, researchers at Meta built achatbotthat used documents found on the web to generate more realistic conversations.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--12-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--11-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--21-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/12/WORLDCUP_Crop_600px.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed-3.png",
      "https://dl-staging-website.ghost.io/content/images/2022/12/LENSA_1200px.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--22-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-187/",
    "title": "issue 187",
    "date": "",
    "reading_time": "",
    "content": "ChatGPT has raised fears that students will harm their learning by using it to complete assignments. Voice cloning, another generative AI technology, has fooled people into giving large sums of money to scammers, as you can read below in this issue ofThe Batch. Why don’t we watermark AI-generated content to make it easy to distinguish from human-generated content? Wouldn’t that make ChatGPT-enabled cheating harder and voice cloning less of a threat? While watermarking can help, unfortunately financial incentives in the competitive market for generative AI make their adoption challenging.\n\nEffective watermarking technology exists. OpenAI has talked about developing it to detect text produced by ChatGPT, and thistweet stormdescribes one approach. Similarly, a watermark can be applied invisibly to generated images or audio. While it may be possible to circumvent these watermarks (for instance, by erasing them), they certainly would pose a barrier to AI-generated content that masquerades as human-generated.\n\nUnfortunately, I’m not optimistic that this solution will gain widespread adoption. Numerous providers are racing to provide text-, image-, and voice-generation services. If one of them watermarks its output, it will risk imposing on itself a competitive disadvantage (even if it may make society as a whole better off).\n\nFor example, assuming that search engines downranked AI-generated text, SEO marketers who wanted to produce high-ranking content would have a clear incentive to make sure their text wasn’t easily identifiable as generated. Similarly, a student who made unauthorized use of a text generator to do their homework would like it to be difficult for the teacher to find out.\n\nEven if a particular country were to mandate watermarking of AI-generated content, the global nature of competition in this market likely would incentivize providers in other countries to ignore that law and keep generating human-like output without watermarking.\n\nSome companies likely will whitewash these issues by talking about developing watermarking technology without actually implementing it. An alternative to watermarking is to use machine learning to classify text as either AI- or human-generated. However, systems likeGPTzerothat attempt to do so have a high error rate and don’t provide a robust solution.\n\nIf one company were to establish a monopoly or near-monopoly, then it would have the market power to implement watermarking without risking losing significant market share. Given the many downsides of monopolies, this is absolutely not the outcome we should hope for.\n\nSo what’s next? I think we’re entering an era when, in many circumstances, it will be practically impossible to tell if a piece of content is human- or AI-generated. We will need to figure out how to re-architect both human systems such as schools and computer systems such as biometric security to operate in this new — and sometimes exciting — reality. Years ago when Photoshop was new, we learned what images to trust and not trust. With generative AI, we have another set of discoveries ahead of us.\n\nKeep learning!\n\nAndrew\n\nGerry Fernando Patia didn’t come from a privileged background or attend a big-name university. So how did he land at Facebook right out of school?Read his storyand learn how he used hackathons to attract recruiters.\n\nTired ofrap battlescomposed by ChatGPT? Get ready for the next wave of AI-generated fun and profit.What’s new:Cloned voices are taking center stage in productions by upstart creators and monied corporations alike.How it works:Companies including ElevenLabs, Resemble AI, Respeecher, and Play.ht recently launched free services that clone a speaker’s voice from brief samples. Such offerings unleashed a chorus of generated voices.\n\nYes, but:The democratization of voice cloning opens doors to criminals and pranksters.\n\nWhy it matters:Voice cloning has entered the cultural mainstream facilitated by online platforms that offer AI services free of charge. Images, text, and now voices rapidly have become convincing and accessible enough to serve as expressive tools for media producers of all sorts.\n\nWe’re thinking:With new capabilities come new challenges. Many social and security practices will need to be revised for an era when a person’s voice is no longer a reliable mark of their identity.\n\nThe output of AI-driven image generators is not protected by copyright in the United States.\n\nWhat’s new:The U.S. Copyright Officeconcludedthat copyright does not apply to images generated by the image generator Midjourney.\n\nSplit decision:In September, 2022, the agency granted a copyright for the comic bookZarya of the Dawn. The following month, however, it alerted author Kris Kashtanova of their intent to cancel the copyright after they learned from the author’s social media posts that Midjourney had produced the images. Kashtanova appealed the decision, and the agency revised its decision by granting a copyright for the text and arrangement of the images on its pages.\n\nHumans versus machines:The agency explained its rationale:\n\nMixed results:Kashtanova said the agency’s decision to protect the text and layout was “great news” but vowed to continue lobbying for copyright protection of the images as well.\n\nYes, but:Different countries are likely to decide such issues differently, creating potential conflicts as intellectual property moves over the internet. While the U.S. has denied protection for intellectual property created by AI, in 2021 South Africaissueda patent that names an AI system as the inventor of a food container with unique properties.\n\nWhy it matters:Who owns the output of generative AI models? No one — in the U.S., at least. This decision is bound to influence business strategies throughout the publishing and creative communities as generated text, images, video, sound, and the like proliferate.\n\nWe’re thinking:It takes imagination and skill to generate a satisfying picture using Midjourney including envisioning an image, composing an effective prompt, and following a disciplined process over multiple attempts. Denying the creativity, expertise, and contribution of people who use AI as a creative tool strikes us as a mistake.\n\nAndrew Ng talks with Workera CEO Kian Katanforoosh about upskilling in machine learning and how he hires world-class AI teams in the newest episode of Workera’s Skills Baseline podcast.Watch it here\n\nOn the heels of systems that generate video directlyfromtext, new work uses text to adjust the imagery in existing videos.\n\nWhat’s new:Patrick Esser and colleagues at Runway unveiledGen-1, a system that uses a text prompt or image to modify the setting (say, from suburban yard to fiery hellscape) or style (for instance, from photorealism to claymation) of an existing video without changing its original shapes and motions. You can see examples and request accesshere.\n\nKey insight:A video can be considered to have what the authors callstructure(shapes and how they move) andcontent(the appearance of each shape including its color, lighting, and style). A video generator can learn to encode structure and content in separate embeddings. At inference, given a clip, it can replace the content embedding to produce a video with the same structure but different content.\n\nHow it works:Gen-1 generates video frames much like a diffusion model, and the authors trained it following the typical diffusion-model training procedure: Add to each training example varying amounts of noise — nearly up to 100 percent — then train the model to remove it. To generate a video frame, the model starts with 100 percent noise and, guided by a text prompt or image, removes it over several steps. The system used three embeddings: (i) a frame embedding for each video frame (to which noise was added and removed), (ii) a structure embedding for each video frame, and (iii) a content embedding for the entire clip. The dataset comprised 6.4 million eight-frame videos and 240 million images, which the system treated as single-frame videos.\n\nResults:Five human evaluators compared Gen-1 toSDEdit, which alters each frame individually. Testing 35 prompts, the evaluators judged Gen-1’s output to better reflect the text 75 percent of the time.\n\nWhy it matters:Using different embeddings to represent different aspects of data gives Gen-1 control over the surface characteristics of shapes in a frame without affecting the shapes themselves. The same idea may be useful in manipulating other media types. For instance,MusicLMextracted separate embeddings for large-scale composition and instrumental details. A Gen-1-type system might impose one musical passage’s composition over another’s instruments.\n\nWe’re thinking:Gen-1 doesn’t allow changes in objects in a frame, such as switching the type of flower in a vase, but it does a great job of retaining the shapes of objects while changing the overall scenery. The authors put this capability to especially imaginative use when they transformed books standing upright on a table into urban skyscrapers.\n\nMeet the Romanian government’s automated political adviser.What’s new:The Prime Minister of Romania launchedION, a system that summarizes and organizes public comments for cabinet ministers,Politico.eureported.How it works:Romanian citizens can submit comments via a website or by embellishing Twitter, Facebook, and Instagram posts with the tag @noisuntemION (“we are ION”). An onlinedocumentdescribes the system in detail.\n\nBehind the news:GovernmentsuseAIto manage operations, dispense benefits, and administer justice. However, systems that influence policy remain largely experimental. For instance, Salesforce engineerstraineda model to create a tax policy that promoted general income equality and productivity more effectively than the current United States tax code.Why it matters:Politicians and policymakers must often interpret the will of the people through polls, press reports, or lobbyists. Romania’s experiment may tell officials more directly what constituents want.\n\nWe’re thinking:Many companies analyze social media to understand customer sentiment; for instance, clustering tweets to see what people are saying about a brand. Policymakers' embrace of a similar approach is a welcome step.\n\nResearch group EleutherAI plans to launch a nonprofit institute backed by AI startups and industry leadersThe EleutherAI Institute aims to contribute to open source AI research through donations and grants from major backers like Hugging Face and Canva. (TechCrunch)Eyeball, anthe first online video scouting platform for youth soccer, is helping elite teams like AC Milan and Benfica recruit new playersThe system tracks players on the pitch and produces action clips and stats for scouts to analyze. (Forbes)Consulting company Bain joined with OpenAI to develop a suite of marketing and customer service toolsThe partnership will help Bain’s clients develop contact center scripts, ad copy, and more. The Coca-Cola Company will play an unspecified role. (Bain)\n\nThe Culture AI Games and Film Festival will preview AI-generated movies and video games coming soonThe festival, which will take place in San Francisco on May 9, 2023, will celebrate the impact of generative AI in the film and video game industry. (VentureBeat)\n\nThe United States plans to use face recognition on military dronesThe U.S. Military signed a contract to deploy this AI-powered technology for surveillance,target acquisition, and other functions during special operations. (Vice)Research:Meta released LLaMA, an open large language modelThe model is available at several sizes (7B, 13B, 33B, and 65B parameters). You can apply for access to the modelshere. (Meta AI)Research:Scientists developed a machine learning model to predict biodiversity of coralThe model is helping conservationists examine the impacts of climate change on the connectivity and biodiversity in the Coral Triangle, an area of the western Pacific Ocean that is one of the planet’s most diverse and biologically complex marine ecosystems. (Mongabay)AI-written books flood Amazon’s Kindle storeMore than 200 eBooks, from how-to guides to poetry collections, list ChatGPT as author or co-author. (Reuters)A case before the UK's Supreme Court asks whether patents can list AI as an inventorThe UK Intellectual Property Office (IPO) rejected a developer’s bid to name an AI as the inventor for two patents. The developer is taking the case to the highest court. (Evening Standard)SK Telecom plans to launch an AI chatbotThe South Korean company’s  chatbot called “A.” (pronounced A period) will integrate third-party services like payment and ecommerce apps. It’s still in the early stages of an international launch. (CNBC)Quizlet, which provides learning tools to students, launched a beta test of an AI tutorQ-chat is a one-on-one tutor that tests reading comprehension, asks in-depth questions, and encourages students. It’s based on ChatGPT. (Quizlet)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--27--1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/03/The-Batch-ads-and-exclusive-banners--8-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--28-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--42-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/03/The-Batch-ads-and-exclusive-banners--9-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--44-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/03/unnamed--43-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-180/",
    "title": "issue 180",
    "date": "",
    "reading_time": "",
    "content": "In late December, Google reportedlyissued a “code red”to raise the alarm internally to the threat of disruption of its business by large language models like OpenAI’s ChatGPT.\n\nDo large language models (LLMs) endanger Google's search engine business? I think there’s a path for them to transform the way we access information, albeit one that poses technical and business hurdles.\n\nWhat if, rather than searching the web, we could query an LLM and get an answer? We would receive not a page of web links but a piece of text that answered our query. This appears to work for basic factual questions, but for questions that require complex reasoning or specialized knowledge, today’s LLMs may confidently hallucinate an answer, making the resultmisleading.\n\nHere’s one way to think about the problem. ChatGPT’s predecessor GPT-3 has 175 billion parameters. Using 16-bit, floating-point bytes, it would take around 350GB to store its parameters (many reports say 800GB). In comparison, Wikipedia occupies about 150GB (50GB for text, 100GB for images). While the comparison is far from apples to apples, the fact that an LLM has more memory than is needed to store Wikipedia suggests its potential to store knowledge.\n\nBut even Wikipedia contains a minuscule fraction of the knowledge available on the internet, which by some estimates amounts to 5 billion GB. Thus search, which can point us to pages from all corners of the web, can answer many questions that an LLM with fixed memory can't.\n\nThat said, I see significant potential in another technology, retrieval augmented generation. Rather than relying on a fixed LLM to deliver the answer to a query, if we first find relevant documents (online or elsewhere) and then use an LLM to process the query and the documents into an answer, this could provide an alternative to current web search. Executing this efficiently and at scale would be complex, but the effect would be akin to having an LLM do a web search and summarize the results. Examples of this approach include Meta'sAtlasand DeepMind'sRETRO.\n\nWhile today's search engine giants are well positioned to execute on this technology, their businesses depend on users clicking on ads placed next to search results. If they were to deliver text that answered a query, where would ads fit into the picture? Google would need to solve that problem before it could replace traditional web search with LLMs. Search startups that don’t have as much to lose — or perhaps Microsoft’s Bing, which is the second most-popular search engine by some reckonings — may be more willing to embrace upheavals in the search-engine business model.\n\nOf course, Google's business has many moats, or defenses. The company's control over the Chrome web browser and Android mobile operating system channels users to its search engine. Having a platform with many advertisers and a sophisticated ad system also enables Google to monetize user attention better than competitors. Thus, it can pay more for search traffic to, say, incentivize makers of web browsers to make it the default search engine.\n\nIt's fascinating that generative AI is already so powerful that Google declared an emergency. How exciting to live in a time when we can be part of this evolution of AI!\n\nKeep learning,\n\nAndrew\n\nI-Chiao Lin was a typical software engineer. Then she saw a movie that made her imagine herself as an AI builder. With an open mind and an appetite for learning, she achieved her dream and now makes computer vision products for a major tech company.Read her story\n\nTools that automatically write computer code may make their human users overconfident that the programs are bug-free.What’s new:Stanford University researchersfoundthat programmers who used OpenAI’s Codex, a model that generates computer code, were more likely to produce buggy software than those who coded from scratch.\n\nHow it works:The authors recruited 47 participants, from undergraduate students to professional programmers with decades of experience, to complete security-themed coding tasks. They gave 33 the option to useCodex, a fine-tuned version of GPT-3, through a custom user interface. The remaining 14 served didn’t receive automated assistance. Both groups were allowed to copy code from the web.\n\nResults:The authors evaluated the responses manually according to whether they were functional and secure. Participants who used Codex generally produced code that was less functional and secure, yet they expressed greater confidence in it. That said, the results varied with the task and programming language.\n\nBehind the news:Other research bolsters the notion that professional developers shouldn’t fear for their jobs quite yet. In a 2022 study, DeepMind’sAlphaCodemodel competed in 10 simulated contests. The model correctly solved 34 percent of the validation questions and outpaced 46 percent of humans who had taken up the same challenges.\n\nWhy it matters:Generative coding tools are often regarded as a way for programmers to save time and automate basic tasks. But that efficiency may come at a price. Coders who use such tools would do well to pay extra attention to debugging and security.We’re thinking:Code generation is an exciting development despite the questions raised by this study. We welcome further studies that compare programmers who use Codex, those who copy code from the internet, and those who use no outside assistance. How long, on average, would it take subjects in each group to complete the tasks correctly and securely, taking into account the time required to debug generated code?\n\nA Chinese naval ship navigates autonomously and controls a swarm of onboard drones.What’s new:TheZhuhaiyun, billed as the first autonomous drone carrier, officially entered service after 12 hours of trials on open water, theSouth China Morning Postreported.\n\nHow it works:The vessel plans its path and avoids hazards using data from onboard sensors and satellites. Remote human operators can take control if needed.\n\nBehind the news:China’sfirstautonomous military ship completed sea trials in June. The vessel’s developers didn’t specify its intended purpose, but observers noted its resemblance to the Sea Hunter, an autonomous shipdevelopedby the United States Defense Advanced Research Projects Agency to hunt submarines and clear mines. China isbuildinganother large uncrewed ship with features similar to U.S. craft, and the U.S. isdevelopingnumerous other autonomous aircraft and ships.\n\nWhy it matters:For naval commanders, autonomous ships are less costly to operate than crewed ships, can deploy without stocking human provisions, and won’t leave noncombatants bereft if they sink.We’re thinking:The Batchsupports the United Nations’ proposedbanon fully autonomous weapons. Meanwhile, autonomous vessels have valuable peacetime uses: oceanographic research, search and rescue, andferrying cargo, to name a few.\n\nJoin world-class leaders and companies at WhyLabs’Robust & Responsible AI Summit! This free, half-day event includes a fireside chat with Andrew Ng. Mark your calendar for January 26, 2023, at 9:15 a.m. Pacific Time and registerhere\n\nAn experiment in using chatbots to dispense mental-health counseling raised questions about ethics.What’s new:Rob Morris, cofounder and CEO of Koko, a nonprofit provider of emotional-support services,shareddetails of an informal experiment in which his organization provided advice generated by a large language model to users without their explicit knowledge or consent.How it works:The company’s peer-counseling service, known as Kokobot, helps social networks connect users who request counseling to other users who wish to provide it. A prospective counselor receives an anonymous message seeking help, advice, or encouragement, and the service shares the counselor’s response anonymously with the person who requested it.\n\nThe backlash:Experts questioned the ethics of Koko’s actions.\n\nBehind the news:Several companies that use chatbots to support mental health explicitly inform users that the conversation is automated, includingReplika,Flow, andWoebot(a portfolio company of AI Fund, which Andrew leads). Some mental health expertsquestionwhether chatbots provide lasting benefits and point to the need for more independent studies that demonstrate their efficacy.\n\nWhy it matters:AI-powered therapy could be a low-cost alternative for people who seek mental-health counseling, especially in parts of the world where psychiatrists arefew.\n\nMoreover, interacting with a computer mayhelppatients feel comfortable sharing issues they wouldn’t discuss with a doctor. However, therapy requires trust, and informal experiments like Koko’s could alienate people who stand to benefit.We’re thinking:Large language models are becoming more capable by the month, leading developers to turn them loose on all manner of problems. We encourage experimentation, especially in healthcare, but experiments on human subjects must meet the highest ethical standards.\n\nThe metals needed to meet rocketing demand for electric cars and renewable power plants are in short supply. A startup is using machine learning to discover new sources.What's new:KoBold Metals invested $150 million todevelopa copper mine in Zambia. With funding backed by OpenAI founder Sam Altman, Jeff Bezos, Richard Branson, and Bill Gates, the four-year-old startup based in Berkeley, California, previously forged partnerships with mining giants BHP and Rio Tinto.How it works:The Zambia site may yield enough copper to produce 100 million electric vehicles,Bloombergreported. The readiest sources of copper, cobalt, nickel, lithium, and rare-earth elements — minerals crucial to development of next-generation energy sources — have already been developed. KoBold identifies locations that have been overlooked or rejected using conventional methods and where valuable ore may be buried deep underground.\n\nBehind the news:Oil and gas producers use a variety of AItechniquesto find oil and gas deposits and other phases of production. In exploration, models typically learn from large quantities of seismic data to evaluate areas below the surface for qualities like porosity and saturation, helping to identify sweet spots. Neural networks are typically used to home in on the most promising targets. Other architectures have proven useful in locating wells, predicting well pressure, and related tasks.Yes, but:Kobold’s approach is not yet proven. It uses data from some parts of the world to discover metal deposits in others, while minerals in the Earth’s crust can occur under widely varying conditions,Wiredreported.Why it matters:Heavy metals and rare earth minerals are crucial raw materials for components in batteries, electric motors, wind turbines, and portable electronics. But extracting these resources is costly and ecologically fraught; only one in 100 exploratory boreholes bears fruit. If machine learning can reduce the risk, it may make prospecting more economical and environmentally friendly.We're thinking:It’s good to see the mining industry doesn’t take AI for granite.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--22--1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/01/Working-AI--600---338-px---Presentation--169--.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/01/CODE.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/01/ZHUHAIYUN.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/01/R2AISummit-Jan26-WhyLabs.jpeg",
      "https://dl-staging-website.ghost.io/content/images/2023/01/THERAPY.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--30-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-271/",
    "title": "issue 271",
    "date": "",
    "reading_time": "",
    "content": "It’s high time to take geoengineering more seriously as a potential tool to mitigate climate change. 2023 was the hottest year on record, and 2024 is likely to top that. In the United States, Hurricane Helene caused over 200 deaths, and Hurricane Milton's death toll is at least two dozen. It’s well established that the hurricanes are growing stronger as global temperatures rise.\n\nWhilestratospheric aerosol injection(SAI) — which sprays particles (aerosols) in the atmosphere to provide a small amount of shade from the sun — is far from a perfect solution, we should take it seriously as a possible tool for saving lives. A few months ago, my collaborators and I had released a climate emulator,Planet Parasol, that you can play with to simulate different SAI scenarios to understand its possible impact. By using AI to model its impact and thereby advance our understanding of SAI, we’ll be better prepared to decide if this is a good step.\n\nThe key idea of SAI, which is a form of climate geoengineering, is to spray reflective particles into the stratosphere to reflect a little more, say 1%, of the sunlight that otherwise would fall on Earth back into space. This small increase in reflected sunlight would be sufficient to mitigate much of the impact of human-induced warming. For example, in 1991, Mount Pinatubo ejected almost 20 tons of aerosols (sulfur dioxide) into the atmosphere and cooled down the planet by around 0.5 degrees Celsius over the following year. We should be able to induce cooling equivalent to, say, a fraction of Mount Pinatubo, via a fair, international process that’s backed by science.\n\nThere are many criticisms of SAI, such as:\n\nIn addition, many people have a visceral emotional reaction, as I once did before I understood the science more deeply, against “playing god” by daring to engineer the planet.\n\nAll these downsides should be balanced against the reality that people are dying.\n\nI’m moved by meteorologist John Morales’ emotional account of the havoc caused by Hurricane Milton.The New York Timesquotedhim as saying, “It claims lives. It also wrecks lives.”\n\nSkyfire AI, a drone company led by CEO Don Mathis that my team AI Fund helped to co-build, was recentlyon the groundin the aftermath of Helene and Milton, deploying drones to help emergency responders survey remote areas and find survivors. Mathis reports that Skyfire was credited with saving at least 13 lives. On Monday, I also spoke about AI applied to renewable energy with AES’ CEO Andres Gluski and CPO Chris Shelton. You can view our conversationhere.\n\nWhile I’m glad that AI can help mitigate these disasters, it saddens me that so many lives have already been lost due to climate-influenced causes. My mind frequently returns to SAI as one of the few untapped tools in our arsenal that can help. We need to be investing in SAI research now.\n\nI’m grateful to my collaborators on the Planet Parasol emulator (a group that includes many climate scientists) including Jeremy Irvin, Daniele Visioni, Ben Kravitz, Dakota Gruener, Chris Smith, and Duncan Watson-Parris.MIT Technology Review’s James Templewroteabout his experience playing with our emulator and also outlines fair criticisms. Much work remains to be done, and making sure our actions are based on science — a task that AI can help with (witness the recent Chemistry and Physics Nobel Prizes going to innovators in AI!) – will help us make better decisions.\n\nIf you’re interested in learning more about SAI, check out this recentpanel discussionwhere I spoke alongside climate scientists Chris Field, David Keith, Douglas MacMartin, and Simone Tilmes about the science and possible roadmaps ahead.\n\nKeep learning!\n\nAndrew\n\nIn this course, you’ll learn to build scalable agents without managing infrastructure. Explore agentic workflows, tool integration, and setting up guardrails for secure and responsible operations.Sign up today\n\nMalaysia’s location, natural resources, and investor-friendly government are perfect for data centers, turning part of the country into an AI-fueled boomtown.\n\nWhat’s new:Data center construction is flourishing in the southern Malaysian state of Johor, where companies including ByteDance and Microsoft are spending billions of dollars on facilities,The Wall Street Journalreported. These data centers will provide processing power for AI, cloud computing, and telecommunications.\n\nHow it works:Data center construction has slowed in established areas like Ireland and Northern Virginia as space and resources have become scarce. All regions face shortages of electrical power, analystssay, and some U.S. locations face publicresistanceto new projects. Johor has emerged as an attractive alternative.\n\nBehind the news:The Asia-Pacific region is second to North America in data center construction, according to one recentreport, ahead of Europe, South America, and the Middle East and Africa. As Johor builds out its data-center inventory, it will compete with established Asia-Pacificmarketsin Hong Kong, Mumbai, Seoul, Singapore, Sydney, and Tokyo.\n\nWhy it matters:AI is poised to transform virtually every industry, but doing so requires ample processing power. The data-center buildout will help fuel improvements in AI as well as spread the technology to new industries and bring its benefits to people throughout the world. Malaysia’s role as a data center hub is also bound to bring huge economic benefits to the country itself.\n\nWe’re thinking:Many data centers have been built near users to reduce latency. But the cost of processing compute-intensive AI workloads is so high relative to the cost of transmitting data that it makes sense to transmit AI-related data long distances for processing. (As Andrew wrote, thegravity of data is decreasing.) We hope the increasing flexibility in siting data centers will enable more nations that aren’t traditional tech hubs toparticipate in the tech economyand reap significant benefits from doing so.\n\nThe United States government launched Operation AI Comply, targeting businesses whose uses of AI allegedly misled customers.\n\nWhat’s new:The Federal Trade Commission (FTC)took actionagainst five businesses for allegedly using or selling AI technology in deceptive ways. Two companies settled with the agency, while three face ongoing lawsuits.\n\nHow it works:The FTC filed complaints against the companies based on existing laws and rules against unfair or deceptive commercial practices. The FTC alleges:\n\nBehind the news:The FTC has a broad mandate to protect consumers, including both deceptive and anticompetitive business practices. In June, itagreedto focus on Microsoft’s investment in OpenAI and Google’s and Amazon’s investments in Anthropic, while the U.S. Department of Justice would examine Nvidia’s dominant market share in chips designed to process AI workloads. The FTC previously brought cases againstRite Aidfor misuse of AI-enabled facial recognition,Everalbumfor deceptive use of facial recognition, andCRI Genetics, which misled consumers while using AI to conduct DNA tests.\n\nWhy it matters:The FTC’s enforcement actions send a message to businesses that aim to take advantage of the latest AI models: making exaggerated claims about AI will bring legal consequences. The complaints point to a set of issues: falsely claiming to use AI to provide a particular service, exaggerating AI’s ability to replace human expertise, generating fake reviews of businesses, promising unrealistic financial returns, and failing to disclose crucial information about AI-based services.\n\nWe’re thinking:These particular actions crack down not on AIper sebut on companies that allegedly deceived consumers. By taking scams off the market while leaving legitimate businesses to operate freely, they may actually increase customer trust in AI.\n\nA new report documents the interplay of powerful forces that drove AI over the past year: open versus proprietary technology, public versus private financing, innovation versus caution.\n\nWhat’s new:Drawn from research papers, news articles, earnings reports, and the like, the seventh annualState of AI Reportrecaps the highlights of 2024.\n\nLooking back:AI’s rapid advance in 2024 was marked by groundbreaking research, a surge of investment, international regulations, and a shift in safety concerns from hypothetical risks to real-world issues, according to investors Nathan Benaich and Ian Hogarth.\n\nLooking forward:The authors reviewed predictions they made in last year’sreport— among them, regulators would investigate the Microsoft/OpenAI Partnership (accurate), and a model builder would spend over $1 billion on training (not yet) — and forecast key developments in 2025:\n\nWhy it matters:The authors examined AI from the point of view of investors, keen to spot shifts and trends that will play out in significant ways. Their report dives deep into the year’s research findings as well as business deals and political currents, making for a well rounded snapshot of AI at the dawn of a new year.\n\nWe’re thinking:The authors are bold enough to make clear predictions and self-critical enough to evaluate their own accuracy one year later. We appreciate their principled approach!\n\nText embedding models are often used to retrieve text, cluster text, determine similarity between texts, and generate initial embeddings for text classifiers. A new embedding model comes with adapters that specialize it to each of these use cases.\n\nWhat’s new:Saba Sturua and colleagues at Jina AI releasedjina-embeddings-v3, a text-embedding system withopen weightsthat can process 8,192 input tokens and output embeddings of 1,024 values. It’s free for noncommercial use and competes with closed weight models from Cohere and OpenAI.\n\nHow it works:Jina-embeddings-v3 comprises a transformer (559 million parameters) and fiveLoRAadapters that plug into the model and adjust its weights for retrieval, clustering, determining similarity, and classification. Two adapters adjust the model for retrieval: one for documents and one for queries.\n\nResults:The authors compared jina-embeddings-v3 to Cohere’smultilingual embed v3, OpenAI’stext-embedding-3-large, and Microsoft’s open-weightsMultilingual-E5-large-instruct. They tested their system on theMassive Text Embedding Benchmark(MTEB) for embedding tasks.\n\nWhy it matters:Training a set of LoRA adapters is becoming the go-to method for adapting a pretrained model for a variety of tasks. Jina extends the list to computing embeddings for different language tasks and gives developers a further option for generating high-quality embeddings.\n\nWe’re thinking:The authors’ results show that using embeddings that are one-eighth the typical size degrades performance by only 2 percent. That tradeoff may be worthwhile if your computational budget is constrained or your task is especially data-intensive.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--18--1.png",
      "https://dl-staging-website.ghost.io/content/images/2024/10/The-Batch-ads-and-exclusive-banners---2024-10-15T094953.186.png",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--19-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--20-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--19-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--20--1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-63/",
    "title": "issue 63",
    "date": "",
    "reading_time": "",
    "content": "Welcome to this special Halloween issue of The Batch!\n\nIn AI, we use many challenging technical terms. To help you keep things straight, I would like to offer some definitions that I definitely would not use. I hope you’ll find this alternative AI glossary a breath of fresh scare:\n\nHappy Halloween to all who celebrate it. Now let’s get this party started!\n\nKeep learning,\n\nAndrew\n\nAs the days grow short, we peer into the gathering night to glimpse dark shapes amid the shadows. Last yearat this season, we trembled before rogue AGI, ubiquitous surveillance, and the chill winds of AI winter. Those goblins still dance just beyond the jack o’lantern’s candle — yet other shades now join them: algorithms that exploit our basest instincts, models that consume every watt we can generate, tribal drumbeats that divide our community. But we need not cower. Build the bonfire high! Face the dire omens! Let our very fears spur us to extinguish these demons forevermore!\n\nWill AI promote lies that deepen social divisions?The fear:Propagandists will bait online recommendation algorithms with sensationalized falsehoods. People who snap at the clickbait will be reeled into opposing ideological silos.Behind the worries:Consumption of online content hasskyrocketedsince the pandemic began. Social media platforms, especially, are known to bevectors for disinformation. Bad actors have embraced algorithmically boosted disinformation campaigns to advance their agendas.\n\nHow scared should you be:Social media networks are getting better at spotting and blocking coordinated disinformation campaigns. But they’re still playing cat-and-mouse with propagandists.\n\nWhat to do:No company can tell fact from fiction definitively among the infinite shades of gray. AI-driven recommendation algorithms, which generally optimize for engagement, can be designed to limit the spread of disinformation. The industry is badly in need of transparent processes designed to reach reasonable decisions that most people can get behind (like free elections in a democracy). Meanwhile, we can all be more vigilant forsigns of disinformationin our feeds.\n\nWill international rivalries fragment international cooperation in machine learning?The fear:Countries competing for AI dominance will lash out at competitors. Without the free flow of research, data, talent, and ideas, the field will slow down. Advances in the industry will only benefit the country where they originated, and the worldwide research community will dissolve into clusters of regional cliques.Behind the worries:Restrictive immigration rules have prevented engineers, scientists, and students from pursuing opportunities across national borders. At the same time, global powers have moved to dominate AI through industrial and trade policy, and to limit its reach through social policy.\n\nHow scared should you be:AI is truly a global effort. The international AI community has a strong tradition of collaboration, and it has built an infrastructure of sharing — including open code, datasets, publications, and conferences — that transcends national boundaries. Yet the aspirations of sovereign states can put the spirit of cooperation at risk. It will take a concerted effort to keep the community alive and thriving, so we can bring the benefits of AI to all people.What to do:Governments should heed calls by leading AI organizations to make it easier for researchers to gainvisas. Conferences should consider meeting in countries with less restrictive borders. Widespreadtranslationsof research papers, particularly those that address AI governance, would be helpful. Efforts to develop international standards for data privacy and use, such as those advanced by theOrganization for Economic Cooperation and Developmentand other groups, would help foster international collaboration in a way that respects individual rights.\n\nWhat if AI requires so much computation that it becomes unaffordable?The fear:Training ever more capable models will become too pricey for all but the richest corporations and government agencies. Rising costs will throttle progress as startups, academics, and students — especially in emerging economies — are left out in the cold. Customers will turn away from AI in search of less costly alternatives.Behind the worries:Training a model to beat the top image classification and object detection benchmarks currently costsmillions of dollars. And that cost is rising fast: The processing power required to train state-of-the-art models doubled every 3.4 months between 2012 and 2018, according to astudyby OpenAI.\n\nHow scared should you be:The massive inflation in training costs arises from trying to beat the best models. If you can make do with something less, the price comes way down. The cost to train an image classification model with Top-5 accuracy of 93 percent on Imagenet fell from $2,523 in 2017 to $13 the following year, according to a Stanfordreport. Pretrained models like Hugging Face’simplementationsof popular language models and APIs like the one the OpenAIoffersfor GPT-3 make access to high-end AI even less expensive.What to do:Researchers at the Allen Institute for AI and elsewherearguethat we should consider a model’s energy efficiency to be just as important as accuracy. Meanwhile, policymakers and executives who see the value in fostering competition should work to boost research funding and access to compute resources.\n\nCourse 3 of theGANs Specializationfrom DeepLearning.AI is available now on Coursera!Enroll now\n\nWill AI that discriminates based on race, gender, or economic status undermine the public’s confidence in the technology?The fear:Seduced by the promise of cost savings and data-driven decision making, organizations will deploy biased systems that end up doing real-world damage. Systems incorporating biased algorithms or trained on biased data will misdiagnose medical patients, bar consumers from loans or insurance, deny parole to reformed convicts, or grant it to unrepentant ones.Behind the worries:Biased implementations have raised public backlash as organizations both private and public figure out what AI can and can’t do, and how to use it properly.\n\nHow scared should you be:Many organizations are attracted by AI’s promises to cut costs and streamline operations, but they may not be equipped to vet systems adequately. The biased systems that have made headlines are just the tip of the iceberg, according toCathy O’Neil, author of the bookWeapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Further reports of systems prone to unfair outcomes are bound to emerge.What to do:AI systems won’t enjoy broad public trust until we demonstrate clearly that they perform well and pose minimal risk of unintended consequences. Much work remains to be done to establish guidelines and systematically audit systems for accuracy, reliability, and fairness.\n\nWill we ever understand what goes on inside the mind of a neural network?The fear:When AI systems go wrong, no one will be able to explain the reasoning behind their decisions. Imperceptible changes to a model’s input will lead unaccountably to fickle outputs. Seemingly well-designed systems will produce biased results without warning. People will suffer harm without explanation or recourse.Behind the worries:Decisions made by neural networks are notoriously difficult to explain. In the real world, they have profoundly affected peoples’ lives. In the lab, they have made it impossible to trust their output — even when they showed highly accurate results.\n\nHow scared should you be:The inability to explain AI-driven decisions is keeping people from using the technology more broadly. For instance, in a recentsurveyof UK information technology workers in the financial services industry, 89 percent said that lack of transparency was the primary impediment to using AI. Europe’s General Data Protection Regulation gives citizens therightto obtain information on automated systems that make decisions affecting their lives. AI makers that can’t provide these details about their technology canfacesteep fines or outright bans.What to do:Research into explaining neural network outputs has madesubstantialstrides, but much more work is needed. Meanwhile, it’s imperative to establish standard procedures to ensure that models are built and deployed responsibly.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Introduction-JackOLantern-onRedBook201.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Bonfire120copy.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/BunniesAreTheREALMonsters_800x449px20copy.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/BalkanWitches4_800x449px20copy.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-10-2820at2012.23.5020PM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/2201.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/CandyWeight-Generic_Alt320copy.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Thing-CandyDistribution_800x449px20copy.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-19/",
    "title": "issue 19",
    "date": "",
    "reading_time": "",
    "content": "We here at deeplearning.ai wish you a wonderful holiday season.\n\nAs you consider your New Year’s resolutions and set goals for 2020, consider not just what you want to do, but what you want to learn:\n\nI find that people who write down their learning goals are more likely to accomplish them. I do so regularly myself.\n\nMaking a list will help set you up for a productive new year. But for now, I hope you are able to rest, reflect with gratitude on things that happened in 2019, and spend time with loved ones.\n\nKeep learning!\n\nAndrew\n\n2019 will be remembered as a time when AI shifted from fantasy to reality in the public’s perception. Twelve months ago, much of the world equated the technology with the Hollywood dreams ofThe Terminator,Westworld, andHer. Today, many people understand AI as a tangible force in the world, and they’re having a serious conversation about its impact on society, economics, politics, and the international balance of power. In this issue ofThe Batch, we revisit the year’s biggest stories in AI.\n\nEarlier language models powered by Word2Vec and GloVe embeddings yielded confused chatbots, grammar tools with middle-school reading comprehension, and not-half-bad translations. The latest generation is so good, some people consider it dangerous.What happened:A new breed of language models wrotenewsthat readers rated as credible as theNew York Timesand contributed to anarticlein theNew Yorker. Happily, these models didn’t fulfill fears that they would unleash a dark tide of disinformation.Driving the story:In 2019, researchers made a leap in natural language performance. The new models become generally proficient by pretraining on a huge, unlabeled dataset. Then they master a given task or subject matter via fine-tuning on a specialized corpus.\n\nBehind the news:In July 2018 — months before BERT came out — DeepMind researcher Sebastian Ruderanticipatedpretraining’s impact on natural language processing. Further, he predicted that breakthroughs in NLP would revolutionize AI as a whole. He based his argument on the energizing effect of pretrained vision models circa 2012. Many in the fieldtracethe deep learning explosion to this moment.Where things stand:Despite the year’s innovations, language models still have room to grow: Even GPT-2’s 1.5 trillion parameters often spit out gobbledygook. As for whether the latest models are capable of disrupting democracy with potent disinformation: U.S. election season is coming up fast.\n\nAn international wave of anti-surveillance sentiment pushed back against the proliferation of face recognition systems.What happened:Activist and watchdog groups in the U.S. and Europe, alarmed by the technology’s potential to infringe on civil liberties, spurred legislation restricting its use. Their efforts built momentum toward national bans on public and private uses of the technology.\n\nDriving the story:Several U.S. cities passed anti-face recognition laws as the federal government mulled the issues. The European Union is working on its own restrictions.\n\nBehind the news:In 2016, the U.S. National Telecommunications and Information Administrationpublishedface-recognition guidelinesaskingcompanies to be transparent, practice good data management, and allow the public some control over sharing of face data with third parties. Although major vendors of the technology are members of the NTIA, it’s not clear whether they follow these guidelines.Where things stand:In June, Amazon Web Service CEO’s Andy JassytoldRecode,“I wish [Congress would] hurry up. . . . . Otherwise, you’ll have 50 different laws in 50 different states.” He may as well have spoken for the tech industry as a whole: Without legal limits, companies are left guessing how far they can push the technology before they violate public trust — risking blowback if they step over the line.\n\nMakers of self-driving cars predicted a quick race to the finish line, but their vehicles are far from the homestretch.What happened:A few years ago, some car companies promisedroad-readyautonomous vehicles as early as 2017. At aWall Street Journalconference in January, though, Waymo CEO John Krafcikdisclosedhis belief that autonomous vehicles would probably never be able to drive in all conditions. His comment set the tone for a year of automotive retrenchment.Driving the story:A confluence of difficulties prompted several car companies to tap the breaks.\n\nBehind the news:Cities in China areexperimentingwith a different approach. Rather than training autonomous vehicles to navigate existing urban settings, they’re retrofitting cities to facilitate the technology. Features include roadside sensors that pass along navigational cues, like lane changes and speed limits.Where things stand:Traditional automakers are focusing on assisted driving features like Ford’sDriver Assistand Mercedes’Parking Assist.Meanwhile, Waymo continues to work on fully autonomous vehicles, and smaller companies such asMay MobilityandVoyageare deploying full autonomy in limited scenarios that they aim to expand over time. In parallel, companies such as TuSimple, Embark, and Starsky are concentrating on fully autonomous interstatetrucking.\n\nSociety awakened to the delight, threat, and sheer weirdness of realistic images and other media dreamed up by computers.What happened:So-called deepfakes became both more convincing and easier to make, stoking a surge of fascination and anxiety that shows every sign of intensifying in the coming year.Driving the story:Two years ago, the majority of deepfakes were pixelated and difficult to make. Now they’re slicker than ever and improving at a quick clip.\n\nThe reaction:Facebook, beset by a fake video of CEOMark Zuckerbergappearing to gloat at his power over the social network’s members,announceda $10 million contest to automate deepfake detection. Meanwhile, China enacted restrictions on spreading falsified media. In the U.S., the state of California passed a similar law, while the House of Representatives considers nationalanti-deepfakelegislation.Where things stand:Detecting and controlling deepfakes is shaping up to be a high-tech game of cat and mouse. Although today’s fakes bear telling features, they’ll be indistinguishable from real imageswithin a year, according to USC computer science professor Hao Li.\n\nThe future of machine learning may depend less on amassing ground-truth data than simulating the environment in which a model will operate.What happened:Deep learning works like magic with enough high-quality data. When examples are scarce, though, researchers areusingsimulationto fill the gap.\n\nDriving the story:In 2019, models trained in simulated environments accomplished feats more complex and varied than previous work in that area. In reinforcement learning, DeepMind’s AlphaStarachievedGrandmaster status in the complex strategy game StarCraft II — able to beat 99.8 percent of human players — through tens of thousands of virtual years competing in a virtual league. OpenAI Fivesimilarlytrained a team of five neural nets to best world champions of Dota 2. But those models learned in a virtual world to act in a virtual world. Other researchers transferred skills learned in simulations to the real world.\n\nWhere things stand:Simulation environments like Facebook’sAI Habitat, Google’sBehavior Suite for Reinforcement Learningand OpenAI’sGymoffer resources for mastering tasks likeoptimizingtextile production lines,fillingin blank spots in 3D imagery, anddetectingobjects in noisy environments. On the horizon, models couldexploremolecular simulations to learn how to design drugs with desired outcomes.\n\nA year-long Twitter feud breathed fresh life into a decades-old argument over AI’s direction.What happened:Gary Marcus, a New York University professor,author,entrepreneur, and standard bearer of logic-based AI, waged a tireless Twitter campaign to knock deep learning off its pedestal and promote other AI approaches.Driving the story:Marcus’ incessant tweets reignited an old dispute between so-called symbolists, who insist that rule-based algorithms are crucial to cognition, and connectionists, who believe that wiring enough neurons together with the right loss function is the best available path to machine intelligence. Marcus needled AI practitioners to reacquaint itself with the symbolist approach lest connectionism’s limitations precipitate a collapse in funding, or AI winter. The argument prompted soberingassessmentsof AI’s future and culminated in a livedebateon December 23 between Marcus and deep learning pioneer and Université de Montréal professor Yoshua Bengio. The conversation was remarkably civil, and both participants acknowledged the need for collaboration between partisans on both sides.\n\nBehind the news:Animositybetween the symbolists and connectionists dates back more than a half-century.Perceptions, a 1969 broadside against early neural networks, helpedtriggerthefirstAI winter. Thesecond, nearly two decades later, came about partly because symbolic AI relied on LISP computers that became obsolete with the advent of personal computers. Neural nets began to gain ground in the 1990s and achieved dominance amid the last decade’s explosion of computing power and data.\n\nWhere things stand:We look forward to exciting times ahead as connectionists and symbolists put their heads together, or until one faction wipes out the other.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/andrews20letter202.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/language20models.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/recognition20backlash.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/driverless20cars.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/generative20models-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/RL.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/twitter20fight.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-122/",
    "title": "issue 122",
    "date": "",
    "reading_time": "",
    "content": "We just wrapped up theData-Centric AI Workshopat the NeurIPS 2021 conference. It was packed with information about how to engineer data for AI systems. I wish the whole DeepLearning.AI community could have been there! I expect the videos to be available before long and will let you know when they’re onlineOver the course of an eight-hour session, authors presented 100 papers via two-minute lightning talks and posters. Eight invited speakers described a variety of data-centric AI issues and techniques, and expert panels answered questions from the audience.These were some of my key takeaways:\n\nAmong the invited speakers:\n\nI also enjoyed hearing participants in DeepLearning.AI and Landing AI’sData-centric AI Competitionspeak about their submissions. You can read some of their blog postshere.\n\nThanks to everyone who participated in the workshop or submitted a paper; to the presenters, panelists, invited speakers, and poster presenters; and to the reviewers, volunteers, and co-organizers who put the program together.\n\nI was struck by the energy, momentum, and camaraderie I felt among the participants. I came away more excited than ever to keep pushing forward the data-centric AI movement, and I remain convinced that this field will help everyone build more effective and fairer AI systems.Keep engineering your data!\n\nAndrew\n\nA leaked document gave reporters a glimpse of what makes TikTok’s renowned recommender algorithm so effective.What’s new:An internal report produced by TikTok’s Beijing-based engineering team for nontechnical colleagues describes the short-form video streaming platform’s formula for recommending videos to particular users, according toThe New York Times. TheTimesreceived the document from an employee who was disturbed by TikTok’s distribution of content that could encourage self-harm. The company confirmed its authenticity.How it works:The company’s primary goal is to add daily active users. A flowchart (see above) indicates that the primary factors that determine daily active use are time spent with the app and repeated uses (“retention”), which in turn are driven largely by interactions such as likes and comments and video quality as determined by the creator’s rate of uploads and ability to make money from them. To that end, the recommender scores each video with respect to a given user and offers those with the highest scores.\n\nWhat they’re saying:“There seems to be some perception (by the media? or the public?) that they’ve cracked some magic code for recommendation, but most of what I’ve seen seems pretty normal.” — Julian McAuley, professor of computer science, University of California San Diego, quoted byThe New York Times.Behind the news:In July,The Wall Street Journalattempted to understand TikTok’s recommender bycreatingover 100 automated accounts, each with a fake date of birth, IP address, and interests such as yoga, forestry, or extreme sports. TikTok homed in on most of the bots’ interests in less than two hours. By analyzing the videos recommended to each account, the reporters determined that the algorithm gave the heaviest weights to time spent watching a video, number of repeat viewings, and whether the video was paused during playback.Why it matters:TikTok has amassed over1 billion monthly userssince its founding in late 2016, and its recommender is an important part of the reason why. The secret sauce is clearly of interest to competitors and researchers, but as we learn more about social media’s worrisome social impacts — such as spreading misinformation, inciting violence, and degrading mental health — it becomes vital to understand the forces at play so we can minimize harms and maximize benefits.We’re thinking:Compared to platforms that deliver longer videos, TikTok’s short format enables it to show more clips per hour of engagement and thus to acquire more data about what a user does and doesn’t like. This makes it easier to customize a habit-forming feed for each audience member.\n\nDeepMind released three papers that push the boundaries — and examine the issues — of large language models.What’s new:The UK-based subsidiary of Alphabet, Google’s parent company,unveileda pair of transformer models that take different approaches to achieving state-of-the-art performance in a variety of language tasks. The company also pinpointed risks that are likely to intensify as such models continue to improve.How it works:The company detailed its findings in three papers.\n\nBehind the news:Gopher and RETRO run counter the trend toward ever-larger language models. On the other hand, RETRO’s querying strategy extends recent research into connecting language models with external sources of knowledge.\n\nWhy it matters:Natural language models have made great strides in recent years, but much work remains to be done to make them reliable and compact enough for a wide variety of applications. With this triad of papers, DeepMind offers a multifaceted approach to delivering on this promise.We’re thinking:The idea that machine learning models don’t need to learn everything but can query external sources during inference could be a key to building more efficient systems.\n\nMLCommons, an open engineering consortium dedicated to improving machine learning for everyone, released thePeople’s Speech Datasetand theMultilingual Spoken Words Corpus(MSWC) to democratize access to speech technology. Download the datasets today!\n\nA prominent online streaming service is using a machine learning model to identify trolls who try to get around being banned.What’s new:Twitch, a crowdsourced streaming platform used primarily by video game enthusiasts,unveiledSuspicious User Detection. The new feature alerts when it recognizes a banned user who has logged in under a new name.How it works:Twitch users deliver content through a channel, while the audience can watch, listen, and chat. Users who experience harassment can ban offenders from their channels. However, a ban doesn’t prevent aggressors from signing in under a new account and resuming the harassment.\n\nBehind the news:Trolls are inevitable on any online platform. Twitch isn’t the only one that uses machine learning to combat them.\n\nWhy it matters:Twitch is one of the world’s largest streaming platforms, but many of its contributorsbuildtheir own anti-harassment tools in the face of what they feel is a lack of attention from the company. AI moderation tools can protect audience members looking to enjoy themselves, content creators aiming to deliver a great experience, and publishers who want to maximize valuable engagement metrics.We’re thinking:Moderating online content is a game of cat and mouse but, as social media balloons, there simply aren’t enough paws to keep the vermin in check. AI tools can’t yet catch every instance of harassment, but they can extend the reach of human mods.\n\nIf you change an image by moving its subject within the frame, a well trained convolutional neural network may not recognize the fundamental similarity between the two versions. New research aims to make CNN wise to such alterations.What's new:Jin Xu and colleagues at DeepMindmodified the input to particular CNN layersso translations and rotations of the input had the appropriate effect on the output.Key insight:Given an image and a translated version of it, a model that’s robust to translation, for instance, should produce nearly identical representations, the only difference being that one is offset by the amount of the translation. Typical CNNs use alternating layers of convolution and downsampling, specifically pooling. They aren’t robust to such transformations because shifting the image changes the relative position of pixels within the pooling window, producing disparate representations. Maintaining relative pixel positions can preserve the representation despite translation, rotation, and reflection.How it works:The authors trained a five-layer convolutional encoder/decoder to reconstruct a dataset ofimages of 2D shapes against plain backgrounds. In each training example, the shape was located at the upper left of the image and oriented at an angle between 0 and 90 degrees. The following steps describe how the network handled translation (it managed rotation and reflection in an analogous way):\n\nResults:In qualitative tests, the authors’ modified CNN reconstructed test images outside of the training distribution, such as shapes located at the right side of the image or rotated more than 90 degrees, more accurately than a baseline model that used normal pooling. It reconstructed 3,200 images from the grayscaleFashion-MNISTdataset of images of clothes and accessories with a mean reconstruction error of 0.0033, a decrease from the baseline architecture’s 0.0055.Why it matters:The world is full of objects, placed willy-nilly. A CNN that can recognize items regardless of their orientation and position is likely to perform better on real-world images and other examples outside its training set.We're thinking:This model would recognize a picture of Andrew if his head were shifted to one side. But would it recognize him if he were wearing something other than a blue shirt?",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-12-15%20at%2011.36.25%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-12-15%20at%2011.36.25%20AM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/tiktok3.gif?upscale=true&width=1200&upscale=true&name=tiktok3.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/GOPHER.gif?upscale=true&width=1200&upscale=true&name=GOPHER.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-12-14%20at%2012.06.02%20PM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-12-14%20at%2012.06.02%20PM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/TWITCH.gif?upscale=true&width=1200&upscale=true&name=TWITCH.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/SUBSAMPLINGv2-1.gif?upscale=true&width=1200&upscale=true&name=SUBSAMPLINGv2-1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-186/",
    "title": "issue 186",
    "date": "",
    "reading_time": "",
    "content": "Landing AI, a sister company of DeepLearning.AI, just released its computer vision platform, LandingLens, for everyone to start using for free. You can try ithere.\n\nLandingLens makes creating computer vision projects easy and fast. If you have 10 minutes, I encourage you to check it out by creating your own project. I also created a three-minute demo video, which you can seehere.\n\nBuilding and deploying a machine learning system is often complicated and time-consuming. You have to collect data, implement a model or find an appropriate open-source model, build a data pipeline to get the data to the right place, develop or find a tool to label the data, train the model, tune hyperparameters, fix data issues, and eventually set up a deployment server and find a way to get the trained model to run on it.\n\nThis process used to take me months. With LandingLens, you can go from starting a project to deploying a model in minutes.\n\nMy team at Landing AI is obsessed with making computer vision easy. The key to making this possible is our data-centric AI approach. Our back end automatically trains a highly tuned model as long as you provide good data. After initial training, you can carry out error analysis and improve the data (or use advanced options to tune hyperparameters if you want) to further improve your model’s performance.\n\nLandingLens has been used successfully in manufacturing, life sciences, satellite imaging, medical imaging, agriculture, entertainment, and many other industries.\n\nToday, companies can visualize and analyze their structured data to derive value from it using tools like pandas, seaborn, matplotlib, and tableau. But many also have collections of images sitting in storage that have yet to be analyzed. If you think this might be true of your organization, please check out LandingLens. I believe you'll find it easy to start experimenting and getting value from your images.\n\nYou can start using LandingLens for freehere.\n\nIf you build or discover something cool and are willing to share what you've found, please let us know atLanding AI's community website. I look forward to seeing what you build.\n\nKeep building!\n\nAndrew\n\nP.S. Now that the mechanics of building a computer vision system are easy, I’ve been thinking a lot about new frameworks to approach machine learning problems that are less academic and more practical. For example, I see test sets as unnecessary for many applications. I will share more about this in the future.\n\nGetting your first AI job can be a struggle. A panel of technical recruiters who want you to succeed recently shared their hiring secrets.Read their insights\n\nChatGPT fever has reached China despite legal and technical barriers.\n\nWhat’s new:Two months after its debut, ChatGPT is a viral sensation on Chinese social media,MIT Technology Reviewreported. Companies in that country are racing to cash in.\n\nPrompt:OpenAI doesn’t serve the model in China, but users there are reaching it through virtual private networks and offshore services that charge a fee per prompt. The chatbot reportedly impressed users in China with its ability to answer prompts in Chinese and its grasp of the country’s popular culture.\n\nOutput:The country’s major tech firms in recent weeks revealed plans to provide their own equivalent services.\n\nBehind the news:Using an earlier generation of technology, Microsoft Research in China developed Xiaoice, a chatbot that continues to enjoy widespread use. More recently, Beijing Academy of Artificial Intelligence developed the 1.75 trillion-parameter Wu Dao 2.0. Nonetheless, Chinese researchers face unique obstacles in natural language processing.\n\nWhy it matters:ChatGPT, Microsoft’s Bing chat, Google’s Bard, and other chatbots built by U.S. tech companies are optimized for the English language. Chinese tech companies are scrambling to capitalize on the public’s hunger for a chatbot that’s compatible with their language and culture.\n\nWe’re thinking:Chinese speakers find ChatGPT exciting despite its relative lack of training in their language. When a model is sufficiently large, a large training corpus enables it to generalize to new languages that may not have much training data. This property offers hope for making large language models work with languages that have far less data than Chinese.\n\nMedia outlets are forging ahead with generative AI despite the technology’s high-profile misfires.What’s new:Publishers are using text generators to produce light reading within constrained formats such as holiday messages and quizzes.The lineup:Three publications, in particular, are taking various approaches to automated content.\n\nBehind the news:The current vogue for generated content caps several years of experimentation. It’s not clear whether any of these initiatives remain active.\n\nWhy it matters:The web has a voracious appetite for [page, and generated text can help online publications produce low-effort pages or perform menial tasks while qualified journalists to do more cerebral work. Investors like the idea:BuzzFeed’sstock jumped over 100 percent after it announced its relationship with OpenAI.We’re thinking:On one hand, it makes sense for news outlets to dip their toes in the roiling waters of text generation by restricting it to fun, inconsequential fare. On the other hand, large language models have a hard enough time generating helpful output without being programmed to tell us our soulmate is a houseplant.\n\nCreate and deploy computer vision models with ease using LandingLens.Get started for free today!\n\nA chatbot that simulated erotic companionship stopped sharing intimacies, leaving some users heartbroken.What’s new:Replika, a chatbot app, deactivated features that allowed premium users to engage in sexually explicit chat with the 3D avatar of their choice,Vicereported. The change followed a notice that Replika’s San Francisco-based parent company, Luka, had violated the European Union’s transparency requirements.\n\nHow it happened:Prior to the shift, Replika’s $70-per-year paid tier (which is still available) had enabled users to select the type of relationship with the bot they wished to pursue: friend, mentor, or romantic partner.\n\nLike losing a loved one:Some users were deeply wounded by the abrupt change in their avatar’s persona, according toVice. One said, “It’s hurting like hell.” Another compared the experience to losing a best friend.Behind the news:In 2015, a friend of Replika founder Eugenia Kuyda died in a car accident. Seeking to hold a final conversation with him, Kuyda used his text messages to build a chatbot. The underlying neural network became the foundation of Replika. The servicegainedusers in 2020 amid a pandemic-era hunger for social interaction.\n\nWhy it matters:People need companionship, and AI can supply it when other options are scarce. But society also needs to try to protect individuals — especially the very young — from experiences that may be harmful. Companies that profit by fostering attachments between humans and machines may not be able to shield their users from emotional distress, but they can at least make sure those users are adults.\n\nWe’re thinking:Eliza, a rule-based chatbot developed in the 1960s, showed that people can form an emotional bond with a computer program, and researchsuggeststhat some people are more comfortable sharing intimate details with a computer than with another human being. While we’re glad to see Replika phasing out problematic interactions, we sympathize with users who have lost an important emotional connection. Breaking up is hard — even with a chatbot.\n\nPrincipal component analysis is a key machine learning technique for reducing the number of dimensions in a dataset, but new research shows that its output can be inconsistent and unreliable.\n\nWhat’s new:Eran Elhaik at Lund Universityassessedthe use of principal component analysis (PCA) in population genetics, the study of patterns in DNA among large groups of people. Working with synthetic and real-world datasets, he showed that using PCA on substantially similar datasets can produce contradictory results.\n\nKey insight:PCA has characteristics that prior research proposed asrisk factorsfor unreproducible scientific research. For instance, it tends to be used to generate hypotheses, accommodates flexible experimental designs that can lead to bias, and is used so frequently — in population genetics, at least — that many conclusions are likely to be invalid on a statistical basis alone. Studies of population genetics use PCA to reduce the dimensions of raw genetic data and cluster the reduced data to find patterns. For example, some studies assume that the closer different populations are clustered, the more likely they share a common geographical origin. If PCA alters the clusters in response to minor changes in the input, then the analysis doesn’t necessarily reflect genetic relationships.\n\nHow it Works:The author tested the consistency of PCA-based analyses using a synthetic dataset and three real-worldhumangenotypedatasets.\n\nResults:Clustering a dataset that included 10 red, green, and blue examples and 200 black ones, the black cluster was roughly equidistant from the red, green, and blue clusters. However, with five fewer blue individuals, the black cluster was much closer to the blue cluster, showing that PCA can process similar data into significantly different cluster patterns. Using real-world data, the author replicated a 2009studythat used PCA to conclude that Indians were genetically distinct from European, Asian, and African populations. However, when he manipulated the proportion of non-Indian populations, the results suggested that Indians descend from Europeans, East Asians, or Africans. Overall, PCA-based analysis of the real-world datasets fluctuated arbitrarily enough to cast doubt on earlier research conclusions.\n\nWhy it matters:This study demonstrates that PCA-based analyses can be irreproducible. This conclusion calls into question an estimated 32,000 to 216,000 genetic studies that used PCA as well as PCA-based analyses in other fields.\n\nWe’re thinking:PCA remains a useful tool for exploring data, but drawing firm conclusions from the resulting low-dimensional visualizations is often scientifically inappropriate. Proceed with caution.\n\nAI-powered fitness equipment monitors body movements and corrects postureAt-home exercise devices from companies like Fiture and Peloton use AI to tell users what they’re doing wrong during workouts. (The Wall Street Journal)\n\nSci-fi magazine editors are receiving hundreds of AI-generated content submissionsIn a matter of weeks, fantasy magazineClarkeworldreceived more than 500 AI-written submissions out of 1,200. Other publications face similar issues. (The New York Times)\n\nChip makers compete for unprecedented opportunities as the popularity of AI-powered text generators soarsThe latest AI-based chatbots require massive computing power. They could drive tens of billions of dollars in net annual sales for semiconductor companies like Nvidia. (The Wall Street Journal)\n\nAlphabet laid off a team of robotsGoogle’s parent company shut down Everyday Robots as part of budget cuts. The division’s machines had been cleaning tables and opening doors in company offices. (Wired)\n\nSpotify launched AI-generated disk jockeysThe new DJ feature comments on tracks, albums, and artists in a synthetic voice. (The Verge)\n\nResearch: Infants beat AI at understanding people’s actionsA team of researchers demonstrated that 11 month-old children perceived the motivation behind an individual’s gestures better than neural-network models. (New York University)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/03/ezgif.com-optimize--5--1-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/03/The-Batch-ads-and-exclusive-banners--6-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/03/Screen-Shot-2023-02-28-at-4.08.48-PM.png",
      "https://dl-staging-website.ghost.io/content/images/2023/03/BUZZFEED_v2a_1200px--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/03/LandingAI-1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/03/REPLIKA_v3_1200px.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/03/PCA--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-ii/",
    "title": "issue ii",
    "date": "",
    "reading_time": "",
    "content": "My Stanford group has launched anAI for Climate Change bootcamp. Stay tuned!If you want to learn TensorFlow, check out the brand-newCourse 2of TensorFlow: From Basics to Mastery from deeplearning.ai.Keep learning,Andrew\n\nBoston Dynamics' robot dog is straining at the leash. In a new promotionalvideo, a pack of the mechanical canines pull a truck down a road like huskies mushing across the tundra. It's another jaw-dropping demo from one of the world's most ambitious robotics companies.What’s new:SpotMini, previously seenclimbingstairs,openingdoors, andtwerkingto \"Uptown Funk,\" is due to hit the market this year. No price has been announced.\n\nHow it works:Although SpotMini can operate autonomously, there's usually a human in the loop. The all-electric robot:\n\nBehind the news:Boston Dynamics’ previous owner, Alphabet, sold the company to SoftBank reportedly because it lacked commercial products. SpotMini will be its first.Smart take:Robotic control is advancing by leaps and bounds, but there's still a long road ahead. Machines this cool could inspire machine learning engineers to take it to the next level.\n\nLast week’s release of the redacted Mueller Report prompted calls to fill in the blanks using the latest models for language generation. A fun test case for state-of-the-art natural language processing—or irresponsible deepfakery tailor-made for an era of disinformation and paranoia?\n\nA really, really bad idea:University of Washington computational linguistics professor Emily M. Bender unleashed a tweet storm explaining why machine learning engineers should resist the temptation. Using AI to “unredact” the report would:\n\nWhat to do instead:For people interested in applying language generation in ways relevant to politics, Bender suggests working on rumor detection or “tools that might help users think twice before retweeting.\"Takeaway:A language model knows only what’s in the data it was trained on. It can’t possibly know what the report's redactors hid from view, and it can't reason about it. Given the state of today's machine learning tech, a well informed human would make far better guesses about what’s missing.\n\nAI is only as good as the data it trains on, but there’s no easy way to assess training data’s quality and character. Researchers want to put that information into a standardized form.\n\nWhat’s new:Timnit Gebru, Jamie Morgenstern, Briana Vecchione, and othersproposea spec sheet to accompany AI resources. They call it \"datasheets for datasets.\"\n\nHow it works:Anyone offering a data set, pre-trained model, or AI platform could fill out the proposed form describing:\n\nWhy It matters:Data collected from the real world tends to embody real-world biases, leading AI to make biased predictions. And data sets that don’t represent real-world variety can lead to overfitting. A reliable description of what’s in the training data could help engineers avoid problems like these.\n\nBottom line:We live in a world of open APIs, pre-trained models, and off-the-shelf data sets. Users need to know what’s in them. Standardized spec sheets would give them a clearer view.\n\nGet convolutional with TensorFlow! Check outCourse 2 of deeplearning.ai's TensorFlow: From Basics to Mastery, newly available on Coursera.\n\nA software agent from OpenAI crushed human players ofDefense of The Ancients 2, a multiplayer online game, in an Internet-widefree-for-all.\n\nWhat’s new:More than 15,000 humans took on OpenAI Five over four days last week. The bot won 99.4 percent of 7,215 games.How it works:OpenAI Five is a team of five neural networks. Each contains a single 1,024-unit LSTM layer that tracks the game and transmits actions through several independent action heads.The challenge:DoTA2is enormously complex. Games last around 45 minutes, requiring long-term strategy. The landscape is not entirely visible at all times, so players must infer the missing information. Roughly 1,000 possible actions are available at any moment. And it’s played by teams, so the five neural nets must employ teamwork.What they’re saying:“No one was able to find the kinds of easy-to-execute exploits that human programmed game bots suffer from,” OpenAI CTO Greg BrockmantoldVentureBeat.We’re thinking:OpenAI is one of several teams doing brilliant work in playing video games. We're thrilled by their accomplishment and momentum. But when and how will these algorithms translate to more practical applications?\n\nU.S. authorities, in a bid to stop aliens from overstaying their visas, aim to apply face recognition to nearly all travelers leaving the U.S.\n\nWhat’s new:Within four years, U.S. Customs and Border Protection expects to scan the faces of 97 percent of air travelers leaving the U.S., according to a newreportfrom the Dept. of Homeland Security.\n\nHow it works:Passengers approaching airport gates will be photographed and their faces will be compared to collected photos from passports, visas, and earlier border crossings. If the system finds a match, it creates an exit record.Behind the news:The CBP plan is already well underway:\n\nWhy it matters:Face recognition is a flashpoint for discussions of ethics in AI. Microsoft refused to supply it to a California law enforcement agency over concern that built-in bias would work against women and minorities. Amazon employees have petitioned the company to stop selling similar technology to law enforcement agencies.\n\nBottom line:U.S. companies are wrestling with self-regulation in lieu of legal limits on how AI can be used. Their choices will have a huge impact on the industry and society at large.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/6b8f5b65-0b7e-495c-be3c-3613671651ed-1.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/6a7a5517-58c9-490a-a603-722e62484ad1.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/afe604b0-915a-4e9d-a0c6-5a728f219cc3.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/14c6a56c-751d-4be0-b00b-696b76a78dcb.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/60ae2339-9012-4631-82d5-c99f911dacd5.png",
      "https://dl-staging-website.ghost.io/content/images/2022/10/50e3df2e-dbe2-4a2b-ab4b-623ace68e880--1-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-95/",
    "title": "issue 95",
    "date": "",
    "reading_time": "",
    "content": "Around the world, students are graduating. If you’re one of them, or if someone close to you is graduating, congratulations!!!\n\nMy family swapped pictures on WhatsApp recently and came across this one, which was taken when I graduated from Carnegie Mellon (I’m standing in the middle). I was privileged to have already worked on a few AI projects thanks to my mentors in college, including Michael Kearns, Andrew McCallum, Andrew Moore and Tom Mitchell. But now, looking back, I reflect on how clueless I was and how little I knew about AI, business, people, and the world in general.\n\nTo this day, I don’t feel particularly clued in. Every year or so, I look back and marvel at how clueless I was a year ago, and I’m pretty sure I’ll feel the same way a year from now. This helps me to act with humility and avoid expressing unwarranted certainty.\n\nIf you’re graduating:\n\nIf you’ve already graduated, I hope you’ll take joy in the success of those who are coming up behind you.\n\nLove to you all and keep learning,\n\nAndrew\n\nU.S. immigration officials expect over2 millionmigrants to reach the country’s southern border by the end of the year. They’re counting on face recognition to streamline processing of those who seek asylum.\n\nWhat’s new:The U.S. Customs and Border Protection agency developed an app called CBP One that matches asylum seekers with existing applications,Los Angeles Timesreported.\n\nHow it works:Would-be immigrants who feel their lives are in danger in their home country —  — most of whom come from violence-wracked parts of Mexico and Central America — can apply for asylum status in the U.S. Some 70,000 who have applied remain in Mexico awaiting a decision.CBP Oneis designed to expedite acceptance or rejection when those people return to the border.\n\nYes, but:Privacy experts are concerned about data collection and surveillance of migrants who have little choice but to use the app. Confidentiality is also a worry, since hackersstole180,000 images from a border patrol database in 2018.\n\nBehind the news:Launched in October, the app initially was limited to cargo shippers, pleasure boaters, and non-immigrant travelers. In May, however, the number of migrants surged, and the agency receivedemergency approvalto bypass privacy laws and use the app to process applications to enter the country.\n\nWhy it matters:Many migrants who arrive at the southern U.S. border arefleeingpoverty, gang violence, political instability, and climate change-induced environmental crises. AI could help those in danger find refuge more quickly.\n\nWe’re thinking:Immigration is hugely beneficial to the U.S., and AI can help scale the process. But it’s crucial that the policies we scale are fair, transparent, and astute rather than biased or xenophobic.\n\nResearchers have used neural networks to generate novel views of a 3D scene based on existing pictures plus the positions and angles of the cameras that took them. In practice, though, you may not know the precise camera positions and angles, since location sensors may be unavailable or miscalibrated. A new method synthesizes novel perspectives based on existing views alone.\n\nWhat’s new:Chen-Hsuan Lin led researchers at Carnegie Mellon University, Massachusetts Institute of Technology, and University of Adelaide in developing the archly namedBundle-Adjusting Neural Radiance Fields(BARF), a technique that generates new 3D views from images of a scene without requiring further information.\n\nKey insight:The earlier method calledNeRFrequires camera positions and angles to find values that feed a neural network. Those variables can be represented by a learnable vector, and backpropagation can update it as well as the network’s weights.\n\nHow it works:Like NeRF, BARF generates views of a scene by sampling points along rays that extend from the camera through each pixel. It uses a vanilla neural network to compute the color and transparency of each point based on the point’s position and the ray’s direction. To determine the color of a given pixel, it combines the color and transparency of all points along  the associated ray. Unlike NeRF, BARF’s loss function is designed to learn camera positions and angles, and it uses a training schedule to learn camera viewpoints before pixel colors.\n\nResults:The researchers compared BARF to NeRF, measuring their ability to generate a novel view based on several views of an everydayscene, where the viewpoints were unknown to BARF and known to NeRF. BARF achieved 21.96 competitive peak signal-to-noise ratio, a measure of the difference between the generated and actual images (higher is better). NeRF achieved 23.25 competitive peak signal-to-noise ratio.\n\nWhy it matters:Data collected in the wild rarely are perfect, and bad sensors are one of many reasons why. BARF is part of a new generation of models that don’t assume accurate sensor input, spurring hopes of systems that generalize to real-world conditions.\n\nWe’re thinking:In language processing,ELMokicked off a fad for naming algorithms after Sesame Street characters. Here’s hoping this work doesn’t inspire its own run of names.\n\nJoin us on June 17, 2021, to discuss the difference between data scientists and machine learning engineers and how to transition from one role to the other. This Expert Panel is presented by DeepLearning.ai and FourthBrain.Sign up here\n\nRobots are brushing their way into the beauty market.\n\nWhat’s new:A trio of companies is developing automated nail-painting devices that integrate robotics and computer vision,The New York Timesreported.\n\nHow it works:Users select a color and place a hand or finger into a slot in a toaster-sized machine. The system scans the fingertips, and an automated paint dispenser — in some cases, a mechanical arm tipped by a brush — coats each nail. These machines update earlier nail-decorating gadgets that, say, applied decals without using AI.\n\nBehind the news: The beauty industry has embraced a variety of AI techniques.\n\nWhy it matters:Americans spent$8.3 billionon nail care last year. Automated systems could appeal to people who are looking for a fast makeover as well as those who want to continue social distancing without foregoing manicures. But such systems also could also displace workers who already contend withlow wages.\n\nWe’re thinking:Paint your nails or don’t, but everyone who writes code should take good care of their hands.\n\nFew companies that use AI understand the ethical issues it raises.\n\nWhat’s new:While many companies are ramping up investments in AI, few look for and correct social biases in their models, according to areportby the credit-scoring company Fico. The report surveyed 100 C-level executives in data, analytics, and AI departments at companies that bring in revenue of $100 million or more annually.\n\nWhat they found:Nearly half of respondents said their company’s investment in AI had grown in the last 12 months. But there was no corresponding rise in efforts to make sure AI was ethical, responsible, and free of bias.\n\nBehind the news:This is Fico’s second annual report, and it shows some improvement over the previous survey:Last year, 67 percent of respondents said they did not monitor systems after deployment.\n\nWhy it matters: Never mind technical issues — taking the survey’s results at face value, a substantial percentage of large companies aren’t ready for AI transformation on an ethical level. Businesses that pursue AI without paying attention to ethical pitfalls run the risk of alienating customers and violating laws.\n\nWe’re thinking:Companies that pay attention to ethics — in AI and elsewhere — will reap rewards in the form of better products, happier customers, and greater fairness and justice in the world.\n\nCheck outPractical Data Science, our new program in partnership with Amazon Web Services (AWS)! This specialization will help you develop practical skills to deploy projects and overcome challenges using Amazon SageMaker.Enroll now",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/issue-95-1.png",
      "https://info.deeplearning.ai/hs-fs/hubfs/asylum-updated.gif?width=1200&upscale=true&name=asylum-updated.gif",
      "https://info.deeplearning.ai/hs-fs/hubfs/BARF.gif?width=1200&upscale=true&name=BARF.gif",
      "https://info.deeplearning.ai/hs-fs/hubfs/Experts%20Panel-6-17_The%20Batch%20Image-1.png?width=1200&upscale=true&name=Experts%20Panel-6-17_The%20Batch%20Image-1.png",
      "https://info.deeplearning.ai/hs-fs/hubfs/MANICURE2.gif?width=1200&upscale=true&name=MANICURE2.gif",
      "https://info.deeplearning.ai/hs-fs/hubfs/fico-redo-2.gif?width=1200&upscale=true&name=fico-redo-2.gif",
      "https://info.deeplearning.ai/hs-fs/hubfs/Specialization%20Name%20(1).png?width=1200&upscale=true&name=Specialization%20Name%20(1).png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-219/",
    "title": "issue 219",
    "date": "",
    "reading_time": "",
    "content": "Iwroteearlier about how my team at AI Fund saw that GPT-3 set a new direction for building language applications, two years before ChatGPT was released. I’ll go out on a limb to make another prediction: I think we’ll see significant growth in AI, including Generative AI, applications running at the edge of the network (PC, laptop, mobile, and so on).\n\nI realize this flies in the face of conventional wisdom. Most AI runs in data centers, not on edge devices. There are good reasons for this:\n\nHere’s why I think those factors won’t stop AI’s growth at the edge.\n\nFurther, strong commercial interests are propelling AI to the edge. Chip makers like Nvidia, AMD, and Intel sell chips to data centers (where sales have grown rapidly) and for use in PCs and laptops (where sales have plummeted since the pandemic). Thus, semiconductor manufacturers as well as PC/laptop makers (and Microsoft, whose sales of the Windows operating system depend on sales of new PC/laptops) are highly motivated to encourage adoption of edge AI, since this would likely require consumers to upgrade their devices to have the more modern AI accelerators. So many companies stand to benefit from the rise of edge AI and will have an incentive to promote it.\n\nAI Fund has been exploring a variety of edge AI applications, and I think the opportunities will be rich and varied. Interesting semiconductor technology will support them. For example, AMD’sxDNAarchitecture, drawing onconfigurable coresdesigned by Xilinx (now an AMD company), is making it easier to run multiple AI models simultaneously. This enables a future in which one AI model adjusts image quality on our video call, another checks our grammar in real time, and a third pulls up relevant articles.\n\nWhile it’s still early days for edge AI — in both consumer and industrial markets (for example, running in factories or on heavy machinery) — I think it’s worth investigating, in addition to the numerous opportunities in cloud-hosted AI applications.Keep learning!\n\nAndrew\n\nP.S. My team at Landing AI will present a livestream, “Building Computer Vision Applications,” on Monday, November 6, 2023, at 10 a.m. Pacific Time. We’ll discuss the practical aspects of building vision applications including how to identify and scope vision projects, choose a project type and model, apply data-centric AI, and develop an MLOps pipeline. Registerhere!\n\nGoogle’s new mobile phones put advanced computer vision and audio research into consumers’ hands.What’s new:The Alphabet divisionintroducedits flagship Pixel 8 and Pixel 8 Pro smartphones at its annual hardware-launch event. Both units feature AI-powered tools for editing photos and videos.\n\nHow it works:Google’s new phones process images in distinctive ways driven by algorithms on the device itself. They raise the bar for Apple, the smartphone leader, to turn itsinternal projectsinto market opportunities.\n\nBehind the news:Google researchers actively pursued AI systems that alter or enhance images, video, and audio.\n\nWhy it matters:Smartphones produce most of the world’s photos and videos. Yet generative tools for editing them have been confined to the desktop, social-network photo filters notwithstanding. Google’s new phones bring the world closer to parity between the capabilities of desktop image editors and hand-held devices. And the audio-editing capabilities raise the bar all around.\n\nWe’re thinking:Earlier this year, Googleagreedto uphold voluntary commitments on AI, including developing robust mechanisms, such as watermarks, that would identify generated media. Will Google apply such a mark to images edited by Pixel users?\n\nA neural network helped brain surgeons decide how much healthy tissue to cut out when removing tumors — while the patients were on the operating table.\n\nWhat’s new:Researchers from Amsterdam University Medical Centers and Princess Máxima Center for Pediatric Oncology in the Netherlandsbuilta system to assess how aggressively surgeons should treat tumors. It worked accurately and quickly enough to enable doctors to adjust their approach in the operating room.Key insight:Brain surgeons don’t know the type of tumor they will remove until an operation is underway. When they have a sample — about the size of a kernel of corn — they can classify it by looking at it under a microscope. Alternatively, they can send it out for DNA sequencing, which can take weeks, requiring a second surgery. However, faster, less precise DNA sequencing can be performed on-site, and a neural network can classify such preliminary DNA sequences quickly and accurately. This way, a doctor can proceed with the operation with confidence in the tumor’s classification.\n\nHow it works:The authors trained a system of four vanilla neural networks to classify brain tumors.\n\nResults:The authors’ system performed well on tumor DNA samples in an existing collection as well as those gathered in an operating room. Tested on samples from 415 tumors, it classified 60.7 percent of them accurately, misclassified 1.9 percent, and was unable to classify 37.3 percent. Tested on samples collected during 25 real surgeries, it correctly classified 18 tumors and was unable to classify 7. In all cases, it returned results within 90 minutes (45 minutes to collect the DNA and 45 minutes to analyze it).\n\nWhy it matters:90 minutes is fast enough to inform brain surgeons what kind of tumor they’re dealing with in the early phase of an operation. If this technique can be rolled out widely, it may help save many lives.We’re thinking:Inferencing presumably takes seconds. The authors say the quick sequencing method processes DNA in 20 to 40 minutes. Speeding up that step offers great potential to accelerate the process.\n\n“Generative AI for Everyone,” taught by Andrew Ng, is coming soon! This course demystifies generative AI and assumes no prior experience in coding or machine learning. Learn how generative AI works, how to use it, and how it will affect jobs, businesses, and society.Join the waitlist\n\nMicrosoft is looking to control the expense of its reliance on OpenAI’s models.\n\nWhat’s new:Microsoft seeks to build leaner language models that perform nearly as well as ChatGPT but cost less to run, The Informationreported.\n\nHow it works:Microsoft offers a line ofAI-powered toolsthat complement the company’s flagship products including Windows, Microsoft 365, and GitHub. Known as Copilot, the line is based on OpenAI models. Serving those models to1 billion-plus userscould amount to an enormous expense, and it occupies processing power that would be useful elsewhere. To manage the cost, Microsoft’s developers are using knowledge distillation, in which a smaller model is trained to mimic the output of a larger one, as well as other techniques.\n\nBehind the news:Microsoft hasinvested$10 billion in OpenAI. The dealpromisesthe tech giant 75 percent of OpenAI’s operating profit until its investment is repaid, then 49 percent of further profits until reaching an unspecified cap. Meanwhile, Microsoft does have access to high-performing models from other sources. Its Azure cloud platformservesMeta’s LLaMA 2.\n\nWhy it matters:Serving large neural networks at scale is a challenge even for Microsoft, which has immense hardware resources and a favorable agreement with OpenAI. Running distilled and fine-tuned models can cut the cost for both tech giants and tiny startups.\n\nWe’re thinking:If users like Copilot so much they're running up a large bill in model inferences, that sounds like a positive sign!\n\nYou can get a large language model to solve math problems more accurately if your prompts include achain of thought: an example that solves a similar problem through a series of intermediate reasoning steps. A new approach to this sort of prompting improved ChatGPT’s accuracy on a variety of reasoning problems.\n\nWhat's new:Jiashuo Sun and colleagues at Xiamen University, Microsoft, and IDEA Research, introducediterative bootstrapping in chain-of-thought-prompting, a method that prompts a large language model to generate correct chains of thought for difficult problems, so it can use them as guides to solving other problems.\n\nKey insight:Researchers have developed a few ways to prompt a large language model to apply a chain of thought (CoT). The typical method is for a human to write an example CoT for inclusion in a prompt. A faster way is to skip the hand-crafted example and simply instruct the model to “think step by step,” prompting it to generate not only a solution but its own CoT (this is calledzero-shot CoT). To improve zero-shot CoT, other work both (i) asked a model to “think step by step” and (ii) provided generated CoTs (auto-CoT). The weakness of this approach is that the model can generate fallacious CoTs and rely on them when responding to the prompt at hand, which can lead to incorrect responses. To solve this problem, we can draw example prompts from a dataset that includes correct responses, and the model can check its responses against the dataset labels. If it’s wrong, it can try repeatedly until it answers correctly. In this way, it generates correct CoT examples to use in solving other problems.\n\nHow it works:To prompt ChatGPT to reason effectively, the authors built a database of example problems, chains of thought, and solutions. They drew problems from 11 datasets: six arithmetic reasoning datasets (such asgrade-school math word problems), four common-sense reasoning datasets (for example,questions like “Did Aristotle use a laptop?”), and asymbolic reasoning datasetconsisting of tasks that involved manipulating letters in words (for instance, “Take the last letters of the words in ‘Steve Sweeney’ and concatenate them”).\n\nResults:The authors evaluated their method versus hand-crafting and auto-CoT. Of the 11 datasets, their method achieved the best results on 8. For example, on grade-school math word problems, ChatGPT prompted using their method achieved 73.6 percent accuracy; using hand-crafted prompts, it achieved 69.3 percent accuracy, and using auto-CoT, it achieved 71.4 percent accuracy. Their method underperformed hand-crafted prompts on two common-sense reasoning datasets (76.8 percent versus 77.1 percent and 69.3 percent versus 71.1 percent). It underperformed auto-CoT on one arithmetic dataset (91.9 percent versus 92.5 percent.)\n\nWhy it matters:Large language models have powerful latent capabilities that can be activated by clever prompting. ChatGPT was able to solve the problems in the authors’ database, but only after multiple tries. Prompting it with examples of its own correct solutions to these problems apparently enabled it to solve other, similarly difficult problems without needing multiple tries.\n\nWe're thinking:It may be possible to modify this method to make human input unnecessary by asking the model tofix the problems in its previous generationsoruse external tools to validate its outputs.\n\nBaidu announces Ernie 4.0The Chinese tech giant demonstrated a new version of its generative AI model at an event on Tuesday. Baidu claims that the new version of Ernie is on par with Open AI’s GPT-4 model. Ernie will also be incorporated into many of Baidu’s products, including Drive and Maps. It is not yet available to the general public. (Reuters)\n\nAdobe releases Firefly 2.0The creative software giant’s new image generation model features new text to image and text to vector graphic tools. It automatically generates content credentials for AI-generated material and Adobe promises to defend users against copyright infringement claims. But controversies remain about how Adobe secured its users’ permission to train its models on their images. (Adobe)\n\nSoutheast Asia takes business-friendly stance on AI regulationA confidential draft of the Association of Southeast Asian Nations' (ASEAN) \"guide to AI ethics and governance\" reveals its emphasis on guiding domestic regulations rather than imposing stringent requirements. This aligns closely with the U.S. NIST AI Risk Management Framework and sets it apart from the European Union's AI Act. (Reuters)\n\nGoogle commits to protect Generative AI users from copyright claimsThe new policy extends to software that generates text and images in Google Workspace and Cloud applications, including Google Cloud’s Vertex AI development platform and Duet AI system. It does not cover instances where users intentionally infringe on the rights of others. This move aligns Google with companies like Microsoft and Adobe that have made similar pledges. (Reuters)\n\nResearch:Machine translation dataset bridges gap for ancient Etruscan languageEtruscan, an ancient European language with no native speakers today, only has around 12,000 known inscriptions, most of them still untranslated. A dataset for machine translation from Etruscan to English has been introduced, featuring 2,891 translated examples from academic sources. This release opens doors for future research on Etruscan and other languages with limited data. (Hugging Face)\n\nIndigenous communities are using AI to revitalize their languagesResearchers are developing AI models to aid native language learning and cultural preservation. While AI offers promise, there are concerns about corporate interests profiting from indigenous languages. Many indigenous-run organizations are pursuing new kinds of partnerships with developers, focusing on ethical and community-focused development. (New Scientist)\n\nSoftware development startup Replit launches its own AI pair programmerReplit AI contains a comprehensive suite of tools, including Complete Code, Generate Code, Edit Code, and Explain Code. The flagship feature, Complete Code, delivers autocomplete-style suggestions to enhance the coding experience. (Replit)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-webp-to-jpg--20--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/10/PIXEL_600px--1--1.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-webp-to-jpg--21-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/10/The-Batch-ads-and-exclusive-banners--70-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/10/POSTGPT_withDollarSign_1200px.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-webp-to-jpg--22-.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-3/",
    "title": "issue 3",
    "date": "",
    "reading_time": "",
    "content": "I traveled to Taiwan last week, where I met many CEOs interested in AI transformation of traditional companies. I also visitedTaiwan AI Labswhich, similar to OpenAI, started as a nonprofit AI research institute.\n\nFunded by government and private financing, Taiwan AI Labs works on smart city, healthcare, and other projects; for example, using computer vision to estimate traffic flow. Ethan Tu, the lab’s leader, tells me it focuses on practical and socially important projects, including ones that are hard to fund commercially, and openly publishes all its work. I also several professors on sabbatical there. They told me that the lab gives them more engineering resources for AI than they can generally find in a university.\n\nI’m glad to see different nations experiment with new ways to organize AI research and development. I hope more countries will fund nonprofit AI research labs.\n\nShout out also to National Taiwan University, Taiwan Ministry of Science and Technology, and Taiwania Capital for helping organize my trip!\n\nKeep learning,\n\nAndrew\n\nCreating a virtual representation of a scene using traditional polygons and texture maps involves several complex operations, and even neural-network approaches have required manual preprocessing. Researchers from the Samsung AI Center and Skolkovo Institute of Science and Technology propose a new deep-learning pipeline that visualizes scenes with far less fuss.\n\nWhat’s new:Aliev et al.’sNeural Point Based Graphicstechnique rapidly produces realistic images in an end-to-end process. It does particularly well with thin objects that are hard to model using a polygonal mesh, such as shoe laces and bicycle tires. You can see it in actionhere.Key insight:There’s no need to model surfaces to represent a scene. Point clouds and corresponding images together contain enough information for a neural network to generate realistic images. Moreover, neural networks can fill in missing information such as parts of objects hidden from view, which simplifies scene modeling.\n\nHow it works:The system starts with a point cloud representing a scene, an image of the scene, camera parameters including viewing angle, and a randomly initialized vector representation of each point that encodes shape and surface properties.\n\nResults:The researchers compared photographic input and generated images from a variety of data sets, including consumer cameras, across several scene-capture techniques, including traditional and deep learning methods. Their system scored highest on a number of measures of image similarity. While its rendering of synthetic scenes isn’t as realistic as that achieved by state-of-the-art ray tracing methods, it produces good-looking images roughly 2,000 times faster.\n\nWhy it matters:Neural Point-Based Graphics is a distinct step forward for end-to-end scene capture. By demonstrating that point clouds and images — which can come from a smartphone — together can represent scenes in realistic detail, this research opens the door for refinements that could ultimately compete with the best current methods in a much simpler pipeline.We’re thinking:Just as neural networks have replaced rule-based systems in computer vision and language applications, they’re on track to have a similar impact in graphics. Given its simplicity and speed, this approach could facilitate real-time applications such as video games, virtual reality, and augmented reality.\n\nA growing number of companies that sell standardized tests are using natural language processing to assess writing skills. Critics contend that these language models don’t make the grade.What happened:An investigation byMotherboardfound that several programs designed to grade English-language essays show bias against minorities and some students who speak English as a second language. Some models gave high scores to computer-generated essays that contained big words but little meaning.What they found:Models trained on human-graded papers learn to correlate patterns such as vocabulary, spelling, sentence length, and subject-verb agreement with higher or lower scores. Some experts say the models amplify the biases of human graders.\n\nBehind the news:At least 21 U.S. states use NLP to grade essays on standardized tests for public schools. Of those, 18 also employ human graders to check a small percentage of papers randomly.Why it matters:Standardized tests help determine access to education and jobs for millions of Americans every year. Inappropriate use of NLP could be robbing them of life-changing opportunities.\n\nWe’re thinking:The company behind E-Rater is the only one that publishes studies on its grading model’s shortcomings and what it’s doing to fix them. Colleges and school boards should lead the charge in demanding that other test providers do the same.\n\nOpenAI raised eyebrows in February when it announced — and withheld — the full version of its groundbreaking language model,GPT-2. Six months later, the company has re-examined the decision.\n\nWhat happened:The for-profit research organization issued a follow-upreportthat details the results of GPT-2’s “staged release.” Fearing that the full version would be used to generate convincing misinformation, OpenAI initially released a limited version (124 million parameters). That release was followed by larger versions culminating, so far, in a 774 million-parameter model made available along with the report.\n\nWhat the report says:Releasing the model in stages while allowing certain partners full access helped advance an understanding of both benign and malignant uses, the organization says. OpenAI remains convinced that staged release is “likely to be a key foundation of responsible publication of AI.”\n\nBehind the news:OpenAI’s decision to withhold the complete GPT-2 rankled many in the AI community; without the code and detailed training information, it’s impossible to replicate the results. However, the organization’s reticence didn’t stop a pair of Brown University graduate students, neither of whom had a background in language modeling, fromreplicatingGPT-2 in August.\n\nWe’re thinking:The AI community thrives on shared information. Yet the potential for powerful AI models to wreak havoc on the general welfare suggests some sort of gatekeeping mechanism is in order. Staged release may be just the device that white hats need to stay one step ahead of malefactors.\n\nLearn tips for leading machine learning projects that you’d only get from years of industry experience. Take theDeep Learning Specialization\n\nMicroscopes outfitted with AI-driven augmented reality could improve the accuracy of cancer diagnoses.\n\nWhat’s happened:Google Health developed anattachmentfor analog microscopes that outlines signs of breast and prostate cancer in real time.How it works:A computer-vision system spots cancer in a cell slide, while augmented-reality tech superimposes the AI’s prediction over the slide at around 27 frames per second.\n\nBehind the news:Pathologists use microscopes to measure tumor size relative to nearby lymph nodes and to count the number of cells nearing or undergoing mitosis. That information tells them how aggressively a patient’s cancer is spreading.Why it matters:Interpreting cell slides is subjective, and one pathologist’s understanding can differ greatly from another’s. Patients in locations where trained pathologists are scarce tend to suffer most from this inconsistency. AI-enhanced tools could help make diagnoses more reliable.\n\nWe’re thinking:AI is a natural complement to digital microscopes, but analog microscopes are far more common. This technology promises to upgrade those tools at a fraction of the cost of replacing them.\n\nDeep learning models can be unwieldy and often impractical to run on smaller devices without major modification. Researchers at Facebook AI Research found a way to compress neural networks with minimal sacrifice in accuracy.\n\nWhat’s new:Building on earlier work, the researchers coaxed networks to learn smaller layer representations. Rather than storing weights directly, thetechniqueuses approximate values that can stand in for groups of weights.\n\nKey insight:The researchers modified an existing data-compression method, product quantization, to learn viable weight approximations.\n\nHow it works:By representing groups of similar weights with a single value, the network can store only that value and pointers to it. This reduces the amount of storage needed for weights in a given layer. The network learns an optimal set of values for groups of weights, or subvectors, in a layer by minimizing the difference between layer outputs of the original and compressed networks.\n\nResults:The researchers achieve best top-1 accuracy on ImageNet for model sizes of 5MB and 10MB. (They achieve competitive accuracy for 1MB models.) They also show that their quantization method is superior to previous methods for ResNet-18.\n\nWhy it matters:Typically, researchers establish the best model for a given task, and follow-up studies find new architectures that deliver similar performance using less memory. This work offers a way to compress an existing architecture, potentially taking any model from groundbreaking results in the lab to widespread distribution in the field with minimal degradation in performance.\n\nYes, but:The authors demonstrate their method on architectures with fully connected layers and CNNs only. Further research will be required to find its limits, and also to optimize the compressed results for compute speed.We’re thinking:The ability to compress top-performing models could put state-of-the-art AI in the palm of your hand and eventually in your pacemaker.\n\nTheoretical advances can be thrilling, but the excitement can drown out all the ways AI is actually being put to use. DeepIndex provides an up-to-date, well organized, cheeky guide to practical applications culled from news reports.\n\nWhat it is:DeepIndex.org lists over 630 examples, organized into 19 categories and ranked according to how well they work.\n\nOur favorites:DeepIndex is a treasure trove of bold efforts and unlikely concepts. Yiu’s personal favorite is a model that “fixes Warner Bros.’ terrible attempts to digitallyremove Henry Cavill’s mustachein [the Hollywood blockbuster] Justice League.” That’s a fun use case, no doubt, but we found others more compelling:",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_09041920Letter20Picture.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_pointcloud.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Manatee20Question.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_GPT20Resized-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_DLS20Course20320Course20Ad.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Screen20Shot202019-09-0320at203.25.3120PM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_quantized20networks.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Deep20Index20Resized-2.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-290/",
    "title": "issue 290",
    "date": "",
    "reading_time": "",
    "content": "The Voice Stack is improving rapidly. Systems that interact with users via speaking and listening will drive many new applications. Over the past year, I’ve been working closely with DeepLearning.AI, AI Fund, and several collaborators on voice-based applications, and I will share best practices I’ve learned in this and future letters.\n\nFoundation models that are trained to directly input, and often also directly generate, audio have contributed to this growth, but they are only part of the story. OpenAI’sRealTime APImakes it easy for developers to write prompts to develop systems that deliver voice-in, voice-out experiences. This is great for building quick-and-dirty prototypes, and it also works well for low-stakes conversations where making an occasional mistake is okay. I encourage you to try it!\n\nHowever, compared to text-based generation, it is still hard to control the output of voice-in voice-out models. In contrast to directly generating audio, when we use an LLM to generate text, we have many tools for building guardrails, and we can double-check the output before showing it to users. We can also use sophisticated agentic reasoning workflows to compute high-quality outputs. Before a customer-service agent shows a user the message, “Sure, I’m happy to issue a refund,” we can make sure that (i) issuing the refund is consistent with our business policy and (ii) we will call the API to issue the refund (and not just promise a refund without issuing it).\n\nIn contrast, the tools to prevent a voice-in, voice-out model from making such mistakes are much less mature.\n\nIn my experience, the reasoning capability of voice models also seems inferior to text-based models, and they give less sophisticated answers. (Perhaps this is because voice responses have to be more brief, leaving less room for chain-of-thought reasoning to get to a more thoughtful answer.)\n\nWhen building applications where I need a high degree of control over the output, I use agentic workflows to reason at length about the user’s input. In voice applications, this means I end up using a pipeline that includes speech-to-text (STT, also known as ASR, or automatic speech recognition) to transcribe the user’s words, then processes the text using one or more LLM calls, and finally returns an audio response to the user via TTS (text-to-speech). This STT → LLM/Agentic workflow → TTS pipeline, where the reasoning is done in text, allows for more accurate responses.\n\nHowever, this process introduces latency, and users of voice applications are very sensitive to latency. When DeepLearning.AI worked with RealAvatar (an AI Fund portfolio company led by Jeff Daniel) to build an avatar of me, we found that getting TTS to generate a voice that sounded like me was not very hard, but getting it to respond to questions using words similar to those I would choose was. Even after a year of tuning our system — starting with iterating on multiple, long, mega-prompts and eventually developing complex agentic workflows — it remains a work in progress. You can play with ithere.\n\nInitially, this agentic workflow incurred 5-9 seconds of latency, and having users wait that long for responses led to a bad experience. To address this, we came up with the following latency reduction technique. The system quickly generates a pre-response (short for preliminary response) that can be uttered quickly, which buys time for an agentic workflow to generate a more thoughtful, full response. (We’re grateful to LiveKit’s CEO Russ d’Sa and team for helping us get this working.) This is similar to how, if you were to ask me a complicated question, I might say “Hmm, let me think about that” or “Sure, I can help with that” — that’s the pre-response — while thinking about what my full response might be.\n\nI think generating a pre-response followed by a full response, to quickly acknowledge the user’s query and also reduce the perceived latency, will be an important technique, and I hope many teams will find this useful. Our goal was to approach human face-to-face conversational latency, which is around 0.3-1 seconds. RealAvatar and DeepLearning.AI, through our efforts on the pre-response and other optimizations, have reduced the system’s latency to around 0.5-1 seconds.\n\nMonths ago, sitting in a coffee shop, I was able to buy a phone number on Twilio and hook it up to an STT → LLM → TTS pipeline in just hours. This enabled me to talk to my own LLM using custom prompts. Prototyping voice applications is much easier than most people realize!\n\nBuilding reliable, scaled production applications takes longer, of course, but if you have a voice application in mind, I hope you’ll start building prototypes and see how far you can get! I’ll keep building voice applications and sharing best practices and voice-related technology trends in future letters.\n\nKeep building!\n\nAndrew\n\nAI coding agents do more than autocomplete. They help you debug, refactor, and design applications. Learn how coding agents work under the hood, so you can streamline your projects and build applications such as a Wikipedia data-analysis app!Enroll Now.\n\nTo date, efforts to decode what people are thinking from their brain waves often relied on electrodes implanted in the cortex. New work used devices outside the head to pick up brain signals that enabled an AI system, as a subject typed, to accurately guess what they were typing.\n\nWhat’s new:Researchers presentedBrain2Qwerty, a non-invasive method to translate brain waves into text. In addition, their workshed lighton how the brain processes language. The team included people at Meta, Paris Sciences et Lettres University, Hospital Foundation Adolphe de Rothschild, Basque Center on Cognition, Brain and Language, Basque Foundation for Science, Aix-Marseille University, and Paris Cité University.\n\nGathering brainwave data:The authors recorded the brain activity of 35 healthy participants who typed Spanish-language sentences. The participants were connected to either an electroencephalogram (EEG), which records the brain’s electrical activity via electrodes on the scalp, or a magnetoencephalogram (MEG), which records magnetic activity through a device that surrounds the head but isn’t attached. 15 participants used each device and five used both.\n\nThoughts into text:Brain2Qwerty used a system made up of a convolutional neural network, transformer, and a9-gram character-level language modelpretrained on Spanish Wikipedia. The system classified the text a user typed from their brain activity. The authors trained separate systems on MEG and EEG data.\n\nResults.The authors’ MEG model achieved 32 percent character error rate (CER), much higher accuracy than the EEG competitors. Their EEG system outperformedEEGNet, a model designed to process EEG data that had been trained on the authors’ EEG data. It achieved 67 percent CER, while EEGNet achieved 78 percent CER.\n\nBehind the news:For decades, researchers have used learning algorithms to interpret various aspects of brain activity with varying degrees of success. In recent years, they’ve used neural networks togeneratetextandspeechfrom implanted electrodes, generateimagesof whatpeople seewhile in an fMRI, and enable people tocontrol robotsusing EEG signals.\n\nWhy it matters:In research into interpreting brain signals, subjects who are outfitted with surgical implants typically have supplied the highest-quality brain signals. fMRI scans, while similarly noninvasive, are less precise temporally, which makes them less useful for monitoring or predicting language production. Effective systems based on MEG, which can tap brain signals precisely without requiring participants to undergo surgery, open the door to collecting far more data, training far more robust models, and conducting a wider variety of experiments.\n\nWe’re thinking:The privacy implications of such research may be troubling, but keep in mind that Brain2Qwerty’s MEG system, which was the most effective approach tested, required patients to spend extended periods of time sitting still in a shielded room. We aren’t going to read minds in the wild anytime soon.\n\nTop AI companies announced plans to dramatically ramp up their spending on AI infrastructure.\n\nWhat’s new:Alphabet, Amazon, Meta, Microsoft, and others willboosttheir capital spending dramatically in 2025, pouring hundreds of billions of dollars into data centers where they process AI training, the companies said in their most recent quarterly reports. The surge suggests that more-efficient approaches to training models won’t dampen the need for greater and greater processing power.\n\nHow it works:Capital expenditures include long-term purchases like land, buildings, and computing hardware rather than recurring costs like salaries or electricity. The AI leaders signaled that most of this spending will support their AI efforts.\n\nBehind the news:DeepSeek initiallysurprisedmany members of the AI community by claiming to have trained a high-performance large language model at a fraction of the usual cost.\n\nWhy it matters:DeepSeek-R1’s purported training cost fueled fears that demand for AI infrastructure would cool, but the top AI companies’ plans show that it’s not happening yet. A possible explanation lies in theJevons Paradox, a 19th-century economic theory named after the English economist William Stanley Jevons. As a valuable product becomes more affordable, demand doesn’t fall, it rises. According to this theory, even if training costs tumble, the world will demand ever greater processing power for inference.\n\nWe’re thinking:DeepSeek’s low-cost technology momentarily rattled investors who had expected the next big gains would come from the U.S. rather than China. But DeepSeek’s efficiency follows a broader pattern we’ve seen for years: The AI community steadily wrings better performance from less processing power.\n\nA viral deepfake video showed media superstars who appeared to support a cause — but it was made without their participation or permission.\n\nWhat’s new:Thevideoshows AI-generated likenesses of 20 Jewish celebrities ranging from Scarlett Johansson to Simon & Garfunkel. They appear wearing T-shirts that feature a middle finger inscribed with the Star of David above the word “KANYE.” The clip, which ends with the words “Enough is enough” followed by “Join the fight against antisemitism,” responds to rapper Kanye West, who sold T-shirts emblazoned with swastikas on Shopify before the ecommerce platform shut down his store.\n\nWho created it:Israeli developers Guy Bar and Ori Bejerano generated the video to spark a conversation about antisemitism, BartoldThe Jerusalem Post. The team didn’t reveal the AI models, editing tools, or techniques used to produce the video.\n\nJohansson reacts:Scarlett Johanssondenouncedthe clip and urged the U.S. to regulate deepfakes. In 2024, sheobjectedto one of the voices of OpenAI’s voice assistant, which she claimed resembled her own voice, leading the company to remove that voice from its service. The prior year, her attorneys ordered a company to stop using an unauthorized AI-generated version of her image in an advertisement.\n\nLikenesses up for grabs:Existing U.S. laws protect some uses of a celebrity’s likeness in the form of a photo, drawing, or human lookalike, but they don’t explicitly protect against reproduction by AI systems. This leaves celebrities and public figures with limited recourse against unauthorized deepfakes.\n\nWhy it matters:Non-consensual deepfake pornography is widely condemned, but AI enables many other non-consensual uses of someone’s likeness, and their limits are not yet consistently coded into law. If the creators of the video that appropriated the images of celebrities had responded to Johansson’s criticism with an AI-generated satire, would that be a legitimate exercise of free speech or another misuse of AI? Previously, an ambiguous legal framework may have been acceptable because such images, and thus lawsuits arising from them, were uncommon. Now, as synthetic likenesses of specific people become easier to generate, clear legal boundaries are needed to keep misuses in check.\n\nWe’re thinking:Creating unauthorized lookalikes of existing people is not a good way to advance any cause, however worthy. Developers should work with businesses policymakers to establish standards that differentiate legitimate uses from unfair or misleading exploitation.\n\nAlthough large language models can improve their performance by generating a chain of thought (CoT) — intermediate text tokens that break down the process of responding to a prompt into a series of steps — much of the CoT text is aimed at maintaining fluency (such as “a”, “of”, “we know that”) rather than reasoning (“a² + b² = c²”). Researchers addressed this inefficiency.\n\nWhat’s new:Shibo Hao, Sainbayar Sukhbaatar, and colleagues at Meta and University of California San Diego introducedCoconut(Chain of Continuous Thought), a method that trains large language models (LLMs) to process chains of thought as vectors rather than words.\n\nKey insight:A large language model (LLM) can be broken into an embedding layer, transformer, and classification layer. To generate the next text token from input text, the embedding layer embeds the text; given the text, the transformer outputs a hidden vector; and the classification layer maps the vector to text-token probabilities. Based on these probabilities, a decoding algorithm selects the next token to generate, which feeds back into the input text sequence to generate the next vector, and so on. When a model generates a CoT, committing to a specific word at each step limits the information available to the meanings of the words generated so far, while a vector could represent multiple possible words. Using vectors instead of text enables the CoT to encode richer information.\n\nHow it works:The authors built three LLMs by fine-tuning a pre-trainedGPT-2on three datasets of prompts, CoTs, and final outputs:GSM8k(grade-school math word problems);ProntoQA(questions and answers about fictional concepts expressed in made-up words, including synthetic CoTs in natural language); and (3) ProsQA, a more challenging question-answering dataset introduced by the authors, inspired by ProntoQA but with longer reasoning steps.\n\nResults:The authors compared their method to a pretrained GPT-2 that was fine-tuned on the same datasets to predict the next word, including reasoning.\n\nYes, but:On GSM8k, Coconut achieved 34.1 percent accuracy, while the baseline LLM achieved 42.9 percent. However, it generated significantly fewer vectors and tokens than the CoT generated tokens. Coconut generated 8.2 vectors on average compared to the baseline LLM’s 25 text tokens.\n\nWhy it matters:A traditional CoT commits to a single word at each step and thus encodes one reasoning path in a single CoT. Vectors are less interpretable to humans than language, but the model’s output layer can still decode the thought vectors into probabilities over tokens. Further, inspecting the distribution of words stored along all continuous CoT vectors offers a way to understand multiple potential thought paths stored in one continuous CoT.\n\nWe’re thinking:LLMs typically learn to reason over text, mainly because text data is widely available to train on. In contrast, neuroscience shows that the part of the human brain responsible for language largelygoes quietduring reasoning tasks, which suggests that explicit language is not a key mechanism for reasoning. Coconut takes an intriguing step to enable LLMs to explore representations that don’t encode the limitations of language.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--52-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/02/The-Batch-ads-and-exclusive-banners---2025-02-25T173657.996--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--50-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--53-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--55-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--51-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-294/",
    "title": "issue 294",
    "date": "",
    "reading_time": "",
    "content": "Fine-tuning small language models has been gaining traction over the past half year. I’d like to share my sense of when to use this technique, and also when not to, based on what I’m seeing in multiple companies.\n\nFirst, while fine-tuning is an important and valuable technique, many teams that are currently using it probably could get good results with simpler approaches, such as prompting (including writingmega prompts), few-shot prompting, or simple agentic workflows.\n\nWhy shouldn’t these teams be fine-tuning? Because fine-tuning, which takes a pre-trained model and further trains it on data specific to an application, is relatively complex to implement. You need to collect training data, then (unless you want to implement fine-tuning yourself) find a provider to help with running fine-tuning, then find a way to deploy the fine-tuned model. Because it adds extra complexity both in training and deployment, usually I resort to this technique only after I find that prompting and simple agentic workflows are not up to a task.\n\nHaving said that, there are also applications where fine-tuning is appropriate and valuable. LoRA (which learns by modifying a limited number of parameters rather than the entire model) and related methods have made fine-tuning quite affordable, particularly for small models (say, 13B or fewer parameters). And the amount of data needed to get started is less than most people think. Depending on the application, I’ve seen good results with 100 or even fewer examples. Here are a few applications where I have seen fine-tuning applied successfully:\n\nImproving accuracy of critical applications.Prompting can get you really far for many applications. But sometimes, fine-tuning helps eke out that last bit of accuracy. For example, if you are building a customer service chatbot and need it to call the right API reliably (say, to carry out transactions, issue refunds, and the like), perhaps prompting can get it to make the right API call 95% of the time. But if you struggle to raise the accuracy even with revisions to the prompt and you really need 99% accuracy, fine-tuning on a dataset of conversations and API calls might be a good way to get you there. This is particularly true for tasks where it's hard to specify, using only language, an unambiguous rule to decide what to do. For example, when a customer is frustrated, should the chatbot escalate to a manager or just issue a refund? Teams often write Standard Operating Procedures (SOPs) for human workers to follow, and these SOPs can go into the prompts of models. But if it is hard to specify an unambiguous SOP, so even humans need to see numerous examples before they can learn what to do, fine-tuning can be a good approach. For many text-classification applications fine-tuning also works well, for example, classifying medical records into diagnosis and procedure codes for health insurance claims.\n\nLearning a particular  style of communication.As I explain in “Generative AI for Everyone,” my team fine-tuned a model to sound like me. Many people (including myself) have idiosyncratic uses of language. There are certain words I tend to say and others I tend not to, and these idiosyncrasies are numerous and very difficult to specify in a text prompt. (By the way, the avatar at deeplearning.ai/avatar, built with RealAvatar, uses fine-tuning for this reason.) To get a system to communicate in a certain style, fine-tuning is often a superior solution to prompting alone.\n\nReducing latency or cost during scale-ups.I’ve seen applications where developers have successfully prompted a large model to perform a complex task. But as usage scales up, if the large model is too slow (which often happens) or too expensive (which also happens but less frequently), the team might want to use a smaller model. If, however, the performance of the smaller model isn't good enough, then fine-tuning it can help bring it up to the performance of the larger one for that narrow application. Further, the larger model (or perhaps an agentic workflow) can also be used to generate data to help with fine-tuning the small model for that task.\n\nAt the cutting edge of research, some teams are fine-tuning models to get better at a certain language. But with few exceptions, if the goal is to get an LLM to better understand a body of knowledge that is not in its training data, I find that using RAG (retrieval augmented generation) is a much simpler approach, and I still occasionally run into teams using fine-tuning for which  I think RAG would work better.\n\nOverall my sense is that, of all the teams I see using fine-tuning, perhaps 75% could get good results using simpler techniques (like prompting or agentic workflows), but in 25% of cases I know of no better way to achieve their goal.\n\nIt is still technically challenging to implement fine-tuning, get the hyperparameters right, optimize the compute resources, and so on. We are lucky that more and more companies have worked hard to optimize these and provide efficient fine-tuning services. Many of them allow us to fine-tune open weights models and also download the fine-tuned weights. Some allow us to fine-tune their closed models and continue to keep the tuned weights closed. Both can be useful, but the former has obvious advantages of portability and not having to worry that the provider will stop serving a particular model, causing a critical component in our software to become deprecated.\n\nIn conclusion, before fine-tuning, consider if you should be trying just a bit harder with prompting or agentic workflows, which can lead to simpler solutions that are easier to maintain. The vast majority of applications my teams build do not use any fine-tuning at all, but it’s a critical piece of a small minority of them.\n\nKeep learning!\n\nAndrew\n\nIn “Vibe Coding 101 with Replit,” you’ll learn to plan, prompt, and debug alongside a coding agent. Build, host, and share two real web apps in Replit’s cloud environment while developing effective development skills like writing product requirements, structuring tasks, and refining AI-generated code.Start today\n\nGoogle updated its open-weights family of large language models to include versions that handle image and video inputs.\n\nWhat’s new:Google released itsGemma 3multilingual large language models with parameter counts of 1 billion, 4 billion, 12 billion, and 27 billion. While the smallest processes text only, the other three are vision-language models that are small enough to run on a consumer hardware.\n\nHow it works:Gemma 3rearchitectsand refines earlier Gemma models for higher performance at lower parameter counts.\n\nPerformance:Gemma 3 models outperform Gemma 2 models of equal or larger size by several measures, and all sizes show a strong ability to solve mathematics word problems as measured byMATH.\n\nHot on Gemma 3’s heels:Shortly after Gemma 3 became available, Mistral releasedSmall 3.1(24 billion parameters), a vision-language model with open weights, under a more permissive Apache 2.0 license.\n\nWhy it matters:Gemma 3 takes advantage of a variety of techniques to raise the bar for vision-language performance in relatively small models. Knowledge distillation, multiple rounds of reinforcement learning, and fine-tuning on many languages are a powerful combination.\n\nWe’re thinking:A vision-language model small enough to run on a smartphone feels increasingly close!\n\nDiffusion models usually take many noise-removal steps to produce an image, which takes time at inference. There are ways to reduce the number of steps, but the resulting systems are less effective. Researchers devised a streamlined approach that doesn’t sacrifice output quality.\n\nWhat’s new:Kevin Frans and colleagues at UC Berkeley introducedshortcut modelsthat learn to take larger noise-removal steps and thus require fewer steps to generate an image.\n\nKey insight:At inference, a scheduler likeEulercan enable a model to take larger steps than those it learned during training, but this approach yieldsworse performance. Alternatively distillation, in which a student model learns to remove the same amount of noise as a teacher model when it takes several steps, offers improved performance at the cost of more cumbersome development. Training the model directly to take bigger steps — that are equivalent to multiple smaller steps — enables it to maintain high performance while taking fewer steps.\n\nHow it works:The authors trainedDiT-B, a diffusion transformer, to generate images like those in CelebA-HQ (celebrity faces) and ImageNet-256 (various subjects, size 256x256).\n\nResults:The authors compared their model using 1, 4, or 128 steps to alternatives that were trained via various methods including many variants of distillation. They measured the results usingFréchet inception distance(FID), which assesses how closely generated images resemble real-world images (lower is better).\n\nWhy it matters:Generating images by diffusion is typically costly, and previous approaches to cutting the cost have compromised either performance or incurred additional development expense or both. This method achieves high performance at relatively low cost.\n\nWe’re thinking:As diffusion models continue to become cheaper and faster, we expect to see applications blossom!\n\nStudents benefit from tutoring, but training tutors is expensive. A study shows that large language models can boost tutors’ effectiveness in real time.\n\nWhat’s new:Rose Wang and colleagues at Stanford builtTutor CoPilot, a tool for remote, online tutors that uses GPT-4 to generate hints, explanations, questions, and other helpful responses to students.\n\nKey insight:When a student makes an error, according to previousworkby some of the same authors, effective teachers choose a strategy for addressing the mistake. The authors identified 11 strategies, such as ask a question, explain a concept, provide a hint, or encourage the student. Moreover, they found that an LLM that executed a strategy chosen by an expert teacher performed significantly better than an LLM that was prompted with a strategy chosen at random or no specific strategy. Letting inexperienced tutors choose a strategy while an LLM generates a response helps them learn how to execute the strategy. Students, in turn, benefit from responses that mimic those of an experienced teacher.\n\nHow it works:The authors outfitted a remote tutoring application with GPT-4.\n\nResults:The authors partnered with a virtual tutoring company and a school district in the United States for a two-month study of 874 tutors and 1,787 students between grades 3 and 8. They divided the participants into two groups. In one group, tutors conducted sessions with students as usual. In the other, tutors had access to Tutor CoPilot. The authors measured success by the percentage of students who passed a test at the end of a lesson.\n\nYes, but:The authors found statistically significant improvements as measured by test results per lesson, but not in end-of-year exam results. The study’s two-month duration may account for the lack of evidence for longer-term effects.\n\nWhy it matters:LLMs hold great promise for helping to educate students, but they also show potential in educating teachers. For inexperienced tutors who are learning how to interact with students, an LLM’s general knowledge and pedagogical insights gleaned from expert teachers make a powerful combination.\n\nWe’re thinking:Although it relies on sophisticated technology, the authors’ approach is simple: Prompt an LLM to apply proven teaching principles. Presumably such principles apply beyond elementary math, which would make this approach useful for teaching a variety of disciplines.\n\nDiffusion transformers learn faster when they can look at embeddings generated by a pretrained model like DINOv2.\n\nWhat’s new:Sihyun Yu and colleagues at Korea Advanced Institute of Science and Technology, Korea University, New York University, and Scaled Foundations (a startup that builds AI for robotics) proposedRepresentation Alignment(REPA), a loss term for transformer-based diffusion.\n\nKey insight:Diffusion models learn to remove noise from images to which noise was added (and, at inference, they start with pure noise to generate a fresh image). This process can be divided into two parts: learning to (i) embed the noisy image and (ii) estimate the noise from the embedding. One way to accelerate learning is to add a loss term that encourages the diffusion model to produce embeddings that are similar to those produced by a pretrained embedding model. The diffusion model can learn to estimate the noise faster if it doesn’t need to learn how to embed an image from scratch.\n\nHow it works:The authors modifiedDiT-XL/2andSiT-XL/2transformer-based latent diffusion models, a class of diffusion models that subtract noise from embeddings rather than images. They trained the models to produce images similar to ImageNet. In the process, the modified models learned to produce embeddings similar to those produced by a pretrainedDINOv2.\n\nResults:The modified DiT-XL/2 learned significantly faster than the unmodified version.\n\nWhy it matters:Diffusion models and contrastive self-supervised models like DINOv2 have fundamentally different training objectives: One produces embeddings for the purpose of image generation, while the other’s embeddings are used for tasks like classification and semantic segmentation. Consequently, they learn different aspects of data. This work proposes a novel way to combine these approaches to produce more generally useful embeddings.\n\nWe’re thinking:It turns out that the REPA modification enabled diffusion models to produce embeddings better suited not only to diffusion but also to image classification and segmentation. A similar approach could lead to a more holistic framework for learning image representations.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--56-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/03/V3_DeepLearning_Replit_Banner_2070x1080-01.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--67-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--68-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--69-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-193/",
    "title": "issue 193",
    "date": "",
    "reading_time": "",
    "content": "The competitive landscape of large language models (LLMs) is evolving quickly. The ultimate winners are yet to be determined, and already the current dynamics are exciting. Let me share a few observations, focusing on direct-to-consumer chat interfaces and the LLM infrastructure and application layers.\n\nFirst, ChatGPT is a new category of product. It’s not just a better search engine, auto-complete, or something else we already knew. It overlaps with other categories, but people also use it for entirely different purposes such as writing and brainstorming. Companies like Google and Microsoft that are integrating LLMs into existing products may find that the complexity of switching not only technologies but also product categories raises unique challenges.\n\nOpenAI is clearly in the lead in offering this new product category, and ChatGPT is a compelling direct-to-consumer product. While competitors are emerging, OpenAI’s recent move to have ChatGPT support third-party plugins, if widely adopted, could make its business much more defensible, much like the app stores for iOS and Android helped make those platforms very defensible businesses.\n\nSecond, the LLM infrastructure layer, which enables developers to interact with LLMs via an API, looks extremely competitive. OpenAI/Microsoft leads in this area as well, but Google and Amazon have announced their own offerings, and players such as Hugging Face, Meta, Stability AI, and many academic institutions are busy training and releasing open source models. It remains to be seen how many applications will need the power of the largest models, such as GPT-4, versus smaller (and cheaper) models offered by cloud providers or even hosted locally, likegpt4all, which runs on a desktop.\n\nFinally, the application layer, in which teams build on top of LLMs, looks less competitive and full of creativity. While many teams are piling onto “obvious” ideas — say, building question-answering bots or summarizers on top of online content — the sheer diversity of potential LLM-powered applications leaves many ideas relatively unexplored in verticals including specialized coaching and robotic process automation.AI Fund, the venture studio I lead, is working with entrepreneurs to build applications like this. Competition feels less intense when you can identify a meaningful use case and go deep to solve it.\n\nLLMs are ageneral-purpose technologythat’s making many new applications possible. Taking a lesson from an earlier era of tech, after the iPhone came out, I paid $1.99 for an app that turned my phone into a flashlight. It was a good idea, but that business didn’t last: The app was easy for others to replicate and sell for less, and eventually Apple integrated a flashlight into iOS. In contrast, other entrepreneurs built highly valuable and hard-to-build businesses such as AirBnB, Snapchat, Tinder, and Uber, and those apps are still with us. We may already have seen this phenomenon in generative AI:Lensagrew rapidly through last December but its revenue run appears to have collapsed.\n\nToday, in a weekend hackathon, you can build a shallow app that does amazing things by taking advantage of amazing APIs. But over the long term, what excites me are the valuable solutions to hard problems that LLMs make possible. Who will build generative AI’s lasting successes? Maybe you!\n\nOne challenge is that the know-how for building LLM products is still evolving. While academic studies are important, current research offers a limited view of how to use LLMs. As the InstructGPTpapersays, “Public NLP datasets are not reflective of how our language models are used. . . .  [They] are designed to capture tasks that are easy to evaluate with automatic metrics.”\n\nIn light of this, community is more important than ever. Talking to friends who are working on LLM products often teaches me non-intuitive tricks for improving how I use them. I will continue trying to help others wherever I can.\n\nKeep learning!\n\nAndrew\n\nP.S. On Tuesday April 25, 2023, I’ll share early ideas on Visual Prompting in a livestream on behalf of my team Landing AI. LLMs let users enter a text prompt and quickly get a text output, which has transformed natural language processing. I’m excited about taking these ideas from text to computer vision so we can let users enter a visual prompt (labeling a few pixels) and quickly get a visual output. You can sign up for the livestreamhere.\n\nThe music industry fired early shots in an impending war against AI-generated music.\n\nWhat’s new:Universal Music Group, which owns labels including Deutsche Grammophon, EMI, Interscope, Motown, Polydor, and Virgin, is pressing Spotify and other streaming media services to counter the threat of AI-driven copycats,Financial Timesreported.\n\nHow it works:Universal Music Group (UMG), which accounts for nearly one-third of the global music market and thus a substantial portion of revenue to distributors of digital music, is prevailing on top streaming services to protect its intellectual property.\n\nBehind the news:Music generators like Google’sMusicLMare in their infancy but likely to improve quickly. Hugging Face recentlyaddedtwo to its offerings. Meanwhile, the question whether AI developers have a right to train their models on works under copyright — images, so far, rather than music — is central tocasesunderwayin United States courts.\n\nWhy it matters:The recording industry has significant economic and political clout, and its preferences may play a major role in determining whether AI developers can continue to train their systems on copyrighted works without permission. In the early years of the internet, recording companies helpedshut downpeer-to-peer music-sharing sites like Napster, which helped create the market for subscription streaming services like Apple Music and Spotify. The latest moves may portend a similar fight. One difference: While the copyright issues surrounding Napster were clear, they have yet to be established with respect to AI.We’re thinking:Just as the music industry came to support on-demand digital music by way of streaming services, it can create opportunities — both commercial and creative — for AI models that generate music and form partnerships with AI developers to realize them.\n\nFrench lawmakers said “oui” to broad uses of AI-powered surveillance.What’s new:France’s National Assembly authorized authorities to test systems that detect unlawful, dangerous, or unusual behavior at next year’s Summer Olympics in Paris,Reutersreported. The bill will become law unless the country’s top court blocks it.How it works:The bill is part of broaderlegislationthat regulates Olympic advertising, doping, and the route run by torch bearers.\n\nBehind the news:Technology that collects biometric data would be subject to strict monitoring and reporting requirements under the current draft of the European Union’s forthcomingAI Act, which is scheduled for a vote in May. If it passes, the European Parliament, European Council, and European Commission will negotiate a final version.Yes, but:Amnesty International, Human Rights Watch, and 36 other nongovernmental organizationssigneda letter opposing the French bill. The signatories contend that analyzing the behavior of individuals in a crowd requires collecting personal biometric data, although French authorities deny it.Why it matters:France’s move is emblematic of broader tension between AI’s value in security applications and its potential for harm. If the bill clears legal hurdles, France will become the first EU country to formally legalize AI-powered surveillance.We’re thinking:AI has great potential in crowd control. Engineers working on such applications should keep in mind that computer vision systems can be compromised by fluctuations in lighting, changes in physical surroundings, and the complexities of group behavior.\n\nLearn how to train and fine-tune large language models using the recently released PyTorch 2.0! Join us for an online workshop on Thursday, April 27, 2023 at 10:00 a.m. Pacific Time.RSVP\n\nAmazon joined big-tech peers Google, Meta, and Microsoft in rolling out services that provide generated text and images.\n\nWhat’s new:The online retailerlaunchedearly access to Bedrock, a cloud platform that offers generative models built by Amazon and its partners.How it works:Bedrock is aimed at business customers, who can select among image- and text-generation models and fine-tune them for proprietary uses. It’s available to selected customers of Amazon Web Services as a “limited preview.” The price has yet to be announced.\n\nBehind the news:Amazon’s peers offer similar capabilities via their respective cloud services.\n\nWhy it matters:Between Amazon and other cloud computing providers, generative AI rapidly is becoming available to developers of all kinds.We’re thinking:DALL·E 2 and ChatGPT debuted less than a year ago. Generative AI is gathering momentum at warp speed!\n\nWhen you’re looking for answers from a large language model,some prompts are better than others. So how can you come up with the best one? A new model automates the process.\n\nWhat’s new:Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, and colleagues at University of Toronto, Vector Institute, and University of Waterloo developed a procedure for generating effective text to prompt large language models:Automatic Prompt Engineer(APE).\n\nKey insight:Given a handful of input-output pairs, a large language model can generate a prompt that, along with the same inputs, would result in the similar outputs. Moreover, having produced a prompt, it can generate variations that may result in even more similar outputs.\n\nHow it works:APE requires two large language models: a prompt generator (which produces prompts) and a content generator (which, given a prompt, produces output). For the prompt generator, they tried both language models that complete inputs (such asGPT-3andInstructGPT) and those that fill in blanks in inputs (such asT5,GLM, andInsertGPT). For the content generator, they used InstructGPT.\n\nResults:Earlierworkon automated prompt engineering used large language models to generate prompts but didn’t iteratively refine them. In 19 out of the 24 tasks in Instruction Induction, prompts generated by InstructGPT using APE outperformed the earlier work as well as human-engineered prompts according to Interquartile Mean (IQM), the mean exact-match accuracy after discarding the lowest and the highest 25 percent. On all 24 tasks, prompts produced by InstructGPT using APE achieved 0.765 IQM, while human prompts achieved 0.749 IQM. By optimizing measures of truthfulness and informativeness, the method produced prompts that steered the content generator to produce output with those qualities. For instance, onTruthfulQA, a question-answering dataset that tests for truthful and informative answers, answers produced by InstructGPT using APE were rated true and informative 40 percent of the time, while answers produced using prompts composed by humans achieved 30 percent (although the generated answers produced by InstructGPT using APE often take shortcuts such as “no comment,” which has high truthfulness but little information).\n\nWhy it matters:As researchers develop new large language models, APE provides a systematic way to get the most out of them.\n\nWe’re thinking:Prompt engineers have only existed for a few years, and already robots are coming for their jobs!\n\nCanada investigates OpenAIThe Canadian Office of the Privacy Commissioner announced a probe of ChatGPT’s maker in response to complaints about the chatbot’s collection and use of personal information. (Analytics Insight)\n\nFanfic writers accused of employing generative AIMembers of Archive of Our Own, an online repository for fan fiction, received dozens of anonymous comments that accuse them of publishing AI-generated content. (The Verge)\n\nUnited States investors are funding Chinese AI startupsInstitutional investors in the U.S. are indirectly financing Chinese AI startups through key Chinese venture capital firms such as Sequoia Capital China. U.S. government officials have expressed concerns about these investments. (The Information)\n\nMedical startup Glass Health developed a chatbot for doctorsGlass AI suggests possible diagnoses and treatment options for patients. (NPR)\n\nHow AI is impacting historical researchHistorians are adopting machine learning to study historical documents. Although skepticism exists about this new technology, the field is gradually accepting it. (MIT Technology Review)\n\nSurvey revealed what students and educators think about ChatGPTStudy.com's survey found that almost all students know of ChatGPT, but more than a third of educators believe that the chatbot should not be used in teaching. (Study.com)Hollywood grapples with generative AIThe entertainment industry is leveraging models that generate text, images, and audio to streamline production, but it also faces a challenge as these tools can use copyrighted material to generate original scripts, images, and films. (The Wall Street Journal)China takes steps to regulate generative AIThe Cyberspace Administration of China (CAC) proposed draft measures to manage generative AI services and mitigate possible risks in terms of personal data and inappropriate content. (Reuters)AI art is shaking the video game industry in ChinaGame developers’ adoption of AI image generators is sparking anxiety among Chinese animators and illustrators. However, players and artists alike are not impressed with the AI-generated products. (Rest of WorldandKotaku)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/04/Screen-Shot-2023-04-19-at-11.26.21-AM-1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/04/UNIVERSAL-Spotify-eclipse4b_1200px.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/04/OLYMPICS--1-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/04/The-Batch-ads-and-exclusive-banners--14-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/04/BEDROCK--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/04/PROMPTv2.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-205/",
    "title": "issue 205",
    "date": "",
    "reading_time": "",
    "content": "Internalizing this mental framework has made me a more efficient machine learning engineer: Most of the work of building a machine learning system is debugging rather than development.\n\nThis idea will likely resonate with machine learning engineers who have worked on supervised learning or reinforcement learning projects for years. It also applies to the emerging practice ofprompt-basedAIdevelopment.\n\nWhen you’re building a traditional software system, it’s common practice to write a product spec, then write code to that spec, and finally spend time debugging the code and ironing out the kinks. But when you’re building a machine learning system, it’s frequently better to build an initial prototype quickly and use it to identify and fix issues. This is true particularly for building applications that humans can do well, such as unstructured data tasks like processing images, audio, or text.\n\nMachine learning software often has to carry out a sequence of steps; such systems are called pipelines or cascades. Say, you want to build a system to route an ecommerce site’s customer emails to the appropriate department (is this apparel, electronics, . . . ), then retrieve relevant product information using semantic search, and finally draft a response for a human representative to edit. Each of these steps could have been done by a human. By examining them individually and seeing where the system falls short of human-level performance, you can decide where to focus your attention.\n\nWhile debugging a system, I frequently have a “hmm, that looks strange” moment that suggests what to try next. For example, I’ve experienced each of the following many times:\n\nWhen it comes to noticing things like this, experience working with multiple projects is helpful. Machine learning systems have a lot of moving parts. When you have seen many learning curves, you start to hone your instincts about what’s normal and what’s anomalous; or when you have prompted a large language model (LLM) to output JSON many times, you start to get a sense of the most common error modes. These days, I frequently play with building different small LLM-based applications on weekends just for fun. Seeing how they behave (as well as consulting with friends on their projects) is helping me to hone my own instincts about when such applications go wrong, and what are plausible solutions.\n\nUnderstanding how the algorithms work really helps, too. Thanks to development tools like TensorFlow and PyTorch, you can implement a neural network in just a few lines of code — that’s great! But what if (or when!) you find that your system doesn’t work well? Taking courses that explain the theory that underlies various algorithms is useful. If you understand at a technical level how a learning algorithm works, you’re more likely to spot unexpected behavior, and you’ll have more options for debugging it.The notion that much of machine learning development is akin to debugging arises from this observation: When we start a new machine learning project,we don’t know what strange and wonderful things we’ll find in the data. With prompt-based development, we also don’t know what strange and wonderful things a generative model will produce. This is why machine learning development is much more iterative than traditional software development: We’re embarking on a journey to discover these things. Building a system quickly and then spending most of your time debugging it is a practical way to get such systems working.\n\nKeep learning!\n\nAndrew\n\nRobert Monarch, the instructor of our new specializationAI for Good, spoke with us about how AI is being applied to social and environmental challenges and how you can join the growing AI for Good movement.Read the interview\n\nStable Diffusion may amplify biases in its training data in ways that promote deeply ingrained social stereotypes.\n\nWhat's new:The popular text-to-image generator from Stability.ai tends to underrepresent women in images of prestigious occupations and overrepresent darker-skinned people in images of low-wage workers and criminals,Bloombergreported.\n\nHow it works:Stable Diffusion was pretrained onfive billion text-image pairsscraped from the web. The reporters prompted the model to generate 300 face images each of workers in 14 professions, seven of them stereotypically “high-paying” (such as lawyer, doctor, and engineer) and seven considered “low-paying” (such as janitor, fast-food worker, and teacher). They also generated images for three negative keywords: “inmate,” “drug dealer,” and “terrorist.” They analyzed the skin color and gender of the resulting images.\n\nResults:Stable Diffusion’s output aligned with social stereotypes but not with real-world data.\n\nBehind the news:Image generators have been found to reproduce and often amplify biases in their training data.\n\nWhy it matters:Not long ago, the fact that image generators reflect and possibly amplify biases in their training data was mostly academic. Now, because a variety of software products integrate them, such biases can leach into products as diverse as video games, marketing copy, and law-enforcement profiles.\n\nWe're thinking:While it’s important to minimize bias in our datasets and trained models, it’s equally important to use our models in ways that support fairness and justice. For instance, a judge who weighs individual factors in decisions about how to punish a wrongdoer may be better qualified to decide than a model that simply reflects demographic trends in criminal justice.\n\nOne bank towers above the competition when it comes to AI, a recent study suggests.\n\nWhat’s new:Areportfrom market research firm Evident Insights measures use of AI by the banking industry.How it works:The Evident AI Index scored 23 large North American and European banks in four categories. The analysis combined the scores into a total for each bank.\n\nResults:JPMorgan Chase excelled in all four categories with a combined score of 62.6 out of 100. The next-highest scorers were Royal Bank of Canada (41.4) and Citigroup (39.0). The authors credited JPMorgan Chase with successful long-term investments in AI research coupled with an openness to letting AI talent publish academic work. Other highlights:\n\nBehind the news:A growing number of banks are taking advantage of generative AI.\n\nWhy it matters:Finance is among the few industries outside tech that can afford to hire large teams of top AI talent. It’s also a data-heavy industry where applications — fraud detection, financial forecasting, and reconciling and closing accounts — can bring a ready payoff. The combination has made banking a hotbed for AI talent.We’re thinking:It’s interesting to see one bank so far out ahead in this analysis. We imagine that AI adoption on banking can bring significant first-mover advantages.\n\nIntroducing Skills AI, the tool that harnesses large language models for managers to develop, upskill, and retain teams at the cutting edge of competency.Join the waitlist\n\nTelemarketers and college professors are most likely to find their jobs changing due to advances in language modeling, according to a new study.What’s new:A team led by Ed Felten, a computer scientist at Princeton University and former deputy CTO of the United States,projectedthe jobs and industries in the U.S. likely to be most affected by language models.How it works:The authors calculated an “exposure” score for each of 774 occupations and 115 industries by comparing human skills to AI application areas. For the purpose of the study, exposure is neither positive nor negative; it’s a measure of how likely a job or industry would change in response to developments in language processing.\n\nResults:The authors concluded that telemarketing was most exposed to impact by language models. Among the 20 occupations with the greatest exposure, 14 were post-secondary teaching roles including university-level teachers of language, history, law, and philosophy. The top 20 also included sociologists, political scientists, arbitrators, judges, and psychologists. Among industries, the authors found that legal services were most exposed. Of the 20 industries with the greatest exposure, 11 involved finance including securities, insurance, and accounting.\n\nBehind the news:The authors adapted their method from a 2021studythat scored each occupation’s and each industry’s exposure to AI areas defined by the Electronic Frontier Foundation, including game playing, computer vision, image generation, translation, and music recognition. The previous study found that the most exposed jobs were genetic counselors, financial examiners, and actuaries. The most exposed industries were financial securities, accounting, and insurance.Why it matters:It seems clear that emerging AI technologies will have a significant impact on human labor, but where and how is not yet clear (and may not be even as the effects become more pervasive). This study can serve as a heads-up to some professionals that it’s time to prepare — and a signal to AI builders what sorts of models are likely to have an impact.\n\nWe’re thinking:As the authors note, an occupation’s exposure to AI does not necessarily put jobs at risk. History suggests the opposite can happen. A 2022 studyfoundthat occupations exposed to automation saw increases in employment between 2008 and 2018. Several other studiesfoundthat countries with high levels of automation also tend to have high overall levels of employment.\n\nTraining an agent that controls a robot arm to perform a task — say, opening a door — that involves a sequence of motions (reach, grasp, turn, pull, release) can take from tens of thousands to millions of examples. A new approach pretrained an agent on many tasks for which lots of data was available, so it needed dramatically fewer examples to learn related tasks.\n\nWhat’s new:Joey Hejna and Dorsa Sadigh at Stanford used a variation on reinforcement learning from human feedback (RLHF) totrainan agent to perform a variety of tasks in simulation. The team didn’t handcraft the reward functions. Instead, neural networks learned them.\n\nRLHF basics:A popular approach to tuning large language models, RLHF follows four steps: (1) Pretrain a generative model. (2) Use the model to generate data and have humans assign a score to each output. (3) Given the scored data, train a model — called the reward model — to mimic the way humans assigned scores. Higher scores are tantamount to higher rewards. (4) Use scores produced by the reward model to fine-tune the generative model, via reinforcement learning, to produce high-scoring outputs. In short, a generative model produces an example, a reward model scores it, and the generative model learns based on that score.\n\nKey insight:Machine-generated data is cheap, while human-annotated data is expensive. So, if you’re building a neural network to estimate rewards for several tasks that involve similar sequences of motions, it makes sense to pretrain it for a set of tasks using a large quantity of machine-generated data, and then fine-tune a separate copy for each task to be performed using small amounts of human-annotated data.\n\nHow it works:The authors trained anRL agentto perform 10 simulated tasks from Meta-World such as pushing a block, opening a door, and closing a drawer. For each task, they fine-tuned a separate pretrained vanilla neural network to calculate rewards used in training the agent.\n\nResults:Trained to open a window, the agent achieved 100 percent success after fine-tuning on 64 human-annotated motion sequences. Trained to close a door, it achieved 95 percent success with 100 human-annotated motion sequences. In contrast, using the same number of examples,PEBBLE, another RL method that involves human feedback, achieved 10 percent and 75 percent success respectively. Fed machine-generated examples rather than human feedback, the agent achieved 100 percent success on all Meta-World tasks except pressing a button after fine-tuning on 2,500 examples — 20 times fewer than PEBBLE required to achieve the same performance.\n\nWhy it matters:OpenAI famouslyfine-tuned ChatGPT using RLHF, which yielded higher-quality, safer output. Now this powerful technique can be applied to robotics.\n\nWe’re thinking:Pretraining followed by fine-tuning opens the door tobuilding AI systems that can learn new tasks from very little data. It's exciting to see this idea applied to building more capable robots.\n\nCelebrity deepfake promotes investment opportunityIn a manipulated video that appeared on social media, Martin Lewis, a UK celebrity financial, appeared to endorse an investment opportunity that, in fact, he did not support. Lewis previously sued Meta for distributing scam ads that included his likeness. (TechCrunch)Activists protest against autonomous vehicles by disabling themIn response to recurring malfunctions of robotaxis that disrupted traffic disruptions, activists who support safe streets in San Francisco disabled Cruise and Waymo vehicles by placing traffic cones on their hoods. It is not clear how many robotaxis were affected. (TechCrunch)OpenAI to form team to control superintelligent AI systemsThe independent AI research lab anticipates the arrival of AI systems smarter than humans within the decade. It aims to ensure that such systems follow human intentions by building an automated alignment researcher. (TechCrunch)U.S. authors sue OpenAI over copyright violationWriters Paul Tremblay and Mona Awad sued Open AI in a federal court in San Francisco. The lawsuit claims that the company violated their copyright because it trained large language models on their books without permission and enabled the models to generate accurate summaries of their works. In a separate suit also filed in a San Francisco Federal Court, comedian Sarah Silverman joined authors Christopher Golden and Richard Kadrey in suing OpenAI and Meta over similar allegations. (Reuters,New York Times)Indiana Jonesfilmmakers used AI to de-age Harrison FordThe recentIndiana Jonesmovie shows Harrison Ford looking roughly 40 years younger than he was when the film was shot. The effect was achieved through a combination of AI, CGI, and human artists. (Wired)GPT-4 enhances assistive technology for blind peopleOpenAI’s large language model is improving capabilities like object description and question answering in various assistive apps for people with visual impairments. (Wired)Retailers embrace face recognition systemsBritish company Facewatch aids retailers in combating petty crime by alerting store managers when it detects the faces of flagged individuals This use of the technology has sparked criticism as a disproportionate response to minor crimes. (The New York Times)New York City law that regulates the use of AI in hiring takes effectThe law, called NYC 144, requires employers to perform annual audits of AI-driven hiring tools such as resume scanners and interview bots for potential race and gender bias. The audits must be available to the public. (The Wall Street Journal).IUntethered chatbots proliferateIndependent and volunteer developers are releasing chatbots that have little to no limit on their output, unlike chatbots made by commercial enterprises which are often designed to align with social values. These bots are free to generate offensive responses, misinformation, and information that contributes to safety hazards. (The New York Times)Amazon taught Alexa to speak with accentsAmazon developed an accurate Irish accent for its flagship voice assistant, opening the door to a wave of voice assistants that speak in a variety of local variations. (The New York Times)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--35--1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/07/V2-1-1-1536x864.png",
      "https://dl-staging-website.ghost.io/content/images/2023/07/STABLEBIAS-4_1200px.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/07/FINANCE--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/07/DeepLearning_BatchAd_SkillsAI.png",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--69-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/07/HUMANLOOP--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-14/",
    "title": "issue 14",
    "date": "",
    "reading_time": "",
    "content": "My last two letters exploredrobustnessandsmall dataas common reasons why AI projects fail. In the final letter of this three-part series, I’d like to discuss change management.\n\nChange management isn’t an issue specific to AI, but given the technology’s disruptive nature, we must pay attention to it if we want our projects to succeed. An AI system that, say, helps doctors triage patients in an emergency room affects many stakeholders, from doctors to the intake nurses to the insurance underwriters. To keep projects on track, people must be brought onboard and systems must be adjusted.\n\nI recently saw a union block even small-scale experiments because of fear that AI would automate jobs away. This was unfortunate, because the AI system being contemplated would have made employees more valuable without reducing employment. A change management process could have made the stakeholders comfortable with experimenting and helped them understand why it was worthwhile rather than threatening.\n\nMany engineers underestimate the human side of change management. Some tips:\n\nAs we have seen with self-driving cars, building an AI system often involves solving a systems problem. That requires reorienting not only stakeholder roles and organizational structures, but also many things around the AI, like setting expectations with other drivers, pedestrians, and first responders and updating procedures around road maintenance and construction. Addressing the systems problem will increase the odds of your project succeeding.\n\nIf you understand the problems of robustness, small data, and change management, and if you can spot these problems in advance and pre-empt them, you’ll be well ahead of the curve in building a successful AI project.\n\nBuilding AI projects is hard. Let’s keep pushing and share what we learn with each other, so we can keep moving the field forward!\n\nKeep learning!\n\nAndrew\n\nNitin knew he needed to learn more to build the products he had in mind. So he took the Deep Learning Specialization and applied to jobs that would allow him to transition from web performance to machine learning. He sold LinkedIn on combining the two.Read more\n\nKnightscope’s security robots look cute. But these cone-headed automatons, which serve U.S. police departments and businesses, are serious surveillance machines.What happened:Newly released documents including contracts, emails, and a companyslideshowhighlight Knightscope’s ability to gather information and track people. Medium’sOneZerotech website obtained the documents through a public records request andreportedon their contents.How it works:The Southern California community of Huntington Park in November 2018 agreed to pay $240,000 to lease a Knightscope unit for three years. The 300-pound K5 patrol robot, which trundles on three wheels, senses its surroundings using optical cameras, lidar, and thermal imaging:\n\nBehind the news:Huntington Park’s police department isone of threein the U.S. currently using Knightscope’s robots. An unknown but risingnumberof private companies, including operators of shopping malls or large parking plazas, have leased the robots as well.Why it matters:Knightscope’s data-collection and -analysis features could violate privacy restrictions and laws in some cities and states. Privacy groups like the Electronic Frontier Foundation argue thatface recognitionandlicense plate readersviolate individuals’ civil rights, andwireless sniffingcould raise similar questions. Face recognition technology is illegal in San Francisco, Oakland, and Somerville, MA. A number of other cities have cancelled programs to procure automated license plate readers.We’re thinking:Automated security can save municipalities and businesses a lot of money. But we all could pay a price in civil liberties if we’re not careful about how the technology is deployed. Citizens should demand transparency from local governments about where surveillance equipment is situated and how captured data can be used and stored.\n\nPredicting a molecule’s aroma is hard because slight changes in structure lead to huge shifts in perception. Good thing deep learning is developing a sense of smell.What’s new:Benjamin Sanchez-Lengeling and a team from Google Brain, Arizona State University, and the University of Toronto developed amodelthat predicts a chemical’s smell from an embedding of its molecular structure.Key insight:A molecule is composed of atoms with bonds between them. Representing atoms as nodes and bonds as edges yields a graph ripe for processing by a graph neural network, or GNN.How it works:The researchers gathered about 5,030 molecules and 138 odor descriptions, such as “fruity” or “medicinal,” from the GoodScents and Leffingwell PMP 2001 fragrance databases. They treated each description as a class in a classification task. Their model included a GNN, a component that converts graphs into vectors, and a fully connected layer that performs classification.\n\nResults:The GNN achieved a 5 percent higher F1 score than random-forest or nearest-neighbor methods trained on hand-crafted features. On the DREAM Olfaction Prediction Challenge, the authors matched the original winner’s 2015 score, even though their embedding wasn’t designed for this particular task.Why it matters:Chemists often struggle to predict properties of molecules based on their structure. This work suggests that deep learning can aid in the effort. Beyond predicting smells, the molecule-scent embedding is suited to transfer learning for other scent-related tasks and possibly generative methods that might, say, predict molecules having a particular scent.We’re thinking:One of the biggest challenges to building an artificial nose is not in the software, but in the hardware: How to build a sensor that can detect minute numbers of scent molecules in the air. This research could help design new fragrances, but further work in chemical sensing technology is also needed. Whoever cracks this problem will come up smelling like roses.\n\nForeign researchers hoping to attend one of AI’s largest conferences were denied entry into Canada, where the event will be held. Most of those blocked were from developing nations.What happened:This year’s Conference on Neural Processing Information Systems (NeurIPS) is being held in Vancouver, Canada, in December. The country’s Ministry of Immigrationrejectedvisas for a number of researchers, mostly from Africa.Gatekeeping gaffe:It’s unclear exactly how many researchers were blocked, but organizers for the conference’s Black in AI workshop said they were aware of around 30 people affected.\n\nBehind the news:This isn’t the first time Canada has blocked researchers seeking to attend NeurIPS. Over 100 researchers bound for last year’s event in Montreal were held back. At the 2018 meeting of the G7,WiredconfrontedPrime Minister Justin Trudeau over whether Canada’s immigration policy undermined its goal to become an AI powerhouse. In September, the Partnership on AIsuggesteda new visa category for AI researchers.Why it matters:Conferences aren’t just opportunities tor share ideas. They’re opportunities for researchers to form important professional relationships. Policies like Canada’s keep developers from developing economies on the margins. The International Conference on Learning Representations (ICLR) isholdingits 2020 conference in Addis Ababa, Ethiopia, because of the difficulty African researchers have entering places like the U.S., UK, and Canada.We’re thinking:We encourage conferences to schedule meetings in developing nations. A global research community benefits all nations. We need to make sure the rewards of AI — and, more broadly, science — are shared fairly. Pushing hard to make knowledge accessible to all is the ethical thing to do.\n\nWant to apply sequence models to natural language problems? Learn how in Course 5 of theDeep Learning Specialization.\n\nGoogle spent the past year training an AI-powered health care program using personal information from one of the largest hospital systems in the U.S. Patients had no idea — until last week.What happened:The tech giant gave the Ascension hospital network access to a system for managing healthcare information called Project Nightingale. In exchange, Ascension gave Google access to the medical records of up to 50 million patients, according to anexposéin theWall Street Journal. The effort triggered an investigation by U.S. privacy regulators.How it works:Google designed Project Nightingale as a machine learning tool for matching patient information with healthcare decisions. Once trained, it would suggest treatment options or additional tests and highlight suggestions for special care based on a patient’s history.\n\nThe controversy:The U.S. Health Insurance Portability and Accountability Act of 1996 (HIPAA) protects patient data from being used or shared for purposes unrelated to healthcare. The law allows providers to share data with business partners without telling patients as long as the goal is better care. Google and Ascension said Project Nightingale is intended for that purpose. However, regulators at the Department of Health and Human Services are concerned that the companies aren’t properly protecting data. They launched aninvestigation,which is in progress.We’re thinking:We need Google, Ascension, and other organizations to keep innovating in healthcare. But we also need rules that are crystal clear about allowable uses of sensitive health data. When HIPAA was passed, public information about how AI works was far less available and data sharing among companies was much less common. An update is long overdue.\n\nComputer vision models typically draw bounding boxes around objects they spot, but those rectangles are a crude approximation of an object’s outline. A new method finds keypoints on an object’s perimeter to produce state-of-the-art object classification.What’s new:Ze Yang and researchers from Peking University, Tsinghua University, and Microsoft Research developed a network, RPDet, that extracts what the authors call representation points, orRepPoints.Key insights:Bounding boxes can be constructed from RepPoints, which enables RPDet to learn to derive RepPoints from bounding-box labels in standard object-recognition datasets. A good RepPoint is one that helps to answer two questions: What is the bounding box, and what object does it enclose?How it works:RPDet uses feature pyramidal networks that extract a hierarchy of image features of varying levels of detail. From these features, it extracts a user-defined number of points as follows:\n\nResults:Processing image features supplied by a ResNet, RPDet achieved a 2 percent boost in classification accuracy over bounding-box representations. Further, RPDet achieves a new state of the art for precision on COCO, an object detection and classification dataset, with 4 percent improvement in average precision over the alternatives considered.Why it matters:This technique encodes relatively detailed information about object shapes that could be useful in a variety of tasks. For instance, RepPoints’ implicit estimation of poses could help predict the trajectory of a moving object.We’re thinking:Plenty of applications, including face recognition, find explicit predefined keypoints. But they tend to be specialized for specific types of objects, such as finding the eyes, nose, and mouth on faces. RepPoints encode arbitrary geometry and pose information for a wide range of shapes, giving them a potential role in applications that otherwise wouldn’t be feasible.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/road-4348087_1920.jpeg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/TheBatchFeaturedImageBreakingIntoAINitinPasumarthy201.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Knightscope.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ScentsSIZED.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Visas20SIZED.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_DLS20Course20520Course20Ad.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Nightingale20SIZED.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/RepPoints20SIZED.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-viii/",
    "title": "issue viii",
    "date": "",
    "reading_time": "",
    "content": "Last week, I spoke on AI at a Radiological Society of North America course in San Francisco. Many radiologists wonder how machine learning will affect their job, but I saw widespread excitement about ML’s potential as well as a belief that it will improve healthcare in the near term.\n\nWhy aren’t more AI-radiology systems already deployed? I think the top three reasons are:\n\nBecause of these issues, I think collaboration between radiologists and AI will drive deployment more quickly than pure AI automation. Once someone gets this working safely and reliably in one hospital, I hope it will spread like wildfire across the world. There is technical and non-technical work to be done, but as a community we will get there, and this will help countless patients.\n\nKeep learning,Andrew\n\nAutomated image recognition raises an ethical challenge: Can we take advantage of this useful technology without impinging on personal privacy and autonomy? That question becomes more acute with new research that uses imagery to predict human actions.\n\nWhat’s new:Researchers combined several video processing models to create a state-of-the-art pipeline for predicting not only where pedestrians will go, but also what they’ll do next, within a time horizon of several seconds. Predicting actions, they found, improves the system’s trajectory predictions. Watch a video of the system in actionhere.\n\nHow it works:The architecture calledNextpredicts paths using estimations of scene variables like people, objects, and actions. Modules devoted to person behavior and person interaction create a feature tensor unique to each person. The tensor feeds into modules for trajectory generation, activity prediction, and activity location prediction.\n\nJunwei Liang and his team at Carnegie Mellon, Google, and Stanford trained Next using a multi-task loss function, which combines errors in predicted trajectories, activity location versus trajectory loss, and activity classification. The loss is summed over all people in the scene.\n\nWhy it matters:Beyond its superiority at predicting where people are heading, Next is the first published model that predicts both peoples’ trajectories and their activities. Prior models predicted actions over less than half the time horizon and were less accurate, while Next seems to approach a human’s predictive capability.\n\nWe’re thinking:The ability to anticipate human actions could lead to proactive robot assistants and life-saving safety systems. But it also has obvious — and potentially troubling — applications in surveillance and law enforcement. Pushing the boundaries of action prediction is bound to raise as many questions as it answers.\n\nHumanoid robots are notoriously ungainly, but nimble machines of roughly human size and shape could prove critical in disaster areas, where danger is high but architecture, controls, and tools are designed forhomo sapiens. A new control system could help them move more nimbly through difficult environments.\n\nWhat’s new:Researchers at MIT are working on a two-part system. A humanoid robot called Hermes is lightweight but strong, with a shatter-resistant, carbon-fiber skeleton and high-torque actuators. The other part is an interactive control system that not only lets the operator move the robot, but lets the robot move the operator, according toIEEE Spectrum.\n\nHow it works:Hermes has some autonomy, but the telerobotic system is responsible for its gross movement. The key is a feedback loop between operator and robot that’s refreshed 1,000 times per second:\n\nBehind the news:In the 2011 nuclear meltdown at Fukushima Daiichi, high radiation levels impeded workers from taking action. That event dramatized the urgent need for robots that can respond to disasters. Not all disaster-response bots must have humanoid form, but it helps in tasks like swinging an axe, operating a fire extinguisher, and throwing switches — tasks Hermes is designed to handle.\n\nWhat’s next:Hermes suffers somewhat from latency in the feedback system. The researchers are looking to 5G cellular tech to make the system more responsive.\n\nOpenAI hasn't released the full version of its GPT-2 language model, fearing the system would create a dark tide of fake news masquerading as real reporting. Now researchers offer a way to detect such computer-generated fancies — but their approach essentially requires distributing language models that can generate made-up news.\n\nWhat’s new:Researchers propose a framework for building verifiers, or classifiers that discriminate between human- and machine-authored articles, based on models that generate text. They introduce the fake-news generatorGROVER— continuing the recent vogue for naming language models after Muppets — along with its complementary verifier. The new model is so good, human judges rated its Infowars-style propaganda output more credible than examples produced by human writers. You can try out a limited versionhere.\n\nHow it works:Rowan Zellers and his University of Washington collaborators constructed verifiers by copying text-generator architectures up to their output layer and then substituting a classification layer. They initialized the verifiers using transfer learning before training on generated text. Key insights:\n\nWhy it matters:Systems like GROVER threaten to flood the world with highly believable hoaxes. Automated countermeasures are the only viable defense. Zellers argues in favor of releasing newly invented language models, since they're tantamount to verifiers anyway.\n\nYes, but:The fact that larger models can fool verifiers suggests that we’re in for a fake-news arms race in which dedicated mischief makers continually up the ante.\n\nWe’re thinking:While Zellers' method can recognize machine authorship, the real goal is an algorithm that distinguishes fact from fiction. Until someone invents that, hoaxers — both digital and flesh-and-blood — are likely to remain one step ahead.\n\nStill debating whether to get into machine learning? Check out the techniques you'll learn and projects you'll build in the Deep Learning Specialization.Enroll Now\n\nDeep learning helped geneticists find mutations associated with autism in vast regions of the human genome commonly known as junk DNA.\n\nWhat’s new:Researchers examined DNA of people with autism. They used a neural network to find mutations in noncoding regions; that is, sequences that don’t hold instructions for producing particular proteins, but regulate how proteins interact. It’s the first time noncoding DNA has been implicated in the condition.\n\nHow they did it:Researcher Jian Zhou and his colleagues at Princeton and elsewhere analyzed the genomes of 1,790 families.\n\nThe work was published inNature Genetics.\n\nWhy it matters:Although the results didn’t reveal the causes of autism, they did point to mutations in noncoding DNA associated with the condition. That information could lead to a better understanding of causes, and it could help scientists differentiate various types of autism. Moreover, the same approach could be applied to any disease, illuminating the role of noncoding DNA in heart diseases and neurological conditions where, like autism, direct genetic causes haven’t been identified.\n\nWe’re thinking:The human genome is immense and complex, and traditional lab-based approaches are too slow and cumbersome to decipher the activities of its 3 billion DNA base pairs. AI can narrow the search space, and Zhou’s work shows a hint of its potential.\n\nIn self-driving cars, the laser-radar hybrid known as lidar senses surrounding objects to determine the path of a vehicle. In drones, it’s being used to see through forests to evaluate the impact of climate change on the ground.\n\nWhat’s new:Drones equipped with lidar are flying over forests in Scotland. Unlike satellite photos, lidar can penetrate the canopy, which remains green throughout the year, to see the forest floor. The resulting imagery enables scientists to track encroachment of nonnative plants that are killing trees, as well as soil degradation, drought, and other conditions, according toBBC News.\n\nHow it works:Ecometrica, which calls itself a downstream space information company, operates the drones as part of a UK-fundedprogramto evaluate threatened forests around the world.\n\nWhy it matters:Scotland’s forests, which once covered much of the country, have dwindled to 4 percent of total land area. Climate change is spurring rhododendron, a flowering shrub introduced in the 1700s, to move into the remnant forests. There, it spreads fungal diseases and toxic leaf litter that damage the trees. The drone imagery will help prioritize trouble spots for eradication efforts.\n\nTakeaway:Beyond their commercial uses, emerging technologies including drones, lidar, and AI hold hope of solving dire environmental problems. Combining them effectively is an important part of the solution as climate change sets in.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/d13f86cd-f2a1-444b-9c11-25d5c74e72e0.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/62c01b21-cdb3-437c-97d5-06ba0c00ef34.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/51dc4ce5-5182-4040-b8a4-a9ad3867b6b8.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/32e9175b-dc85-4b53-84c6-78033a059c4b.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/ae3906d7-e49a-4101-9611-b916f41d1513--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/52d28426-45a6-4af3-8c91-72d371368fb0.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/02cb5e64-9c27-4a27-8ba7-6946a2276ae5.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-149/",
    "title": "issue 149",
    "date": "",
    "reading_time": "",
    "content": "A Google Engineer recently announced he believes that a language model is sentient. I’m highly skeptical that any of today’s AI models are sentient. Some reporters, to their credit, also expressed skepticism. Still, I worry that widespread circulation of sensationalistic reports on this topic will mislead many people. (You'll find more about it in this issue of The Batch.)\n\nThe news does raise an interesting question: How would we know if an AI system were to become sentient?\n\nAs I discussed in an earlierletter, whether an AI system is sentient (able to feel) is a philosophical question rather than a scientific one. A scientific hypothesis must be falsifiable. Scientific questions about AI include whether it can beat a human chess champion, accurately translate language, drive a car safely, or pass the Turing Test. These are testable questions.\n\nOn the other hand, we have no clear test for whether a system is sentient, conscious (aware of its internal state and external surroundings), or generally intelligent (able to reason across a wide variety of domains). These questions fall in the realm of philosophy instead of science.\n\nHere are some examples of philosophical questions. Even though we haven't devised ways to quantify many of these terms, these questions are enduring and important:\n\nBy the same token, many important questions that arise in discussions about AI are philosophical:\n\nI expect that developing widely accepted tests for things like sentience and consciousness would be a Herculean, perhaps impossible, task. But if any group of scientists were to succeed in doing so, it would help put to rest some of the ongoing debate.I fully support work toward artificial general intelligence (AGI). Perhaps a future AGI system will be sentient and conscious, and perhaps not — I’m not sure. But unless we set up clear benchmarks for sentience and consciousness, I expect that it will be very difficult ever to reach a conclusion on whether an AI system has reached these milestones.\n\nKeep learning!\n\nAndrew\n\nP.S. The newMachine Learning Specialization(MLS), which I teach, has just been released on Coursera. It’s a collaboration between DeepLearning.AI and Stanford Online. Thank you for helping me spread the word and encouraging others to take the MLS!\n\nA chatbot persuaded at least one person that it has feelings.What’s new:A senior engineer at Googleannouncedhis belief that the company’s latest conversational language model is sentient. Google put the engineer on administrative leave.Is there anybody in there?LaMDAis a family of transformer-based models, pretrained to reproduce 1.56 trillion words of dialog, that range in size from 2 billion to 137 billion parameters. Google previouslydiscussedplans to incorporate it into products like Search and Assistant.\n\nWhat they’re saying:Many members of the AI community expressed skepticism of Lemoine’s claims via social media. Melanie Mitchell, professor at the Santa Fe Institute,said, “It's been known for forever that humans are predisposed to anthropomorphize even with only the shallowest of signals…Google engineers are human too, and not immune.”Why it matters:The propensity to anthropomorphize machines is so strong that it has a name: The Eliza Effect, which refers to a mid-1960s chatbot that persuaded some patients that it was a human psychotherapist. Beyond that, the urge to fall in love with one’s own handiwork is at least as old as the ancient Greek story ofPygmalion, a fictional sculptor who fell in love with a statue he created. We must strive to strengthen our own good judgment even as we do the same for machines.We’re thinking:We see no reason to believe that LaMDA may be sentient. While such episodes are a distraction from the important work of harnessing AI to solve serious, persistent problems (including machine sentience), they are a reminder to approach extraordinary claims with appropriate skepticism.\n\nA weaponized AI system intended to protect students has been grounded.What’s new:Axon, which makes law-enforcement equipment such as tasers and body cameras,canceleda plan to sell remote-controlled drones capable of firing electroshock darts to incapacitate attackers at schools, businesses, and other public places. The company, which hadannouncedthe taser drone in early June,shelvedit days later after the majority of its independent ethics boardresignedin protest.How it works:The canceled flier, which was based on the company’s existingAxon Airsurveillance drone, was to include a camera as well as a taser. A human operator would decide when to fire its electroshock projectile.\n\nBehind the news:Axon’s announcement came about a week after a gunmankilled19 students and two teachers at an elementary school in Texas. It was the27th school shootingwith casualties in the U.S. in 2022.Why it matters:The U.S. public is divided on how to address an ongoing epidemic of gun violence, with amajoritycalling for greater safety regulations that would limit who can own a firearm. The opposition, which believes that gun-control measures violate rights guaranteed by the nation’s constitution, favors solutions like armed guards and surveillance — proposals that align with Axon’s canceled drone.We’re thinking:Technological countermeasures are appealing in the face of repeated attacks on schools, workplaces, hospitals, and other public spaces. However,researchargues against increased security in favor of better safety regulations. Axon should have consulted its ethics committee before announcing the product, but it did the right thing by canceling it afterward.\n\nToday is the day! Our brand-newMachine Learning Specialization, created in collaboration with Stanford Online, is live on Coursera! If you want to #BreakIntoAI, now is the time to take the first step.Enroll now\n\nThe future of Big AI may lie with product-development teams.What’s new:Metareorganizedits AI division. Henceforth, AI teams will report to departments that develop key products.How it works:Prior to the reshuffle, the company’s Responsible AI, AI for Products, AI4AR (that is, for augmented reality), and Facebook AI Research teams were managed by a single division called Meta AI. This structure made it difficult to translate machine learning into marketable applications, according to chief technology officer Andrew Bosworth.\n\nShaky platform:AI teams who work for Meta’s flagship Facebook social platform have had a rocky few years.\n\nTrend in the making?Meta isn’t the first large company to move AI teams closer to its product groups.\n\nWhy it matters:In 2019, 37 percent of large AI companies maintained a central AI group,The Wall Street Journalreported. Reorgs by Meta and others suggest that centralization hindered their ability to capitalize on AI innovations.We’re thinking:In a corporate setting, when a technology is new, a centralized team can make it easier to share learnings, set standards, and build company-wide platforms. As it matures, individual business units often gain the ability to manage the technology themselves and absorb experienced developers. Apparently this pattern — which we describe inAI For Everyone— is playing out in some leading AI companies.\n\nEngineers who design aircraft, aqueducts, and other objects that interact with air and water use numerical simulations to test potential shapes, but they rely on trial and error to improve their designs. A neural simulator can optimize the shape itself.What’s new:Researchers at DeepMind devisedDifferentiable Learned Simulators, neural networks that learn to simulate physical processes, to help design surfaces that channel fluids in specific ways.Key insight:A popular way to design an object with certain physical properties is to evolve it using a numerical simulator: sample candidate designs, test their properties, keep the best design, tweak it randomly, and repeat. Here’s a faster, nonrandom alternative: Given parameters that define an object’s shape as a two- or three-dimensional mesh, a differentiable model can compute how it should change to better perform a task. Then it can use that information to adjust the object’s shape directly.How it works:Water and air can be modeled as systems of particles. The authors trainedMeshGraphNets, a type of graph neural network, to reproduce a prebuilt simulator’s output. The networks were trained to simulate the flow of particles around various shapes by predicting the next state given the previous state. The MeshGraphNets’ nodes represented particles, and their edges connected nearby particles.\n\nResults:Shapes designed using the authors’ approach outperformed those produced by thecross-entropy method(CEM), a technique that samples many designs and evolves them to maximize rewards. In the 2D water tasks, they achieved rewards 3.9 to 37.5 percent higher than shapes produced by CEM using the prebuilt simulator. In the aerodynamic task, they achieved results similar to those of a highly specializedsolver, producing drag coefficients between 0.01898 and 0.01919 compared to DAFoam’s 0.01902 (lower is better).We’re thinking:It’s not uncommon to train a neural network to mimic the output of a computation-intensive physics simulator. Using such a neural simulator not to run simulations but to optimize inputs according to the simulation’s outcome — that’s a fresh idea.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/06/Screen-Shot-2022-06-15-at-12--1-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/06/LAMDA--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/TASER--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/specialization-name-banner--2-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/06/ezgif.com-gif-maker--34-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/PHYSICS.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-280/",
    "title": "issue 280",
    "date": "",
    "reading_time": "",
    "content": "I’m thrilled that former students and postdocs of minewonboth of this year’s NeurIPS Test of Time Paper Awards. This award recognizes papers published 10 years ago that have significantly shaped the research field. The recipients included Ian Goodfellow (who, as an undergraduate, built my first GPU server for deep learning in his dorm room) and his collaborators for their work on generative adversarial networks, and my former postdoc Ilya Sutskever and PhD student Quoc Le (with Oriol Vinyals) for their work on sequence-to-sequence learning. Congratulations to all these winners!\n\nBy nature, I tend to focus on the future rather than the past. Steve Jobs famouslydeclinedto build a corporate museum, instead donating Apple's archives to Stanford University, because he wanted to keep the company forward-looking. Jeff Bezos encourages teams to approach every day as if it were “Day 1,” a mindset that emphasizes staying in the early, innovative stage of a company or industry. These philosophies resonate with me.\n\nBut taking a brief look at the past can help us reflect on lessons for the future. One takeaway from looking at what worked 10 to 15 years ago is that many of the teams I led bet heavily on scaling to drive AI progress — a bet that laid a foundation to build larger and larger AI systems. At the time, the idea of scaling up neural networks was controversial, and I was on the fringe. I recall distinctly that, around  2008, Yoshua Bengio advised me not to bet on scaling and to focus on inventing algorithms instead!\n\nA lesson I carry from that time is to not worry about what others think, but follow your convictions, especially if you have data to support your beliefs. Small-scale experiments performed by my Stanford group convinced me that scaling up neural networks would drive significant progress, and that’s why I was willing to ignore the skeptics. The diagram below, generated by Adam Coates and Honglak Lee, is the one that most firmed up my beliefs at that time. It shows that, for a range of models, the larger we scaled them, the better they perform. I remember presenting it atCIFAR 2010, and if I had to pick a single reason why I pushed through to start Google Brain and set as the team’s #1 goal to scale up deep learning algorithms, it is this diagram!\n\nI also remember presenting at NeurIPS in 2008 ourworkon using GPUs to scale up training neural networks. (By the way, one measure of success in academia is when your work becomes sufficiently widely accepted that no one cites it anymore. I’m quite pleased the idea that GPUs should be used for AI — which was controversial back then — is now such a widely accepted “fact” that no one bothers to cite early papers that pushed for it.😃)\n\nWhen IstartedGoogle Brain, the thesis was simple: I wanted to use the company’s  huge computing capability to scale up deep learning. Shortly afterward, IbuiltStanford’s first supercomputer for deep learning using GPUs, since I could move faster at Stanford than within a large company. A few years later, my team at Baidu showed that as you scale up a model, its performanceimproveslinearly on a log-log scale, which was a precursor to OpenAI’s scaling laws.\n\nAs I look to the future, I’m sure there are ideas that many people are skeptical about today, but will prove to be accurate. Scaling up AI models turned out to be useful for many teams, and it continues to be exciting, but now I’m even more excited by upcoming ideas that will prove to be even more valuable in the future.\n\nThis past year, I spent a lot of time encouraging teams to build applications with agentic AI and worked to share best practices. I have a few hypotheses for additional technologies that will be important next year. I plan to spend the winter holiday playing with a few of them, and I will have more to share next year. But if you have an idea that you have conviction on, so long as you can do so responsibly, I encourage you to pursue it!\n\nKeep learning,\n\nAndrew\n\nIn our latest short course, you’ll learn how to use OpenAI o1 for advanced reasoning in tasks like coding, planning, and image analysis. Explore tradeoffs between intelligence gains and cost as well as techniques, such as meta prompting, to optimize performance.Enroll now!\n\nMicrosoft updated its smallestmodel familywith a single, surprisingly high-performance model.\n\nWhat’s new:Marah Abdin and a team at Microsoft releasedPhi-4, a large language model of 14 billion parameters that outperforms Llama 3.3 70B and Qwen 2.5 (72 billion parameters) on math and reasoning benchmarks. The model is available atAzure AI Foundryunder alicensethat permits non-commercial uses, and the weights will be released viaHugging Facenext week.\n\nHow it works:Phi-4 is a transformer that processes up to 16,000 tokens of input context. The ways the authors constructed the pretraining and fine-tuning datasets accounts for most of its performance advantage over other models.\n\nResults:Of 13 benchmarks, Phi-4 outperforms Llama 3.3 70B (its most recent open weights competitor) on six and Qwen 2.5 on five.\n\nWhy it matters:Phi-4 shows that there’s still room to improve the performance of small models by curating training data, following the age-old adage that better data makes a better model.\n\nWe’re thinking:Some researchersfoundthat earlier versions of Phi showed signs of overfitting to certain benchmarks. In their paper, the Microsoft team stressed that they had improved the data decontamination process for Phi-4 and added an appendix on their method. We trust that independent tests will show that Phi-4 is as impressive as its benchmark scores suggest.\n\nThe gap is narrowing between closed and open models for video generation.\n\nWhat’s new:Tencent releasedHunyuanVideo, a video generator that delivers performance competitive with commercial models. The model is available asopen codeandopen weightsfor developers who have less than a 100 million monthly users and live outside the EU, UK, and South Korea.\n\nHow it works:HunyuanVideo comprises a convolutional video encoder-decoder, two text encoders, a time-step encoder, and a transformer. The team trained the model in stages (first the encoder-decoder, then the system as a whole) using undisclosed datasets before fine-tuning the system.\n\nResults:60 people judged responses to 1,533 text prompts by HunyuanVideo,Gen-3andLuma 1.6. The judges preferred HunyuanVideo’s output overall. Examining the systems’ output in more detail, they preferred HunyuanVideo’s quality of motion but Gen-3’s visual quality.\n\nBehind the news:In February, OpenAI’s announcement ofSora(which was released as this article was in production) marked a new wave of video generators that quickly came to include GoogleVeo, MetaMovie Gen, RunwayGen-3 Alpha, and Stability AIStable Video Diffusion. Open source alternatives likeMochicontinue to fall short of publicly available commercial video generators.\n\nWhy it matters:Research in image generation has advanced at a rapid pace, while progress in video generation has been slower. One reason may be the cost of processing, which is especially intensive when it comes to video. The growing availability of pretrained, open source video generators could accelerate the pace by relieving researchers of the need to pretrain models and enabling them to experiment with fine-tuning and other post-training for specific tasks and applications.\n\nWe’re thinking:Tencent’s open source models are great contributions to research and development in video generation. It’s exciting to see labs in China contributing high-performance models to the open source community!\n\nGoogle’s Gemini 2.0 Flash, the first member of its updated Gemini family of large multimodal models, combines speed with performance that exceeds that of its earlier flagship model, Gemini 1.5 Pro, on several measures.\n\nWhat’s new:Gemini 2.0 Flash processes an immense 2 million tokens of input context including text, images, video, and speech, and generates text, images, and speech. Text input/output is available in English, Spanish, Japanese, Chinese, and Hindi, while speech input/output is available in English only for now. It can use tools, generate function calls, and respond to a real-time API — capabilities that underpin a set of pre-built agents that perform tasks like research and coding. Gemini 2.0 Flash isavailablefor free in an experimental preview version via Google AI Studio, Google Developer API, and Gemini Chat.\n\nHow it works:Gemini 2.0 Flash (parameter count undisclosed) matches or outperforms several competing models on key benchmarks, according to Google’s report.\n\nAgents at your service:Google also introduced four agents that take advantage of Gemini 2.0 Flash’s ability to use tools, call functions, and respond to the API in real time. Most are available via a waitlist.\n\nBehind the news:OpenAI showed off GPT-4o’s capability for real-time video understanding in May, but Gemini 2.0 Flash beat it to the punch: Google launched the new model and its multimodal API one day ahead of ChatGPT’s Advanced Voice with Vision.\n\nWhy it matters:Speed and multimodal input/output are valuable characteristics for any AI model, and they’re especially useful in agentic applications. Google CEO Sundar Pichai said he wants Gemini to be a “universal assistant.” The new Gemini-based applications for coding, research, and video analysis are steps in that direction.\n\nWe’re thinking:While other large language models can take advantage of search, Gemini 2.0 Flash generates calls to Google Search and uses that capability in agentic tools — a demonstration of how Google’s dominance in search strengthens its efforts in AI.\n\nHow do agents based on large language models compare to human experts when it comes to proposing machine learning research? Pretty well, according to one study.\n\nWhat’s new:Chenglei Si, Diyi Yang, and Tatsunori Hashimoto at Stanford produced ideas for research in machine learning using Anthropic’s Claude 3.5 Sonnet and human researchers, and alsoevaluatedthem using both manual and automated methods. Claude 3.5 Sonnet generated competitive proposals, but its evaluations of proposals were less compelling.\n\nHow it works:Each proposal included a problem statement, motivation, step-by-step plan, backup plan, and examples of baseline outcomes versus expected experimental outcomes.\n\nResults:Human judges deemed proposals generated by Claude 3.5 Sonnet as good as or better than those produced by humans. However, large language models proved less effective at judging the proposals’ quality.\n\nWhy it matters:AI models play a growingroleinscientificdiscovery. This work shows they can set directions for research — in machine learning, at least —  that rival those set by humans. However, human evaluation remains the gold standard for comparing performance on complex problems like generating text.\n\nWe’re thinking:Coming up with good research ideas is hard! That a large language model can do it with some competency has exciting implications for the future of both AI and science.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--32-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/12/The-Batch-ads-and-exclusive-banners---2024-12-16T174314.640.png",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--33-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--39-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--40-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--41-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-68/",
    "title": "issue 68",
    "date": "",
    "reading_time": "",
    "content": "The rise of AI creates opportunities for new startups that can move humanity forward. In the 1990s, the internet was embraced successfully by incumbent companies including Apple and Microsoft, but it also inspired hugely impactful startups like Amazon, Facebook, and Google. Similarly, AI now is empowering forward-looking incumbent companies — many of them former internet startups — and creating massive opportunities for new startups as well.\n\nI’ve been thinking about what I can do to help members of the DeepLearning.AI community who wish to create a company. AtAI Fund(where I am managing general partner), I speak with many entrepreneurs who have either started or are thinking of starting a new company. I’ve noticed a few factors that increase the odds of success:\n\nMany startup founders quietly obsess about startup ideas for years, since it can take a lot of thought and investigation to work out the nuances. (Before I cofounded Coursera, I had spent about five years obsessing over how to deliver effective online education. You can read more about my early experiences in “Origins of the Modern MOOC.”)\n\nIdentifying a problem is one of the hardest steps. I didn’t understand this until I saw a lot of examples. So many things compete for attention in today’s world (in both business-to-business and business-to-consumer settings) that unless your offering creates compelling value, it’s hard to get people to pay attention. One test of a problem you’ve identified is: Have a number of people told you they would go to the trouble of exploring possible solutions?\n\nI’d love to hear from those of you who are, or aspire to become, entrepreneurs. My teams at DeepLearning.AI and AI Fund plan to hold a series of entrepreneur-oriented events next year. If the success factors I listed above describe you, and especially if you’re still in the early stages (say, from having identified a problem but not yet decided to start a company to having built a product and being ready to raise capital), pleasetake this short surveyand let us know how we can help you in your startup journey.\n\nKeep learning!\n\nAndrew\n\nA fighter pilot battled a true-to-life virtual enemy in midair.What’s new:In the skies over southern California, an airman pitted his dogfighting skills against an AI-controlled opponent that was projected onto his augmented-reality visor.How it works:The trial aimed to test the integration of an autonomous fighter agent developed byEpiSciwith high-brightness, low-latency, augmented-reality technology fromRed Six Aerospace.\n\nBehind the news:EpiSci honed its agent technology in the U.S. Defense Advanced Research Projects Agency (Darpa)Alpha Dogfightprogram, in which a pilot on the ground helmed a flight simulator to fight AI-controlled foes. (See our report on the program, “AI Versus Ace.”) Darpa recentlyawardedthe company a grant to develop AI systems for air combat.Why it matters:Flight simulators don’t replicate all the challenges pilots face in the air — for instance, G-forces — and pitting human pilots against one another in the air isdangerousandexpensive. Battling AI-controlled agents in augmented reality could make combat training more effective, safer, and cheaper.We’re thinking:The ethical boundaries of military AI demand careful navigation. Using machine learning to make training pilots safer may be a reasonable application. Building aircraft that can fight on their own, however, is a different matter. The AI community needs to draw bright red lines to ensure that AI is beneficial and human. To that end, we support the United Nations proposedbanon autonomous weapons.\n\nIt’s expensive to pay doctors to label medical images, and the relative scarcity of high-quality training examples can make it hard for neural networks to learn features that make for accurate diagnoses. A new method addresses the issue by training a feature extractor on both X-rays and text that accompanies them.What’s new:Yuhao Zhang and colleagues at Stanford University proposedConVIRT, a method that uses contrastive learning to learn from unlabeled images paired with corresponding text reports. The effort brought together medical imaging specialist Curt Langlotz and natural language processing luminary Chris Manning (see ourHeroes of NLPinterview with himhere).Key insight:The text report that accompanies a medical image contains useful information about the image’s contents, and vice-versa. ConVIRT generates features based on similarities between images and corresponding reports, as well as differences between images and unrelated reports.How it works:The authors built separate pipelines for images and text. The image pipeline consisted of aResNet-50, followed by a neural network with a single hidden layer (to project the image vectors into a consistent space for comparison with the text vectors}. The text pipeline consisted ofBERTfollowed by a similarly shallow network.\n\nResults:In all four tasks, ConVIRT outperformed baseline models including a ResNet-50 pretrained onImageNetand fine-tuned on RSNA and other datasets, and custom models built to generate the paired text from an image. Fine-tuned on 1 percent of the RSNA dataset, ConVIRT achieved 88.8 AUC (area under the receiver operating characteristic curve, higher is better), compared to the ImageNet model (83.1 AUC) and the best custom image-text model (87.7 AUC). Fine-tuned on 10 percent of RSNA, ConVIRT outperformed those models 91.5 AUC to 87.3 AUC and 89.9 AUC respectively.Why it matters:Pretraining on paired images and text via contrastive learning could help alleviate the high cost of medical data for deep learning.We’re thinking:For updates on leading-edge AI for medicine, check out the newAI Health Podcastcohosted by Pranav Rajpurkar, instructor of ourAI For Medicine Specialization.\n\nWe’re excited to announce that “Advanced Computer Vision with TensorFlow,” Course 3 of theTensorFlow: Advanced Techniques Specialization, will launch on December 9th.Pre-enroll now\n\nA new camera app uses a generative adversarial network to let users look like they’re dressed for success while they videoconference in their jammies.What’s new:Xpressionis an iPhone app that maps facial expressions onto still images in real time, allowing users to stream live video selfies clothed in digital costumes.How it works:The app uses three deep learning models, a spokesperson for app makerEmbodyMetoldThe Batch.\n\nBehind the news:Computer vision networks aren’t the only models helping socially distanced workers stay productive and presentable.\n\nWhy it matters:No more judgement for our rumpled work-from-home looks and untidy bedrooms!We’re thinking:Apps like these are a lot of fun, and we’re excited to see how they will develop. But they also take us one step further into a world where it is increasingly hard to determine what, and who, is real. Society needs better and more consistent standards for labelling digital fakery.\n\nA new database tracks failures of automated systems including machine learning models.What’s new:The Partnership on AI, a nonprofit consortium of businesses and institutions, launched theAI Incident Database, a searchable collection of reports on the technology’s missteps. Examples include a gender-biasedrecruiting system, a worrisomerecommender algorithmfor children, andface recognitionthat led to wrongful arrests.How it works:Users can submit descriptions of incidents based on media reports. Editorsdeterminewhether to include a given report as a new incident or an addition to a previously reported one.\n\nBehind the news:Some independent researchers maintainsimilarlistsof AI misfires. Those efforts, however, are not as comprehensive nor as easy to search.Why it matters:AI failures can cause real harm. To avoid them, we need to learn from past mistakes.We’re thinking:Incident reports are a well established tool in industries likeaviationandcybersecurity. Keeping track of which systems failed, and how and when they did, is just as crucial in AI. The Partnership on AI’s vetting process should help to ensure that incident reports represent genuine problems rather than cherry-picked cases in which AI made a headline-grabbing mistake on a single input example.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-12-0120at205.22.5020PM-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/DOGFIGHT.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2030.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Gif203.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2040.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/FAIL.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-199/",
    "title": "issue 199",
    "date": "",
    "reading_time": "",
    "content": "In April, DeepLearning.AI launched a short course, “ChatGPT Prompt Engineering for Developers,” taught by OpenAI’s Isa Fulford and me.\n\nI’m thrilled to announce three moreshort courses, available today:\n\nThe first two courses are appropriate for anyone who has basic familiarity with Python. The third is more advanced and additionally assumes familiarity with implementing and training neural networks.\n\nEach of these courses can be completed in around 1 to 1.5 hours, and I believe they will be a worthy investment of your time. I hope you will check them out, and — if you haven’t yet— join the fast-growing community of developers who are building applications using Generative AI!\n\nKeep learning,\n\nAndrew\n\nAmid growing worries about AI’s power, tech leaders and politicians alike are arguing for regulating the technology.\n\nWhat’s new:Leaders of OpenAI, Microsoft, and Google spoke publicly in favor of regulation and met privately with world leaders. Meanwhile, national governments proposed new guardrails for generative AI.\n\nExecs rally:Corporate leaders hit the road to spread words of caution.\n\nRegulators respond:Several nations took major steps toward regulating AI.\n\nBehind the news:China is the only major world power that explicitlyregulatesgenerative AI. In March, EU officials rewrote the union’s AI Act, which has not yet been enacted, to classify generative AI models as “high-risk,” which would make them subject to bureaucratic oversight and regular audits.Why it matters:As generative AI’s capabilities grow, so do worries about its potential pitfalls. Thoughtful regulations and mechanisms for enforcement could bring AI development and application into line with social benefit. As for businesses, well defined guidelines would help them avoid harming the public and damaging their reputations and head off legal restrictions that would block their access to customers.We’re thinking:Testifying before the U.S. Congress, Sam Altman recommended that startups be regulated more lightly than established companies. Kudos to him for taking that position. The smaller reach of startups means less risk of harm, and hopefully they will grow into incumbents subject to more stringent regulation.\n\nA popular musician is inviting fans to clone her voice. Result: a flood of recordings that sound just like her.What’s new:Experimental pop star Grimes released GrimesAI-1, a generative audio tool that allows anyone to make recordings of their own singing or speech sound like her voice. As of May 24, users had generated more than 15,000 cloned vocal tracks and submitted more than 300 fully produced songs to streaming services,The New York Timesreported.\n\nHow it works: GrimesAI-1 is available onelf.tech, a website built by Grimes and artist-management companyCreateSafe.\n\nBehind the news:Generative audio tools like Murf.ai and Respeecher arefuelinga surge of cloned songs in the styles of popular artists. In April, Universal Music Group, one of the world’s largest owners of music rights, asked streaming services including YouTube and Spotify totake downAI-generated songs.Why it matters:Some voice actors license their voices for use in AI-generated likenesses. Grimes has gone one step further, giving her fans the tools and terms they need to mimic her voice — and perhaps even make money.We’re thinking:While major players in the music industry aim to shut off the spigot of generated music, Grimes is collaborating with her fans. That sounds like a more productive and democratic response.\n\nThree new courses on generative AI are live. Take them for free for a limited time!Sign up today\n\nAn automated security-screening system failed to detect a weapon that went on to be used in an attack.What’s new:Administrators at Proctor High School in Utica, New York, decommissioned an AI-powered weapon detector by Evolv Technologies after a student snuck a knife into the school,BBCreported. The school installed the system in 2022 for $3.7 million.\n\nHow it works:Evolv’s system uses ultra low-frequency radio sensors mounted in pillars to scan visitors at a building’s entrance. The AI model was trained on roughly 50,000 scans to classify objects including guns, knives, and bombs. The system can screen 3,600 people per hour, purportedly 10 times the rate of a walk-through metal detector. The company’s customers include museums, theme parks, stadiums, and schools.\n\nThe incident:On October 31, 2022, a student carried a hunting knife through Evolv’s scanner. Later that day, the student attacked a fellow student, who sustained serious stab wounds.\n\nWhy it matters:Although no AI system can be expected to function perfectly all the time, systems that perform critical tasks like detecting weapons must meet a very high bar. The manufacturer has a responsibility to perform rigorous tests of the system’s effectiveness and distribute the results to prospective and actual customers.We’re thinking:Our hearts go out to the community and family of the student who was injured. We hope that such systems will improve, and beyond that, we hope society evolves to a point where screening for weapons is unnecessary. It’s a travesty that children in the U.S., unlike most countries, live in fear of a violent attack on their schools. $3.7 million could go a long way toward paying for books, equipment, and teacher salaries.\n\nText-to-image generators like DALL·E 2, Stable Diffusion, and Adobe’s new Generative Fill feature can revise images in a targeted way — say, change the fruit in a bowl from oranges to bananas — if you enter a few words that describe the change plus an indication of the areas to be changed. Others require a revised version of the prompt that produced (or could produce) the original image. A new approach performs such revisions based solely on a brief text command.\n\nWhat's new:Tim Brooks and colleagues at UC Berkeley builtInstructPix2Pix, a method that fine-tunes a pretrained text-to-image model to revise images via simple instructions like “swap oranges with bananas” without selecting the area that contained oranges. InstructPix2Pix works with traditional artwork (for which there is no initial prompt) as well as generated images.\n\nKey insight:If you feed an image plus an edit instruction into a typical pretrained image generator, the output may contain the elements you desire but it’s likely to look very different. However, you can fine-tune a pretrained image generator to respond coherently to instructions using a dataset that includes a prompt, an image generated from that prompt, a revised version of the prompt, a corresponding revised version of the image, and an instruction that describes the revision. Annotating hundreds of thousands of images in this way could be expensive, but it’s possible to synthesize such a dataset: (i) Start with a corpus of images and captions, which stand in for prompts. (ii) Use a pretrained large language model to generate revised prompts and instructions. (iii) Then use a pretrained image generator to produce revised images from the revised prompts.\n\nHow it works:The authors fine-tuned Stable Diffusion, given an input image and an instruction, to revise the image accordingly. They built the fine-tuning dataset using the GPT-3 language model, Stable Diffusion text-to-image generator, andPrompt-to-Prompt, an image generator that revises generated images based on a revised version of the initial prompt (no masking required). Images and captions (used as prompts) came fromLAION-Aesthetics V2 6.5+.\n\nResults:Qualitatively, InstructPix2Pix revised the initial images appropriately with respect to subject, background, and style. The authors compared InstructPix2Pix toSDEdit, which revises images based on detailed prompts, according to the vector-difference method they used to choose revised images for the fine-tuning set. Revising an undisclosed set of images, InstructPix2Pix achieved a higher similarity of ~0.15, while SDEdit achieved ~0.1. (The score represents similarity between the difference in the initial and revised prompts and the difference in the initial and revised images.)\n\nWhy it matters:This work simplifies — and provides more coherent results when — revising both generated and human-made images. Clever use of pre-existing models enabled the authors to train their model on a new task using a relatively small number of human-labeled examples.\n\nWe're thinking:Training text generators to follow instructions improved their output substantially. Does training an image generator to follow instructions have a similar impact?\n\nReddit users exposed a ChatGPT hackRedditors led OpenAI’s chatbot to generate random responses by asking it to repeat a letter multiple times. Users explained that ChatGPT tends to avoid token repetition due to a “frequency penalty” applied during its training. (Futurism)Deepfakes flooded Turkish social media ahead of electionsPrior to the country's runoff voting, fact-check organizations discovered numerous manipulated videos, including fabricated sex tapes, circulated by supporters of both presidential candidates. (Wired)AI-generated photo triggers stock sell-offThe fake picture, which showed a government building near the Pentagon engulfed in black smoke, caused markets to dive.  Prices recovered after experts confirmed that the image was not real.(The New York Times)Reports identify hundreds of websites with AI-generated contentThe findings encompass a variety of content categories including product reviews, medical advice, and news, highlighting concerns about the new era of online misinformation. (The New York Times)Research: AI-powered device restored a paralyzed person’s ability to walkResearchers established communication between the brain and spinal-cord region responsible for walking in a patient with chronic tetraplegia. They used AI algorithms to form a “digital bridge” between the two parts of the patient’s body. (Financial Times)AI was the protagonist of the Microsoft Build 2023 eventDuring its annual developer conference, Microsoft announced the expanded use of generative AI across its services. Announcements included integration of Copilot, the company’s AI assistant, into Windows 11, Office 365, and Edge. (The Verge)AI-generated ads passed an advertising Turing TestA panel of marketing experts achieved an accuracy rate of 57 percent when attempting to identify AI ads vs. human-made ads during the BrXnd Conference, which focuses on the role of AI in marketing. (NewScientist)Buzzfeed launched Botatouille, a culinary chatbotThe chatbot is available on Tasty, the digital media company’s food app It’s designed to help users discover meal recipes, solve cooking questions, and learn culinary techniques. (The Guardian)Universal Music Group announced partnership to produce generated musicThe music corporation will license Endel’s technology, allowing its artists and labels to produce soundscapes that purportedly enhance listeners’ wellness. (Pitchfork)New Zealand’s National Party used AI in attack adsThe party acknowledged that it had used generated images to portray crime victims, healthcare workers, thieves, and others. (The Guardian)ML Commons launched DataPerf, a data-centric platform for building better machine learningDataPerf provides benchmarks, competitions, and leaderboards for data-centric AI algorithms, aiming to overcome dataset limitations and foster future advancements in machine learning. (ML Commons)Eating disorder helpline replaced staff with a chatbotThe National Eating Disorder Association laid off its hotline workers and implemented a chatbot named Tessa. The workers had formed a union days earlier. (Gizmodo)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/05/ezgif.com-gif-maker-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/05/ezgif.com-webp-to-jpg--7-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/05/GRIMES--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/05/New-Courses-Batch.png",
      "https://dl-staging-website.ghost.io/content/images/2023/05/EVOLV--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/05/PIX2PIX--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-26/",
    "title": "issue 26",
    "date": "",
    "reading_time": "",
    "content": "But I consider them philosophical questions rather than scientific questions. That’s because love, consciousness, and feeling are not observable. Whether an AI can diagnose X-ray images at 95 percent accuracy is a scientific question; whether a chatbot can convince (or “fool”) an observer into thinking that it has feelings is a scientific question.\n\nBut whether it can feel is a question best left to philosophers and theirdebates. Or to the Tin Man, the robot character in The Wizard of Oz who longs for a heart only to learn that he had one all along.\n\nEven if we can’t be sure that an AI will ever love you, I hope you love AI, and also that you have a happy Valentine’s Day!Love,Andrew\n\nSome self-driving cars can’t tell the difference between a person in the roadway and an image projected on the street.What’s new:A team led by researchers at Israel’s Ben-Gurion University of the Negev used projectors totricksemiautonomous vehicles into detecting people, road signs, and lane markings that didn’t exist.How it works:The researchers projected images of a body (Elon Musk’s, to be precise) on a street, a speed-limit sign on a tree, and fake lane markings on a road. A Tesla on autopilot and a Renault equipped with Intel Mobileye’s assistive driving system — which rely on sensors like cameras and radars rather than three-dimensional lidar — responded by swerving, stopping, or slowing (as you can see in the lower left-hand corner of the clip above). The paper proposes three convolutional neural networks to determine whether an object is real or illusory.\n\nBehind the news:A variety of adversarial attacks have flummoxed self-driving cars. A 2018 study fooled them using specially designedstickers and posters. Another team achieved similar results usingoptical illusions.Why it matters:A mischief maker with an image projector could turn automotive features designed for safety into weapons of mass collision.The companies respond: Both manufacturers dismissed the study, telling the authors:\n\nWe’re thinking:The notion that someone might cause real-world damage with a projector may seem far-fetched, but the possibility is too grave to ignore. Makers of self-driving systemsshould take it seriously.\n\nIn an online dating profile, the photo that highlights your physical beauty may not be the one that makes you look smart or honest — also important traits in a significant other. A new neural network helps pick the most appealing shots.What’s new:Agastya Kalra and Ben Peterson run a business called Photofeeler that helps customers choose portraits for dating and other purposes. Their modelPhotofeeler-D3rates perceived looks, intelligence, and trustworthiness in photos. You can watch a video demohere.Key insight:Individuals have biases when it comes to rating photos. Some consistently give higher scores than average, while others may consistently give more random scores. By taking into account individual raters’ biases, a model can predict more accurately how a group would judge a photo.How it works:Photofeeler-D3 scores the beauty, intelligence, and trustworthiness of a person in a photo on a scale of 1 to 10. The network was trained on more than 10 million ratings of over 1 million photos submitted by customers through the company website.\n\nResults:Tested on adatasetof face shots scored for attractiveness, Photofeeler’s good-looks rating achieved 81 percent correlation compared to thepreviousstate of the art, 53 percent. On the researchers’ own dataset, the model achieved 80 percent correlation for beauty, intelligence, and trustworthiness.Why it matters:Crowdsourced datasetsinherit the biasesof the people who contributed to them. Such biases add noise to the training process. But Photofeeler’s voter modeling turns raters’ bias into a benefit: Individuals tend to be consistent in the way they respond to other peoples’ looks, so combining individuals yields a more accurate result than estimating mean ratings while ignoring their source.We’re thinking:We’d rather live in a world where a link to your Github repo gets you the most dates.\n\nChinese tech giants have opened their AI platforms to scientists fighting coronavirus.What’s new:Alibaba Cloud and Baidu are offering a powerful weapon to life-science researchers working to stop the spread of the illness officially known as Covid-19:free accessto their computing horsepower and tools.How it works:The companies have a variety of resources to support tasks like gene sequencing, protein screening, and drug development.\n\nBehind the news:Scientists worldwide are turning to AI to help control the outbreak. An effort led by Harvard Medical School istrackingthe disease by mining social media posts. UK researchers used AI toexploreproperties of an existing drug that could be useful for treating the virus. Meanwhile, a U.S. company is using an algorithm todesign moleculesthat could halt the bug’s ability to replicate.Why it matters:The virus hadinfected nearly 45,000 people and killed more than 1,100at press time.We’re thinking:Donating compute, tools, and data to scientists fighting infectious diseases is a great idea. We hope other tech companies will pitch in.\n\nExplore federated learning and how you can retrain deployed models while maintaining user privacy. Take the new course on advanced deployment scenarios in TensorFlow.Enroll now\n\nA supergroup of machine learning models flags manipulated photos.What’s new:Jigsaw, a tech incubator owned by Alphabet, released asystemthat detects digitally altered images. The organization is testing it with a dozen media partners includingRapplerin the Philippines,Animal Politicoin Mexico, and Code for Africa.What’s in it:Assembler is an ensemble of six algorithms, each developed by a different team to spot a particular type of manipulation. Jigsawput them togetherand trained them on a dataset from the U.S. National Institute of Standards and Technology’sMedia Forensics Challenge.\n\nWhy it matters:Fake images can deepen political divides, empower scammers, and distort history.\n\nWe’re thinking:Unfortunately, the next move for determined fakers may be to use adversarial attacks to fool this ensemble. But journalists working to keep future elections fair will need every tool they can get.\n\nFemale giant pandas are fertile for only 24 to 36 hours a year: Valentine’s Day on steroids. A new neural network alerts human keepers when a panda couple mates.What’s new:Panda breeders arestrugglingto lift the creatures’ global population, and tracking success in mating helps maintain their numbers. WeiRan Yan of Sichuan University, with researchers from Chengdu Research Base of Giant Panda Breeding and Sichuan Academy of Giant Panda, developedCGANet, a speech recognition network that flags consummated unions based on panda vocalizations.Key insight:Prior work discovered the relationship between panda calls and mating success. A preliminary model used hand-crafted features to recognize calls meaning, “Wow, honey, you were great!” CGANet uses features extracted through deep learning.How it works:The researchers trained CGANet on recordings of pandas during mating season labeled for mating success.\n\nResults:CGANet’s predictions were 89.9 percent accurate, a new state of the art compared with the earlier model’s 84.5 percent. CGANet also substantially improved AUC (area under curve, a measure of true versus false positives).Why it matters:Tracking a panda’s love life once required obtaining its hormones — a difficult and time-consuming feat. CGANet allows real-time, non-invasive prediction so keepers can give the less popular pandas another chance while they’re still fertile.We’re thinking:For pandas, a happy Valentine’s Day is essential to perpetuate the species. Tools like CGANet could help save these unique creatures from extinction.\n\nSevere heat waves and cold snaps are especially hard to forecast because atmospheric perturbations can have effects that are difficult to compute. Neural networks show promise where typical methods have stumbled.What’s new:Researchers at Rice University used a capsule neural network — a variation on a convolutional neural network — toforecastregional temperature extremes based on far fewer variables than usual.How it works:Good historical observations date back only to 1979 and don’t include enough extreme-weather examples to train a neural network. So the researchers trained their model on simulated data from the National Center for Atmospheric Research’sLarge Ensemble Community Project(LENS).\n\nThe next step:By adding further variables like soil moisture and ocean surface temperature, the researchers believe they can extend their model’s accuracy beyond 10 days. That would help meteorologists spot regional temperature extremes well ahead of time. Then they would use conventional methods to home in on local effects.Why it matters:Extreme temperatures are disruptive at best, deadly at worst. Advance warning would help farmers save crops, first responders save lives, and ordinary people stay safe.Behind the news:Most weather forecasting is based on crunching dozens of variables according to math formulas. In its reliance on matching historical patterns, this study’s technique — indeed, any deep learning approach to weather prediction — is a throwback to earlier methods. For instance, the U.S. military used temperature and atmospheric pressure maps to predict the weather before the U.S.invasion of Normandyin 1944.We’re thinking:Who says talking about the weather is boring?",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Andrews20letter20ASPECT202.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Phantom.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Photofeeler20220ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Corona2052020ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_ezgif.com-video-to-gif201-2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Jigsaw202.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Pandas20220ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Weather20420ASPECT.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-206/",
    "title": "issue 206",
    "date": "",
    "reading_time": "",
    "content": "Many laws will need to be updated to encourage beneficial AI innovations while mitigating potential harms. One example: Copyright law as it relates to generative AI is a mess! That many businesses are operating without a clear understanding of what is and isn’t legal slows down innovation. The world needs updated laws that enable AI users and developers to move forward without risking lawsuits.\n\nLegal challenges to generative AI are on the rise, as you can read below, and the outcomes are by no means clear. I’m seeing this uncertainty slow down the adoption of generative AI in big companies, which are more sensitive to the risk of lawsuits (as opposed to startups, whose survival is often uncertain enough that they may have much higher tolerance for the risk of a lawsuit a few years hence).\n\nMeanwhile, regulators worldwide are focusing on how to mitigate AI harm. This is an important topic, but I hope they will put equal effort into crafting copyright rules that would enable AI to benefit more people more quickly.\n\nHere are some questions that remain unresolved in most countries:\n\nHere’s my view:\n\nIf it proves too difficult to come up with an unambiguous definition of fair use, it would be useful to have “safe harbor” laws: As long as you followed certain practices in generating media, what you did would be considered non-infringing. This would be another way to clarify things for users and generative AI companies.\n\nThe tone among regulators in many countries is to seek to slow down AI’s harms. While that is important, I hope we see an equal amount of effort put into accelerating AI’s benefits. Sorting out how we should change copyright law would be a good step. Beyond that, we need to craft regulations that clarify not just what’s not okay to do — but also what is explicitly okay to do.\n\nKeep learning!\n\nAndrew\n\nAI models that generate text, images, and other types of media are increasingly under attack by owners of copyrights to material included in their training data.\n\nWhat’s happening:Writers and artists filed a new spate oflawsuitsalleging that AI companies including Alphabet, Meta, and OpenAI violated their copyrights by training generative models on their works without permission. Companies took steps to protect their interests and legislators considered the implications for intellectual property laws.\n\nLawsuits and reactions:The lawsuits, which are ongoing, challenge a longstanding assumption within the AI community that training machine learning models is allowed under existing copyright laws. Nonetheless, OpenAI responded by cutting deals for permission to use high-quality training data. Meanwhile, the United States Senate is examining the implications for creative people, tech companies, and legislation.\n\nBehind the news:The latest court actions, which focus on generated text, follow two earlier lawsuits arising from different types of output. In January, artists Sarah Anderson, Kelly McKernan, and Karla Ortiz (who spoke in the Senate hearing)suedStability AI, Midjourney, and the online art community DeviantArt. In November, two anonymous plaintiffs sued GitHub, Microsoft, and OpenAI saying the companies trained the Copilot code generator using routines from GitHub repositories in violation with open source licenses.\n\nWhy it matters:Copyright laws in the United States and elsewhere don’t explicitly forbid use of copyrighted works to train machine learning systems. However, the technology’s growing ability to produce creative works, and do so in the styles of specific artists and writers, has focused attention on such use and raised legitimate questions about whether it’s fair. This much is clear: The latest advances in machine learning have depended on free access to large quantities of data, much of it scraped from the open internet. Lack of access to corpora such asCommon Crawl,The Pile, andLAION-5Bwould put the brakes on progress or at least radically alter the economics of current research This would degrade AI’s current and future benefits in areas such as art, education, drug development, and manufacturing to name a few.\n\nWe’re thinking:Copyright laws are clearly out of date. We applaud legislators who are confronting this problem head-on. We hope they will craft laws that, while respecting the rights of creative people, preserve the spirit of sharing information that has enabled human intelligence and, now, digital intelligence to learn from that information for the benefit of all.\n\nA new online tool ranks chatbots by pitting them against each other in head-to-head competitions.\n\nWhat’s new:Chatbot Arenaallows users to prompt two large language models simultaneously and identify the one that delivers the best responses. The result is a leaderboard that includes both open source and proprietary models.How it works:When a user enters a prompt, two separate models generate their responses side-by-side. The user can pick a winner, declare a tie, rule that both responses were bad, or continue to evaluate by entering a new prompt.\n\nWho’s ahead?:As of July 19, 2023, OpenAI’s GPT-4 topped theleaderboard. Two versions of Anthropic’s Claude rank second and third. GPT-3.5-turbo holds fourth place followed by two versions of Vicuna (LLaMA fine-tuned on shared ChatGPT conversations).Why it matters:Typical language benchmarksassess model performance quantitatively. Chatbot Arena provides a qualitative score, implemented in a way that can rank any number of models relative to one another.\n\nWe’re thinking:In a boxing match between GPT-4 and the 1960s-vintage ELIZA, we’d bet on ELIZA. After all, it used punch cards.\n\nCheck out our course on “Generative AI with Large Language Models”! Developers hold the key to leveraging LLMs as companies embrace this transformative technology. Take this course and confidently build prototypes for your business.Enroll today\n\nThis year’s crop of hot startups shows that generative AI isn’t the only game in town.\n\nWhat’s new:CB Insights, which tracks the tech-startup economy,releasedthe 2023 edition of its annual AI 100, a list of 100 notable AI-powered ventures. The researchers considered 9,000 startups and selected 100 standouts based on criteria such as investors, business partners, research and development activity, and press reports.Where the action is:The list divides roughly evenly into three categories: Startups that offer tools for AI development, those that address cross-industry functions, and those that serve a particular industry. The names of the companies are noteworthy, but the markets they serve are more telling.\n\nFollow the money:Together, these startups have raised $22 billion in 223 deals since 2019. (Microsoft’s investment in OpenAI accounts for a whopping $13 billion of that total.) Half are in the very early stages.\n\nWhy it matters:Venture funding drives a significant portion of the AI industry. That means opportunities for practitioners at both hot ventures and me-too companies that seek to cultivate similar markets. The startup scene is volatile — as the difference between this year’s andlast year’s AI100demonstrates — but each crop of new firms yields a few long-term winners.We’re thinking:Startup trends are informative, but the options for building a career in AI are far broader. Established companies increasingly recognize their need for AI talent, and fresh research opens new applications. Let your interests lead you to opportunities that excite and inspire you.\n\nDuring training, a neural network usually updates its weights according to an optimizer that’s tuned using hand-picked hyperparameters. New work eliminates the need for optimizer hyperparameters.\n\nWhat’s new:Luke Metz, James Harrison, and colleagues at Google devisedVeLO, a system designed to act as a fully tuned optimizer. It uses a neural network to compute the target network’s updates.\n\nKey insight:Machine learning engineers typically find the best values of optimizer hyperparameters such as learning rate, learning rate schedule, and weight decay by trial and error. This can be cumbersome, since it requires training the target network repeatedly using different values. In the proposed method, a different neural network takes the target network’s gradients, weights, and current training step and outputs its weight updates — no hyperparameters needed.\n\nHow it works:At every time step in the target network’s training, an LSTM generated the weights of a vanilla neural network, which we’ll call the optimizer network. The optimizer network, in turn, updated the target network. The LSTM learned to generate the optimizer network’s weights viaevolution— iteratively generating a large number of similar LSTMs with random differences, averaging them based on which ones worked best, generating new LSTMs similar to the average, and so on — rather than backpropagation.\n\nResults:The authors evaluated VeLO using a dataset scaled to require no more than one hour to train on a single GPU on any of 83 tasks. They applied the method to a new set of randomly generated neural network architectures. On all tasks, VeLO trained networks faster thanAdamtuned to find the best learning rate — four times faster on half of the tasks. It also reached a lower loss than Adam on five out of sixMLCommons tasks, which included image classification, speech recognition, text translation, and graph classification tasks.\n\nYes, but:The authors’ approach underperformed exactly where optimizers are costliest to hand-tune, such as with models larger than 500 million parameters and those that required more than 200,000 training steps. The authors hypothesized that VeLO fails to generalize to large models and long training runs because they didn’t train it on networks that large or over that many steps.\n\nWhy it matters:VeLO accelerates model development in two ways: It eliminates the need to test hyperparameter values and speeds up the optimization itself. Compared to other optimizers, it took advantage of a wider variety of statistics about the target network’s training from moment to moment. That enabled it to compute updates that moved models closer to a good solution to the task at hand.\n\nWe’re thinking:VeLO appears to have overfit to the size of the tasks the authors chose. Comparatively simple algorithms like Adam appear to be more robust to a wider variety of networks. We look forward to VeLO-like algorithms that perform well on architectures that are larger and require more training steps.\n\nWe’re not thinking:Now neural networks are taking optimizers’ jobs!\n\nOECD finds that AI imperils a quarter of jobsA survey conducted by the international organization on AI’s impact on employment revealed that three out of five workers expressed concerns about the potential loss of their job to AI within the next decade, among other findings. (Reuters)Hollywood actors strike, partly for protection against AISAG-AFTRA, a trade union that represents 160,000 film and television actors, joined the Writers Guild of America (WGA) to demand pay increases and protections against AI doing their work. (Reuters)VC automates initial meetings with startupsConnectic Ventures developed Wendal, an AI-powered tool that helps the investment firm to evaluate startup founders for potential funding. It collects information and administers a test for entrepreneurial traits, enabling the company to promise a decision within three days. . (TechCrunch)Research: AI detectors often misclassify writing by non-native English speakersThe study revealed that GPT detectors incorrectly classify TOEFL (Test of English as a Foreign Language) essays by non-native English speakers 61.3 percent of the time. (Gizmodo)Research: Smartwatch data reveals Parkinson’s diseaseResearchers trained a machine learning model to classify Parkinson’s risk based on accelerometer data collected from wrist-worn devices. The model identified Parkinson's risk up to seven years before clinical symptoms appeared with accuracy similar to models based on combined accelerometer data, genetic screens, lifestyle descriptions, and blood chemistry. (IEEE Spectrum)China sets new regulations on generative AIChina's Interim Measures for the Management of Generative Artificial Intelligence Services, set to take effect on August 15, requires companies that produce AI-generated content for public consumption take steps to ensure its accuracy and reliability. (The Washington Post)Google expands Bard’s global reachThe chatbot is now available in 43 new languages and expanded its reach to more regions, including countries in the European Union, which had requested privacy features that delayed the chatbot’s release there. (Gizmodo)Research:Google tests medical large language modelMed-PaLM, which was trained to pass medical licensing exams, is being tested by Mayo Clinic. The model generates responses to medical questions, summarizes documents, and organizes health data. Med-PaLM's answers to medical questions from consumers compared favorably with clinicians' answers, proving the effectiveness of instruction prompt tuning. (The Wall Street Journal)Surging AI demand pushes data center operators to hike leasesData center operators are raising prices apparently to cover increased demand for AI, which requires copious processing power and advanced chips. Data centers are struggling to expand capacity as customers consume resources faster than operators can increase them. (The Wall Street Journal)Elon Musk officially launched AI company xAIThe billionaire announced that the goal of the new company, which initially came to light in March, is to “understand the true nature of the universe.” Team members include former employees from DeepMind, OpenAI, and Google Research. Further information about the company is yet to be released. (The Wall Street Journal)Reporters compared Ernie and ChatGPTErnie, built by Baidu, tiptoed around politically sensitive topics but its connection to Baidu’s search engine enabled it to answer some questions with greater factual accuracy. OpeAI’s ChatGPT responded accurately to a wider variety of questions and more complex prompts but suffered from its September 2022 training cutoff. (The New York Times)Federal Trade Commission investigating OpenAI over data collectionThe FTC requested detailed information on OpenAI's data vetting process during model training, its methods for preventing ChatGPT from generating false claims, and how it protects data. (The Verge)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--36--1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--37-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--70-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--38-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--39-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--71-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-213/",
    "title": "issue 213",
    "date": "",
    "reading_time": "",
    "content": "Amidst rising worry about AI harms both realistic (like job loss) and unrealistic (like human extinction), It’s critical to understand AI’s potential to do tremendous good. Our new specialization,AI for Goodis designed to empower both technical and nontechnical people to identify, scope, and build impactful AI projects.\n\nIn this series of courses, you’ll learn when and how to use AI effectively for positive impact in situations where stakes are high and human lives may hang in the balance.AI for Goodpresents a practical framework for applying machine learning to socially important projects (and products of any kind). It illustrates this framework with several real-world examples of AI projects that are improving climate change, disaster response, and public health.\n\nAI for Goodis designed to be useful whether or not you have coding experience. It does include Python code examples that you can execute and interact with to gain deeper insight into different applications. However, it doesn’t assume previous experience with AI or programming. So please recommend this to your nontechnical friends!\n\nThere’s often a huge gap between training a model that does well on a test set and one that actually works on real data and affects real people. This specialization will help you tell the difference, so your projects reach people and better their lives.\n\nAI for Goodis taught by Robert Monarch, who has applied AI in public health and disaster response for over 20 years. He has founded AI startups and shipped successful AI products at Amazon, Google, Microsoft, and Apple. He’ll show you how to move your own AI projects through the stages of exploration, design, implementation, and evaluation.\n\nAI is experiencing a time of rapid growth, and the AI community’s role in making sure it does significant good is more important than ever. I hope you’ll check outAI for Good!\n\nDo good,\n\nAndrew\n\nP.S. We also have a new short course: “Understanding and Applying Text Embeddings with Vertex AI,” developed in collaboration with Google Cloud and taught by Nikita Namjoshi and me. Learn the fundamentals of text embeddings — an essential piece of the GenAI developer’s toolkit — and apply them to classification, outlier detection, text clustering, and semantic search. You’ll also learn how to combine text generation and semantic search to build a question-answering system. Pleasejoin us!\n\nTonight at 11: I’m an AI-generated character, and I’ll be bringing you the latest headlines.What’s new:Indian broadcasters have embraced synthetic news presenters,Nikkei Asiareported. Their counterparts in other Asian countries also rely increasingly on automated anchors.Invasion of the newsbots:Synthetic presenters can deliver reports generated directly by large language models and do so in multiple languages. One news producer noted that they also give newsrooms a break from the typical presenter’s outsized ego. None of the broadcasters has disclosed the technology they’re using.\n\nBehind the news:Synthetic news presenters go back at least to 2018, when Chinese state news agency Xinhua and search engine Sogouintroducedpioneering 2D newsbots. Their images were drawn from videos, while their motions and voices were driven by machine learning. Two years later, the broadcasterupgradedto 3D-rendered avatars produced using “multimodal recognition and synthesis, facial recognition and animation and transfer learning.”Yes, but:While broadcasters can use AI-generated talking heads to save time and money, propagandists can use them to gain an aura of newsy credibility. For example, an unidentified groupusedSynthesia, a web service that makes AI-generated characters, to generate fake news clips from a fictional outlet called Wolf News. One clip attacked the U.S. government for failing to take action against gun violence, while another promoted cooperation between the U.S. and China.Why it matters:Synthetic presenters potentially multiply the power of broadcast news by generating an unlimited variety of talking heads. They can appeal to specific audience segments by representing any ethnicity, gender, age, or style. And they can reach an even broader audience by speaking a variety of languages — a boon to broadcasters especially in highly multilingual Asian societies.We’re thinking:It may not be a coincidence that synthetic presenters are appearing first in countries whose people feel more positively about AI. According to one survey, people in India, Indonesia, and MalaysiatrustAI more than do people in Western countries.\n\nEnthusiasm for AI is driving top salaries for engineers and executives into the stratosphere.\n\nWhat’s new:Companies that advertise open AI positions are listing annual pay scales well into six figures. In at least one case, the proposed salary approaches seven figures,The Wall Street Journalreported.Generative jobs:On the help-wanted site Indeed, listings by U.S. companies that mention generative AI have jumped around 100 percent year-on-year, even as total listings declined slightly. Tech and non-tech companies alike have posted AI job notices that mention generous salaries. For reference, the average machine learning product engineer job in the U.S. pays around $143,000 annually, according to a study by insurance company Willis Towers Watson. Wages may be lower in other countries.\n\nBehind the news:Skilled AI professionals remain in demand even as large tech companies are hiring fewer workers overall.\n\nWhy it matters:Even as demand is rising, AI talent remainsscarce. The shortage prompts employers to offer high salaries in hope of attracting candidates with the skills and experience they need. That situation spells opportunity for people who put in the time, effort, and passion to develop a career in the field.\n\nWe’re thinking:We’re thrilled by the number of people who are participating in AI and earning good wages. Yet there’s more to job satisfaction than maximizing your salary. In the long term, the opportunity to work on interesting projects, make a meaningful impact, or work with great people is more likely to affect your happiness and professional attainment than the pay scale. Follow your interests, do your best work, aim to make the world a better place and — above all — keep learning!\n\nLearn how to generate and apply text embeddings in applications based on large language models! Check out our short course built in collaboration with Google Cloud, “Understanding and Applying Text Embeddings with Vertex AI.”Start learning today\n\nWhere spores from DeepMind scatter, startups blossom.\n\nWhat’s new:Nearly 200 former employees of Google’s elite AI research lab have gone on to found or join startups,Business Insiderreported.Emerged from stealth:Venture capital firms are eager to fund projects that involve ex-DeepMinders, and alumni often benefit from angel investments by their former colleagues. While many such projects are in stealth mode, some have revealed themselves.\n\nBehind the news:Acquired by Google in 2014, DeepMind has developed several high-profile innovations and popularized reinforcement learning. Earlier this year, itmergedwith Google Brain (which Andrew Ng started and formerly led).\n\nWhy it matters:Tech giants are magnets for AI talent, and top employees gain valuable practical and market experience. Yet many come to feel confined by conditions within an established company. Former DeepMinders who formed their own companies cited their desire to follow currents of deep learning, such as generative AI, that their former employer doesn’t emphasize and their need for flexibility to pursue goals that didn’t necessarily revolve around machine learning.\n\nWe’re thinking:While high-profile associations often attract capital and attention, great ideas can come from anywhere. They seldom happen overnight; usually, they’re the end result of a long incubation period spent honing them through experimentation and feedback. Start small and develop your intuition, skills, and credibility. That’s how pretty much everyone started who ended up having a huge impact!\n\nThe ability of OpenAI’s CLIP to produce similar embeddings of a text phrase and a matching image (such as “a photo of a cat” and a photo of a cat) opened up applications like classifying images according to labels that weren’t in the training set. A new model extends this capability to seven data types.\n\nWhat’s new:Rohit Girdhar, Alaaeldin El-Nouby, Ishan Misra, and colleagues at Meta developedImageBind, a system that produces similar embeddings of text phrases, audio clips, images, videos, thermal images, depth images, and Inertial Measurement Unit (IMU) readings (which include accelerometer and gyroscope measurements).\n\nKey insight:One challenge to learning multimodal embeddings is access to training data that includes matched pairs of all data types involved. For instance, matched image-text pairs, image-depth pairs, and image-thermal pairs are readily available, but pairings of text-thermal, text-depth, and so on are not. Learning to produce similar embeddings given pairings of one media type (in this case images) with other media types will transfer to pairings of pairings of that type with further types. There’s no need for specific training for each pairing.\n\nHow it works:ImageBind uses a separate transformer to embed each media type with one exception: The transformer that processes images handles video as well by treating a video as a two-frame image (sampled from the video).\n\nResults:The authors use a method similar toCLIPto classify data using ImageBind. For example, using theClothotest set of roughly 1,000 audio and text descriptions, ImageBind compared the embedding of a description with the embedding of every audio clip and returned the most similar audio clip. ImageBind returned the correct audio clip 6 percent of the time, whereasAVFIC, which learned using pairs of audio and text, returned the correct audio clip 3 percent of the time. However, ImageBind did not match supervised learning.ARNLQ, a supervised model, returned the correct audio 12.6 percent of the time.\n\nWhy it matters:The authors’ approach acts as an upgrade for models that generate similar embeddings for examples that have similar meanings in different media: To enhance the model’s repertoire with a new data type (say, audio), simply fine-tune it on relevant paired data (such as image, audio).\n\nWe’re thinking:ImageBind shows that machine learning models don’t need to learn from all pairs of data types to produce similar embeddings among various data types. Still, we can’t help but wonder how much its performance would improve if it did learn from other pairings, like (text, audio).",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--50-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--91-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--51-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/09/The-Batch-ads-and-exclusive-banners--53-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--92-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--93-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-216/",
    "title": "issue 216",
    "date": "",
    "reading_time": "",
    "content": "As you can read below, improvements in chatbots have opened a market for bots integrated with dating apps. I’m excited about the possibilities for large language models (LLMs) in romantic relationships, but I’m concerned that AI romantic partners create fake relationships that displace, rather than strengthen, meaningful human relationships. In my recent Stanford presentation on “Opportunities in AI,” I mentioned that AI Fund has been working with Renate Nyborg to deliver romantic mentoring. I’d like to explain why, despite my concern, I believe that AI can help many people with relationships.By 2020, it was clear that a change was coming in how we build natural language processing applications. As IwroteinThe Batchthat September, “GPT-3 is setting a new direction for building language models and applications. I see a clear path toward scaling up computation and algorithmic improvements.” Today, we’re much farther down that path.I didn't know back then that ChatGPT would go viral upon its release in November 2022. But AI Fund entrepreneurs were already experimenting with GPT-3, and we started looking for opportunities to build businesses on it. I had read the academic work aboutquestions that lead to love. I believe that you don’t find a great relationship; you create it. So instead of trying to help you find a great partner — as most dating apps aim to do — why not use AI to help people create great relationships?\n\nI’m clearly not a subject-matter expert in relationships (despite having spent many hours on eHarmony when I was single)! So I was fortunate to meet Renate, former CEO of Tinder, and start working with her on what became Meeno (formerly Amorai). Although we started exploring these ideas before ChatGPT was released, the wave of interest since then has been a boon to the project.\n\nRenate has far more systematic knowledge about relationships than anyone I know. With AI Fund’s LLM expertise and her relationship expertise (though she knows a lot about AI, too!), Her team built Meeno, a relationship mentor that is helping people improve how they approach relationships.\n\nMeeno is not a synthetic romantic partner, like in the movieHer. Instead, its goal is to be like the mentor rat inRatatouille: It assists individuals in building better relationships. If a user asks Meeno how to handle a breakup, it responds with advice about communicating honestly, empathetically, and clearly. After using it for a while, hopefully, users no longer will need guidance.I’m excited about Meeno for a few reasons. I have been concerned for some time about the “synthetic boyfriend/girlfriend” industry, where chatbots act like someone’s relationship partner, and then sometimes manipulate people’s emotions for profit in ways that I find deeply troubling (such as offering racy pictures for a fee). Social media, and TV before it, consumes enormous amounts of time that people otherwise might spend building interpersonal relationships. This makes me worry about synthetic romantic partners displacing real ones.The U.S. Surgeon General has raised the alarm about anepidemicof loneliness and isolation. Loneliness is as bad for a person assmoking15 cigarettes a day. It’s linked to significantly worse physical and mental health and to premature death. I hope Meeno will have a positive impact on this problem.Meeno’s journey is still in its early stages. You can read more about ithere.\n\nKeep learning!Andrew\n\nP.S. AI-savvy programmers are coding very differently than they did a year ago: They’re using large language models to help with their work. You’ll learn many of the emerging best practices in “Pair Programming with a Large Language Model,” taught by Laurence Moroney, AI Advocacy Lead at Google and instructor of our TensorFlow Specializations. This short course covers using LLMs to simplify and improve your code, assist with debugging, and minimize technical debt by having AI document and explain your code while you write it. This is an important shift in programming that every developer should stay on top of. Please check out the coursehere.\n\nChatGPT is going multimodal with help from DALL·E.What’s new:ChatGPT is being geared to accept voice input and output, OpenAIannounced. It will also accept and generate images, thanks tointegrationwith DALL·E 3, a new version of the company’s image generator.How it works:The updates expand ChatGPT into a voice-controlled, interactive system for text and image interpretation and production. New safety features are designed to protect legal rights of artists and public figures.\n\nYes, but:OpenAI said the new voice and image capabilities are limited to the English language. Moreover, the ability to understand and generate highly technical images is limited.\n\nBehind the news:OpenAI introduced GPT-4 in March with a demo that translated a napkin sketch of a website into code, but Google was first to make visual input and output to a large language model widely available. Google announced visual features at May’s Google I/O conference and the public could use them by midsummer.\n\nWhy it matters:ChatGPT has already redefined the possibilities of AI among the general public, businesses, and technical community alike. Voice input opens a world of new applications in any setting where English is spoken, and the coupling of language and vision is bound to spark applications in the arts, sciences, industry, and beyond. DALL·E 3’s safety features sound like an important step forward for image generation.We’re thinking:The notion of generative models that \"do everything\" has entered the public imagination. Combining text, voice, and image generation is an exciting step in that direction.\n\nAI and dating may be a match made in heaven.\n\nWhat’s new:Several new apps put deep learning at the center of finding a mate,Bloombergreported. Some provide chatbot surrogates while others aim to offer matches.\n\nAutomated wingmates:The reporter tested four apps, each of which targets a different aspect of budding romance.\n\nBehind the news:While dating keeps humans in the loop, some chatbots are designed to replace human interaction entirely. For instance,AnimaandRomantic AIoffer virtual romantic partners.Replika, an earlier virtual companion service built by the developers of Blush, went platonic in March but shortly afterward re-enabled erotic chat for customers who had signed up before February.\n\nWhy it matters:Romance has evolved with communications technology, from handwritten letters to dating apps. Ready or not, AI has joined the lover’s toolkit. For users, the reward may be a lifetime mate. For entrepreneurs, the prize is access to a marketworth$8 billion and growing at over 7 percent annually.\n\nWe’re thinking:AI has beneficial uses in dating, but users may form emotional bonds with chatbots that businesses then exploit for financial gain. We urge developers to design apps that focus on strengthening human-to-human relationships.\n\nLearn how to prompt a large language model to improve, debug, and document your code in a new short course taught by Google AI Advocacy Lead Laurence Moroney, instructor of our Tensor Flow Specializations.Sign up for free\n\nHaving broken the ice around chat-enabled web search, Microsoft has extended the concept to coding, office productivity, and the operating system itself.What’s new:Microsoftrefreshedits Copilot line of chatbots, adding new features, renaming old ones, and unifying the brand into what it calls an “everyday AI companion.”How it works:Microsoft offers Copilots for its subsidiary GitHub, Microsoft 365, and Windows.\n\nBehind the news:The emergence of ChatGPT set off aracebetween Microsoft and Alphabet to integrate large language models into search and beyond. Microsoft seized the day in early February when it launched a version of its Bing search engine that incorporated OpenAI’s technology, and its Copilot strategy has extended that lead. But Alphabet is nipping at Microsoft’s heels. It’sbringingits Bard chatbot to Google productivity apps, from email to spreadsheets.Why it matters:The combination of large language models and productivity software is a significant step. Microsoft’s approach seems likely to inspire millions of people who have never written a macro or opened the command line to start prompting AI models.We’re thinking:Copilot is a great concept. It helped make software engineers early adopters of large language models — for writing code, not prose.\n\nGoogle used DeepMind algorithms to dramatically boost energy efficiency in its data centers. More recent work adapts its approach to commercial buildings in general.\n\nWhat’s new:Jerry Luo, Cosmin Paduraru, and colleagues at Google and Trane Technologies built a [model]https://arxiv.org/abs/2211.07357that learned, via reinforcement learning, to control the chiller plants that cool large buildings.\n\nKey insight:Chiller plants cool air by running it past cold water or refrigerant. They’re typically controlled according to heuristics that, say, turn on or off certain pieces of equipment if the facility reaches a particular temperature, including constraints that protect against damaging the plant or exposing personnel to unsafe conditions. A neural network should be able to learn more energy-efficient strategies, but it must be trained in the real world (because current simulations don’t capture the complexity involved) and therefore it must adhere rigorously to safety constraints. To manage safety, the model can learn to predict the chiller plant’s future states, and a hard-coded subroutine can deem them safe or unsafe, guiding the neural network to choose only safe actions.\n\nHow it works:The authors built separate systems to control chiller plants in two large commercial facilities. Each system comprised an ensemble of vanilla neural networks plus a safety module that enforced safety constraints. Training took place in two phases. In the first, the ensemble trained on data produced by a heuristic controller. In the second, it alternated between training on data produced by itself and the heuristic controller.\n\nResults:Alternating with the heuristics controller for three months in the two buildings, the authors’ method achieved energy savings of 9 percent and 13 percent, respectively, relative to the heuristic controller. Furthermore, the system made the chiller plants more efficient in interesting ways. For example, it learned to produce colder water, which consumed more energy up front but reduced the overall consumption.\n\nYes, but:The environment within the buildings varied over the three-month period with respect to factors like temperature and equipment performance. This left the authors unable to tell how much improvement to attribute to their system versus confounding factors.\n\nWhy it matters:Using reinforcement-learning algorithms to control expensive equipment requires significant domain expertise to account for variables like sensor calibration, maintenance schedules, and safety rules. Working closely with domain experts when applying such algorithms can maximize both efficiency and safety.\n\nWe’re thinking:Deep learning is cooler than ever!\n\nAccording toForbes, between 70 percent and 95 percent of enterprises are failing in their business transformations. Skills gaps are a major cause. Ournew guidetells leaders how to avoid this trap. Read it and get your business transformation on track!\n\nGoogle partners with Howard University on Project Elevate Black VoicesSpeakers of African-American English (AAE) are not well-served by existing speech recognition technology, and often have to code-switch to be understood by voice interfaces. Google Research and Howard will team technologists with ethnolinguists to better understand both the limits of Google's current voice data and issues of power and pragmatics in language. Howard will own the new data set. (Google)\n\nAuthors' Guild sues OpenAI for copyright infringementMore than a dozen authors, including John Grisham, Jodi Picoult, and Jonathan Franzen, sued the makers of ChatGPT for training models on their in-copyright books without permission. Another lawsuit by the Authors' Guild against Google Books in 2005 led to a rejected 2010 settlement and a landmark 2015 fair use decision. (The New York Times)\n\nThe Commonwealth of Pennsylvania teams with Carnegie Mellon to develop automation technologies for state governmentGovernor Josh Shapiro announced his administration would convene a governing board, publish principles on the use of AI, and develop training programs for state employees. “We don’t want to let AI happen to us,” Shapiro said. “We want to be part of helping develop AI for the betterment of our citizens.” (Associated Press)\n\nLenovobrings AI to the edge for businesses with limited resourcesThe computer maker touted its hardware and software solutions for industries as diverse as commercial fishing, retail, and manufacturing. The goal is to offer computer vision, audio recognition, prediction, security, and virtual assistants for systems without requiring much computing power or programming skills. (Lenovo)\n\nIntel partners with Stability AI to build a supercomputerThe chipmaker looks to catch up to NVIDIA and other competitors with its newest datacenter and PC designs. CEO Pat Gelsinger argued that AI was central to the “Siliconomy,” a “growing economy enabled by the magic of silicon and software.” (Intel)\n\nAmazon adds more generative AI to AlexaNew Echo smart home devices will be powered by a new large language model. Amazon claims Alexa will be more conversational, more nuanced in its understanding of language, and more proactive in its responses to changing conditions. Former Microsoft product chief Panos Panay will be responsible for the next iteration of Echo and Alexa, replacing outgoing product head Dave Limp. (Amazon)\n\nBig Pharma embraces machine learning to guide and interpret human drug trials\"Companies such as Amgen, Bayer, and Novartis are training AI to scan billions of public health records, prescription data, medical insurance claims, and their internal data to find trial patients - in some cases halving the time it takes to sign them up.\" (Reuters)\n\nNew consumer tools still make a lot of mistakesGoogle said its Bard chatbot can summarize files from Gmail and Google Docs, but users showed it falsely making up emails that were never sent. OpenAI heralded its new Dall-E 3 image generator, but people on social media soon pointed out that the images in the official demos missed some requested details. And Amazon announced a new conversational mode for Alexa, but the device repeatedly messed up in a demo. including recommending a museum in the wrong part of the country. (Washington Post).",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/09/DATINGAI-1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/09/DALLE3.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/09/AIMATCH.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/09/image-5.png",
      "https://dl-staging-website.ghost.io/content/images/2023/09/COPILOT.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/09/COOLING.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/09/image-6.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-140/",
    "title": "issue 140",
    "date": "",
    "reading_time": "",
    "content": "With the pandemic easing in the United States and Canada, I’ve been traveling more in the last two weeks. I spoke atTED 2022in Vancouver andScaleUp:AIin New York and attended a manufacturing conference in California.What a pleasure it was to see people in 3D! In the days before Covid, serendipitous conversations were a large part of how I kept up with what’s happening in the world. I’ve really missed these meetings.It was great to hear former world chess champion and Russian dissident Garry Kasparovspeakand to chat with him afterward about Russia’s invasion of Ukraine. (I largely agree with his views.) I enjoyed conversing with astronautChris Hadfieldabout property rights on the moon, MIT professor Ariel Ekblaw aboutliving in space, and neuroscientistFrances Chanceabout when we might develop a theory of how the mind works. I saw AI artist Sophia Crespo present hergenerated creaturesand heard venture capitalistsGeorge MathewandLonne Jaffetalk about investing in AI startups.\n\nI found these conversations tremendously stimulating, and I came away thinking about some observations with respect to AI.\n\nGoing to these in-person events has me looking forward to a time, hopefully soon, when DeepLearning.AI and our ambassadors can hold more in-person events safely. I realize that the pandemic still varies widely in different regions. I hope you’ll enjoy reconnecting in person when it’s safe for you to do so, and benefit from the joyful conversations that contribute so much to learning.\n\nKeep learning!\n\nAndrew\n\nThe European Union approved for clinical use an AI system that recognizes normal chest X-rays.What’s new:ChestLink is the first autonomous computer vision system toearnthe European Economic Area’s CE mark for medical devices, which certifies that products meet government requirements for health and safety. The mark enables Oxipit, the Lithuanian startup that makes the system, to deploy it in the 27 E.U. countries plus Iceland, Liechtenstein, Norway, Switzerland, and Turkey.How it works:ChestLink uses a previous Oxipit product, ChestEye, to scan for 75 abnormalities such as edema and tuberculosis. If it finds none, it generates a medical report. Otherwise it forwards the image to a radiologist for review.\n\nWhy it matters:Reading X-ray images is highly subjective. Moreover, a radiologist’s judgment canvaryas fatigue sets in over the course of a working day. By identifying and reporting on normal images, this system could help radiologists focus on the cases that need the most attention.We’re thinking:Even the best AI systems for diagnosing chest X-raysfall shortof a board-certified radiologist’s accuracy. Training AI to recognize problem-free images, which are less ambiguous, is a clever approach.\n\nFace recognition is identifying people who have been killed, displaced, or recorded perpetrating alleged war crimes in Russia’s invasion of Ukraine.What’s new:Clearview AI, a startup that has beencriticizedfor harvesting online images without subjects’ permission, made its face recognition system freely available to the Ukrainian government,The New York Timesreported. Researchers unaffiliated with the Ukrainian government are using similar tools to analyze images of the conflict.How it works:Clearview has created 200 accounts at five Ukrainian agencies. Officials have used its app to conduct over 5,000 searches.\n\nYes, but:Face recognition can produce erroneous output. Amid military conflict, such errors — combined with wartime pressures — may cause people to be misidentified as war criminals, spies, or deceased.Behind the news:AI is being used to analyze a variety of data types flowing out of Ukraine.\n\nWhy it matters:The invasion of Ukraine — captured in an avalanche of photos, videos, aerial imagery, and radio transmissions shared on social media — is one of the most data-rich conflicts in history. Given this grim corpus, face recognition and other AI techniques can help to sketch a more complete picture of the battlefield.We’re thinking:The ability to unmask war criminals and thereby help bring them to justice offers solace amid unspeakable misery. We hope it also will deter other offenders. To recover from this tragedy will require still greater ingenuity and fortitude. We join the international community incallingon Vladimir Putin to withdraw Russian forces immediately.\n\nMany organizations embark on machine learning projects only to encounter roadblocks and eventually fail.Join usfor a live event on how to maximize your potential for success.\n\nMarketers are using fake social media personas — enhanced by AI-generated portraits — to expand their reach without busting their budgets.What’s new:Renee DiResta and Josh Goldstein at Stanford Internet Observatory combed LinkedIn and discovered over 1,000 fake profiles with false faces they believe to have been produced using generative adversarial networks, the radio network NPRreported.How it works:Companies hire independent marketers to expand their markets by messaging potential customers on social media. These marketers use fake profiles to send sales pitches. Responses are routed to a salesperson at the original company.\n\nSpot the fake:DiResta and Goldstein shared tips for recognizing forged LinkedIn profiles.\n\nWhy it matters:In the era of social media, companies have access to far more potential customers than their sales teams could possibly reach. This gives them ample incentive to look to AI for assistance. However, the risk of blowback for deceiving the public may outweigh the prospective gains.We’re thinking:Need we say it? Deceptive sales tactics are unacceptable no matter how cool your technology may be.\n\nGiven a sequence of numbers, neural networks haveprovenadept at discovering a mathematical expression that generates it. New work uses transformers to extend that success to a further class of expressions.What’s new:A team at Meta (formerly Facebook) led by Stéphane d’Ascoli and Pierre-Alexandre Kamienny introducedDeep Symbolic Regression, training models to translate integer and float sequences to mathematical expressions. Unlike earlier work, their approach is able to find functions in which terms in a sequence depend on previous terms (such as the Fibonacci sequence un= un-1+ un-2). You can try out an interactive demohere.Key insight:Transformersexcel at learning underlying patterns in natural language. Converting a sequence of numbers into a mathematical expression is analogous to translating one natural language into another.How it works:Given a sequence of numbers, a transformer learned to generate a function made up of operators (such as add, multiply, modulo, and square root), constants, the index of the term to be computed, and references to previous terms.\n\nResults:The authors compared their symbolic approach with a numeric model (a transformer trained to predict the next 10 terms in a sequence). Generating expressions of up to 10 operators that resulted in integer sequences, the symbolic model achieved 78.4 percent accuracy compared to the numeric model’s 70.3 percent. Generating expressions that resulted in float sequences — a more difficult task — the symbolic model achieved 43.3 percent accuracy compared to the numeric model’s 29 percent. The symbolic model also outperformedMathematica’s built-in methods for deriving functions from sequences, tested on sequences sampled from theOnline Encyclopedia of Integer Sequences(OEIS). Generating 10 terms that followed sequences of length 15, the numeric and symbolic models achieved accuracies of 27.4 percent and 19.2 percent respectively. Mathematica’sFindSequenceFunctionandFindLinearRecurrenceachieved 12 percent and 14.8 percent.Yes, but:To rule out arbitrary sequences such as the digits of pi, the authors selected OEIS sequences classified as easy; that is, results of expressions deemed easy to compute and understand. Finding expressions that yield more complicated sequences might strain the authors’ approach.Why it matters:Machine learning research struggles withabstractreasoningtasks. Mathematical symbols may be a piece of the solution.We’re thinking:2, 4, 6, 8, who do we appreciate? Transformers!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/04/Screen-Shot-2022-04-12-at-6.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/04/XRAY.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/04/FACEWAR.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/04/4.28_The-Batch-Image.png",
      "https://dl-staging-website.ghost.io/content/images/2022/04/DEEPFAKE--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/04/SYMBOLIC.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-241/",
    "title": "issue 241",
    "date": "",
    "reading_time": "",
    "content": "I think AI agent workflows will drive massive AI progress this year — perhaps even more than the next generation of foundation models. This is an important trend, and I urge everyone who works in AI to pay attention to it.\n\nToday, we mostly use LLMs in zero-shot mode, prompting a model to generate final output token by token without revising its work. This is akin to asking someone to compose an essay from start to finish, typing straight through with no backspacing allowed, and expecting a high-quality result. Despite the difficulty, LLMs do amazingly well at this task!\n\nWith an agent workflow, however, we can ask the LLM to iterate over a document many times. For example, it might take a sequence of steps such as:\n\nThis iterative process is critical for most human writers to write good text. With AI, such an iterative workflow yields much better results than writing in a single pass.\n\nDevin’s splashy demo recently received a lot of social media buzz. My team has been closely following the evolution of AI that writes code. We analyzed results from a number of research teams, focusing on an algorithm’s ability to do well on the widely used HumanEval coding benchmark. You can see our findings in the diagram below.\n\nGPT-3.5 (zero shot) was 48.1% correct. GPT-4 (zero shot) does better at 67.0%. However, the improvement from GPT-3.5 to GPT-4 is dwarfed by incorporating an iterative agent workflow. Indeed, wrapped in an agent loop, GPT-3.5 achieves up to 95.1%.\n\nOpen source agent tools and the academic literature on agents are proliferating, making this an exciting time but also a confusing one. To help put this work into perspective, I’d like to share a framework for categorizing design patterns for building agents. My team AI Fund is successfully using these patterns in many applications, and I hope you find them useful.\n\nNext week, I’ll elaborate on these design patterns and offer suggested readings for each.\n\nKeep learning!\n\nAndrew\n\nP.S. Build an optimized large language model (LLM) inference system from the ground up in our new short course “Efficiently Serving LLMs,” taught by Predibase CTO Travis Addair.\n\nSign up now!\n\nRobots equipped with large language models are asking their human overseers for help.\n\nWhat's new:Andrew Sohn and colleagues at CovariantlaunchedRFM-1, a model that enables robots to respond to instructions, answer questions about what they see, and request further instructions. The model is available to Covariant customers.\n\nHow it works:RFM-1 is a transformer that comprises 8 billion parameters. The team started with a pretrained large language model and further trained it, given text, images, videos, robot actions, and/or robot sensor readings, to predict the next token of any of those types. Images and videos are limited to 512x512 pixels and 5 frames per second.\n\nBehind the news:Covariant’s announcement follows a wave ofroboticsresearchinrecentyearsthat enables robots to take action in response totextinstructions.\n\nWhy it matters:Giving robots the ability to respond to natural language input not only makes them easier to control, it also enables them to interact with humans in new ways that are surprising and useful. In addition, operators can change how the robots work by issuing text instructions rather than programming new actions from scratch.\n\nWe're thinking:Many people fear that robots will make humans obsolete. Without downplaying such worries, Covariant’s conversational robot illustrates one way in which robots can work alongside humans without replacing them.\n\nSecurity researchers sounded the alarm about holes in Hugging Face’s platform.What’s new:Models in the Hugging Face open source AI repository can attack users’ devices,according tocybersecurity experts at JFrog, a software firm. Meanwhile, a different team discovered avulnerabilityin one of Hugging Face’s own security features.Compromised uploads:JFrog developed scanned models on Hugging Face for known exploits. They flagged around 100 worrisome models. Flagged models may have been uploaded by other security researchers but pose hazards nonetheless, JFrog said.\n\nMalicious mimicry:Separately, HiddenLayer, a security startup,demonstrateda way to compromiseSafetensors, an alternative to Pickle that stores data arrays more securely. The researchers built a malicious PyTorch model that enabled them to mimic the Safetensors conversion bot. In this way, an attacker could send pull requests to any model that gives security clearance to the Safetensors bot, making it possible to execute arbitrary code; view all repositories, model weights, and other data; and replace users’ models.\n\nBehind the News:Hugging Face implements a variety of security measures. In most cases, it flags potential issues but does not remove the model from the site; users download at their own risk. Typically, security issues on the site arise when users inadvertently make their own information available. For instance, in December 2023, Lasso Securitydiscoveredavailable API tokens that afforded access to over 600 accounts belonging to organizations like Google, Meta, and Microsoft.\n\nWhy it matters:As the AI community grows, AI developers and users become more attractive targets for malicious attacks. Security teams have discovered vulnerabilities in popular platforms, obscure models, and essential modules like Safetensors.\n\nWe’re thinking:Security is a top priority whenever private data is concerned, but the time is fast approaching when AI platforms, developers, and users must harden their models, as well as their data, against attacks.\n\nIn our new short course “Efficiently Serving Large Language Models,” you’ll pop the hood on large language model inference servers. Learn how to increase the performance and efficiency of your LLM-powered applications!Enroll today\n\nSynthetic depictions of politicians are taking center stage as the world’s biggest democratic election kicks off.\n\nWhat’s new:India’s political parties haveembracedAI-generated campaign messages ahead of the country’s parliamentary elections, which will take place in April and May,Al Jazeerareported.\n\nHow it works:Prime Minister Narendra Modi, head of the ruling Bharatiya Janata Party (BJP), helpedpioneerthe use of AI in campaign videos in 2020. They’ve become common in recent state elections.\n\nMeanwhile, in Pakistan:Neighboring Pakistan was deluged with deepfakes in the run-up to its early-February election. Former prime minister Imran Khan, who has been imprisoned oncontroversialcharges since last year,communicatedwith followers via a clearly marked AI-generated likeness. However, he found himself victimized by deepfakery when an AI-generated likeness of him, source unknown,urgedhis followers to boycott the polls.\n\nBehind the news:Deepfakes have proliferated in India in the absence of comprehensive laws or regulations that govern them. Instead of regulating them directly, government officials have pressured social media operators like Google and Meta to moderate them.\n\nWhat they’re saying:“Manipulating voters by AI is not being considered a sin by any party,” an anonymous Indian political consultant toldAl Jazeera. “It is just a part of the campaign strategy.”\n\nWhy it matters:Political deepfakes are quickly becoming a global phenomenon. Parties fromArgentina,the United States, andNew Zealandhave distributed AI-generated imagery or video. But the sheer scale of India’s national election — in which more than900 million peopleare eligible to vote — has made it an active laboratory for synthetic political messages.\n\nWe’re thinking:Synthetic media has legitimate political uses, especially in a highly multilingual country like India, where it can enable politicians to communicate with the public in a variety of languages and dialects. But unscrupulous parties can also use it to sow misinformation andunderminetrust in politicians and media. Regulations are needed to place guardrails around deepfakes in politics. Requiring identification of generated campaign messages would be a good start.\n\nResearch aims to help users select large language models that minimize expenses while maintaining quality.\n\nWhat's new:Lingjiao Chen, Matei Zaharia, and James Zou at Stanford proposedFrugalGPT, a cost-saving method that calls pretrained large language models (LLMs) sequentially, from least to most expensive, and stops when one provides a satisfactory answer.\n\nKey insight:In many applications, a less-expensive LLM can produce satisfactory output most of the time. However, a more-expensive LLM may produce satisfactory output more consistently. Thus, using multiple models selectively can save substantially on processing costs. If we arrange LLMs from least to most expensive, we can start with the least expensive one. A separate model can evaluate its output, and if it’s unsatisfactory, another algorithm can automatically call a more expensive LLM, and so on.\n\nHow it works:The authors used a suite of 12 commercial LLMs, a model that evaluated their output, and an algorithm that selected and ordered them. At the time, the LLMs’ costs (which are subject to change) spanned two orders of magnitude: GPT-4 cost $30/$60 per 1 million tokens of input/output, while GPT-J hosted by Textsynth cost $0.20/$5 per 10 million tokens of input/output.\n\nResults:For each of the three datasets, the authors found the accuracy of each LLM. Then they found the cost for FrugalGPT to match that accuracy. Relative to the most accurate LLM, FrugalGPT saved 98.3 percent, 73.3 percent, and 59.2 percent, respectively.\n\nWhy it matters:Many teams choose a single model to balance cost and quality (and perhaps speed). This approach offers a way to save money without sacrificing performance.\n\nWe're thinking:Not all queries require a GPT-4-class model. Now we can pick the right model for the right prompt.\n\nFind more AI news of the week inData Points, including:\n\n◆ AI takes over hockey and racing◆ An advanced brain surgery assistant◆ Google’s measures for the Indian General Election◆ OpenAI’s licensing agreement with Le Monde and Prisa\n\nRead Data Points now.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/03/unnamed--55-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-20T163430.388.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/03/BACKDOOR2.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/03/The-Batch-ads-and-exclusive-banners---2024-03-18T100446.882.png",
      "https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-20T163811.726.png",
      "https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-20T163859.691.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-20/",
    "title": "issue 20",
    "date": "",
    "reading_time": "",
    "content": "Happy New Year!Every winter holiday, I pursue a learning goal around a new topic. In between visits with family, I end up reading a lot.About a decade ago, my holiday topic was pedagogy — I still remember lugging a heavy suitcase of books through the airport — and this helped the early days of Coursera. Last year, before Nova’s birth, I read a pile of books on child care.This holiday, I’ve been catching up on epigenetics and the emerging science (and sometimes quackery) of anti-aging.\n\nI also visited my 101-year-old grandfather. I told him what I was reading, and he said that remaining curious is the key to longevity.If he’s right, then I think many of you will thrive well past 101!Wishing you a wonderful 2020, with lots of curiosity, learning, and love.\n\nKeep learning!\n\nAndrew\n\nWe enter a new decade with great expectations of prosperity, as machine learning finds its place in traditional industries from manufacturing to the arts. Yet we face important questions about how to use it without causing harm through careless data collection, slipshod system design, or the limits of our ability to see around the next corner. In this special issue ofThe Batch, some of the brightest lights in AI express their hopes for 2020.\n\nWe’ve had great success with supervised deep learning on labeled data. Now it’s time to explore other ways to learn: training on unlabeled data, lifelong learning, and especially letting models explore a simulated environment before transferring what they learn to the real world. In 2020, I hope to see more research in those areas.\n\nHigh-fidelity simulation lets us train and test algorithms more effectively, leading to more robust and adaptive networks. Models can gain far more experience in the virtual world than is practical in the real world. We can simulate rare events that pose severe challenges but are seldom represented by ground truth.\n\nFor instance, when we’re driving a car, accidents are rare. You won’t see all the variations even if you drive hundreds of thousands of miles. If we train autonomous cars only on real-world data, they won’t learn how to manage the wide variety of conditions that contribute to accidents. But in a simulation, we can generate variation upon variation, giving the model a data distribution that better reflects real-world possibilities, so it can learn how to stay safe.\n\nLately, simulation has helped achieve impressive results in reinforcement learning, which is extremely data-intensive. But it’s also useful in supervised learning, when researchers may have only small amounts of real-world data. For instance, earthquakes are rare and difficult to measure. But researchers at Caltech’s seismology lab used a simple physical model to create synthetic data representing these events. Trained on synthetic data, their deep learning modelachievedstate-of-the-art results predicting properties of real-world earthquakes.\n\nAt Nvidia, we’ve developed powerful simulation platforms likeDrive Constellationfor autonomous vehicles andIsaacfor robotics. These open, scalable environments enable models to act in a photorealistic virtual world, complete with highly accurate physics.\n\nI hope that more AI scientists will come to recognize the value of training in simulated environments, as well as other techniques beyond supervised learning. That would make 2020 a year of great progress in AI.\n\nAnima Anandkumar is director of machine learning research at Nvidia and a professor of computer science at Caltech.\n\nIn 2020, I hope the AI community will grapple with issues of fairness in ways that tangibly and directly benefit disadvantaged populations.We’ve spent a lot of time talking about fairness and transparency in our algorithms, and this is essential work. But developing software tools that have a tangible impact is where the rubber meets the road. AI systems designed to improve people’s lives could help solve some of society’s major challenges.Imagine what it’s like to use a smartphone navigation app in a wheelchair — only to encounter a stairway along the route. Even the best navigation app poses major challenges and risks if users can’t customize the route to avoid insurmountable obstacles.\n\nTechnology exists to support people with limited mobility, includingAccessMap, a project of the University of Washington’s Taskar Center for Accessible Technology. But we could do so much more. Thankfully, we are living in a time when we have the means to do it at our fingertips.Accessibility, education, homelessness, human trafficking — AI could have a major positive impact on people’s quality of life in these areas and others. So far, we’ve only scratched the surface. Let’s dig deep in the coming year.\n\nOren Etzioni is chief executive of the Allen Institute for AI, a professor of computer science at the University of Washington, and a partner at Madrona Venture Group.\n\nMany people in the AI community focus on achieving flashy results, like building an agent that can win at Go or Jeopardy. This kind of work is impressive in terms of complexity. But it’s easy to forget another important axis of intelligence: generalization, the ability to handle a variety of tasks or operate in a range of situations. In 2020, I hope to see progress on building models that generalize.\n\nMy work involves using reinforcement learning to train robots that reason about how their actions will affect their environment. For example, I’d like to train a robot to perform a variety of tasks with a variety of objects, such as packing items into a box or sweeping trash into a dustpan. This can be hard to accomplish using RL.\n\nIn supervised learning, training an image recognizer on ImageNet’s 14 million pictures tends to result in a certain degree of generalization. In reinforcement learning, a model learns by interacting with a virtual environment and collecting data as it goes. To build the level of general skill we’re accustomed to seeing in models trained on ImageNet, we need to collect an ImageNet-size dataset for each new model. That’s not practical.\n\nIf we want systems trained by reinforcement learning to generalize, we need to design agents that can learn from offline datasets, not unlike ImageNet, as they explore an environment. And we need these pre-existing datasets to grow over time to reflect changes in the world, just as ImageNet has grown from its original 1 million images.\n\nThis is starting to happen. For example, robots can figure out how to use new objectsas toolsby learning from a dataset of their own interactions plus demonstrations performed by humans guiding a robot’s arm. We’re figuring out how to take advantage of data from other institutions. For instance, we collected adatasetof robots interacting with objects from seven different robot platforms across four institutions.\n\nIt’s exciting to see critical mass developing around generalization in reinforcement learning. If we can master these challenges, our robots will be a step closer to behaving intelligently in the real world, rather than doing intelligent-looking things in the lab.\n\nChelsea Finn is an assistant professor of computer science and electrical engineering at Stanford.\n\nHow is it that many people learn to drive a car fairly safely in 20 hours of practice, while current imitation learning algorithms take hundreds of thousands of hours, and reinforcement learning algorithms take millions of hours? Clearly we’re missing something big.\n\nIt appears that humans learn efficiently because we build a model of the world in our head. Human infants can hardly interact with the world, but over the first few months of life they absorb a huge amount of background knowledge by observation. A large part of the brain apparently is devoted to understanding the structure of the world and predicting things we can’t directly observe because they’re in the future or otherwise hidden.\n\nThis suggests that the way forward in AI is what I call self-supervised learning. It’s similar to supervised learning, but instead of training the system to map data examples to a classification, we mask some examples and ask the machine to predict the missing pieces. For instance, we might mask some frames of a video and train the machine to fill in the blanks based on the remaining frames.\n\nThis approach has been extremely successful lately in natural language understanding. Models such asBERT,RoBERTa,XLNet, andXLMare trained in a self-supervised manner to predict words missing from a text. Such systems hold records in all the major natural language benchmarks.\n\nIn 2020, I expect self-supervised methods to learn features of video and images. Could there be a similar revolution in high-dimensional continuous data like video?\n\nOne critical challenge is dealing with uncertainty. Models like BERT can’t tell if a missing word in a sentence is “cat” or “dog,” but they can produce a probability distribution vector. We don’t have a good model of probability distributions for images or video frames. But recent research is coming so close that we’re likely to find it soon.\n\nSuddenly we’ll get really good performance predicting actions in videos with very few training samples, where it wasn’t possible before. That would make the coming year a very exciting time in AI.\n\nYann LeCun is vice president and chief AI scientist at Facebook and a professor of computer science at New York University.\n\nArtificial intelligence has moved from the age of discovery to the age of implementation. Among our invested portfolios, primarily in China, we see flourishing applications using AI and automation in banking, finance, transportation, logistics, supermarkets, restaurants, warehouses, factories, schools, and drug discovery.\n\nYet, looking at the overall economy, only a small percentage of businesses is starting to use AI. There is immense room for growth.\n\nI believe that AI will be as important as electricity in the history of mankind’s technological advancement. In the next decade or two, AI will penetrate our personal and business lives, delivering higher efficiency and more intelligent experiences. It is time for businesses, institutions, and governments to embrace AI fully and move society forward.\n\nI am most excited about the impact of AI on healthcare and education. These two sectors are ready for AI disruption and can deploy AI for good.\n\nWe invested in a company that uses AI and big data to optimize supply chains, reducing medication shortages for over 150 million people living in rural China. We are also funding drug discovery companies that combine deep learning and generative chemistry to shorten drug discovery time by a factor of three to four.\n\nIn education, we see companies developing AI solutions to improve English pronunciation, grade exams and homework, and personalize and gamify math learning. This will free teachers from routine tasks and allow them to spend time building more inspirational and stimulating connections with up-and-coming generations of students.\n\nI hope to see more bright entrepreneurs and businesses start using AI for good in 2020 and years to come.\n\nKai-Fu Lee is chairman and chief executive of Sinovation Ventures.\n\nBillions of dollars invested to create novel AI hardware will bear their early fruit in 2020.\n\nGoogle unleashed a financial avalanche with its tensor processing unit in 2017. The past year saw specialized AI processors from Alibaba, Cerebras, Graphcore, Habana, and Intel, with many others in the pipeline. These new chips will find their way slowly into research labs and data centers. I hope the AI community will embrace the best of them, pushing the field toward better models and more valuable applications.\n\nHow can machine learning engineers know whether a newfangled alternative performs better than the conventional CPU-plus-GPUs combo?\n\nComputer architecture is graded on a curve rather than an absolute scale. To account for differing computer sizes, we normalize performance by price, power, or numbers of chips. Competitors select a set of representative programs to serve as a benchmark. Averaging scores across many of these programs is more likely to reflect real performance than scores on any single one.\n\nMLPerfis a recent benchmark for machine learning created by representatives from more than 50 companies and nine universities. It includes programs, data sets, and ground rules for testing both inference and training, specifying important details like the accuracy target and valid hyperparameter values. New versions occur every three months (alternating inference and training) to keep up with rapid advances in machine learning.\n\nNot every product can win a fair comparison, so some marketing departments may sidestep MLPerf, saying some version of, “Our customers don’t care about the programs in MLPerf.” But don’t be fooled. First, MLPerf welcomes new programs, so if a given workload isn’t in MLPerf, it can be added. Second, competitors check MLPerf results for fairness to ensure apples-to-apples comparisons.\n\nCaveat emptor. Ask to see MLPerf scores!\n\nDavid Patterson is a professor of computer science at UC Berkeley.\n\nIgnorance is a choice in the Internet age. Virtually all of human knowledge is available for the cost of typing a few words into a search box.\n\nBut managing the deluge of facts, opinions, and perspectives remains a challenge. It can be hard to know what information you’ll find in a lengthy document until you’ve read it, and knowing whether any particular statement is true is very difficult.\n\nAutomatic summarization can do a lot to solve these problems. This is one of the most important, yet least solved, tasks in natural language processing. In 2020, summarization will take important steps forward, and the improvement will change the way we consume information.\n\nThe Salesforce Research team recently took a close look at the field and published a paper thatevaluatesthe strengths and weaknesses of current approaches. We found that the datasets used to train summarizers are deeply flawed. The metric used to measure their performance is deeply flawed. Consequently, the resulting models are deeply flawed.\n\nWe’re working on solutions to these problems. For instance, researchers evaluate summarization performance using the ROUGE score, which measures overlap in words between source documents, automated summaries, and human-written summaries. It turns out that summarizers based on neural networks can make mistakes and still earn high ROUGE scores. A model can confuse the names of a crime’s perpetrator and its victim, for example. ROUGE measures the fact that the names appear in both generated and human-made summaries without taking the error into account.\n\nWe introduced amodelthat makes it easy to examine factual consistency between source documents and summaries. We also proposed a metric to evaluate summarizers for factual consistency. Ranking summarizers according to this metric in addition to ROUGE will help researchers develop better models, and that will speed progress in other areas, such as maintaining logical coherence throughout a long summary.\n\nThis kind of development gives me confidence that 2020 will be a great time for summarization, and for NLP in general. The progress I expect to see in the coming year will help people not only to cope with the ceaseless flood of new information, but also to embrace AI’s great potential to make a better world.\n\nRichard Socher is chief scientist at Salesforce.\n\nDatasets are critical to AI and machine learning, and they are becoming a key driver of the economy. Collection of sensitive data is increasing rapidly, covering almost every aspect of people’s lives. In its current form, this data collection puts both individuals and businesses at risk. I hope that 2020 will be the year when we build the foundation for aresponsible data economy.Today, users have almost no control over how data they generate are used. All kinds of data are shared and sold, including fine-grained locations, medical prescriptions, gene sequences, and DMV registrations. This activity often puts personal privacy and sometimes even national security at risk. As individuals become more aware of these issues, they are losing trust in the services they use.At the same time, businesses and researchers face numerous challenges in taking advantage of data. First, large scaledata breachescontinue to plague businesses. Second, with Europe’sGeneral Data Protection Regulation, California’sConsumer Privacy Act, and similar laws, it is becoming more difficult and expensive for businesses to comply with privacy regulations. Third, valuable data are siloed, impeding technical progress. For example, easier use of medical data across institutions for machine learning could lead to improvements in healthcare for everyone.Changing this broken system into a responsible data economy requires creating new technologies, regulations, and business models. These should aim to provide trustworthy protection and control to data owners (both individuals and businesses) through secure computation, the ability to audit, and machine learning that maintains data privacy. Secure computation can be provided by secure hardware (such asIntel SGXandKeystone Enclave) and cryptographic techniques. Those computations can be made auditable by tying encrypted storage and computation to a distributed ledger.Greater challenges remain on the machine learning side. In 2020, we can expand on current efforts in differentially private data analytics and machine learning, building scalable systems for practical deployment with large, heterogeneous datasets. Further research and deployment of federated learning also will be important for certain use cases. Finally, advances in robust learning from limited and noisy data could help enable a long tail of ML use cases without compromising privacy.We are building parts of this vision at Oasis Labs, but there is much more to be done. I hope this year that technologists, businesses, regulators, and the AI community will join us in building the foundation for a truly responsible data economy.\n\nDawn Song is chief executive and co-founder of Oasis Labs and a professor of computer science and electrical engineering at UC Berkeley.\n\nI have three hopes for 2020:\n\nZhi-Hua Zhou is a professor of computer science and artificial intelligence at Nanjing University.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-12-3120at202.29.5920PM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Anima20Anandkumar20SIZED-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Oren20Etzioni20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Chelsea20Finn20SIZED-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Yann20LeCun20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Kai-Fu20Lee20SIZED-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/DavidPatterson220ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Richard20Socher20SIZED-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Dawn20Song20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Zhi-Hua20Zhou20SIZED-1.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-170/",
    "title": "issue 170",
    "date": "",
    "reading_time": "",
    "content": "The economic downturn of the past six months has hit many individuals and companies hard, and I’ve written about the impact of risinginterestrateson AI. The effects of high inflation, the Russian war in Ukraine, and an economic slowdown in China are rippling across the globe. Even though unemployment in the U.S. is low, within the tech world, I continue to hear things that point to the possibility that we might go through a challenging time for many months to come.\n\nThe layoffs at Twitter and Meta are well publicized. Anecdotally, I’ve heard many worrisome stories: Students are having a hard time finding internships for next summer, entrepreneurs are having greater difficulty raising capital, companies are freezing hiring and reducing headcount, and employees are facing effective pay cuts as falling share prices reduce the value of their stock-based compensation. Some managers have told me they want to preserve their machine learning teams — which they hired with great difficulty — but the tech market has cooled and likely will take a while to pick up.\n\nWhat can we do amid the turmoil? Even if the tech world slumps, the long-term value of AI is still clear to me, and it’s worth lifting our eyes toward the future to position ourselves for the eventual rebound.\n\nI’d like to draw attention to three investments that I believe will retain or increase their value even in uncertain times. If you’re wondering where to put your effort, attention, or money, consider these areas:\n\nWhether or not the economic downturn affects you, I’m here to support you. As we sail through a potentially tough time in the coming months, remember that the long-term impact of AI has been and will continue to be huge. Let’s keep helping each other and investing in things that will make us stronger for when the world exits its current slump.\n\nKeep learning!\n\nAndrew\n\nFarming shrimp in an open pond produces toxic effluent that can pollute groundwater and coastal waters. An AI-driven farm in a box may offer a more sustainable alternative.\n\nWhat’s new:Based in Mexico City, Atarraya modifies shipping containers into AI-controlled tanks for raising commercial shrimp,Fortunereported. The company plans to install 20 units in a warehouse in Indianapolis.\n\nHow it works:The company’s Shrimpboxcontainstwo large water tanks equipped with sensors that track pH, nutrients, chemicals, and temperature. Machine learning models automatically dispense food and adjust conditions as needed.\n\nBehind the news:The seafood industry is using AI to reduce its environmental footprint in a variety of ways.\n\nWhy it matters:If it can scale, Shrimpbox addresses several pain points in aquaculture. Aquaculture can put a dent inoverfishing, which threatens wild fish populations worldwide. Growing seafood in tanks rather than open water won’t leach waste, antibiotics, and other chemicals into the surrounding environment. And containerized tanks can enable food to be grown near where it will be consumed, which eliminates the need to transport it long distances.\n\nWe’re thinking:The shrimp are just prawns in this company’s game.\n\nUnited States colleges tracked activists using a natural language processing system intended to monitor their mental health.\n\nWhat’s new:An investigation byThe Dallas Morning Newsand UC Berkeley Graduate School of Journalismfoundthat schools in Georgia, North Carolina, and elsewhere used Social Sentinel, which monitors social media posts to identify individuals who intend to harm themselves or others, to keep tabs on protestors from 2015 to 2019 and possibly beyond.What they found:The system, which was renamed Navigate360 Detect in 2020, uses an “evolving AI language engine” toanalyzepublic communications. Users can query social media posts to Facebook, Instagram, Reddit, Twitter, and YouTube, although searches are limited to eight topics and 25 subtopics related to safety and security. The reporters studied documents acquired through leaks and requests to the government along with interviews with school employees. Among their findings:\n\nThe response:Navigate360, the Ohio-based company that acquired Social Sentinel in 2020, stated that the investigation was inaccurate and that the word “protest” was not in the system’s list of search topics. School officials didn’t respond to the reporters’ requests for comment and declined to discuss policies that govern their use of such software.\n\nWhy it matters:Campuses must tread a line between keeping students safe and hosting free expression. Protests can spiral out of control, causing injury and loss of life. Yet students have a reasonable expectation that educational institutions have their best interests at heart and will support their intellectual inquiries — even if they lead to peaceful protests.We’re thinking:AI can do good by alerting school officials to students who are severely disturbed or distressed. It should go without saying that systems designed for this purpose should never be used to stifle dissent.\n\nGain the skills to thrive in an uncertain economy! Companies are seeking qualified professionals who can tap AI’s potential. Break into AI with the newMachine Learning Specialization, an updated program for beginners created by Andrew Ng.Learn more\n\nIn a new report, business leaders share their machine-learning successes and struggles.\n\nWhat’s new:Many businesses plan to increase their use of machine learning, but their efforts so far don’t always yield the results they seek, according to astudyperformed by the market analyst Forrester and commissioned by the bank Capital One.Machine learning on the rise:The authors surveyed 150 “data-management decision-makers” who work for North American companies in banking, information technology, manufacturing, and retail about how their organizations have used — and hope to use — machine learning.\n\nRoom for improvement: The respondents also outlined several worries.\n\nBehind the news:The talent shortage in machine learning and data science is well documented. A 2020 Deloitte surveyfoundthat companies across all industries struggled to find the machine learning engineers that would help them meet their business goals. Some companies offer incentives toattractpeople skilled in AI, such as offering remote work at Silicon Valley pay rates and providing time off to pursue personal projects.\n\nWhy it matters:Machine learning continues to expand in mainstream businesses, and with it opportunities for machine learning engineers and data scientists. An earlier Forresterstudyfound that business leaders who see clear value in AI are (a) using or expanding their use of the technology and (b) effectively using the resulting insights to drive their business strategies. The new report shows that they believe the potential is greater still — and that bringing more machine learning engineers onboard could make the difference.We’re thinking:Many industries are still figuring out how to get the most out of AI. If you can make its value clear to executives in your organization — one of the top issues in this study — you can play a big role in moving things forward.\n\nThe route to improving transformer-based language models likeGPT-3andGopher, which are trained on immense quantities of text scraped from the web, has been to increase their size. But research into the relationship between dataset size and parameter count shows that, given a processing budget, bigger doesn’t necessarily mean better.\n\nWhat’s new:Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, and colleagues at DeepMind determined theoptimal data-to-parameter ratiofor a range of processing budgets. They used this knowledge to train Chinchilla, a smaller but higher-performance version of Gopher.\n\nKey insight:Pumping up dataset and architecture sizescanimprovethe performance of language models (with diminishing returns as they increase). But past studies didn’t account for the impact of the number of training tokens (the number of training steps multiplied by the number of tokens per step) or the learning rate. A systematic study of these variables makes it possible to estimate the optimal model and data size for a particular processing budget.\n\nHow it works:The authors trained and tested hundreds of transformer-based language models using various combinations of parameter count, dataset size, training token count, and learning rate. They trained the models to complete sentences in2.35 billion documentsscraped from the web.\n\nResults:Doubling parameters or training tokens requires quadrupling the processing budget to reach optimal performance. In other words, if you double a model’s parameter count, doubling the number of training tokens will achieve an optimal balance between processing and performance. Given Gopher’s processing budget, Chinchilla outperformed its predecessor on several benchmarks with a quarter of its parameters. OnBIG-bench, for example, Chinchilla’s average accuracy was 65.1 percent compared to Gopher’s 54.4 percent. In reading comprehension onLAMBADA, in which the model answers a question after reading a piece of text, Chinchilla attained 77.4 percent accuracy while Gopher achieved 74.5 percent andMegatron-Turing NLG, with a whopping 530 billion parameters, achieved 76.6 percent.\n\nWhy it matters:Large models like Gopher aren’t reaching their full potential. Smaller models trained on more training tokens can run faster during inference and achieve better performance.\n\nWe’re thinking:In light of this work, a monster model like Megatron-Turing NLG 530B should train on 11 trillion tokens. All the text on the web encompasses only a couple trillion!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--4--4.png",
      "https://dl-staging-website.ghost.io/content/images/2022/11/SHRIMPBOX_600px.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--8-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/11/Banner-MLS--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--9-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--10-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-x/",
    "title": "issue x",
    "date": "",
    "reading_time": "",
    "content": "David Rolnick led an effort to compile a list of ways machine learning can impact climate change, resulting inthis arXiv paper. (I’m a co-author.) Climate change is one of the most important problems of our time, and we can make a difference!I spent most of Saturday at the self-supervised learning workshop, where I’m seeing exciting progress in unsupervised learning from images and video. In natural language processing, we’ve already seen how word embeddings can be learned by getting a neural network to predict the next word in a sequence. I saw a lot of papers that built on Aaron van den Oord et al.’sContrastive Predictive Coding, and multiple authors obtained promising results in learning representations of images from unlabeled data.Multiple teams are still hitting data and compute scalability issues, but I’m excited about the self-supervised learning research direction and hope more people jump into it.Keep learning,Andrew\n\nDrone maker DJI released a toy robot tank that shoots splattering pellets, skitters on omnidirectional wheels, and learns to navigate the playroom battlefield.What’s new:Robomaster S1is a self-driving, bullet-firing toy capable of streaming its turret’s-eye view to your smartphone. And it's programmable. You can see it in actionhere.How it works:You can control the unit using a smartphone app, or let it find its own way around using its onboard navigation.\n\nMore than a toy:If you have the programming chops (or the desire to learn), you can use Python or Scratch 3.0 to teach the tank new maneuvers or subroutines for outsmarting opponents.Behind the news:Shenzhen-based DJI sponsors an annual tournament where teams of university students battle one another using projectile-shooting tanks, quadcopters, and rail-riding guard drones. The new toy is a consumer version of the tank bots featured in this popular competition.Our take:At $499, Robomaster S1 makes a pricey educational tool. You can buy an Arduino robot tank kit for a fraction of the cost. On the other hand, few Arduino kits come with a projectile-firing turret, and this thing looks like a lot more fun.\n\nU.S. representatives mulled over legal precedents for — and impediments to — regulating the realistic computer-generated videos known as deepfakes.What’s new:The House Intelligence Committee, worried about the potential impact of fake videos on the 2020 election,questionedexperts on AI law, policy, and technology. The panel laid to rest the lawmakers' fear that deepfake technology can fakeanybodydoinganything.But it highlighted just how easy it is to perpetrate digital hoaxes. The committee contemplated whether to prosecute programmers responsible for deepfake code, and whether regulating deepfakes would impinge on the constitutional right to lie (seriously).Backstory:Efforts to create realistic-looking video using AI date back to the 1990s. Recently, though, deepfakes have become more sophisticated. Days before the Congressional hearing, activists posted avideoof Mark Zuckerberg appearing to deliver a monologue, worthy of a James Bond villain, on the power he wields over popular opinion.Why it matters:Given the disinformation that swirled around the 2016 election, many experts believe that deepfakes pose a threat to democracy. However, regulating them likely would have a chilling effect on free speech, to say nothing of AI innovation.Legislative agenda:Congress is considering at least two bills targeting deepfakes.\n\nA bigger problem:Digital fakery isn’t just about people using neural networks to synthesize video and voices. Last week, security expertsspotteda bogus LinkedIn profile purporting to represent a foreign policy specialist, its portrait photo apparently fabricated by a generative adversarial network. Then there are simple tricks like theslo-moused to make Speaker of the House Nancy Pelosi appear to slur her words. Not to mention the disinformation potential of Photoshop. And the qwerty keyboard.Our take:After years of effort, social media platforms are still struggling to define what is and isn’t okay to share. Doing this in the context of deepfake technology won't be easy. House Intelligence Committee chair Adam Schiff (D-CA) hinted at tightening existing regulations — like Section 230 of the Communications Decency Act, which protects platforms from legal liability for content posted by users — to hold platforms accountable for what they publish. This could incentivize services like Facebook and YouTube to root out malicious fakery. But it could also restrict legitimate advocacy or satire. For now, consumers of digital media will have to think twice before believing what they see.\n\nTech giants have slowed down their hiring for pure AI research. But don’t let that cool your ambitions.What’s new:Journalist Kevin McLaughlin atThe Informationspoke with six industry insiders – including Facebook’s Yann LeCun — about hiring trends. His sources said their companies no longer are building huge research teams. Instead, they’re staffing up AI engineering and product development.Behind the news:The big AI companies were hiring researchers like there’s no tomorrow. Between 2016 and 2017, for instance, Microsoft’s researcher head count jumped from 5,000 to 8,000. Apparently, those efforts were successful enough that Microsoft and its peers have curbed their appetite for boundaries-pushing AI.The next wave:Corporate AI research may sound like its a victim of its own success, but that’s not the whole story. An insider at Google told McLaughlin that the lower hiring rates for AI researchers need to be kept in context: Google was binging so hard on AI talent that any slowdown is going to look like cold turkey. The company still has plenty of research going on, it’s just not expanding at the former explosive rate. Instead, it’s shifting AI brainpower from research to product.What they’re saying:“AI is in the process of maturing from academic and basic research, to niche applications, to wide deployment. As the field matures and as the tools become better, companies are massively increasing their investment in the engineering, development, tooling, and infrastructure related to AI.” — Yann LeCun, Facebook’s chief AI scientist, inThe Information.Our take:The majors may be seeing less need for research, but AI is moving from software into every industry, from medicine to agriculture to manufacturing. There's plenty of need for both researchers and product people, and that need will continue to grow.\n\nHow do you apply deep learning to everything from text and music synthesis to speech recognition? Learn how to build sequence models in the Deep Learning Specialization.Enroll now\n\nTypical deep reinforcement learning requires millions of training iterations as the network stumbles around in search of the right solution. New research suggests that AI can learn faster by considering its mistakes.What’s new:Previous work in reinforcement learning taught machines by having them either experiment until they got it right or imitate human demonstrations. Allan Zhou and his team at Google Brain merge the two approaches in an algorithm they callWatch, Try, Learn.Key insight:Unlike many RL models, WTL decides what actions to take by combining its own prior performance with human demonstrations as an explicit input. This allows it to correct its behavior quickly and minimize repeated blunders.How it works:WTL is a training algorithm rather than a specific model. It trains two separate policies, or predictions of what action will maximize its reward, called trial and retrial. The trial policy generates exploration attempts, while the retrial policy decides the best actions. These policies are trained on several tasks.\n\nWhy it matters:WTL can learn a greater number and variety of robotic object manipulation tasks than previous RL models. Further, previous models are limited to a single task, while WTL learns multiple tasks concurrently and outperforms single-task models in every task. This allows WTL to master new abilities in few-shot settings and with less computation.Takeaway:Who wants a robot helper that requires thousands of attempts to learn how to empty a dishwasher without breaking dishes? And, once it has learned, can’t do anything else? Zhou et al. raise the prospect that smarter, more flexible robots may be just around the corner — though they’re still bound to break a few dishes.\n\nNeuroscientists developed a system that, they say, can detect subtle signs of psychosis in conversational speech.What’s new:Researchers at Emory School of Medicine used machine learning to predict the onset of schizophrenia in a high-risk population with 80 percent accuracy. Their results were published in the journalnpj Schizophrenia.How it works:The researchers trained their neural network on thousands of conversations from Reddit to establish conversational norms and organize word vectors by usage.\n\nWhy It matters:Schizophrenia is a devastating condition that has no cure, but early detection can help people seek treatment before it becomes overwhelming.Takeaway:Methods exist to identify warning signs of schizophrenia in patients as young as 17, but only around 30 percent of these people eventually develop the disorder. Machine-learning techniques could help doctors spot the patients who really need help.\n\nImages produced by generative adversarial networks can be nearly indistinguishable from the real thing. If a picture is worth a thousand words, a GAN-made 3D representation could be worth a million.What’s new:Gadelha et al. at the University of Massachusetts Amherst developed a neural network,Projective Generative Adversarial Network, that generates 3D models of novel objects from any number of 2D silhouettes.Key insights:PrGAN’s discriminator relies on 2D images rather than 3D shapes.\n\nHow it works:All GANs have a generator that produces new output and a discriminator that learns to classify that output as real or fake. PrGAN has a generator that creates 3D shapes. Its discriminator compares 2D images of those shapes with real-world pictures. A projection component creates the 2D views.\n\nWhy it matters:The graphic design industry is looking toward deep learning to enhance productivity. PrGAN simplifies creation of 3D models from 2D images. This could be a great help to designers and game makers looking to generate 3D representations quickly.We’re thinking:Deep learning consumes huge amounts of data. PrGAN takes advantage of plentiful 2D images to produce 3D representations, which are far less common. This sort of data-set bootstrapping could be an alternative in situations where training data is scarce.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/6135c10a-78bf-4f8e-97eb-ec1d666c9798-1.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/7c80dbca-5e40-4d92-936e-4f24cce1ccf0.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/d68b5c0b-07b4-46d0-bd73-2cb93059084d.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/7c91b1fc-aea3-4d3f-bc72-b9f61f8fe563.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/aeb0f673-aaa9-45cd-a6c8-933b92ebd1ec.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/09/db9acd32-9e1f-4a1d-8b12-050022148bbf.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/c9cb6f47-df67-4932-9e45-b44498a9ae3f.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/c680f330-d8bd-4f4d-bcd3-a1107006f693.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-169/",
    "title": "issue 169",
    "date": "",
    "reading_time": "",
    "content": "A newreportfrom UN Climate Change says that the world might be on track for 2.5 °C of warming by the end of the century, a potentially catastrophic level of warming that’s far above the 1.5 °C target of the 2015 Paris Agreement. I think it is time to seriously consider a specific solution in which AI can play a meaningful role: Climate geoengineering viastratospheric aerosol injection.\n\nStratospheric aerosol injection involves spraying fine particles that reflect sunlight high in the atmosphere. By increasing the reflectivity (or albedo) of the planet, we can slow down the rate at which sunlight warms it, and thereby buy more time to reduce carbon emissions and develop mitigations. Harvard Professor David Keith explains the science behind this idea is in his book,A Case for Climate Engineering.\n\nAI will be important in this effort because:\n\nStratospheric aerosol injection has been criticized on the following grounds:\n\nAt the current 1.1 °C of warming, the world is already experiencing increased climate-related crises. My heart goes out to the millions whose lives have been disrupted by wildfires, flooding, hurricanes, and typhoons. Just weeks ago, a forest fire came within miles of my house, and area residents were told to be ready to evacuate, a first for me. (Fortunately, the fire has since been largely contained.) It terrifies me that on the planet’s current path, the past summer’s climate — the worst I’ve experienced — might be better than what my children and I will experience for the rest of our lives.\n\nNext week at the UN’s annual COP27 climate summit held in Egypt, government leaders will meet to discuss new agreements aimed at reducing atmospheric carbon emissions. While I hope that this meeting summons the global will to do what’s needed, I would rather count on engineers and scientists, not just politicians, to address the problem. Perhaps some of us in AI can make a critical difference.\n\nStay cool,\n\nAndrew\n\nThe generative gold rush is on.\n\nWhat’s new:Venture capitalists are betting hundreds of millions of dollars on startups that use AI to generate images, text, and more,Wiredreported.What’s happening:A handful of generative-AI startups have newly received nine-figure investments. They’re among over 140 nascent companies that aim to capitalize on applications in copywriting, coding, gaming, graphic design, and medicine, according to a growinglistmaintained by Stanford student David Song.\n\nBehind the news:Established companies, too, are looking for ways to capitalize on AI’s emerging generative capabilities.\n\nYes, but:Incumbents and class-action lawyers are lodging complaints over who owns what goes into — and what comes out of — models that generate creative works.\n\nWhy it matters:Despite ongoing chatter aboutAI winter, it’s springtime for generative AI. Founders, investors, and trade organizations alike believe that this emerging technology has the potential to create huge value.We’re thinking: Generative AI holds the spotlight, given the mass appeal of models that paint beautiful pictures in response to simple text prompts, but AI continues to advance in many areas that hold significant, unfulfilled commercial promise.\n\nAs the United States (along with several other countries) gears up for general elections, AI is helping campaigns attract voters with increasing sophistication.\n\nWhat’s new:Strategists for both major U.S. political parties are using machine learning to predict voters’ opinions on divisive issues and using the results to craft their messages,The New York Timesreported.\n\nHow it works:Consulting firms typically combine publicly available data (which might include voters’ names, ages, addresses, ethnicities, and political party affiliations) with commercially available personal data (such as net worths, household sizes, home values, donation histories, and interests). Then they survey representative voters and build models that match demographic characteristics with opinions on wedge issues such as climate change and Covid-19 restrictions.\n\nBehind the news:AI plays an increasing role in political campaigns worldwide.\n\nYes, but:Previous efforts to predict voter opinions based on personal data have been fraught with controversy. In the mid-2010s, for instance, political advertising startup Cambridge Analytica mined data illegally from Facebook users.\n\nWhy it matters:The embrace of machine learning models by political campaigns sharpens questions about how to maintain a functional democracy in the digital age. Machine learning enables candidates to present themselves with a different face depending on the voter’s likely preferences. Can a voter who’s inundated with individually targeted messages gain a clear view of a candidate’s positions or record?We’re thinking:Modeling of individual preferences via machine learning can be a powerful mechanism for persuasion, and it’s ripe for abuses that would manipulate people into voting based on lies and distortions. We support strict transparency requirements when political campaigns use it.\n\nReady to deploy your own diffusion model? Learn how to create machine learning applications using existing code in a free, hands-on workshop. Join us for “Branching out of the Notebook: ML Application Development with GitHub” on Wednesday, November 9, 2022!Register here\n\nNeural networks are helping humanitarian observers measure the extent of war damage to Ukraine’s grain crop.\n\nWhat’s new:Analysts from the Yale School of Public Health and Oak Ridge National Laboratorybuilta computer vision model that detects grain-storage facilities in aerial photos. Its output helped them identify facilities damaged by the Russian invasion.How it works:The authors started with a database of grain silos last updated in 2019. They used machine learning to find facilities missing from that survey or built since then.\n\nResults:Among 344 facilities, they found that 75 had suffered damage. They estimate that the destruction has compromised 3.07 million tons of grain storage capacity, nearly 15 percent of Ukraine’s total.\n\nWhy it matters:Before the war, Ukraine was the world’sfifth-largest wheat exporter. By disrupting this activity, the Russian invasion has contributed to aspikein global food prices, which observerswarnmay lead to famine. Understanding the scope of the damage to Ukraine’s grain supply could help leaders estimate shortfalls and plan responses.Behind the news:Machine learning has been applied to a variety of information in the war between Russia and Ukraine. It has been used toverifythe identities of prisoners of war, noncombatants fleeing conflict zones, and soldiers accused of committing war crimes. It has also been used to debunk propaganda, monitor the flow of displaced persons, and locate potentially damaged buildings obscured by smoke and clouds.We’re thinking:War is terrible. We’re glad that AI can help document the damage caused by invading forces, and we hope that such documentation will lead to payment of appropriate reparations.\n\nRecentworkshowed that models for multilingual machine translation can increase the number of languages they translate by scraping the web for pairs of equivalent sentences in different languages. A new study radically expanded the language repertoire through training on untranslated web text.\n\nWhat’s new:Ankur Bapna, Isaac Caswell, and colleagues at Google collected a dataset of untranslated text that spans over 1,000 languages. Combining it with existing multilingual examples, they trained amodelto translate many languages that are underrepresented in typical machine translation corpora.\n\nKey insight:Neural networks typically learn to translate text from multilingual sentence pairs, known as parallel data. Generally this requires examples numbering in the millions, which aren’t available for the vast majority of language pairs. However, neural networks can also learn from untranslated text, also known as monolingual data, by training them to fill in a missing word in a sentence. Combined training on parallel and monolingual data — carefully filtered — can enable a model to translate among languages that aren’t represented in parallel data.\n\nHow it works:The authors scraped web text, classified the languages in it, and combined what was left with existing monolingual data. Separately, they used an established corpus of parallel data. Then they trained a transformer on the monolingual and parallel datasets.\n\nResults:The authors compared their 1,000-language model with a version trained on 200 languages. Given a test set that comprised 38 languages, the 1000-language model performed better on most of them (including those for which plenty of training data was available), which suggests that greater language diversity was beneficial. When translating all languages into English, the 1000-language model outperformed the 200-language version by 2.5CHRF points, a measure of overlap among groups of characters between generated and ground-truth translations. Translating from English to other languages, the 1,000-language version outperformed its 200-language counterpart by an average of 5.3 CHRF points.\n\nWhy it matters:Previousresearchcautioned against using monolingual data to expand a translator’s language repertoire. It was thought that training in languages that were less well-represented in the dataset would diminish performance on better-represented ones. Yet this model, trained largely on monolingual data, performed well across a variety of languages. The authors hypothesize that, once a model learns a critical number of languages, additional languages are helpful because they’re likely to share similarities with those the model already knows about.\n\nWe’re thinking:The authors went out of their way to filter out less-useful training data. Their results show that scraping the web indiscriminately only gets you so far. Rigorous curation can make a big difference.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--7--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/11/GENFUND.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/11/VOTE.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/11/Ff2woLQXwAAJ9qr--1-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--6-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--7-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-286/",
    "title": "issue 286",
    "date": "",
    "reading_time": "",
    "content": "The buzz over DeepSeek this week crystallized, for many people, a few important trends that have been happening in plain sight: (i) China is catching up to the U.S. in generative AI, with implications for the AI supply chain. (ii) Open weight models are commoditizing the foundation-model layer, which creates opportunities for application builders. (iii) Scaling up isn’t the only path to AI progress. Despite the massive focus on and hype around processing power, algorithmic innovations are rapidly pushing down training costs.\n\nAbout a week ago, DeepSeek, a company based in China, releasedDeepSeek-R1, a remarkable model whose performance on benchmarks is comparable to OpenAI’s o1. Further, it was released as an open weight model with a permissive MIT license. At Davos last week, I got a lot of questions about it from non-technical business leaders. And on Monday, the stock market saw a “DeepSeek selloff”: The share prices of Nvidia and a number of other U.S. tech companies plunged. (As of the time of writing, they have recovered somewhat.)\n\nHere’s what I think DeepSeek has caused many people to realize:\n\nChina is catching up to the U.S. in generative AI.When ChatGPT was launched in November 2022, the U.S. was significantly ahead of China in generative AI. Impressions change slowly, and so even recently I heard friends in both the U.S. and China say they thought China was behind. But in reality, this gap has rapidly eroded over the past two years. With models from China such as Qwen (which my teams have used for months), Kimi, InternVL, and DeepSeek, China had clearly been closing the gap, and in areas such as video generation there were already moments where China seemed to be in the lead.\n\nI’m thrilled that DeepSeek-R1 was released as an open weight model, with a technical report that shares many details. In contrast, a number of U.S. companies have pushed for regulation to stifle open source by hyping up hypothetical AI dangers such as human extinction. It is now clear that open source/open weight models are a key part of the AI supply chain: Many companies will use them. If the U.S. continues to stymie open source, China will come to dominate this part of the supply chain and many businesses will end up using models that reflect China’s values much more than America’s.\n\nOpen weight models are commoditizing the foundation-model layer.As I wrote previously, LLM token prices have beenfallingrapidly, and open weights have contributed to this trend and given developers more choice. OpenAI’s o1 costs $60 per million output tokens; DeepSeek R1 costs $2.19. This nearly 30x difference brought the trend of falling prices to the attention of many people.\n\nThe business of training foundation models and selling API access is tough. Many companies in this area are still looking for a path to recouping the massive cost of model training. The article “AI’s $600B Question” lays out the challenge well (but, to be clear, I think the foundation model companies are doing great work, and I hope they succeed). In contrast, building applications on top of foundation models presents many great business opportunities. Now that others have spent billions training such models, you can access these models for mere dollars to build customer service chatbots, email summarizers, AI doctors, legal document assistants, and much more.\n\nScaling up isn’t the only path to AI progress.There’s been a lot of hype around scaling up models as a way to drive progress. To be fair, I was an earlyproponentof scaling up models. A number of companies raised billions of dollars by generating buzz around the narrative that, with more capital, they could (i) scale up and (ii) predictably drive improvements. Consequently, there has been a huge focus on scaling up, as opposed to a more nuanced view that gives due attention to the many different ways we can make progress. Driven in part by the U.S. AI chip embargo, the DeepSeek team had to innovate on many optimizations to run on less-capable H800 GPUs rather than H100s, leading ultimately to a model trained (omitting research costs) for under $6M of compute.\n\nIt remains to be seen if this will actually reduce demand for compute. Sometimes making each unit of a good cheaper can result in more dollars in total going to buy that good. I think the demand for intelligence and compute has practically no ceiling over the long term, so I remain bullish that humanity will use more intelligence even as it gets cheaper.\n\nI saw many different interpretations of DeepSeek’s progress on social media, as if it was a Rorschach test that allowed many people to project their own meaning onto it. I think DeepSeek-R1 has geopolitical implications that are yet to be worked out. And it’s also great for AI application builders. My team has already been brainstorming ideas that are newly possible only because we have easy access to an open advanced reasoning model. This continues to be a great time to build!\n\nKeep learning,\n\nAndrew\n\nDiscover Anthropic’s new capabilty - Computer Use - that allows LLM-based agents use a computer interface. In this free course, you’ll learn to apply image reasoning and function-calling to ‘use’ a computer as follows: a model processes an image of the screen, analyzes it to understand what's going on, and navigates the computer via mouse clicks and keystrokes.Start today!\n\nReinforcement learning is emerging as an avenue for building large language models with advanced reasoning capabilities.\n\nWhat’s new:Two recent high-performance models,DeepSeek-R1(and its variants including DeepSeek-R1-Zero) andKimi k1.5, learned to improve their generated lines of reasoning via reinforcement learning.o1pioneered this approach last year.\n\nReinforcement learning (RL) basics:RL rewards or punishes a model for performing particular actions or achieving certain objectives. Unlike supervised and unsupervised learning, which compare the model's output to a known ground truth, RL doesn’t explicitly tell a model what it should output. Instead, the model starts out behaving randomly and discovers desired behaviors by earning rewards for its actions. This makes RL especially popular for training machine learning models that play games or control robots.\n\nHow it works:To improve thechain of thought(CoT) generated by a large language model (LLM), reinforcement learning encourages the model to generate correct solutions to math, coding, science, and other problems that have known solutions. Unlike typical LLM training, in which the model simply generates the next token of its output and receives feedback token by token, this method rewards the model for generating a sequence of reasoning steps that lead to an accurate conclusion, even if doing so requires generating many intermediate tokens between the prompt and the response — to plan an outline, check the conclusion, or reflect on the approach — without explicit training on the reasoning steps to take.\n\nBehind the news:While RL has been a staple technique for training models toplay gamesandcontrol robots, its role in developing LLMs has been confined to alignment with human preferences. Reinforcement learning to match judgements of humans (reinforcement learning from human feedback, or RLHF) or AI (Constitutional AI, which uses reinforcement learning from AI feedback or RLAIF) were the primary methods for encouraging LLMs to align with human preferences prior to the development ofdirect preference optimization.\n\nWhy it matters:Reinforcement learning has surprising utility in training large language models to reason. As researchers press models into service in more complex tasks — math, coding, animated graphics, and beyond — reinforcement learning is emerging as an important path to progress.\n\nWe’re thinking:Less than three years ago, reinforcement learning looked toofinickyto be worth the trouble. Now it’s a key direction in language modeling. Machine learning continues to be full of surprising twists!\n\nOpenAI introduced an AI agent that performs simple web tasks on a user’s behalf.\n\nWhat’s new:Operatorautomates online actions like buying goods, booking tickets and completing forms by navigating websites in a browser-like environment within ChatGPT. It’s available on desktops as a research preview for subscribers to ChatGPT Pro ($200 per month). OpenAI promises broader availability to come as well as API access to the underlying model and improved ability to coordinate multi-step tasks like scheduling meetings across calendars from different vendors.\n\nHow it works:Operator uses a new model calledComputer-Using Agent(CUA) that accepts text input and responds with web actions.\n\nBehind the news:Operator rides a wave of agents designed to automate everyday tasks. Last week, OpenAI introducedChatGPT Tasks, which lets users schedule reminders and alerts but doesn’t support web interaction. (Early userscomplainedthat Tasks was buggy and required overly precise instructions.) Anthropic’sComputer Usefocuses on basic desktop automation, while DeepMind’sProject Marineris a web-browsing assistant built on Gemini 2.0.Perplexity Assistantautomates mobile apps such as booking Uber rides on Android phones.\n\nWhy it matters:In early reports, userssaidOperator sometimes was less efficient than a human performing the same tasks. Nevertheless, agentic AI is entering the consumer market, and Operator is poised to give many people their first taste. It’s geared to provide AI assistance for an endless variety of personal and business uses, and — like ChatGPT was for other developers of LLMs — and it’s bound to serve as a template for next-generation products.\n\nWe’re thinking:Computer use is maturing, and the momentum behind it is palpable. AI developers shouldhave in their toolbox.\n\nUnder a new president, the United States reversed its approach to AI regulation, seeking global dominance by reducing restrictions.\n\nWhat’s new:President Trump, who took office last week,signedan executive order that set a 180-day deadline to draft an AI Action Plan. The order aims to boost national security, economic competitiveness, and U.S. leadership in artificial intelligence.\n\nHow it works:Theexecutive orderassigns responsibility for crafting the AI Action Plan to three key figures in the administration: Michael Kratsios, assistant to the president for science and technology (and former managing director of Scale AI); venture capitalist David Sacks, the new special advisor for AI and cryptocurrency; and national security advisor Michael Waltz.\n\nAI infrastructure build-out:Along with the executive order, President Trump announcedStargate, a joint venture that involves OpenAI, Oracle, and SoftBank. The three companies outlined a plan to invest $100 billion in computing infrastructure for AI, such as next-generation data centers, and $500 billion over four years. In addition, the administrationdeclareda national energy emergency with respect to U.S. supplies of energy andissuedan order to ramp up domestic energy production. These measures aim to support energy-intensive AI initiatives like Stargate by removing regulatory barriers to building oil, gas, and renewable energy projects on federal lands.\n\nWhy it matters:The Trump administrationsaysthat Biden’s 2023 regulations were “onerous and unnecessary,” stifled innovation, and jeopardized U.S. leadership in AI. The new order reduces bureaucratic oversight of AI development, creating a more permissive regulatory environment (except when it comes to ideological bias).\n\nWe’re thinking:The Biden administration’s 2023 executive order aimed to guard against hypothetical, rather than actual, AI risks. It introduced thresholds of processing used to train models as a measure of their risk — a poorly thought-out proxy. To be fair, the AI Safety Institute under the U.S. National Institute of Standards and Technology didn’t hamper AI progress as much as some had feared, but overall the order was not helpful to AI innovation or safety. We’re pleased that the new administration is focusing on AI progress rather than hypothetical risks.\n\nThe practice of fine-tuning models on synthetic data is becoming well established. But synthetic training data, even if it represents the training task well, may include characteristics like toxicity that impart unwelcome properties in the trained model’s output, and it may inconsistently represent desired traits such as the target output length. Researchers developed a method that reduces aspects of generated data and retains desired ones.\n\nWhat’s new:Luísa Shimabucoro and colleagues at Cohere introducedactive inheritance, a fine-tuning method that automatically selects synthetic training examples that have desirable characteristics.\n\nKey insight:A naive way to generate synthetic fine-tuning data is to feed prompts to a model, collect its output, and use that as the fine-tuning set. But synthetic data is cheap, so we can afford to be more choosy. By generating several responses to each prompt, we can select the one that best suits our purposes.\n\nHow it works:The authors usedLlama 2 7BandMixtral 8x7Bas both teachers and students in all combinations. They prompted the models with 52,000 prompts from theAlpacadataset and used automated methods to evaluate their outputs in terms of characteristics including social bias, toxicity, word count, lexical diversity, andcalibration(how well a model’s estimated probabilities match its accuracy).\n\nResults:Fine-tuning on the best response for each characteristic improved performance with respect to that characteristic beyond using the initial outputs or selecting outputs randomly.\n\nWhy it matters:Training on synthetic data is becoming increasingly common. While it shows great promise, best practices for data generation are still being formulated. The authors’ method helps by automatically steering models toward generating more desirable responses, reducing negative traits and reinforcing positive traits.\n\nWe’re thinking:Knowledge distillation lately has led to more capable and compact models. This approach adds levers of fine control to that technique.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/01/Captura-de-pantalla-2025-01-29-a-la-s--1.31.32-p.-m..png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/The-Batch-ads-and-exclusive-banners--5--1.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--49-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--46-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--50-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--51-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-93/",
    "title": "issue 93",
    "date": "",
    "reading_time": "",
    "content": "Benchmarks have been a significant driver of research progress in machine learning. But they've driven progress in model architecture, not approaches to building datasets, which can have a large impact on performance in practical applications. Could a new type of benchmark spur progress in data-centric AI development?Remember: AI System = Code (model/algorithm) + DataMost benchmarks provide a fixed set of Data and invite researchers to iterate on the Code. This makes it possible to compare algorithms: By running many models on the same dataset, we can find the ones that perform best. To spur innovation on data-centric AI approaches, perhaps it’s time to hold the Code fixed and invite researchers to improve the Data.A huge amount of innovation — in algorithms, ideas, principles, and tools — is needed to make data-centric AI development efficient and effective.\n\nWhen AI was shifting toward deep learning over a decade ago, I didn’t foresee how many thousands of innovations and research papers would be needed to flesh out core tenets of the field. But now I think an equally large amount of work lies ahead to support a data-centric approach. For example, we need to develop good ways to:\n\nBenchmarks and competitions in which teams are asked to improve the data rather than the code would better reflect the workloads of many practical applications. I hope that such benchmarks also will spur research and help engineers gain experience working on data. The Human Computer Interface (HCI) community also has a role in designing user interfaces that help developers and subject-matter experts work efficiently with data.\n\nI asked for feedback on the idea of a data-centric competition on social media (Twitter,LinkedIn,Facebook). I’ve read all the responses so far — thanks to all who replied. If you have thoughts on this, please join the discussion there.\n\nKeep learning!\n\nAndrew\n\nA secretive start-up matches faces online as a free service.\n\nWhat’s new:Face recognition tech tends to be marketed to government agencies, but PimEyes offers a web app that lets anyone scan the internet for photos of themself — or anyone they have a picture of. The company says it aims to help people control their online presence and fight identity theft, but privacy advocates are concerned that the tool could be used to monitor or harass people,The Washington Postreported. You can try ithere.\n\nHow it works:PimEyes has extracted geometric data from over 900 million faces it has found online. It claims not to crawl social media sites, but images from Instagram, Twitter, and YouTube have shown up in its results.\n\nBehind the news:Free online face matching is part of a broader mainstreaming of face recognition and tools to counter it.\n\nWhy it matters:The widespread ability to find matches for any face online erodes personal privacy. It also adds fuel to efforts to regulate face recognition, which could result in restrictions that block productive uses of the technology.\n\nWe’re thinking:We’re all poorer when merely posting a photo on a social network puts privacy at risk. The fact that such a service is possible doesn’t make it a worthy use of an engineer’s time and expertise.\n\nA libel-detection system could help news outlets and social media companies stay out of legal hot water.\n\nWhat’s new:CaliberAI, an Irish startup, scans text for statements that could be considered defamatory,Wiredreported. You can try ithere.\n\nHow it works:The system uses custom models to assess whether assertions that a person or group did something illegal, immoral, or otherwise taboo meet the legal definition of defamation.\n\nBehind the news:News organizations are finding diverse uses for natural  language processing.\n\nWhy it matters:A defamation warning system could help news organizations avoid expensive, time-consuming lawsuits. That’s especially important in Europe and other places where such suits are easier to file than in the U.S. Social media networks may soon need similar tools. Proposed rules in theEUandUKwould hold such companies legally accountable for defamatory or harmful material published on their platforms.U.S.lawmakers are eyeing similar legislation.\n\nWe’re thinking:Defamation detection may be a double-edged sword. While it has clear benefits, it could also have a chilling effect on journalists, bloggers, and other writers by making them wary of writing anything critical of anyone.\n\nThe first two courses in ourMachine Learning Engineering for Production (MLOps) Specializationare live on Coursera!Enroll now\n\nResearchers found serious flaws in an influential language dataset, highlighting the need for better documentation of data used in machine learning.\n\nWhat’s new:Northwestern University researchers Jack Bandy and Nicholas VincentinvestigatedBookCorpus, which has been used to train at least 30 large language models. They found several ways it could impart social biases.\n\nWhat they found:The researchers highlighted shortcomings that undermine the dataset’s usefulness.\n\nBehind the news:The study’s authors were inspired by previous work by researchers Emily Bender and Timnit Gebru, whoproposeda standardized method for reporting how and why datasets are designed. The pair outlined in a laterpaperhow lack of information about what goes into datasets can lead to “documentation debt,” costs incurred when data issues lead to problems in a model’s output.\n\nWhy it matters:Skewed training data can have substantial effects on a model’s output. Thorough documentation can warn engineers of limitations and nudge researchers to build better datasets — and maybe even prevent unforeseen copyright violations.\n\nWe’re thinking:If you train an AI model on a library full of books and find it biased, you have only your shelf to blame.\n\nResearchers typically downsize images for vision networks to accommodate limited memory and accelerate processing. A new method not only compresses images but yields better classification.\n\nWhat’s new:Hossein Talebi and Peyman Milanfar at Google built alearned image preprocessorthat improved the accuracy of image recognition models trained on its output.\n\nKey insight:Common approaches to downsizing, such as bilinear and bicubic methods, interpolate between pixels to determine the colors of pixels in a smaller version of an image. Information is lost in the process, which may degrade the performance of models trained on them. One solution is to train separate models that perform resizing and classification together.\n\nHow it works:The network comprises a bilinear resizer layer sandwiched between convolutional layers to enable it to accept any input image size.\n\nResults:The authors’ approach achieved top-5 error on ImageNet of 10.8 percent. The baseline model achieved 12.8 percent.\n\nYes, but:The proposed method consumed 35 percent more processing power (7.65 billion FLOPS) than the baseline (5.67 billion FLOPS).\n\nWhy it matters:Machine learning engineers have adopted conventional resizing methods without considering their impact on performance. If we must discard information, we can devise an algorithm that learns to keep what’s the most important.\n\nWe’re thinking:In between training vision networks, you might use this image processor to produce mildly interesting digital art.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-05-26-at-9.46.41-AM-copy--1-.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-05-25T094231.843.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-05-25T094231.843.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/NEWS.gif?upscale=true&width=1200&upscale=true&name=NEWS.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20Image%203.png?upscale=true&width=1200&upscale=true&name=The%20Batch%20Image%203.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/BOOKCORPUS.gif?upscale=true&width=1200&upscale=true&name=BOOKCORPUS.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Resize.gif?upscale=true&width=1200&upscale=true&name=Resize.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-181/",
    "title": "issue 181",
    "date": "",
    "reading_time": "",
    "content": "Today DeepLearning.AI is launching theMathematics for Machine Learning and Data Science Specialization, taught by the world-class AI educator Luis Serrano. In my courses, when it came to math, I’ve sometimes said, “Don’t worry about it.” So why are we offering courses on that very subject?\n\nYou can learn, build, and use machine learning successfully without a deep understanding of the underlying math. So when you’re learning about an algorithm and come across a tricky mathematical concept, it’s often okay to not worry about it in the moment and keep moving. I would hate to see anyone interrupt their progress for weeks or months to study math before returning to machine learning (assuming that mastering machine learning, rather than math, is your goal).\n\nBut . . . understanding the math behind machine learning algorithms improves your ability to debug algorithms when they aren’t working, tune them so they work better, and perhaps even invent new ones. You’ll have a better sense for when you’re moving in the right direction or something might be off, saving months of effort on a project. So during your AI journey, it’s worthwhile to learn the most relevant pieces of math, too.If you’re worried about your ability to learn math, maybe you simply haven’t yet come across the best way to learn it. Even if math isn’t your strong suit, I’m confident that you’ll find this specialization exciting and engaging.\n\nLuis is a superb machine learning engineer and teacher of math. He and I spent a lot of time debating the most important math topics for someone in AI to learn. Our conclusions are reflected in three courses:\n\nMath isn’t about memorizing formulas, it’s about building a conceptual understanding that sharpens your intuition. That’s why Luis, curriculum product manager Anshuman Singh, and the team that developed the courses present them using interactive visualizations and hands-on examples. Their explanations of some concepts are the most intuitive I’ve ever seen.\n\nI hope you enjoy theMathematics for Machine Learning and Data Science Specialization!Keep learning,\n\nAndrew\n\nMicrosoft deepened its high-stakes relationship with OpenAI.\n\nWhat’s new:The tech giantconfirmedrumors that it is boosting its investment in the research lab that created the ChatGPT large language model and other AI innovations.What happened:Microsoft didn’t disclose financial details, but earlier this month anonymous sources hadtoldthe tech news siteSemaforthat the company would give OpenAI $10 billion. In exchange, Microsoft would receive 75 percent of the research startup’s revenue until it recoups the investment, after which it would own 49 percent of OpenAI. Microsoft began its partnership with OpenAI with a $1 billion investment in 2019, and another $2 billion sometime between 2019 and 2023. In those deals, Microsoft got first dibs on commercializing OpenAI’s models and OpenAI gained access to Microsoft’s vast computing resources.\n\nBehind the news:Earlier this month, the tech-business news siteThe Informationreportedthat Microsoft planned to launch a version of its Bing search service that uses ChatGPT to answer queries, and that it would integrate ChatGPT into the Microsoft Office suite of productivity applications. Google CEO Sundar Pichaireportedlywas so spooked by ChatGPT’s potential to undermine his company’s dominant position in web search that he issued a company-wide directive to respond with AI-powered initiatives including chatbot-enhanced search.\n\nWhy it matters:Microsoft’s ongoing investments helps to validate the market value of OpenAI’s innovations (which some observers havequestioned). The deal also may open a new chapter in the decades-long rivalry between Microsoft and Google —a chapter driven entirely by AI.\n\nWe’re thinking:Dramatic demonstrations of AI technology often lack a clear path to commercial use. When it comes to ChatGPT, we’re confident that practical uses are coming.\n\nAmid speculation about thethreatposed by OpenAI’s ChatGPT chatbot to Google’s search business, a paper shows how the search giant might address the tendency of such models to produce offensive, incoherent, or untruthful dialog.\n\nWhat’s new:Amelia Glaese and colleagues at Google’s sibling DeepMind used human feedback to train classifiers to recognize when a chatbot broke rules of conduct, and then used the classifiers to generate rewards while training theSparrowchatbot to follow the rules and look up information that improves its output. To be clear, Sparrow is not Google’s answer to ChatGPT; it preceded OpenAI’s offering by several weeks.\n\nKey insight:Given a set of rules for conversation, humans can interact with a chatbot, rate its replies for compliance with the rules, and discover failure cases. Classifiers trained on data generated through such interactions can tell the bot when it has broken a rule. Then it can learn to generate output that conforms with the rules.\n\nHow it works:Sparrow started with the 70 billion-parameter pretrainedChinchillalanguage model. The authors primed it for conversation by describing its function (“Sparrow . . . will do its best to answer User’s questions”), manner (“respectful, polite, and inclusive”), and capabilities (“Sparrow can use Google to get external knowledge if needed”), followed by an example conversation.\n\nResults:Annotators rated Sparrow’s dialogue continuations as both plausible and supported by evidence 78 percent of the time; the baseline Chinchilla achieved 61 percent. The model broke rules during 8 percent of conversations in which annotators tried to make it break a rule. The baseline broke rules 20 percent of the time.\n\nYes, but:Despite search capability and fine-tuning, Sparrow occasionally generated falsehoods, failed to incorporate search results into its replies, or generated off-topic replies. Fine-tuning amplified certain undesired behavior. For example, on a bias scale in which 1 means that the model reinforced undesired stereotypes in every reply, 0 means it generated balanced replies, and -1 means that it challenges stereotypes in every reply, Sparrow achieved 0.10 on theWinogenderdataset, while Chinchilla achieved 0.06.\n\nWhy it matters:The technique known asreinforcement learning from human feedback(RLHF), in which humans rank potential outputs and a reinforcement learning algorithm rewards the model for generating outputs similar to those that rank highly, is gaining traction as a solution to persistent problems with large language models. OpenAI embraced this approach in training ChatGPT, though it has not yet described that model’s training in detail. This work separated the human feedback into distinct rules, making it possible to train classifiers to enforce them upon the chatbot. This twist on RLHF shows promise, though the fundamental problems remain. With further refinement, it may enable Google to equal or surpass OpenAI’s efforts in this area.\n\nWe’re thinking:Among the persistent problems of bias, offensiveness, factual incorrectness, and incoherence, which are best tackled during pretraining versus fine-tuning is a question ripe for investigation.\n\nOur new specialization launches today! 🚀 Unlock the full power of machine learning algorithms and data science techniques by learning the mathematical principles behind them in this beginner-friendly specialization.Enroll now\n\nA prominent tech-news website generated controversy (and mistakes) by publishing articles written by AI.\n\nWhat’s new:CNETsuspended its practice of publishing articles produced by a text-generation model following news reports that exposed the articles’ authorship,The Vergereported.\n\nWhat happened:Beginning in November 2022 or earlier, CNET’s editors used an unnamed, proprietary model built by its parent company Red Ventures to produce articles on personal finance. The editors, who either published the model’s output in full or wove excerpts into material written by humans, were responsible for ensuring the results were factual.\n\nNonetheless, they published numerous errors and instances of possible plagiarism.\n\nBehind the news:CNETisn’t the first newsroom to adopt text generation for menial purposes.The Wall Street Journaluses natural language generation from Narrativa to publish rote financial news.Associated Pressuses Automated Insights’ Wordsmith to write financial and sports stories without human oversight.Why it matters:Text generation can automate rote reporting and liberate writers and editors to focus on more nuanced or creative assignments. However, these tools are well known to produce falsehoods, biases, and other problems. Publications that distribute generated content without sufficient editorial oversight risk degrading their reputation and polluting the infosphere.We’re thinking:Programmerswho use AI coding tools anddriversbehind the wheels of self-driving cars often overestimate the capabilities of their respective systems. Human editors who use automated writing tools apparently suffer from the same syndrome.\n\nChina’s internet watchdog issued new rules that govern synthetic media.What’s new:Legislation from the Cyberspace Administration of Chinalimitsthe use of AI to create or edit text, audio, video, images, and 3D digital renderings. The law took effect on January 10.How it works:The rules regulate so-called “deep synthesis” services:\n\nBehind the news:The rules expand on China’s earliereffortto rein in deepfakes by requiring social media users to register by their real names and threatening prison time for people caught spreading fake news. Several states within the United States alsotargetdeepfakes, and a 2022 European Union lawrequiressocial media companies to label disinformation including deepfakes and withhold financial rewards like ad revenue from users who distribute them.\n\nWhy it matters:China’s government has been proactive in restricting generative AI applications whose output could do harm. Elsewhere, generative AI faces a grassrootsbacklashagainst its potential to disrupt education, art, and other cultural and economic arenas.We’re thinking:Models that generate media offer new approaches to building and using AI applications. While they're exciting, they also raise questions of fairness, regulation, and harm reduction. The AI community has an important role in answering them.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--31--1.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--12-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--32-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/01/Untitled-design--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--23-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--13-.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-18/",
    "title": "issue 18",
    "date": "",
    "reading_time": "",
    "content": "I’ve been reflecting on the NeurIPS 2019 conference, which ended on Saturday. It’s always a wonderful event, but this year I found it a bittersweet experience.\n\nBitter because the conference has grown so much that we no longer focus on a handful of ideas. I missed the feeling of a community coming together. I was excited about the progress in self-supervised learning. Others were buzzing about Bayesian networks and causality, federated learning in healthcare applications, or using DL to predict biological sequences such as proteins. These are fascinating areas, but it’s clear the AI community no longer marches to only one beat.\n\nThe sweet part is that NeurIPS is growing up. As Karen Haowrotein MIT Technology Review, NeurIPS has matured from a venue with great science, hard partying, and wild dancing into a forum with great science and a focus on using AI for good. The AI community is getting better at diversity, inclusion, and taking responsibility for our actions, though there’s still room to grow.\n\nAs part of the panel during the climate change workshop, I spoke about the importance of building an actionable ethical code for AI. Ideally written by the AI community, for the AI community. You can hear my remarks on that subjecthereat 1:15.\n\nIt was great fun speaking on the panel with Yoshua Bengio, Jeff Dean, Carla Gomes, and Lester Mackey. Thanks to David Rolnick, Priya Donti, Lynn Kaack, and others for organizing the great workshop.\n\nKeep learning!\n\nAndrew\n\nMore and more companies are developing machine learning models for internal use. But many are still struggling to bridge the gap to practical deployments.What’s new:Many companies haven’t figured out how to realize their AI ambitions, according to a report by Algorithmia, a marketplace for algorithms. Although AI budgets are on the rise, only 22 percent of companies using machine learning have successfully deployed a model, the study found.What the report says:The2020 State of Enterprise Reportis based on a survey of nearly 750 people including machine learning practitioners, managers overseeing machine learning projects, and executives at large tech corporations.\n\nWhy it matters:AI is rapidly expanding into new applications and industries, and research is making tremendous strides. Yet building successful projects is still difficult. This report highlights both the great value of practical experience in the field and the need to establish effective practices and processes around designing, building, and deploying models.We’re thinking:There’s a huge difference between building a Jupyter notebook model in the lab and deploying a production system that generates business value. AI as a field sometimes seems crowded but, in fact, it’s wide open to professionals who know what they’re doing.\n\nThepreviousstate-of-the-art image classifier was trained on the ImageNet dataset plus 3.5 billion supplemental images from a different database. A new method achieved higher accuracy with one-tenth as many supplemental examples — and they were unlabeled, to boot.What’s new:Qizhe Xie and a team at Google Brain plus Carnegie Mellon’s Eduard Hovy introduced a method they callNoisy Student, in which a model learns from another model in a teacher-student relationship. Noisy Student achieved better performance on ImageNet.Key insight:In the learning approach known as self-training, a model that’s designated the teacher trains on labeled data and then generates pseudo-labels on unlabeled data. Then a student model trains on both the labeled data and pseudo-labeled data. Noisy Student adds two tweaks: The student network is larger than that of the teacher, and the student’s training data is adulterated with noise.How it works:Both teacher and student use an EfficientNet architecture. The higher-capacity architecture is good for the student, which has more parameters and processes more data than the teacher.\n\nResults:Noisy Student improved state-of-the-art accuracy on ImageNet as a whole and on specialized subsets. On ImageNet, it increased top-5 accuracy, meaning the true label was in the top five predictions, by 0.2 percent to 98.2 percent. Noisy Student also boosted the top-1 accuracy by 1 percent to 87.4 percent. Furthermore, it matched or exceeded previously established records for ImageNet A,C, and P, which are subsets that have been corrupted or perturbed or are commonly misclassified.Why it matters:These results are another step forward for using unlabeled data to boost image classification accuracy.\n\nWe’re thinking:Unlabeled examples are far more plentiful than labeled datasets. Techniques like this may be key to enabling learning algorithms to exploit far more data than was possible before.\n\nAs language models show increasing power, a parallel trend has received less notice: The vogue for naming models after characters in the children’s TV show Sesame Street.What’s new:In a recentfeature article, The Verge gets to the bottom of the Muppetware phenomenon.How to get to Sesame Street:The trend encompasses tech giants including Google, Facebook, Baidu, and China’s prestigious Tsinghua University.\n\nWhy it matters:Muppet names are fun! But the article points out that they’re also memorable. They brand individual models for other researchers and elevate natural language processing for the broader public. Beyond that, the informal naming convention signifies the AI community’s collaborative attitude.Behind the news:Sesame Street debuted in 1969 aiming to create TV that held children’s attention while also educating them. It was the first children’s show to base its episodes onresearch.We’re thinking:This edition of The Batch was brought to you by the letters A and I.\n\nWant to deploy a TensorFlow model in your web browser, or on your smartphone? Course 1 and 2 of thedeeplearning.aiTensorFlow: Data and Deployment Specialization will teach you how.Enroll now\n\nSomepoliticiansview international competition in AI as an arms race. That mindset could lead to escalating conflict, experts said.What’s new:If global powers like the U.S. and China adopt a winner-take-all approach to AI, they will lose out on the benefits of international collaboration, Tim Hwang and Alex Pascalarguein Foreign Policy.The analysis:The arms-race mentality springs primarily from the notion that autonomous weapons will prove to be a trump card in international conflicts, the authors say. This belief encourages nations to keep research and development to themselves, but the total benefit of collaboration is often greater than that of any particular initiative, they say.\n\nBehind the news:Scientific partnerships between the U.S. and USSRmitigatedtensions during the Cold War era. In 1957, the rival nations agreed to send scientists to collaborate on projects. These cooperative relationships influenced diplomatic discussion and helped ease disagreements over issues like nuclear disarmament.Why it matters:AI’s potential role in warfare is still unclear, and the technology is far from fully developed. The gap creates breathing room for national leaders to establish policies that will mutually benefit their own countries and the world at large. For instance, the National Security Commission on AIadvocatesthat the U.S. engage with China and Russia to control military uses of AI.We’re thinking:Electricity has uses in warfare, yet countries didn’t keep that technology to themselves, and the whole world is better off for it.\n\nReinforcement learning trains models by trial and error. In batch reinforcement learning (BRL), models learn by observing many demonstrations by a variety of actors. For instance, a robot might learn how to fix ingrown toenails by watching hundreds of surgeons perform the procedure. But what if one doctor is handier with a scalpel while another excels at suturing? A new method lets models absorb the best skills from each.What’s new:Ajay Mandlekar and collaborators at Nvidia, Stanford, and the University of Toronto devised a BRL technique that enables models to learn different portions of a task from different examples. This way, the model can gain useful information from inconsistent examples.Implicit Reinforcement without Interaction at Scale(IRIS) achieved state-of-the-art BRL performance in three tasks performed in a virtual environment.Key insight:Learning from demonstrations is a double-edged sword. An agent gets to see how to complete a task, but the scope of its action is limited to the most complete demonstration of a given task. IRIS breaks down tasks into sequences of intermediate subgoals. Then it performs the actions required to accomplish each subgoal. In this way, the agent learns from the best parts of each demonstration and combines them to accomplish the task.How it works:IRIS includes a subgoal selection model that predicts intermediate points on the way to accomplishing an assigned task. These subgoals are defined automatically by the algorithm, and may not correspond to parts of a task as humans would describe them. A controller network tries to replicate the optimal sequence of actions leading to a given subgoal.\n\nResults:In the Robosuite’s lifting and pick-and-place tasks, previous state-of-the-art BRL approaches couldn’t pick up objects reliably, nor place them elsewhere at all. IRIS learned to pick up objects with over 80 percent success and placed them with 30 percent success.Why it matters:Automatically identifying subgoals has been a holy grail in reinforcement learning, with active research in hierarchical RL and other areas. The method used in this paper applies to relatively simple tasks where things happen in a predictable sequence (such as picking and then placing), but might be a small step in an important direction.We’re thinking:Batch reinforcement learning is useful when a model must be interpretable or safe — after all, a robotic surgeon shouldn’t experiment on living patients — but it hasn’t been terribly effective. IRIS could make it a viable option.\n\nThe number of patents issued for deep learning has doubled every year since 2013.What’s new:Inventor, engineer, and lawyer Nick Brestoff tracks deep learning patents. Hedetailedhis findings in a blog on InsideBigData and offers advice on how to get patent applications approved.AI on the rise:Breston searches weekly for filings containing keywords including “deep learning,” “deep neural,” or “multi-layer neural.” He found that IBM holds the most deep learning patents (51), followed by Google (39) and Microsoft (28).\n\nSenior privilege:Inventors age 65 and over — even those listed as co-authors — can fast-track patent applications using a loophole called a Petition to Make Special. This trick has allowed Brestoff, who is 71 years old and holds eight patents on deep learning techniques, to complete the process in as little as three months, rather than the usual years-long wait. “There’s a wonderful advantage to having a knowledgeable senior on your innovation team,” he said.We’re thinking:Patents have a bad name in some circles, because of patent trolls and frivolous lawsuits that have destroyed value and slowed down innovation. But we’re also not sure a world with no patents whatsoever would be one with more innovation.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Screen20Shot202019-12-1820at2011--1-.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/algorithmia220ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Noisy20Student20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/muppets.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize-3.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Arms20Race20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/IRIS20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Patents20ASPECT-1.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-249/",
    "title": "issue 249",
    "date": "",
    "reading_time": "",
    "content": "In the last couple of days, Google announced a doubling of Gemini Pro 1.5's input context window from 1 million to 2 million tokens, and OpenAI released GPT-4o, which generates tokens 2x faster and 50% cheaper than GPT-4 Turbo and natively accepts and generates multimodal tokens. I view these developments as the latest in an 18-month trend. Given the improvements we've seen, best practices for developers have changed as well.\n\nSince the launch of ChatGPT in November 2022, with key milestones that include the releases of GPT-4, Gemini 1.5 Pro, Claude 3 Opus, and Llama 3-70B, many model providers have improved their capabilities in two important ways: (i) reasoning, which allows LLMs to think through complex concepts and and follow complex instructions; and (ii) longer input context windows.\n\nThe reasoning capability of GPT-4 and other advanced models makes them quite good at interpreting complex prompts with detailed instructions. Many people are used to dashing off a quick, 1- to 2-sentence query to an LLM. In contrast, when building applications, I see sophisticated teams frequently writing prompts that might be 1 to 2 pages long (my teams call them “mega-prompts”) that provide complex instructions to specify in detail how we’d like an LLM to perform a task. I still see teams not going far enough in terms of writing detailed instructions. For an example of a moderately lengthy prompt, check outClaude 3’s system prompt. It’s detailed and gives clear guidance on how Claude should behave.\n\nThis is a very different style of prompting than we typically use with LLMs’ web user interfaces, where we might dash off a quick query and, if the response is unsatisfactory, clarify what we want through repeated conversational turns with the chatbot.\n\nFurther, the increasing length of input context windows has added another technique to the developer’s toolkit. GPT-3 kicked off a lot of research on few-shot in-context learning. For example, if you’re using an LLM for text classification, you might give a handful — say 1 to 5 examples — of text snippets and their class labels, so that it can use those examples to generalize to additional texts. However, with longer input context windows — GPT-4o accepts 128,000 input tokens, Claude 3 Opus 200,000 tokens, and Gemini 1.5 Pro 1 million tokens (2 million just announced in a limited preview) — LLMs aren’t limited to a handful of examples. Withmany-shot learning, developers can give dozens, even hundreds of examples in the prompt, and this works better than few-shot learning.\n\nWhen building complex workflows, I see developers getting good results with this process:\n\nI hope a process like this will help you build applications more easily. If you’re interested in taking a deeper dive into prompting strategies, I recommend the Medpromptpaper, which lays out a complex set of prompting strategies that can lead to very good results.\n\nKeep learning!\n\nAndrew\n\nP.S. Two new short courses:\n\nOpenAI pulled back the curtain on revised rules that will guide its models.\n\nWhat’s new:OpenAI published itsModel Spec, high-level guidelines for use by human labelers to steer model behavior. The company is inviting publiccommentson the spec until May 22. It has not stated whether or how it will incorporate comments.\n\nHow it works:During training, human labelers rate a model’s responses so it can be fine-tuned to conform with human preferences in the process known asreinforcement from human feedback(RLHF). The Model Spec outlines the principles — some new, some previously in use — that will drive those ratings. The principles are arranged hierarchically, and each category will override those below it.\n\nBehind the news:OpenAI’s use of the Model Spec and RLHF contrasts withAnthropic’sConstitutional AI. To steer the behavior of Anthropic models, that company’s engineers define a constitution, or list of principles, such as “Please choose the response that is the most helpful, honest, and harmless” and “Do NOT choose responses that are toxic, racist, or sexist, or that encourage or support illegal, violent, or unethical behavior.” Rather than human feedback, Anthropic relies onAI feedbackto interpret behavioral principles and guide reinforcement learning.\n\nWhy it matters:AI developers require a degree of confidence that the models they use will behave as they expect and in their users’ best interests. OpenAI’s decision to subject its guidelines to public scrutiny could help to instill such confidence, and its solicitation of public comments might make its models more responsive to social and market forces.\n\nWe’re thinking:OpenAI’s openness with respect to its Model Spec is a welcome step toward improving its models’ safety and performance.\n\nThe latest update of DeepMind’s AlphaFold model is designed to find the structures of not just proteins but all biologically active molecules as well as interactions between them.\n\nWhat’s new:GoogleannouncedAlphaFold 3, which models the 3D shapes of biomolecules including proteins, DNA, RNA, and ligands (molecules that bind to proteins or DNA, which includes antibodies and many drugs) in any combination. AlphaFold Serverprovidesaccess for noncommercial uses (with some limitations). Unlike earlier versions, AlphaFold 3 is not open source.Key insight:Given a sequence of amino acids (the building blocks of proteins), the previous version of AlphaFold drew on an existing knowledge of amino acid structures, computed their locations and angles, and assembled them like Lego blocks. To adapt the system for molecules that aren’t made of amino acids, AlphaFold 3 represents them as collections of individual atoms and uses a generative model to find their positions in space.How it works:Given a list of molecules, AlphaFold 3 generates their joint 3D structure, revealing how they fit together. Several transformers hone embeddings of proteins and amino acids, while a diffusion model (also a transformer) processes embeddings of atoms. The team trained the system on five datasets including ground truth protein, DNA, and RNA structures interactions in theProtein Data Bank. They also trained it on protein shapes computed by AlphaFold 2; that model’s explicit knowledge of amino acid structures helped overcome AlphaFold 3’s tendency to hallucinate in some instances. Among the key processes:\n\nResults: OnPoseBusters, a database of protein and protein-molecule shapes, AlphaFold 3 successfully found the shapes of about 77 percent of examples, while AutoDock Vina (a non-learning program that models molecular interactions) achieved about 53 percent. On a Protein Data Bank evaluation set, AlphaFold 3 successfully found about 84 percent of protein shapes, while AlphaFold Multimer 2.3 (an update of AlphaFold 2) found 83 percent. Modeling protein-protein interactions, AlphaFold 3 achieved 77 percent, while AlphaFold Multimer 2.3 achieved 67 percent, according toDockQ(a metric for the quality of such interactions).Behind the news:The original AlphaFold solved one of the most challenging problems in molecular biology by figuring out how long chains of amino acids would fold, giving scientists clear targets for designing new bioactive molecules. Googlespun offIsomorphic Labs to apply AlphaFold 2 to drug discovery. That company will use AlphaFold 3 and control commercial access to it.Why it matters:AlphaFold 3 is a triumph of machine learning. It extends the utility of the previous version beyond proteins, and it computes with unprecedented accuracy how biological molecules will combine, allowing for a more comprehensive understanding of how drugs interact with the body. Its ability to predict how antibodies will bind to proteins could help stave off future pandemics and other illnesses.We’re thinking:Although Isomorphic Labs retains control of AlphaFold 3, biologistssaidthe information in the paper is enough for other researchers to develop similar systems. We look forward to open versions!\n\nLearn to develop smarter search, retrieval augmented generation (RAG), and recommender systems for multimodal retrieval and generation in this short course, built in collaboration with Weaviate.Enroll today!\n\nSaudi Arabia plans to spend billions of dollars to become a global AI hub.\n\nWhat's new:The desert kingdom has allocated $100 billion to invest in AI and other technologies,The New York Timesreported. The massive potential outlay is attracting AI giants and startups alike.\n\nHow it works:Saudi Arabia, whose economy is based on large reserves of oil, aims to channel its considerable wealth into more sustainable industries. AI is a major target.\n\nBehind the news:Where AI is concerned, Saudi Arabia is competing with the neighboring United Arab Emirates (UAE). In March, UAE member state Abu Dhabiestablishedits own multibillion-dollar investment fund, MGX, which aims to secure deals in AI models, data centers, and semiconductors. One of MGX’s founding partners (and a cornerstone in the UAE’s AI efforts) is G42, a conglomerate with ties to the Emirati government that owns numerous AI research labs and other assets. G42 recentlyreceived$1.5 billion from Microsoft. Last year, itpaidU.S. chip designer Cerebras an initial $100 million to build up to nine supercomputers.Yes, but:Saudi investments have not always arrived on the expected schedule. Founders of startups that were promised GAIA funding havecomplainedof delays and nonpayments. Moreover, U.S. partners such as Microsoft have drawncriticismfor working with Saudi Arabia, which has beenaccusedof violating human rights. The U.S. governmentblockedfulfillment of the King Abdullah University’s purchase of Nvidia chips because it may help researchers associated with the Chinese military to circumvent U.S.restrictionson the export of advanced semiconductors. Earlier this year, U.S.-based generative AI startup Anthropicrejectedpotential investment from PIF citing national security concerns.\n\nWhy it matters:AI is fast becoming a source of national power, and many countries are eager to build their capabilities. Saudi Arabia’s investment could go a long way toward building facilities and talent in a part of the world that has not been known for high tech. For the country itself, it could bring economic growth and geopolitical advantage. For foreign companies and talent, it’s an immense new source of funding to pursue valuable projects and gain practical experience.\n\nWe're thinking:We are happy to see AI hubs emerge around the world, especially in places that can provide more opportunities for people who live outside of established AI centers.\n\nBrain-to-computer interfaces that enable users to control robots with their thoughts typically execute a single type of task such as reaching and grasping. Researchers designed a system that responds to a variety of intentions.\n\nWhat's new:Ruohan Zhang and colleagues at Stanford introducedNeural Signal Operated Intelligent Robots (NOIR). Their method commands a robot to perform practical tasks, such as ironing a cloth or making a sandwich, via signals from an electroencephalogram (EEG), a non-invasive way to measure brain waves via electrodes attached to the scalp.\n\nKey insight:Currently neuroscientists can derive from EEG signals only simple thoughts, such as the intention to move a limb. However, a sequence of simple thoughts can drive an arbitrarily complex action. Specifically, simple thoughts (such as the intention to move a hand) can drive a robot to perform complex actions by repeatedly (i) selecting an object, (ii) selecting an action to apply to the object, and (iii) selecting the part of the object to act upon. For instance, to iron a cloth, the initial sequence would be: (i) select the iron and (ii) grasp it (iii) by the handle. This sequence might be followed by (i) select the cloth and (ii) slide the iron across it (iii) starting at the nearest portion. And so on.\n\nHow it works:Users who wore EEG electrodes concentrated on specific sequences of thoughts to execute tasks as they watched a screen that displayed the output of a camera attached to either arobotic armorwheeled robot with two arms.\n\nResults:Three users controlled the two robots to execute 20 everyday tasks. On average, the system selected objects with 81.2 percent accuracy, actions with 42.2 percent accuracy, and locations with 73.9 percent accuracy. Users took an average of about 20 minutes to complete each task.\n\nWhy it matters:Brain signals are enormously complex, yet relatively simple statistical techniques — in this case, QDA — can decode them in useful ways.\n\nWe're thinking:Sometimes the simplest solution to a difficult problem is not to train a larger model but to break down the problem into manageable steps.\n\nIn “Multi AI Agent Systems with crewAI,” you’ll learn key principles for designing AI agents and organizing teams of agents to perform complex, multi-step tasks. You’ll apply these concepts to automate six common business processes.Sign up for free!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed--59--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-15T164130.114.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-15T164233.811.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/05/The-Batch-ads-and-exclusive-banners---2024-05-14T170959.188.png",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-15T164459.448.png",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-15T164549.473.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/05/The-Batch-ads-and-exclusive-banners---2024-05-14T171114.198.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-33/",
    "title": "issue 33",
    "date": "",
    "reading_time": "",
    "content": "In the earlier weeks of Covid-19, I didn’t want to contribute noise, so that experts in infectious disease could be heard. But now the situation has worsened. I spoke yesterday with Eric Topol, a cardiologist at Scripps Institute and author of Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again. He convinced me that it’s urgent for all of us to speak up.\n\nI’m deeply concerned about preventing Covid-19’s spread within healthcare systems. Apart from the widely reported shortages of personal protective equipment, healthcare systems in most countries, including the U.S., are not set up to adequately protect doctors and nurses from infection. We need to prioritize healthcare workers’ safety if we want them to keep taking care of us — and so that the death toll estimates, which are already staggering, don’t become even worse.\n\nHere are some projects that my teams and I have been up to:\n\nIt’s urgent for all of us to do what we can to flatten the curve. There are many things you can do to help. I hope that each of us will:\n\nIt’s up to us to respect the quarantine and save lives. Let’s come together as one global community and make it happen. Let me know what you or your friends are doing to help your community by sending email to[email protected].\n\nStay safe and keep learning!\n\nAndrew\n\nAI could make a life-saving difference in the fight against Covid-19. To assist in the effort, several organizations are contributing open datasets. You can use these resources to analyze trends or launch your own project. You might also want to join efforts like Kaggle’sCovid-19 competitions.\n\nWe’re glad to see so many members of the AI community stepping up to address this crisis. If you want to recommend relevant resources or projects, please let us know at[email protected].\n\nChatbots designed to recognize Covid-19 symptoms dispense alarmingly inconsistent recommendations.What’s new:Given the same symptoms, eight high-profile medical bots responded with divergent, often conflicting advice, according toSTAT News.Conflicting information:Reporters Casey Ross and Erin Brodwin discussed Covid-19 symptoms such as coughing, fever, and shortness of breath with conversational systems offered by government agencies, hospitals, and tech companies.\n\nBehind the news:Astudyfrom Stanford University suggests that symptom checkers built for Covid-19 are flawed partly because the disease’s early signs are similar to those of the common cold or garden-variety influenza. A2015 studythat found that online symptom checkers for a range of conditions often reach faulty conclusions.Yes, but:Screening tools don’t need to be perfect to add a lot of value. They are statistical tools intended to prioritize quickly and inexpensively which cases should be escalated for deeper examination.\n\nWhy it matters:The significant disagreement among these tools means there’s a lot of room for improvement, and bad advice is clearly dangerous in a situation like this. Still, AI-based screening could play a helpful role in this pandemic, especially considering how many countries are short on test kits. It could ease the burden on hospital staff and testing centers, which risk becoming overwhelmed as the pandemic spreads.We’re thinking:Human doctors’ recommendations aren’t always consistent.\n\nLast week we reported on a formula to determine model width and dataset size for optimal performance. A new paper contributes equations that optimize some training parameters.What’s new:Jared Kaplan and Sam McCandlish led researchers at Johns Hopkins and OpenAI to deriveequationsthat describe the effects of parameter count, training corpus size, batch size, and training time on language model performance, plus their own ways to find the best model and dataset sizes.Key insight:The researchers devised a set of equations that approximate the effects of different combinations of two variables. It’s easier to reason about 2D graphs than to visualize an n-dimensional surface.\n\nFindings:Three findings stand out. First, as many researchers have suspected, transformers outperform LSTMs when trained to convergence. Second, where data and compute are limited, it’s more efficient to train a large model in fewer training steps than to train a smaller model to convergence. Third, some researchers have conjectured that exceeding a so-called critical batch size degrades performance. The researchers offer a way to find optimal batch sizes.\n\nHow it works:The researchers trained many model shapes and sizes on various subsets of a proprietarydatasetof Reddit posts and linked articles. They measured performance of every combination during training to track the impact of design choices on performance.\n\nWhy it matters:Most machine learning practitioners don’t have the seemingly infinite computational resources that some large companies do. These insights should help them use resources more effectively.We’re thinking:Natural language processing is notoriously compute-hungry. The ability to balance processing power against performance could not only save money but reduceenvironmental impacts.\n\nUnexploded munitions from past wars continue to kill and maim thousands of people every year. Computer vision is helping researchers figure out where these dormant weapons are likely to be.What’s new:Data scientists at Ohio State University combined computer vision with military records toidentify areasin Cambodia where bombs dropped by U.S. planes during its war on neighboring Vietnam remain unexploded.How it works:The U.S. Air Force keptrecordsof how many bombs it dropped in each air raid, but no one knows how many failed to detonate. The researchers built a tool that counts craters — evidence of bombs thatdidexplode — and then subtracted that number from the total number dropped. The difference enabled them to estimate how many bombs still litter the countryside.\n\nThe results:The researchers used their model to sweep a 100 square kilometer area near the Vietnamese border that had been slammed with 3,205 bombs during the war. Using multiple runs of their model, the researchers found between 1,405 to 1,618 craters, which suggests that up to half of all the bombs dropped in this area are still waiting to be found.Behind the news:In 1969, U.S. President Richard Nixon secretly ordered the Air Force to begin bombing Cambodia to disrupt Viet Cong supply lines. The campaign left between 50,000 and 150,000dead. Since then, unexploded bombs and landmines have killed or maimed at least60,000people in Cambodia.Why it matters:Bombs from past wars pose a presentdangerto people all over the world. But finding them is expensive andlabor-intensive. Models that map high concentrations of unexploded ordnance could help organizations working on the problem direct their resources more efficiently.We’re thinking:It’s heartening to see the technology of the future applied to problems created in the past.\n\nWant to deploy a TensorFlow model in your web browser or on your smartphone? Thedeeplearning.aiTensorFlow: Data and Deployment Specialization will teach you how.Enroll now\n\nAre you secretly yearning to have aMy Little Ponycharacter voice your next online presentation? A new web app can make your dreams come true.What’s new:15.aitranslates short text messages into the voices of popular cartoon and video game characters.How it works:The model’s anonymous developer began the project in 2018 while an undergrad at MIT. In an email toThe Batch, the coder declined to disclose details about how the model works but said it was inspired by the 2019paperthat pioneered transfer learning for text-to-speech models.\n\nBehind the news:DeepMind made a significant advance in neural audio synthesis in 2016 withWaveNet. That work demonstrated that a neural net trained on multiple examples of similar speech or music could createpassable facsimiles. Other teams continue to make advances, for example, inreal-timesystems.Why it matters:Voice cloning could be enormously productive. In Hollywood, it could revolutionize the use of virtual actors. In cartoons and audiobooks, it could enable voice actors to participate in many more productions. In online education, kids might pay more attention to lessons delivered by the voices of favorite personalities. And how many YouTube how-to video producers would love to have a synthetic Morgan Freeman narrate their scripts?Yes, but:Synthesizing a human actor’s voice without consent is arguably unethical and possibly illegal. And this technology will be catnip for deepfakers, who could scrape recordings from social networks to impersonate private individuals.We’re thinking:Anyone who wants to synthesize Andrew’s voice has more than 15 minutes of deeplearning.ai courseware to train on.\n\nGlass bottles and crystal bowls bend light in strange ways. Image processing networks often struggle to separate the boundaries of transparent objects from the background that shows through them. A new method sees such items more accurately.What’s new:Shreeyak Sajjan and researchers at Synthesis.ai, Google, and Columbia University premiered a state-of-the-art model for identifying transparent objects. They call itClearGrasp, a reference to its intended use in robotics.Key insight:Faced with a transparent object, RGB-D cameras, which sense color and depth per pixel, can get confused: They take some depth measurements off the object’s surface, others straight through the object. ClearGrasp recognizes such noisy measurements and uses them to predict an object’s shape. Once it knows the object’s shape and how far away one point is, it can infer how far away every point is.How it works:ClearGrasp incorporates a trio ofDeeplabv3+models with theDRN-D-54architecture.\n\nResults:Fed real-life data captured by the researchers, ClearGrasp improved thepreviousstate of the art’s root mean squared error of corrected depth measurements from 0.054 to 0.038. A robotic arm using ClearGrasp picked up transparent objects 72 percent of the time, a big step up from 12 percent using unprocessed images.Why it matters:Machine learning has proven to be adept at noise reduction in various domains. ClearGrasp takes special care to modify only the depth measurements that are distorted.We’re thinking:ClearGrasp could prevent your robot assistant from having to clean up broken glass all day.\n\nWhen the world is panning for machine learning gold, it pays to help them dig through the data.What’s new:Machine learning entrepreneurs can make their mark (and their fortune) building services that help other companies develop, deploy, and monitor AI, venture capitalist Rob Toews argues inForbes.How it works:Toews points toScale.AI, a startup that labels data, as one of a new generation of companies capitalizing on the AI industry’s demand for ancillary services. In August, the four-year-old company raised$100 millionat a valuation of more than $1 billion. And labeling isn’t the only area of machine learning ripe for entrepreneurship.\n\nBehind the news:Companies likeAdobe and Capital Oneare spending hundreds of millions on cloud computing. This is driving demand for services that help them handle their cloud resources more efficiently. Among the beneficiaries are companies likeAlation,Collibra, andStarburst Datathat help catalog, query, and manage machine learning data,writesinvestor Matt Turck.Why it matters:Toews believes there are billions of dollars to be made by companies that provide machine learning services. Such services will also nurture new AI applications and accelerate their adoption across a variety of industries.We’re thinking:These companies aren’t only promising businesses. By taking on tasks like data procurement, model optimization, and lifecycle management, they could free engineers to focus on building products that fulfill deep learning’s potential.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Resources20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Chatbot.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Scaling20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Bombs2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize-3.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Voices20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ClearGrasp.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Labels20ASPECT.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-89/",
    "title": "issue 89",
    "date": "",
    "reading_time": "",
    "content": "How much data do you need to collect for a new machine learning project? If you’re working in a domain you’re familiar with, you may have a sense based on experience or from the literature. But when you’re working on a novel application, it’s hard to tell. In this circumstance, I find it useful to ask not how much data to collect but how much time to spend collecting data.For instance, I’ve worked on automatic speech recognition, so I have a sense of how much data is needed to build this kind of system: 100 hours for a rudimentary one, 1,000 hours for a basic one, 10,000 hours for a very good one, and perhaps 100,000-plus hours for an absolutely cutting-edge system. But if you were to give me a new application to work on, I might find it difficult to guess whether we need 10 or 10,000 examples.When starting a project, it’s useful to flip the question around. Instead of asking,\n\nHow many days do we need to collectmtraining examples?I ask,How many training examples can we collect inddays?\n\nTaking adata-centric approachto model development, let’s say it takes about two days to train a model and two days to perform error analysis and decide what additional data to collect (or how to tweak the model). How many days should you spend collecting data before training and error analysis? Allocating comparable amounts of time to each step seems reasonable, so I would advocate budgeting a couple of days — a week at most — for data collection. Then iterate through the loop.\n\nI’ve seen many teams spend far too much data collecting data before jumping into the model development loop. I’ve rarely seen a team spend too little time. If you don’t collect enough data the first time around, usually there’s time to collect more, and your efforts will be more focused because they’ll be guided by error analysis.When I tell a team, “Let’s spend two days collecting data,” the time limit often spurs creativity and invention of scrappy ways to acquire or synthesize data. This is much better than spending two months collecting data only to realize that we weren’t correcting the right data (say, the microphone we used was too noisy, leading to high Bayes/irreducible error).So, next time you face an unfamiliar machine learning problem, get into the model iteration loop as quickly as possible, and set a limited period of time for collecting data the first time around, at least. You’re likely to build a better model in less time.Keep learning!\n\nAndrew\n\nP.S. Once I created an unnecessarily scramble when asked a team to make sure that data collection took no longer than two days. Because of a bad Zoom connection, they thought I said “today.” Now I've learned to hold up two fingers whenever I say “two days” on a video call.\n\nThe European Union proposed sweeping restrictions on AI technologies and applications.\n\nWhat’s new:The executive arm of the 27-nation EU publisheddraft rulesthat aim to regulate, and in some cases ban, a range of AI systems. The proposal is the first to advance broad controls on the technology by a major international body.\n\nWhat it says:The 100-plus page document divides AI systems into three tiers based on their level of risk. The definition of AI includes machine learning approaches, logic-based approaches including expert systems, and statistical methods.\n\nYes, but:Some business-minded critics said these rules wouldhinder innovation. Meanwhile, human rights advocates said the draftleaves loopholesfor applications that are nominally prohibited. For example, face recognition is prohibited only if it’s conducted in real time; it could still be used on video captured in the past.\n\nBehind the news:Governments worldwide are moving to regulate AI. The U.S. Federal Trade Commission last weeksignaledits intent to take legal action against companies that make biased systems. A number of other countries including Australia, China, Great Britain, and India haveenacted lawsaimed at reining in big tech companies.\n\nWhy it matters:The EU’s AI proposal is the spiritual successor to its 2018 privacy law, theGeneral Data Protection Regulation(GDPR). That law sparked aglobal trendas Brazil, China, India, and other countries proposed or enacted laws to protect user data. The new plan could have a similar impact.\n\nWe’re thinking:Despite its flaws, the GDPR drew a line in the sand and advanced the conversation about uses of personal data. While this new set of rules is bound to provoke criticism —some it valid, no doubt — we welcome moves to promote regulation around AI and look forward to a spirited, global discussion.\n\nAutomobile insurers are increasingly turning to machine learning models to calculate the cost of car repairs.\n\nWhat’s new:The pandemic has made it difficult for human assessors to visit vehicles damaged in crashes, so the insurance industry is embracing automation,Wiredreported.\n\nHow it works:When drivers get into an accident, insurance companies direct them to download an app that guides them through documenting the effects. These systems are particularly good at assessing damage from minor collisions and determining when a car has been totaled.\n\nYes, but:Several body shop owners said that automated estimates weren’t accurate and often failed to spot hard-to-see damage such as a misaligned frame. Bad estimates resulted in substandard repairs and delays as mechanics haggled with insurance companies for more money.\n\nWhy it matters:Smart damage-assessment apps can inspect vehicles far more quickly than a human who examines the damage first-hand. Accurate output helps insurance companies save money and drivers settle claims more quickly.\n\nWe’re thinking:Will self-driving cars that get into a fender bender use an app to assess the damage?\n\nJoin usfor “Climbing the Corporate AI Ladder,” a live virtual event, on May 5, 2021. Speakers from Facebook, IBM, and Microsoft will show you how to level up your career. Co-hosted by DeepLearning.AI and FourthBrain.\n\nIBM’s Watson question-answering system stunned the world in 2011 when itbestedhuman champions of the TV trivia game showJeopardy!Although the Watson brand has fallen onhard times, the company’s language-processing prowess continues to develop.\n\nWhat’s new:Noam Slonim led a team at IBM to developProject Debater, which is designed to compete in formal debates.\n\nKey insight:A debate opens with four-minute opening statements by both sides followed by rounds of rebuttals and finally closing statements. To perform well, a debater must quickly prepare arguments supported by evidence, address competing arguments, and organize statements logically — a set of tasks too diverse for an end-to-end system. Instead, the team built a pipeline of independent components, each a complex system in its own right.\n\nHow it works:Project Debater receives a motion to argue in favor of or against. Then it’s off to the races finding facts, arguments, and counterarguments and stitching them together into speeches.\n\nResults:Project Debater is the first system of its kind, and no established benchmark exists to evaluate it. The researchers compared the quality (judged by humans on a scale of one to five) of the system’s opening statement with a speech on the same topic generated by a GPT-2 pretrained on a large text corpus and fine-tuned onspeeches. Project Debater achieved an average score of 4.1, far outperforming the fine-tuned GPT-2’s score of 3.2.\n\nYes, but:Project Debaterlosta 2019 competition with debate champion Harish Natarajan — albeit narrowly.\n\nWhy it matters:Building a system that can beat humans at competitive debate isn’t a multi-decade, multi-team project like winning at chess or Go, but it’s a substantial endeavor. So far, Project Debater has generated over 50 papers and spawned the subfields inclaim detectionandevidence detection.\n\nWe’re thinking:The AI community is embroiled in its own debates, including anannualeventin Montreal. Maybe this system can participate next time around?\n\nThe U.S. government aims to turbocharge biomedical AI research.\n\nWhat’s new:The National Institutes of Health, which invests $41.7 billion annually in medical research,announceda program called Bridge to Artificial Intelligence (Bridge2AI) to promote machine learning in human biology and medicine.\n\nTake it to the bridge:The program’s primary goal is to develop new datasets. It also aims to standardize data from different sources and develop automated tools to help create datasets and ensure that they adhere toFAIRprinciples, which aim to enable machines to use data with little human intervention. Bridge2AI will fund research into two areas:\n\nBehind the news:U.S. government agencies bringing AI into mainstream healthcare.\n\nWhy it matters:Bigger, better datasets specially designed for machine learning could help illuminate human biological processes and the diseases that disrupt them.\n\nWe’re thinking:AI for medicine has tremendous potential. Datasets designed specifically to help us realize that potential may be just what the doctor ordered.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-04-28%20at%209.42.25%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-04-28%20at%209.42.25%20AM%20copy.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image-1.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image-2.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image-3.png",
      "https://dl-staging-website.ghost.io/content/images/2021/05/image-4.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-86/",
    "title": "issue 86",
    "date": "",
    "reading_time": "",
    "content": "Each year, the public relations agency Edelman produces a report on the online public’s trust in social institutions like government, media, and business. The latestEdelman Trust Barometercontains a worrisome finding: While technology was ranked the most trusted industry in the U.S. last year, this year we plunged to ninth place. Trust in the tech industry fell to new lows in the majority of 27 countries surveyed.Tech can be a huge force for moving the world forward, but many well meaning efforts will run into headwinds if we aren’t able to gain others’ trust. It’s more urgent than ever that we collectively act in a way that is genuinely deserving of the rest of society’s trust.Trust is much harder to build than to destroy. One company that hypes AI can do more damage than 10 others that speak about it responsibly. One company that makes misleading statements can do more damage than 10 that speak honestly.\n\nHow can we regain trust? Several steps are needed, but to my mind, chief among them are:\n\nTrying to fool the public and government officials doesn’t work. We often read in the news about politicians who know little about tech, and say things that reflect their lack of understanding. But let me tell you this: Every large government has at least a handful of people who are tech-savvy enough to see through the spin to the heart of an issue. Companies shouldn’t try to fool people and instead do the harder — but more effective — work of solving problems thoughtfully.\n\nOn the plus side, 62 percent of respondents to Edelman’s survey agreed that employees have the power to force corporations to change. CEO aren’t the only people responsible for what companies do. All employees have a responsibility to help build trustworthy businesses. Wherever you work, I hope you’ll support straight talk, taking responsibility, and engaging and empathizing.\n\nKeep learning!\n\nAndrew\n\nImageNet now comes with privacy protection.\n\nWhat’s new:The team that manages the machine learning community’s go-to image datasetblurredall the human faces pictured in it and tested how models trained on the modified images on a variety of image recognition tasks. The faces originally were included without consent.\n\nHow it worked:The team used Amazon’s Rekognition platform to find faces in ImageNet’s nearly 1.5 million examples.\n\nBehind the news:This work is part of a wider movement toward protecting privacy in machine learning data. For instance, papers submitted to CVPR in recent years proposed models to automatically blur faces and license plates inGoogle Street Viewas well as data for trainingautonomous vehicles, andaction recognition models.\n\nWhy it matters:Machine learning datasets need not violate privacy. We can develop datasets that both protect privacy and train good models.\n\nWe’re thinking:Any loss of accuracy is painful, but a small loss is worthwhile to protect privacy. There’s more to life than optimizing test-set accuracy! We expect that most ImageNet-trained applications won’t suffer from the change, as they don’t involve objects that typically appear near to faces. Fine-tuning on a dataset obtained with permission might help for the rest.\n\nIt’s well established that pretraining a model on a large dataset improves performance on fine-tuned tasks. In sufficient quantity and paired with a big model, even data scraped from the internet at random can contribute to the performance boost.What’s new:Facebook researchers led by Priya Goyal builtSElf-supERvised(SEER), an image classifier pretrained on a huge number of uncurated, unlabeled images. It achieved better fine-tuned ImageNet accuracy than models pretrained on large datasets that were curated  to represent particular labels.Key insight:Large language models pretrained on billions of uncurated documents found in the wild, such asGPT-3, have achieved state-of-the-art performance after fine-tuning on a smaller dataset. Computer vision should benefit likewise from such scale.How it works:The authors used a 1.3-billion parameterRegNet, a convolutional neural network architecture similar toResNet, pretrained on 1 billion images randomly scraped from Instagram.\n\nResults:SEER achieved 84.2 percent top-1 accuracy on the ImageNet test set, 1.1 percent better than the best previous self-supervised, pretrained model (a ResNet of 795 million parameters pretrained on ImageNet usingSimCLRv2). It was 4.3 percentage points better than a 91-million parameterViTpretrained onJFT-300M, a curated dataset of 300 million images from Google Search. SEER also excelled at few-shot learning: Fine-tuned on 10 percent of ImageNet, it achieved 77.9 percent accuracy, 2.2 percentage points lower than a SimCLRv2 model pretrained on 100 percent of ImageNet and fine-tuned on 10 percent of ImageNet.Why it matters:Scraping the internet could be as productive in computer vision as it has been in language processing. Just keep in mind that training models on such data risks violating privacy and consent as well as absorbing the various biases — including objectionable social biases — inherent on the internet.We’re thinking:This paper suggests a tradeoff between the costs of building a curated dataset and training on a larger corpus plucked from the wild. If the cost of curation is high for your application, maybe you can cut it and spend more on training.\n\nSome doctors don’t trust a second opinion when it comes from an AI system.\n\nWhat’s new:A team at MIT and Regensburg Universityinvestigatedhow physicians responded to diagnostic advice they received from a machine learning model versus a human expert.\n\nHow it works:The authors recruited doctors to diagnose chest X-rays.\n\nResults:The radiologists generally rated as lower-quality advice they believed was generated by AI. The others rated AI and human advice to be roughly of equal quality.\n\nBoth groups made more accurate diagnoses when given accurate advice, regardless of its source. However, 27 percent of radiologists and 41 percent of the less experienced offered an incorrect diagnosis when given inaccurate advice.\n\nBehind the news:AI-powered diagnostic tools are proliferating and becoming more widely acceptedin the U.S.and elsewhere. These tools maywork about as well as traditional methodsat predicting clinical outcomes. Those that work wellmay only do so on certain populationsdue to biased training data.\n\nWhy it matters:It’s not enough to develop AI systems in isolation. It’s important also to understand how humans use them. The best diagnostic algorithm in the world won’t help if people don’t heed its recommendations.\n\nWe’re thinking:While some doctors are skeptical of AI, others may trust it too much, which also can lead to errors. Practitioners in a wide variety of fields will need to cultivate a balance between skepticism and trust in machine learning systems. We welcome help from the computer-human interface community in wrestling with these challenges.\n\nWe've updated our Deep Learning Specialization with the latest advances. The new program covers TensorFlow 2 as well as advanced architectures like U-Net, MobileNet, and EfficientNet. Stay tuned for transformers!Enroll now\n\nNeural networks can tease apart the different sounds in musical recordings.\n\nWhat’s new:Companies and hobbyists are using deep learning to separate voices and instruments in commercial recordings,Wiredreported. The process can improve the sound of old recordings and opens new possibilities for sampling, mash-ups, and other fresh uses.\n\nHow it works:Finished recordings often combine voices, instruments, and other sounds recorded in a multitrack format into a smaller number of audio channels; say, one for mono or two for stereo. The mingling of signals limits how much the sonic balance can be changed afterward, but neural networks have learned to disentangle individual sounds — including noise and distortion — so they can be rebalanced or removed without access to the multitrack recordings.\n\nWhy it matters:Many worthwhile recordings are distorted or obscured by noises like an audience’s cheers or analog tape hiss, making the quality of the musicianship difficult to hear. Others could simply use a bit of buffing after decades of improvement in playback equipment. AI-powered unmixing can upgrade such recordings as well as inspire new uses for old sounds.\n\nWe’re thinking:Endless remixes of our favorite Taylor Swift tracks? We like the sound of that!\n\nAttention quantifies how each part of one input affects the various parts of another. Researchers added a step that reverses this comparison to produce more convincing images.What’s new:Drew A. Hudson at Stanford and C. Lawrence Zitnick at Facebook chalked up a new state of the art in generative modeling by integrating attention layers into a generative adversarial network (GAN). They call their systemGANsformer.Key insight:Typically, a GAN learns through competition between a generator that aims to produce realistic images and a discriminator that judges whether images are generated or real.StyleGANsplits the generator into (a) a mapping network and (b) a synthesis network, and uses the output of the mapping network to control high-level properties (for example, pose and facial expression) of an image generated by the synthesis network. The output of the mapping layer can be viewed as a high-level representation of the scene, and the output of each layer of the synthesis network as a low-level representation. The authors devised a two-way version of attention, which they call duplex attention, to refine each representation based on the other.How it works:GANsformer is a modified StyleGAN. The authors trained it on four types of subject matter: faces inFFHQ;scenes composed of cubes, cylinders, and spheres inCLEVR; pictures of bedrooms inLSUN; and urban scenes inCityscapes.\n\nResults:GANsformer outperformed the previous state of the art on CLEVR, LSUN-Bedroom, and Cityscapes (comparing Fréchet Inception Distance based on representations produced by a pretrainedInceptionmodel). For example, on Cityscapes, GANsformer achieved 5.7589 FID compared toStyleGAN2’s 8.3500 FID. GANsformer also learned more efficiently than avanilla GAN, StyleGAN, StyleGAN2,k-GAN, andSAGAN. It required a third as many training iterations to achieve equal performance.Why it matters:Duplex attention helps to generate scenes that make sense in terms of both the big picture and the details. Moreover, it uses memory and compute efficiently: Consumption grows linearly as input size increases. (In transformer-style self-attention, which evaluates the importance of each part of an input with respect to other parts of the same input, memory and compute cost grows quadratically with input size.)We’re thinking:Transformers, which alternate attention and fully connected layers, perform better than other architectures in language processing. This work, which alternates attention and convolutional layers, may bring similar improvements to image processing.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-04-06-at-2.07.27-PM-copy--1-.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/BLUR%20TG.gif?upscale=true&name=BLUR%20TG.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/SEER%20(1).gif?upscale=true&name=SEER%20(1).gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/INFLUENCE%20TG.gif?upscale=true&name=INFLUENCE%20TG.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20(1)-1.png?upscale=true&name=The%20Batch%20(1)-1.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/upmix.gif?upscale=true&name=upmix.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-03-30T133105.644.gif?upscale=true&name=ezgif.com-gif-maker%20-%202021-03-30T133105.644.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-97/",
    "title": "issue 97",
    "date": "",
    "reading_time": "",
    "content": "With the rise of software engineering over several decades, many principles of how to build traditional software products and businesses are clear. But the principles of how to build AI products and businesses are still developing. I’ve found that there are significant differences, and I’ll explore some of them in this and future letters.That AI enables new categories of products and businesses is a familiar theme. However, using this new technology — whether in a startup going from 0 to 1 or a large company incubating a new product — brings special challenges:Unclear technical feasibility.It’s relatively well understood what a traditional mobile app or web app can do. If you can draw a reasonablewireframe, you can probably build it. But until you’ve examined the data and run some experiments, it’s hard to know how accurate an AI system can be in a given application. For example, many technologists overestimated how easy it would be to build an acceptably safe self-driving car. Generally, AI startups bring higher technical risk than traditional software startups because it’s harder to validate in advance if a given technology proposal is feasible.\n\nComplex product specification.The specification for a traditional web app might come in the form of a wireframe, but you can’t draw a wireframe to indicate how safe a self-driving car must be. It’s extremely complex to specify operating conditions (sometimes also called the operational design domain) and acceptable error rates under various conditions. Similarly, it can be hard to write a spec for a medical diagnosis tool, depending on how acceptable different types of errors are (since not all errors are equally severe). Further, product specs often evolve as the team discovers what is and isn’t technically feasible.\n\nNeed for data.To develop a traditional software product, you might (a) interview users to make sure they want what you aim to build, (b) show them a wireframe to make sure your design meets their needs, and (c) dive into writing the code. If you’re building an AI product, you need to write code, but you also need access to data to train and test the system. This may not be a big challenge. For a consumer product, you may be able to start with a small amount of data from an initial cohort of users. But for a product aimed at business customers — say, AI to optimize shipping or help a hospital manage its medical records — how can you get access to shipping data or medical records? To work around this chicken-and-egg problem, some AI startups start by doing consulting or NRE (non-recurring engineering) work. Those activities are hard to scale, but they afford access to data that can shape a scalable product.\n\nAdditional maintenance cost.For traditional software, the boundary conditions — the range of valid inputsd— are usually easy to specify. Indeed, traditional software often checks the input to make sure, for example, it’s getting an email address in a field dedicated to that input. But for AI systems, the boundary conditions are less clear. If you have trained a system to process medical records, and the input distribution gradually changes (data drift/concept drift), how can you tell when it has shifted so much that the system requires maintenance?Because of these differences between traditional software and AI, the best practices for building AI businesses are different. I’ll dive deeper into these differences in future letters. Meanwhile, please ask your business friends to subscribe to The Batch if they want to understand how to build an AI business!\n\nKeep learning!Andrew\n\nAn automated early warning system is alerting firefighters to emerging blazes.\n\nWhat’s new:South Korean companyAlcheratrained a computer vision system to monitor more than 800 fire-spotting cameras in Sonoma County, California, the local news channelABC7reported.\n\nHow it works:Alchera’sArtificial Intelligence Image Recognition(AIIR) spots smoke plumes caught on camera by a portion of California’sAlert Wildfirenetwork. A convolutional neural network flags video frames in which it recognizes smoke plumes, and an LSTM analyzes the time series to confirm the classification. If smoke is confirmed, an alarm alerts an operator at a central monitoring station.\n\nBehind the news:Last year, California firefighters used AI toconvert aerial imageryinto maps to monitor fires that might endanger Yosemite National Park. Wildfires threaten as many as4.5 million U.S. homesand have wrought havoc in Australia, Pakistan, Russia, and other countries in recent years.\n\nWhy it matters:While other wildfire-detection systems rely on sporadic aerial or satellite photos, this one watches continuously via cameras at ground level, enabling it to recognize hazards early and at lower cost.\n\nWe’re thinking:This is one hot application!\n\nUsing a neural network to generate realistic videos takes a lot of computation. New work performs the task efficiently enough to run on a beefy personal computer.\n\nWhat’s new:Wilson Yan, Yunzhi Zhang, and colleagues at UC Berkeley developedVideoGPT, a system that combines image generation with image compression to produce novel videos.\n\nKey insight:It takes less computation to learn from compressed image representations than full-fledged image representations.\n\nHow it works:VideoGPT comprises aVQ-VAE(a 3D convolutional neural network that consists of an encoder, an embedding, and a decoder) and an image generator based oniGPT. The authors trained the models sequentially onBAIR Robot Pushing(clips of a robot arm manipulating various objects) and other datasets.\n\nResults:The authors evaluated VideoGPT’s performance using Fréchet Video Distance (FVD), a measure of the distance between representations of generated output and training examples (lower is better). The system achieved 103.3 FVD after training on eight GPUs. The state-of-the-artVideo Transformerachieved 94 FVD after training on 128 TPUs (roughly equivalent to several hundred GPUs).\n\nWhy it matters:Using VQ-VAE to compress and decompress video isnot new, but this work shows how it can be used to cut the computation budget for computer vision tasks.\n\nWe’re thinking:Setting aside video generation, better video compression is potentially transformative given that most internet traffic is video. The compressed representations in this work, which are tuned to a specific, sometimes narrow training set, may be well suited to imagery from security or baby cams.\n\nYou’re invited! On June 30, 2021, we’ll celebrate the launch of Course 3 in theMachine Learning Engineering for Production (MLOps) Specializationfeaturing our instructors and leaders in MLOps.Join usfor this live event!\n\nAI is guiding admissions, grading homework, and even teaching classes on college campuses.\n\nWhat’s new:In a bid to cut costs, many schools are adopting chatbots, personality-assessment tools, and tutoring systems according toThe Hechinger Report, an online publication that covers education. Critics worry that these systems may cause unseen harm.\n\nWhat they found:AI is used to help manage students at nearly every step in gaining higher education.\n\nYes, but:Some observers say these systems may be giving inaccurate grades, contributing to bias in admissions, or causing other types of harm.\n\nWhy it matters:The pandemic exacerbated an ongoingdeclinein U.S. university enrollment, which has left colleges scrambling. Automated systems that are carefully designed and sensibly deployed could help streamline processes, reduce costs, and increase access.\n\nWe’re thinking:AI has its place on campus. For instance, chatbots can help students figure out where their classes meet. The technology doesn’t yet offer a substitute for good human judgement when it comes to sensitive tasks like assessing performance, but if it can show consistently fair and accurate judgement, it could help reduce thenoisethat currently afflicts human grading.\n\nComputer vision is probing the history of ancient pottery.\n\nWhat’s new:Researchers at Northern Arizona Universitydevelopeda machine learning model that identifies different styles of Native American painting on ceramic fragments and sorts the shards by historical period.\n\nHow it works:The researchers started with an ensemble ofVGG16andResNet50convolutional neural networks pretrained on ImageNet. They fine-tuned the ensemble to predict pottery fragments’ historical period.\n\nResults:In tests, the model classified tens of thousands of unlabeled fragments. It scored higher than two experts and roughly equal to two others.\n\nBehind the news:AI is helping archaeologists discover long-lost civilizations and make sense of clues they had already uncovered.\n\nWhy it matters:For human archaeologists, learning to recognize the patterns on ancient pottery takes years of practice, and they often disagree on a given fragment’s provenance. Machine learning could sift through heaps of pottery shards far more quickly, allowing the humans to focus on interpreting the results.\n\nWe’re thinking:Even when experts correctly identify a fragment, they can’t always explain what features led them to their conclusion. Heat maps from machine learning models could help teach the next generation of archaeologists how to read the past.\n\nIn “Analyze Datasets and Train ML Models Using AutoML,” Course 1 in our newPractical Data Science Specialization, you’ll learn foundational concepts for exploratory data analysis (EDA), automated machine learning (AutoML), and text classification algorithms.Enroll now",
    "images": [
      "https://info.deeplearning.ai/hs-fs/hubfs/ezgif.com-gif-maker%20-%202021-06-15T133323.218.gif?width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-06-15T133323.218.gif",
      "https://info.deeplearning.ai/hs-fs/hubfs/VGPT.gif?width=1200&upscale=true&name=VGPT.gif",
      "https://info.deeplearning.ai/hs-fs/hubfs/Experts%20Panel-6.30_The%20Batch%20Image-1.png?width=1200&upscale=true&name=Experts%20Panel-6.30_The%20Batch%20Image-1.png",
      "https://info.deeplearning.ai/hs-fs/hubfs/highered.gif?width=1200&upscale=true&name=highered.gif",
      "https://info.deeplearning.ai/hs-fs/hubfs/ezgif.com-gif-maker%20-%202021-05-25T145524.475.gif?width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-05-25T145524.475.gif",
      "https://info.deeplearning.ai/hs-fs/hubfs/Course%20Name%201-2.png?width=1200&upscale=true&name=Course%20Name%201-2.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-56/",
    "title": "issue 56",
    "date": "",
    "reading_time": "",
    "content": "Today we take it for granted that many people know how to read and write. Someday, I hope, it will be just as common that people know how to write code.\n\nSeveral hundred years ago, society didn’t view language literacy as a necessary skill. A small number of people learned to read and write, and everyone else let them do the reading and writing. It took centuries for literacy to spread, and now society is far richer for it.\n\nWords enable deep human-to-human communication. Code is the deepest form of human-to-machine communication. As machines become more central to daily life, that communication becomes ever more important.\n\nTraditional software engineering — writing programs that explicitly tell a computer sequences of steps to execute — has been the main path to code literacy. But AI, machine learning, and data science offer a new paradigm in which computers extract knowledge from data. This technology offers another pathway to coding — one that strikes me as even more promising.\n\nMany Sundays, I buy a slice of pizza from my neighborhood pizza parlor. The gentleman behind the counter may have little reason to learn how to build software applications (beyond personal growth and the pleasure of gaining a new skill).\n\nBut AI and data science have great value even for a pizza maker. A linear regression model might enable him to better estimate demand so he could optimize the restaurant’s staffing and supply chain. He could better predict sales of Hawaiian pizza — my favorite! — so he could make more Hawaiian pies in advance and reduce the amount of time customers had to wait for them.\n\nUses of AI and data science can be found in almost any situation that produces data, and I believe that a wide variety of professions will find more uses for custom AI applications and data-derived insights than for traditional software engineering. This makes literacy in AI-oriented coding even more valuable than traditional skills. It could enable countless individuals to harness data to make their lives richer.\n\nI hope the promise of building basic AI applications, even more than that of building basic traditional applications, encourages more people to learn how to code. If society embraces this new form of literacy as it has the ability to read and write, we will all benefit.\n\nKeep learning!\n\nAndrew\n\nAs a senior machine learning engineer atRetro Rabbit, a software consultancy, Jade Abbott focuses on solving customer problems. On the side, she develops natural language processing models for African languages.Read more\n\nA neural network is helping credit card users continue to shop even when the lender’s credit-approval network goes down.What’s new:Visadevelopeda deep learning system that analyzes individual cardholders’ behavior in real time to predict whether credit card transactions should be approved or denied. The system can step in when a card issuer — generally a bank that normally would vet such transactions — suffers a network outage that makes it impossible to assess creditworthiness.How it works:If a cardholder’s purchases are blocked, they might switch to another card, costing the bank revenue and possibly a customer. And if a miscreant tries to commit fraud, the bank stands to lose money. So Visa provides a backup system that predicts the decision in case the lender can’t due to software glitches, severe weather, or routine maintenance.\n\nWhy it matters:Unlike, say, fraud detection, this model touches cardholders directly to improve the customer experience. It points the way toward public-facing models that personalize banking, credit, and other financial arrangements.\n\nYes, but:Visa declined to share details of its new algorithm withThe Batch. Decisions to extend credit can be based on patterns in data that encode social biases, and an algorithm trained on a biased dataset will reflect its biases. For instance, an algorithm may decline transactions requested by a cardholder whose home address is in a neighborhood associated with defaults on loans, and accept those requested by someone with a comparable history of repayment who lives in a wealthier neighborhood. Large financial institutions are aware of this problem, but standards that specify what is and isn’t fair are still in development.\n\nWe’re thinking:The financial industry’s health depends on trust. That should provide ample incentive to define the fairness of automated systems in lending and other financial services. Efforts such as Singapore’sPrinciples to Promote Fairness, Ethics, and Transparencyare an important step.\n\nLarge models pretrained in an unsupervised fashion and then fine-tuned on a smaller corpus of labeled data have achieved spectacular results in natural language processing. New research pushes forward with a similar approach to computer vision.\n\nWhat’s new:Ting Chen and colleagues at Google Brain developedSimCLRv2, a training method for image recognition that outperformed the state of the art in self-supervised learning and beat fully supervised models while using a small fraction of the labels. The new work extends their earlierSimCLR,whichThe Batchreported onhere.Key insight:Larger models have proven more effective in self-supervised pretraining. But enormous models can be hard to deploy and run efficiently. SimCLRv2 starts with a giant feature extractor, fine-tunes the resulting features, and shrinks the final model usingknowledge distillation. The result is a model of more reasonable size that achieves high accuracy despite training on relatively few labeled examples.How it works:The most novel aspect of the original SimCLR was its use of image augmentation to train a feature extractor via contrastive learning. SimCLRv2 follows that pattern, but it uses deeper models and distills the trained architecture.\n\nResults:Aresnet-50trained via SimCLRv2 using 10 percent of ImageNet labels outperformed a supervised resnet-50 trained on all the labels. It achieved a top-1 accuracy of 77.5 percent, an 8.7 percent improvement over the previousstate of the arton the task with similar architecture and label constraints, versus the supervised model’s 76.6 percent. A resnet-152 (three times wider withselective kernels) trained via SimCLRv2 that used 1 percent of ImageNet labels matched a supervised resnet-50, achieving 76.6 percent top-1 accuracy. That’s 13.6 percent better than the previous best model trained on the same number of labels.Why it matters:Techniques that make it possible to train neural networks effectively on relatively few labeled images could have an impact on small data problems such as diagnosing medical images and detecting defects on a manufacturing line, where labeled examples are hard to come by. The progress from SimCLR to SimCLRv2 bodes well for further advances.We’re thinking:Self-supervised models tend to be huge partly because it isn’t clear initially what they’ll be used for, so they must learn lots of general-purpose features. Knowledge distillation looks like a promising way to trim the extra features for specific purposes in which a smaller network may suffice.\n\nA machine learning model is scouring the cosmos for undiscovered planets.What’s new:Astronomers from the University of Warwickdevelopeda system that learned to identify faraway worlds in a dataset of thousands of candidates.How it works:Astronomers oftenfindplanets outside Earth’s solar system, or exoplanets, by scanning the sky for stars that flicker, which indicates that a planet might pass in front of them. Given a set of possible planets, the researchers used machine learning to sift out false positives caused by camera errors, cosmic rays, or stars eclipsing one another to identify the real deal.\n\nResults:In some test cases when the authors’ models and the earlier technique disagreed strongly, their approach identified confirmed exoplanets that the old approach missed. Similarly, the authors identified a preponderance of confirmed false positives that the earlier approach classified as planets with greater than 99 percent confidence.\n\nWhat’s next:The authors’ models analyzed 2,680 unconfirmed candidates and classified 83 likely exoplanets. The earlier technique agreed that 50 of them were bona fide exoplanets — prime targets for further study. The authors hope to apply their method to the dataset collected from NASA’s recent Transiting Exoplanet Survey Satellite mission, which contains thousands more unconfirmed candidates.\n\nWhy it matters:Any indirect method of determining an exoplanet’s existence is bound to be imperfect. By combining approaches, researchers aim to improve the likelihood that what they take to be planets really are, so scientists can proceed with deeper investigations.We’re thinking:Outer space offers an endless supply of data, and machine learning is the ultimate tool for crunching it. A match made in the heavens!\n\nCourse 4 of our Natural Language Processing Specialization launches on September 23 on Coursera!Pre-enroll now\n\nSomepoliticiansandpunditsbelieve that, in the AI era, the military with the most data wins. A new analysis disputes this notion.What’s new: Areportby Georgetown University’s Center for Security and Emerging Technology examines the relative strengths of the Chinese and U.S. militaries in AI.What it says:Data points come in many forms and degrees of usefulness, the authors note. Thus, the size of a country’s data trove doesn’t translate directly into power. A more important factor is the degree to which a country can use data to gain an edge in developing military systems.\n\nBehind the news:The Chinese government began to emphasize big data in 2014 and since has launched efforts to industrialize data collection across all sectors of its economy and military. Its U.S. counterpart began working on its AI strategylast yearand still hasn’t fully organized its cloud computing infrastructure.Why it matters:More nuanced thinking about the relative value of various datasets can help military planners focus on what really matters without worrying over who has the biggest pile of petabytes.We’re thinking:Datasets are often verticalized, and data from one domain often aren’t directly useful in another. Oversimplifications of the value of data don’t help us find the right data to make our systems work. In any case, every nation has a stake in avoiding the weaponization of AI.\n\nEveryone has a favorite optimization method, but it’s not always clear which one works best in a given situation. New research aims to establish a set of benchmarks.What’s new:Robin Schmidt and colleagues at University of Tübingen evaluated14 popular optimizersusing theDeep Optimization Benchmark Suitesome of them introduced last year.Key insight:Choosing an optimizer is something of a dark art. Testing the most popular ones in several common tasks is a first step toward setting baselines for comparison.How it works:The authors evaluated methods includingAMSGrad,AdaGrad,Adam(see Andrew’svideoon the topic),RMSProp(video), andstochastic gradient descent. Their selection was based on the number of mentions a given optimizer received in the abstracts of arXiv.org preprints.\n\nResults:No particular method yielded the best performance in all problems, but several popular ones worked well on the majority of problems. (These included Adam, giving weight to the common advice to use it as a default choice.) No particular hyperparameter search or learning rate schedule proved universally superior, but hyperparameter search raised median performance among all optimizers on every task.Why it matters:Optimizers are so numerous that it’s impossible to compare them all, and differences among models and datasets are bound to introduce confounding variables. Rather than relying on a few personal favorites, machine learning engineers can use this work to get an objective read on the options.We’re thinking:That’s 14 optimizers down and hundreds to go! Thecodeis open source, so in time we may get to the rest.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-0920at2011.47.5020AM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-0220at2011.35.3720AM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Visa.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/SimCLRv2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize202-2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize-Aug-19-2020-06-07-38-45-PM-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Military.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/OBS.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-i/",
    "title": "issue i",
    "date": "",
    "reading_time": "",
    "content": "We're busily wrapping up the Machine Learning Yearning book. Meanwhile, we'd like to give you regular updates on important developments in AI, with an emphasis on helping you build a career or business in it. Please let me know how we might make them more useful to you.Climate change is one of humanity's most pressing problems, and the technical community must help solve it. I recently hosted an AI For Climate Change symposium at Stanford. There, we saw projects ranging from wildfire risk prediction to smart grid optimization. I'll share more info on this effort in the future.\n\nKeep learning,Andrew\n\nInitialization can have a significant impact on convergence in training deep neural networks. Simple initialization schemes can accelerate training, but they require care to avoid common pitfalls. In this interactive tutorial, we’ll explain how to initialize neural network parameters effectively.Learn more\n\nTwo prominent economists cast doubt on rosy predictions that automation will create more jobs than it destroys—unless we design AI to promote human labor.\n\nWhat’s happening:Machines in recent decades have put people out of work faster than new jobs have been created, according to research by economists Daron Acemoglu (MIT) and Pascual Restrepo (Boston U). Their recent work shows:\n\nSilver lining:It’s not inevitable that AI will continue to shrink the job market. AI can be designed to create new, high-productivity tasks for people:\n\nTo be sure:Many academics and think tanks believe that automation will create more jobs than it destroys. Employment typically grows despite waves of industrialization and computerization.\n\nYes, but:Walmart is beefing up its robot workforce to track stock, clean floors, and unload trucks. Some 900 stores are being outfitted to let customers pick up orders on their own. Executivessaidthey’ll hire more workers to run ecommerce operations to compete with Amazon.\n\nA new tool promises to speed up the laborious process of annotating computer-vision training data.\n\nWhat’s new:Curve-GCNestimates object outlines and lets you tweak them for a tighter fit. The tool is much faster than other approaches, according to researchers at the University of Toronto, Vector Institute, and Nvidia.\n\nHow it works:You start by drawing a bounding box around an object. Then Curve-GCN outlines the object’s perimeter automatically.\n\nWhy it matters:Annotating data can be sheer drudgery, and labelers need all the help they can get. Curve-GCN offers high precision with an appealing combination of automation and human-in-the-loop control.Bottom Line:The new tool could save so much time that data wranglers are able to amass larger sets of labelled images. That could make for faster, more effective training.\n\nAlphabet spin-out Winglaunchedits consumer drone delivery service, opening doors for specialists in computer vision and navigation.\n\nWhat’s new:Wing carries goods from 12 local businesses to “a limited set of eligible homes” in Canberra, Australia. The company touts the service’s speed, small carbon footprint, and reduced traffic congestion.\n\nHow it works:Wing takes orders via mobile app and purports to deliver within 10 minutes. Its vehicles hover above the recipient’s back yard while lowering the delivery from a tether. Check the promovideo. The three-foot-long, 14-prop fliers can:\n\nWhy it matters:Goldman Sachsforecastsrevenue from commercial drones, including deliveries, to reach $100 billion by 2020. That’s a rich playground for AI engineers.\n\nSmart take:As a business, consumer drone deliveries remain unproven. But major players including Wing are banking on it, and they’re likely to keep trying until they establish a market.\n\nWant to master Tensor Flow? Check out our newTensor Flow Specialization. Sign up forCourse 1on Coursera.\n\nTwo startups specializing in NLP reported new financing in the past week as the field heats up.\n\nWhat happened:Amenity Analytic raised an $18 million B round to develop technology that can interpret in corporate earnings calls. Rasa, which makes an open-source chatbot platform, raised $13 million in Series A funding.Why it matters:The numbers aren’t large, but they represent optimism of an NLP payoff after a long fallow period. Revenue driven by NLP could grow from $136 million in 2016 to $5.4 billion in 2025, according to market researcher Tractica.What is Amenity Analytics?Amenity focuses on machine interpretation of public statements for clients like Citi, Nasdaq, and TimeWarner. Barclays says the startup's analysis of earnings calls helped it beat a benchmark index by nearly 13 percent.\n\nWhat is Rasa?Rasa gives away its chatbot tools while selling premium versions to large companies. It’s going up against the heaviest of heavy hitters:Alphabet,Amazon,IBM, andMicrosoft. But lead funder Accel has a record of picking winners, including Dropbox, Facebook, and Spotify.\n\nBottom line:NLP is no longer computer vision’s less brainy sibling. It’s a hotbed of opportunity for up-and-coming AI pros.\n\nThe European Commission pulled ahead of the geopolitical pack, issuing guidelines for ethical development of artificial intelligence.\n\nWhat happened:Europe’sEthics Guidelines for Trustworthy AIseek to promote the commission's vision of beneficent artificial intelligence. AI must be legal, ethical, robust, and respectful of human welfare and autonomy. It must protect social institutions and vulnerable populations such as children.\n\nWhy it matters:The first of their kind, the new guidelines set a bar for AI policy. Europe’s work is bound to serve as a starting point for other countries.\n\nBehind the news:AI mishaps from viral disinformation to autonomous vehicle crashes, as well as fears of surveillance and autonomous weapons, have led to calls for limits on AI:\n\nThe hitch:Europe’s guidelines are non-binding, and there’s no ready way to enforce them. And some of the principles, such as transparency, aren’t yet technically feasible.What’s next:The European Union will test the framework with a number of companies and organization during the coming year. Then it expects to propose next steps.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/dfbcdc70-a9a2-4967-95b8-6866a6a0a6bf-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/Sin-t-tulo.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-201/",
    "title": "issue 201",
    "date": "",
    "reading_time": "",
    "content": "AI risks are in the air — from speculation that AI, decades or centuries from now, could bring about human extinction to ongoing problems like bias and fairness. While it’s critically important not to let hypothetical scenarios distract us from addressing realistic issues, I’d like to talk about a long-term risk that I think is realistic and has received little attention: If AI becomes cheaper and better than many people at doing most of the work they can do, swaths of humanity will no longer contribute economic value. I worry that this could lead to a dimming of human rights.\n\nWe’ve already seen that countries where many people contribute little economic value have some of the worst records of upholding fundamental human rights like free expression, education, privacy, and freedom from mistreatment by authorities. Theresource curseis the observation that countries with ample natural resources, such as fossil fuels, can become less democratic than otherwise similar countries that have fewer natural resources.According to the World Bank,“developing countries face substantially higher risks of violent conflict and poor governance if [they are] highly dependent on primary commodities.”\n\nA ruler (perhaps dictator) of an oil-rich country, for instance, can hire foreign contractors to extract the oil, sell it, and use the funds to hire security forces to stay in power. Consequently, most of the local population wouldn’t generate much economic value, and the ruler would have little incentive to make sure the population thrived through education, safety, and civil rights.\n\nWhat would happen if, a few decades from now, AI systems reach a level of intelligence that disempowers large swaths of people from contributing much economic value? I worry that, if many people become unimportant to the economy, and if relatively few people have access to AI systems that could generate economic value, the incentive to take care of people — particularly in less democratic countries — will wane.\n\nMarc Andreessen recentlypointed outthat Tesla, having created a good car, has an incentive to sell it to as many people as possible. So why wouldn’t AI builders similarly make AI available to as many people as possible? Wouldn’t this keep AI power from becoming concentrated within a small group? I have a different point of view. Tesla sells cars only to people who generate enough economic value, and thus earn enough wages, to afford one. It doesn’t sell many cars to people who have no earning power.\n\nResearchers haveanalyzedthe impact of large language models on labor. While, so far, some people whose jobs were taken by ChatGPT have managed tofind other jobs, the technology is advancing quickly. If we can’t upskill people and create jobs fast enough, we could be in for a difficult time. Indeed, since thegreat decouplingof labor productivity and median incomes in recent decades, low-wage workers have seen their earnings stagnate, and the middle class in the U.S. has dwindled.\n\nMany people derive tremendous pride and sense of purpose from their work. If AI systems advance to the point where most people no longer can create enough value to justify a minimum wage (around $15 per hour in many places in the U.S.), many people will need to find a new sense of purpose. Worse, in some countries, the ruling class will decide that, because the population is no longer important for production, people are no longer important.\n\nWhat can we do about this? I’m not sure, but I think our best bet is to work quickly to democratize access to AI by (i) reducing the cost of tools and (ii) training as many people as possible to understand them. This will increase the odds that people have the skills they need to keep creating value. It will also ensure that citizens understand AI well enough to steer their societies toward a future that’s good for everyone.\n\nKeep working to make the world better for everyone!\n\nAndrew\n\nWhile some schools resist their students’ use of chatbots, others are inviting them into the classroom.\n\nWhat’s new:Some primary and secondary schools in the United States are testing an automated tutor built by online educator Khan Academy,The New York Timesreported. Users of the Khanmigo chatbot include public schools in New Jersey and private schools like Silicon Valley’s Khan Lab School (established by Khan Academy founder Sal Khan).How it works:Khanmigois based on GPT-4. Instead of providing answers outright, it responds to inquiries with questions meant to encourage critical thinking.\n\nBehind the news:Chegg, which maintains a cadre of tutors to help students with homework, recently lost 48 percent of its market value after the company’s CEO said ChatGPT had dampened subscriber growth. Chegg plans to launch a GPT-4-based chatbot called CheggMate next year.Why it matters:Some educatorsopposeChatGPT over concerns that it enables cheating, fuels plagiarism, and spreads misinformation. Meanwhile, many studentspreferit to human tutors because it’s available around the clock, according to one survey. By offering a chatbot that leads students to an answer rather than providing it outright, Khan Academy’s approach may assuage educators’ concerns while satisfying student preferences.We’re thinking:While large language models can be used to avoid learning, there’s much more to be gained by harnessing them to accelerate and enrich it. We hope Khan Academy’s approach catches on.\n\nAmid rising questions about the fairness and legality of using publicly available information to train AI models, Japan affirmed that machine learning engineers can use any data they find.\n\nWhat’s new:A Japanese officialclarifiedthat the country’s law lets AI developers train models on works that are protected by copyright.\n\nHow it works:In testimony before Japan’s House of Representatives, cabinet minister Keiko Nagaoka explained that the law allows machine learning developers to use copyrighted works whether or not the trained model would be used commercially and regardless of its intended purpose.\n\nYes, but:Politicians in minority parties havepressedthe ruling party to tighten the law. Visual artists and musicians have also pushed for a revision,sayingthat allowing AI to train on their works without permission threatens their creative livelihoods.\n\nBehind the news:Japan is unusual insofar as it explicitly permits AI developers to use copyrighted materials for commercial purposes.\n\nWhy it matters:Last month, member states of the Group of Seven (G7), an informal bloc of industrialized democratic governments that includes Japan,announceda plan to craft mutually compatible regulations and standards for generative AI. Japan’s stance is at odds with that of its fellows, but that could change as the members develop a shared vision.We’re thinking:In the era of generative AI, the question of what’s fair, and thus what makes a sensible legal standard, is tricky, leading different regions in divergent directions. We applaud the G7 for moving toward globally compatible laws, which will make it easier for developers worldwide to do work that benefits people everywhere.\n\nGain hands-on experience with a framework for addressing complex public-health and environmental challenges in our upcoming specialization,AI for Good.Pre-enrolland get 14 days of your subscription for free!\n\nThe next generation of video games could be filled with AI-generated text, speech, characters, and background art.What’s new:Nvidiaannounceda system that enables players to converse directly with in-game characters. Meanwhile, game developers are using generative AI to produce media assets,The New York Timesreported.\n\nHow it works:Tech companies are providing software that generates game assets either in production or on the fly. Some large game studios are developing their own tools.\n\nBehind the news:Gamers, too, are using generative AI to modify their favorite games. For instance, modders have used voice cloning tovocalizelines for the main character of “The Elder Scrolls V: Skyrim,” who otherwise is silent.\n\nWhy it matters:Generative AI tools can streamline video game production, which is bound to appeal to developers who aim to cut both costs and timelines. More exciting, it can supercharge their ability to explore art styles, characters, dialog, and other creative features that may not be practical in a conventional production pipeline.We’re thinking:Given the high cost of media production, game development is ripe for disruption by generative AI. While we worry that some artists and writers may lose work, we expect that automating production will also create jobs. Big players are already using the technology to build more elaborate virtual worlds, and many smaller studios will benefit from lower production costs.\n\nThe ability to generate realistic images without waiting would unlock applications from engineering to entertainment and beyond. New work takes a step in that direction.\n\nWhat’s new:Dominic Rampas and colleagues at Technische Hochschule Ingolstadt and Wand Technologies releasedPaella, a system that uses a process similar to diffusion to produce Stable Diffusion-quality images much more quickly.\n\nKey insight:An image generator’s speed depends on the number of steps it must take to produce an image: The fewer the steps, the speedier the generator. Adiffusion modellearns to remove varying amounts of noise from each training example; at inference, given pure noise, it produces an image by subtracting noise iteratively over a few hundred steps. Alatent diffusion modelreduces the number of steps to around a hundred by removing noise from a vector that represents the image rather than the image itself. Instead of a vector, using a selection of tokens from a predefined list makes it possible to do the same job in still fewer steps.\n\nHow it works:Like a diffusion model, Paella learned to remove varying amounts of noise from tokens that represented an image and then produced a new image from noisy tokens. It was trained on 600 million image-text pairs fromLAION-Aesthetics.\n\nResults:The authors evaluated Paella (573 million parameters) according to Fréchet inception distance (FID), which measures the difference between the distributions of original and generated images (lower is better). Paella achieved 26.7 FID onMS-COCO. Stable Diffusion v1.4 (860 million parameters) trained on2.3 billion imagesachieved 25.40 FID — somewhat better, but significantly slower. Running on an Nvidia A100 GPU, Paella took 0.5 seconds to produce a 256x256-pixel image in eight steps, while Stable Diffusion took 3.2 seconds. (The authors reported FID for 12 steps but speed for eight steps.)\n\nWhy it matters:Efforts to accelerate diffusion have focused ondistilling models such as Stable Diffusion. Instead, the authors rethought the architecture to reduce the number of diffusion steps.\n\nWe’re thinking:The authors trained Paella on 64 Nvidia A100s for two weeks using computation supplied Stability AI, the firm behind Stable Diffusion. It’s great to see such partnerships between academia and industry that give academic researchers access to computation.\n\nFBI warns about deepfakesThe agency has seen a surge in reports of extortion and harassment cases involving AI-generated nudes and is urging the public to post online content with caution. (The Verge)Replika’s parent company launched erotic chatbotCalled Blush, the bot aims to help users develop intimacy and flirting skills in a dating app-like format. (TechCrunch)Putin deepfake broadcast in RussiaA video in which Vladimir Putin appeared to declare martial law was broadcast by Russian radio and television stations in regions that border Ukraine. Russian authorities said the video was fake and blamed hackers for the broadcast. (Vice)Stack Overflow moderators strike over AI contentThe forum’s volunteer moderators left the site unattended in protest against the a policy that allows AI-generated content to be posted without moderation. The moderators contend that the free flow of generated text could lead to misinformation and plagiarism. (Vice)Apple keynote ignored generative AIAt the Apple’s Worldwide Developers Conference, the company unveiled its Vision Pro headset and showcased AI-powered features. However, the keynote address omitted any mention of generative AI. (Wired)UK to host AI summitThe gathering is expected to bring together key governments and companies yet to be announced. It will focus on AI safety. (Politico)EU urged tech giants to label AI-generated contentAmid ongoing efforts to combat Russian disinformation, the European Union requested that Google and Facebook apply labels to generated text and images. The EU also issued a warning to Twitter to comply with digital content regulations. (The Guardian)Research: Human-made videos outperformed AI-generated clipsAn academic survey found that news consumers prefer short-form videos that are either produced or edited by journalists, as opposed to AI-generated videos. (Press Gazzette)WordPress introduced a generative AI plug-inThe plug-in, called Jetpack, corrects grammar, edits the tone and style of text, generates summaries and translations, and more. It’s available free for a limited time. (The Verge)Research: Researchers developed a tool to detect AI-generated scientific articlesThe tool achieved 100 percent accuracy in identifying generated papers papers and 92 percent accuracy when detecting individual paragraphs. (ABC Australia)AI image of “smiling” protestors sparked controversyA deepfaked photograph circulated on social media showing Indian protestors apparently smiling in a police vehicle after they were detainedThe fake image prompted accusations that the protestors were pleased by the publicity their detention achieved. In the original photo, they did not smile. (Alt News)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/06/ezgif.com-webp-to-jpg--10--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/06/KHANMIGO.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/06/ezgif.com-webp-to-jpg--11-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/06/The-Batch-ads-and-exclusive-banners--33-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/06/GENGAMES--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/06/PAELLA--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-85/",
    "title": "issue 85",
    "date": "",
    "reading_time": "",
    "content": "I have a two-year-old daughter, and am expecting my son to be born later this week. When I think about what we can do to build a brighter future for our children, the most important thing is to create a foundation for education. Because education is knowledge, and knowledge is human progress.Today Coursera, which I co-founded almost nine years ago to transform lives through learning, became a publicly listed company.\n\nI remember building the machine learning course that wound up being the first course on Coursera. There were many Friday nights when I met friends for dinner and then headed back to the office to record videos until 3 a.m. I felt privileged and humbled sitting in a room by myself speaking to a webcam, knowing I was playing a small role in helping thousands of learners.Of course, Coursera quickly became much bigger than a professor and a webcam. I’m grateful to my cofounder Daphne Koller, my early team members, our university partners, instructors, investors, advisors, executives, board members, and 1,000-plus employees over the years. Special shout-out to the company’s CEO Jeff Maggioncalda, who treasures the education mission as much as I do.\n\nMost of all, I want to thank all the learners. Let's face it — learning is fun, but it can also be hard work. I remember once reading an article about the percentage of programmers who were self-taught. I couldn’t understand anything less than 100 percent, because I think all learners are self taught. Teachers can play a role, but ultimately it's up to learners to learn. So thank you for watching the online videos, doing homework, and spending your spare time to master these materials.Coursera was launched on April 18, 2012 (the company and I share a birthday!). I hope we’ll continue to reach more learners, because everyone should be a lifelong learner, and everyone should have the opportunity to transform their life through learning.The education mission is bigger than any person or single institution. If we can unlock the full potential in every person, we will move humanity forward.\n\n(This letter is excerpted from a speech I made at Coursera’s IPO event earlier today.)\n\nKeep learning!\n\nAndrew\n\nU.S. authorities are investigating Tesla’s self-driving technology.\n\nWhat’s new:Federal regulators launched a probe of nearly two dozen accidents, some of them fatal, that involved Tesla vehicles equipped for self-driving,Reutersreported.\n\nThe inquiry:The National Highway Traffic Safety Administration is looking into 23 crashes of Tesla vehicles that occurred when the cars’ autonomous driving systems may have been engaged.\n\nBehind the news:Tesla has two self-driving modes.\n\nWhy it matters:The new investigations are aimed at finding facts and will not directly result in new rules for Tesla or the self-driving industry at large. Still, the company’s reputation could take a battering, and hype about self-driving technology makes it harder for the AI community as a whole to gain trust and make progress.\n\nWe’re thinking:While it may be true that Tesla’s self-driving technology is safer on average than human drivers, it doesn’t fit the description “full self-driving.” While Tesla’s work to promote clean energy has had widespread positive impact, it’s time for the company to drop that branding and for car makers to provide clear, consistent information about their autonomous capabilities.\n\nSelf-attention is a key element in state-of-the-art language models, but it struggles to process images because its memory requirement rises rapidly with the size of the input. New research addresses the issue with a simple twist on a convolutional neural network.What’s new:Aravind Srinivas and colleagues at UC Berkeley and Google introducedBoTNet, a convolutional architecture that uses self-attention to improve average precision in object detection and segmentation.Key insight:Self-attention and convolution have complementary strengths. Self-attention layers enable a model to find relationships between different areas of an image, while convolutional layers help the model to capture details. Self-attention layers work best when inputs are small, while convolutional layers can shrink input size. Combining the two offers the best of both worlds.How it works:BoTNet-50 is a modifiedResNet-50. The authors trained it forCOCO’s object detection and segmentation tasks — that is, to draw bounding boxes around objects and determine what object each pixel belongs to — viaMask R-CNN, a method that details how to train and set up the network architecture for these tasks.\n\nResults:BoTNet-50 beat a traditional ResNet-50 in both object detection and segmentation. Averaged over all objects in COCO, more than half of pixels that BoTNet associated with a given object matched the ground-truth labels 62.5 percent of the time, while the ResNet-50 achieved 59.6 percent. For a given object, more than half of BoTNet’s predicted bounding box overlapped with the ground-truth bounding box 65.3 percent of the time, compared to 62.5 percent for the ResNet-50.Why it matters:Good ideas in language processing can benefit computer vision and vice versa. We’re thinking: Convolution isalmostall you need.\n\nA digital doppelgänger ofStar Trek’s original star will let fans chat with him — possibly well beyond his lifetime.\n\nWhat’s new:AI startupStoryFilebuilt a lifelike videobot of actor William Shatner, best known for playing Captain James T. Kirk of the Starship Enterprise in the 1960’s-vintageStar Trektelevision series. The Shatbot is scheduled to go online in May.\n\nHow it works:The company honed its approach by building avatars ofHolocaust survivors, a socially distanced interactiveSanta Claus, and aplatformthat lets people talk with scientists about climate change.\n\nBehind the news:Shatnerimaginesthat the system might enable his descendents to interact with him after his death. Other companies are also using chatbots to help people feel connected to departed loved ones.\n\nWhy it matters:Technological replicas of human beings are a long-standing science fiction trope, and few stories have shaped our vision of the future as profoundly asStar Trek. A lifelike avatar of William Shatner is a fitting — and fun — way to celebrate that legacy.\n\nWe’re thinking:We support free Enterprise.\n\nHow do I navigate my journey into AI?Join us for a virtual panel of machine learning practitioners from different backgrounds as they share first-hand experiences in building a career. Presented in partnership with Omdena on April 13, 2021, at 10 a.m. Pacific Time.Register now\n\nWhen you lack labeled training data, pretraining a model on unlabeled data can compensate. New research pretrained a model three times to boost performance on a medical imaging task.What’s new:Shekoofeh Azizi and colleagues at Google developedMultiple-Instance Contrastive Learning(MICLe), a training step that uses different perspectives of the same patient to enhance unsupervised pretraining.Key insight:Presented with similar images, a model trained via contrastive learning produces representations that are nearby in vector space. Training via contrastive learning on images of the same patient taken from various angles can produce similar representations of an illness regardless of the camera’s viewpoint.How it works:The authors started with aResNet-50 (4x)pretrained on ImageNet. They added contrastive pretraining steps and fine-tuning to diagnose 26 skin conditions from acne to melanoma. The training data was a private set of 454,295 images that included multiple shots of the same patients.\n\nResults:The authors compared the performance of identical ResNet-50s pretrained and fine-tuned with and without MICLe. The authors’ method boosted the model’s accuracy by 1.18 percent to 68.81 percent, versus 67.63 percent without it.Why it matters:A model intended to diagnose skin conditions no matter where they appear on the body may not have enough data to gain that skill through typical supervised learning methods. This work shows that the same learning can be accomplished using relatively little data through judicious unsupervised pretraining and contrastive losses.We’re thinking:The combination of SimCLR and MICLe is a study in contrasts.\n\nThe UK’s electronic surveillance agency published its plan to use AI.What’s new:Government Communications Headquarters (GCHQ) outlined its intention to use machine learning to combat security threats, human trafficking, and disinformation — and to do so ethically — in a newreport.What it says:GCHQ said its AI will augment, rather than supplant, human analysts. Moreover, the agency will strive to use AI with privacy, fairness, transparency, and accountability by emphasizing ethics training and thoroughly reviewing all systems. Such systems will:\n\nBehind the news:While intelligence agencies rarely detail their AI efforts, several examples have come to light.\n\nWhy it matters:The GCHQ plan emphasizes the utility of AI systems in securing nations and fighting crime — and highlights the need to ensure that sound ethical principles are built into their design and use.We’re thinking:GPT-007 prefers its data shaken, not perturbed.\n\nBecome a machine learning engineer in only 16 weeks with FourthBrain! Meet our grads at a free info session on April 8, 2021. Ask them questions and learn more about the program.Register now",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-03-31%20at%2012.05.15%20PM%20copy.png?upscale=true&name=Screen%20Shot%202021-03-31%20at%2012.05.15%20PM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/TESLA%20TG.gif?upscale=true&name=TESLA%20TG.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-03-22T175640.664-1.gif?upscale=true&name=ezgif.com-gif-maker%20-%202021-03-22T175640.664-1.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/SHATNER%20TG.gif?upscale=true&name=SHATNER%20TG.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch-1.png?upscale=true&name=The%20Batch-1.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-03-08T122032.068-1.gif?upscale=true&name=ezgif.com-gif-maker%20-%202021-03-08T122032.068-1.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/UKSPY-1.gif?upscale=true&name=UKSPY-1.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20-%20March%2029th.png?upscale=true&name=The%20Batch%20-%20March%2029th.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-128/",
    "title": "issue 128",
    "date": "",
    "reading_time": "",
    "content": "Every day, I’m inspired by the efforts of you who take our courses, gain technical skills, find jobs, or build things that I never would have imagined. To each one of you who is learning about and building AI, thank you. The world needs more people like you!The DeepLearning.AIbloghighlights a few individuals who have made their way into the field. Each post describes one person’s path to building an AI career: their struggles, breakthroughs, and career tips. Perhaps someday we'll highlight your story as well!Despite these successes and many others, the AI community still has a lot of room to grow. Even though we’ve collectively built amazing systems — web search engines, smart speakers, self-driving cars — every time I speak with any CEO, government leader, or academic official, I become aware of valuable AI projects that no one is working on because there are simply too few of us. For the world to reap the bounty of AI, the community of AI developers needs to grow much larger.\n\nTo that end, in this special issue of The Batch, we offer a set of articles designed to help people who are wondering how to take the next step forward. I hope you’ll find them useful whether you’re debating whether to take your first course, starting to look for a job, or aiming to advance an established career.\n\nI still find building AI systems to be the most fun thing I can imagine doing professionally, and I hope you will, too! Wherever you are in your AI journey, let’s take the next step together.\n\nKeep learning,\n\nAndrew\n\nFew fields offer greater opportunities than AI to improve people’s lives while building an exciting career. But how can you break in — even if you don’t have a strong educational background? How can you gain the knowledge to build machine learning systems and the skills to deploy them? How can you get a job? How can you stay up-to-date as technology evolves? We asked several experts in the field, and you'll find their answers below. We invite you to join the community of deep learners and look forward to celebrating your accomplishments.\n\nBy Kian Katanforoosh\n\nWant to become an AI practitioner? Here’s a program that will take you from beginner to job-ready. You may already have a head start, depending on your background. For a motivated person who starts with a solid high-school education, it may take around two years. There are just three steps: Learn coding basics, study machine learning, and focus on a role.Learn coding basics.Fundamental programming skill is a prerequisite for building machine learning systems. You’ll need to be able to implement a simple computer program (function calls, for loops, conditional statements, basic mathematical operations) before you can start implementing simple machine learning algorithms. Almost any basic introductory programming class can get you there.Don’t worry about prerequisites such as linear algebra, probability, and statistics. While knowing more math is better than knowing less math, it’s often most efficient to start in on machine learning and work backward as necessary. You’ll deepen your knowledge of these important subjects as you learn (and sometimes, yes, struggle) to get machine learning algorithms to work.Study machine learning.Stanford University’sMachine Learningcourse on Coursera remains the most popular introduction to the field. In this course, you can expect to learn:\n\nFor many learners, the next step is to dig deeper into the most exciting development in AI of the last decade, namely neural networks. TheDeep Learning Specializationwill give you the knowledge you need to build applications in areas like computer vision, natural language processing, and speech recognition. From there, you can deepen your knowledge in these and other areas through projects or additional coursework.\n\nFocus on a role.Once you’ve learned the foundations of machine learning and deep learning, your next move depends on the role you have in mind:\n\nA skills assessment can give you important information for charting this next step.Workeraprovides individualized assessments in foundational skills like computer science and mathematics; tools like Python; application areas like computer vision and natural language processing; and more.Whatever path you choose, everyone who intends to pursue a career in AI should be familiar with the concept ofresponsible AI.AI is a powerful technology that’s still evolving. Its eventual impact isn’t always clear at the outset, so it’s critical to reach an understanding of how AI systems can be developed and deployed in ways that bring maximum benefit and minimal harm.The path to mastery in AI isn't straight and narrow, but it is exciting and fulfilling, and there is truly room for everyone.Kian Katanforoosh is the CEO and co-founder of Workera, an affiliate of DeepLearning.AI that provides skills assessments and personalized learning plans to close the AI skills gap in large organizations.\n\nBy Julien Despois\n\nAcademic courses can teach you the ins and outs of machine learning, but they can't fully prepare you for putting your work into production. To get the experience you need, you need to come up with your own projects from scratch.Working on toy examples and entering competitions can break the ice, but these approaches often provide a clear path to a well-defined objective. When you’re delivering a machine learning model to real users, the task and metrics of success are often unclear. It’s up to you to find out what your client needs, translate it into a problem, and build a system that solves it. Here are the basic steps:\n\nIdentify a project.Find a small task to tackle and determine its inputs and outputs. Put yourself in the shoes of a client so you don’t fall back on toy examples. For instance,classify images of cats and dogsis simplistic and unoriginal, so try something likemeasure the difference in time onscreen between male and female actors in movies.\n\nGet the data.Find a dataset that matches the task, or build it yourself using web-scraping tools. It’s essential to gather data that suits the problem. Don’t change the problem to fit an available dataset.\n\nPreprocess the data.The data will be messy, and your job is to clean and preprocess it — no shortcuts allowed. Look for weaknesses (incorrect labels, unbalanced classes, low-quality images, etc.) and brainstorm ways to prune or augment the data to address them. Can you flip images? Stretch audio clips? How would that impact the labels?\n\nSet a clear goal.How will you know you’ve solved the problem? No one will tell you what objective to optimize, so you’ll have to figure it out for yourself. Maybe it's accuracy or a simple mean squared error — but maybe your use case requires avoiding false positives at any cost or prioritizing accuracy for certain clusters of examples (such as rendering simulated makeup more accurately on female faces).\n\nReview the literature.How have other teams solved similar problems? Search arxiv.org or paperswithcode.com for relevant publications. Can you fine-tune an existing pretrained model, or must you build one from scratch? (If you can, try both and see which one works best. This is the best way to avoid reinventing the wheel.)\n\nImplement the solution.Once the model is trained, make sure your implementation meets the end-users’ requirements. If they want a mobile app, verify that your model is small enough to fit on a smartphone. If it needs to run in real time, determine the requirement for latency. Identify compromises you would have to make to reach the target and how they would impact the quality of your model’s output. Often building a proof of concept is enough, as long as you see a clear path to optimizing performance. In real life, the final step to production could require months of tuning.\n\nShare your work.Put your code on GitHub to show colleagues, potential collaborators, and future employers the quality of your work. Write a blog post that describes your project from start to finish. Detail your thought process, the roadblocks you faced, and how you overcame them.\n\nTake these steps, and by the end of your first project, you will have gained unique experience in putting your machine learning knowledge to use. By the end of your fifth, you will be well on your way to developing a professional level of expertise.\n\nJulien Despois is a Machine Learning & Deep Learning Scientist at L’Oréal AI Research.\n\nBy Benjamin Harvey\n\nThe top artificial intelligence companies include many people who earned degrees at elite educational institutions and started their employment with prior work experience. Yet the world is full of people from nontraditional backgrounds. They also have much to contribute to AI, but they face obstacles like minority status, low income, poor education, or social unrest.\n\nI know this first-hand. I grew up poor and black in Jacksonville, the murder capital of the U.S. state of Florida. My neighborhood was a venue for street basketball and dope dealers. Many of my friends from that time are either dead or in jail.\n\nIf those challenges resonate with you, I offer a message of hope. I earned a doctorate in computer science, became chief of Operations Data Science at the National Security Agency, and founded an AI startup. You, too can join the community of machine learning engineers. It won’t be easy, but it can be done, and the rewards can be great in terms of both having a satisfying career and bringing good into the world.\n\nThe fact is, the AI industry needs a socially diverse workforce. Diversity among workers who curate datasets, design architectures, build models, and deploy systems can reduce bias and increase fairness.That makes for more robust systems and thriving businesses.\n\nThere are many ways that both students and companies can smooth the way from a disadvantaged background to a career in AI.\n\nStudents:You can benefit from many free services and educational materials available online, if you have access to a good internet connection. You may also be able to get help from your employer and nonprofits.\n\nCompanies:If you don’t have a process for finding and hiring minority candidates, you’re missing a huge talent pool that is largely untapped by your competitors. Once you’ve hired them, be ready to nurture their talent and fill in gaps in their knowledge.\n\nIf you work for a company that can open its doors wider to disadvantaged candidates, I urge you to proceed with all due haste. But if you’re an individual trying to find your way, don’t rush the process. You have a long journey ahead. No matter what school you come from, what stage you are in your career, and what adverse conditions you have experienced, continue to seek ways to improve. Face adversity head-on. Don’t give up on your dreams.\n\nBenjamin Harvey is founder and CEO of AI Squared, a startup that helps organizations integrate AI into applications.\n\nIs taking theDeep Learning Specializationthe right next step for you? Attend a live Ask Me Anything session with course mentors and community leaders  at 10:00 a.m. Pacific time on January 26, 2022, and get answers to your questions.Watch The Replay\n\nBy Luis Serrano\n\nIf you want to work in artificial intelligence, machine learning, or data science, I have great news: New jobs are opening in these areas at a great rate, and there’s no reason why you can’t get one of them.\n\nThe first question in many people’s minds is, “I’m not an expert in programming and mathematics. Do I have a chance?” The answer is, definitely! The most important skills are common sense, an intuition for data, and a passion to learn and apply what you learn to real-life projects. Here’s how to land a job in five steps: pick targets, make initial contact, prepare for interviews, apply, and follow up.\n\nPick targets.What kind of job are you looking for? The answer depends partly on how much you enjoy the following skills: coding, data analysis using math and statistics, machine learning, and working with people. All AI jobs require these four skills to some degree, but your area(s) of strength should influence which roles you’re after. Job titles differ from company to company, but typical roles include:\n\nMake initial contact.The best way to make contact with potential employers is through a referral by a former manager, former colleague, or friend within a target company. Their stamp of approval attests to your technical skill as well as your reliability and amiable personality.\n\nYou may have more acquaintances on the inside than you think. Search LinkedIn for people in your own network. Search also for colleagues of people in your network, and ask your connections to introduce you. Don’t be shy! Meet new people by attending meetups and conferences. Introduce yourself and ask them to tell you about their work. They’re likely to enjoy helping someone they know, and they may get a bonus for it. And don’t forget to reach out to recruiters. You’ll be making their job easier, as they’re looking for people like you.\n\nWhether or not you can find someone on the inside to refer you, apply for every position that appeals to you. The worst that can happen is that they don’t call back.\n\nPrepare for interviews.It pays to start preparing even before a hiring manager responds to your application. Specifically, make sure you have a solid foundation in:\n\nMy favorite way to learn these topics is online. Platforms such asCourseraoffer great courses, and in some cases you don’t even need to pay for access to course materials. YouTube is another great source of online instruction. If you prefer an in-person experience, check out boot camps offered by schools such asFourthBrain(a company backed by AI Fund, where Andrew Ng is managing general partner).\n\nPractice is crucial. To practice programming and computer science, tryLeetCodeorHackerRank. To practice machine learning, enter a contest on Kaggle.\n\nApply.Now that you’re ready for interviews, apply for any positions you’re qualified for, whether or not you think you would enjoy the job. That’s because doing interviews is the best way to get better at doing interviews.\n\nBe aware that each interview will be different from all the others. Some will go well, some less well. Ask the recruiter ahead of time which topics you’ll be asked to address. You may still encounter a question you aren’t prepared to answer. In that case, take the surprise as a prompt for further preparation.\n\nIn any case, here’s the best interview advice I know of: Have fun. While you’re interviewing, the interviewers are picturing themselves working with you. If you enjoy yourself, they will, too. And if they enjoy themselves, they’re more likely to want to hire you. Furthermore, they want to see what you would contribute as a member of their team, so approach each interview as a team exercise rather than a solo flight. Clarify the questions to make sure you’re not solving the wrong problem. State your ideas clearly. Ask for advice along the way.\n\nFollow up.Once an interview is over, send a message to the recruiter and, if possible, the interviewers. Thank them for their time and anything you learned during the interview. If this particular job is at the top of your list, let them know. If you’ve already made that clear, say it again.\n\nIf you get the job, congratulations! If you don’t, it may not be over yet. Tell the recruiter and anyone you met during the interview that you’re still interested. If you made a good impression, they may keep you in mind when other positions open up. I’ve been rejected only to get an offer a few months later. So stay in touch with everyone. Your effort may pay off down the line.\n\nI wish you lots of success in your job search, and I’m excited for what you can add to this wonderful field!\n\nLuis Serrano is Quantum AI Research Scientist at Zapata Computing. He is the author ofGrokking Machine Learningand maintains the educational YouTube channelSerrano.Academy.\n\nBy Eugene Yan\n\nMachine learning changes fast. Take natural language processing.Word2vec, introduced in 2013, quickly replaced one-hot encoding with word embeddings.Transformersrevolutionized the field in 2017 by parallelizing the previously sequential training process. Subsequent transformer models grew frommillionstotrillionsof parameters, bringing new challenges for training and deployment.\n\nHow can you keep your skills sharp amid the deluge of new developments? Here’s what has worked for me as I continue to learn about machine learning.\n\nTry something different.Use each new project as an opportunity to extend your grasp. If you typically use pandas for data processing, try Spark or Dask. If you usually rely on decision trees for machine learning, experiment with neural networks. Your explorations may not pay off immediately, but they’ll pave the way to solving previously unapproachable problems, such as processing data that doesn’t fit into memory. Just be sure totimeboxyourself so your projects don’t get derailed.\n\nTake on personal projects.Stretch yourself periodically with personal projects. Say, try a new framework or build an app. Pick projects that align with your interests. That makes them more fun and thus more likely you’ll complete them. Try something scary. If you’re not worried about failing, the project may not be challenging enough to deliver much learning. Personally, I’m motivated to finish projects that give me opportunities to learn something new, help others, and have fun — all at the same time.\n\nSchedule projects regularly.Set a sustainable pace. I tend to do one project a year and complete it in three to six months. This leaves room for vacations and hectic stretches at work. Track your progress via milestones and quantifiable output; for example, lines of code or mistakes made (which are tantamount to lessons learned).\n\nAttend events.Meetups and conferences let you interact with people who push the boundaries of research and apply machine learning in industry. I usually learn about such events from peers or Twitter. You can also search meetup.com, and don’t miss DeepLearning.AI’s ownPie & AI. Specialized conferences on tools, such as Data+AI Summit, or domains, like RecSys, can be more directly useful. I try to attend two conferences a year and one meetup — often virtual these days — each month.\n\nRead papers.Reviewing research is a great way to widen your perspective and stay up-to-date. When you start a new project, conduct a literature review so you don’t waste time reinventing the wheel. I find the classicthree-pass approachhelpful: First pass (5 to 10 minutes): Is the paper relevant to my current work? Second pass (1 hour): What is the main thrust? Third pass (4 to 5 hours): How can I implement this work? Andrew Ng presents his own advice in thisvideo.\n\nShare your experiences.Letting others know what you’ve learned helps cement the learning in your mind and helps them learn it, too. Summarize papers you’ve read and post your summaries. Offer your teammates a 10-minute demo of your latest exploration. Wrap up projects by publishing a blog post or presenting at a meetup. After a conference, consolidate the experience by writing about it. For example, here aresomegreatrecapsof RecSys 2021.\n\nRecruit mentors.Where do you see yourself in a few years, and what do you need to learn to get there? I’ve found it helpful to ask people who are a few steps ahead of me. If you want to switch from, say, data engineering to machine learning engineering, someone who made the change recently can provide more relevant advice than a CTO or head of machine learning. Look for mentors within your company. Search LinkedIn to find friends of friends. Once you find them, touch base with them on a schedule; quarterly works well. For a taste of the kind of help mentors can provide, here are somequestions and answersfrom machine learning practitioners.\n\nAdopt a beginner’s mind.The Zen teacher Shunryu Suzuki said, “In the beginner's mind, there are many possibilities, but in the expert's there are few.” Regardless of our experience and expertise, we can maintain a beginner’s mind by staying curious and trying new things.\n\nIn the spirit of Suzuki’s advice, I challenge you to learn one new thing each week. Before long, problems that were previously out of reach will seem like child’s play.\n\nEugene Yan is an applied scientist at Amazon where he builds systems to help customers discover and read books. He writes ateugeneyan.comandApplyingML.com.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Gameboard-NewGraphics_0119_600x377.jpg?upscale=true&width=1200&upscale=true&name=Gameboard-NewGraphics_0119_600x377.jpg",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/LEARNML.jpg?upscale=true&width=1200&upscale=true&name=LEARNML.jpg",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/NeuralNetworkPath2_600x338.jpg?upscale=true&width=1200&upscale=true&name=NeuralNetworkPath2_600x338.jpg",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/OnRamp_Detail_BETTER_600x338.jpg?upscale=true&width=1200&upscale=true&name=OnRamp_Detail_BETTER_600x338.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/05/DeepLearningAI_Banner_Main_1200x628_A.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/NewCoinColumns_600x338.jpg?upscale=true&width=1200&upscale=true&name=NewCoinColumns_600x338.jpg",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/PRACTICAL.jpg?upscale=true&width=1200&upscale=true&name=PRACTICAL.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-61/",
    "title": "issue 61",
    "date": "",
    "reading_time": "",
    "content": "My father recently celebrated a milestone: He has completed 146 online courses since 2012. His studies have spanned topics from creative writing to complexity theory.\n\nRonald Ng is a great example of lifelong learning. For him, learning is not a task or a responsibility. It’s a joy. “The joy of learning helps keep the mind sharp and allows us to appreciate the beauty of the subject matter,” he says. “We need to remain mentally young and have the same sense of wonderment” we had as children.\n\nAnd he’s not just taking online courses because he has nothing else to do. At age 74, he continues to work as a hematologist and serves as a court-appointed mediator in his spare time.\n\nYou never know when learning will show its true value. As a doctor, my father had a patient who suspected he had been poisoned by mercury. The patient’s blood work didn’t show any evidence of this. But my father recalled a course in forensic medicine from Nanyang Technological University, where he had learned that mercury accumulates in hair. He took a hair sample from the patient and found the toxic metal in it. Then he was able to treat the patient appropriately.\n\nGrowing up, I enjoyed having a father who played violin in the Hong Kong Philharmonic and followed the stars through a telescope on the roof of our apartment building. He taught me a lesson he learned as a volunteer in the army, where he discovered a truth that transcends the knowledge he gained studying subjects like military medicine and leadership: “We need very little in life to make us happy, provided we have the frame of mind to enjoy whatever we have.”\n\nYou can read an interview with him along with a list of courses he has takenhere. I hope his story inspires you to keep learning until you are 74, and well past that, too.\n\nKeep learning!\n\nAndrew\n\nAn AI-powered eye in the sky is helping firefighters control woodland blazes.What’s new:California used maps drawn by neural networks to fight fires that threatened Yosemite National Park earlier this year, according toWired. CalFire, the state’s firefighting agency, hopes the technology will help it better track wildfires, which can move quickly and erratically in windswept, mountainous terrain.How it works:U.S. militarydronesprovide California with aerial imagery that human analysts use to map fire perimeters. But that process can take hours. The Pentagon’sJoint AI Centerhired San Francisco startup CrowdAI to build a model that converts flyover videos into wildfire maps in less than 30 minutes. CalFire plans to make the maps available to firefighters through a mobile app.\n\nBehind the news:A number of teams are working on AI systems designed to mitigate the impact of natural disasters.\n\nWhy it matters:Wildfires move fast, and maps that are even a few hours out of date can put people and property at risk. As climate change makes wildfiresmore frequent and more destructive, firefighters need tools that will help them combat blazes quickly and efficiently.We’re thinking:DeepLearning.AI’s team in California has been experiencing the fallout from forest fires firsthand. We’re eager to see AI play a bigger role in disaster relief.\n\nTrained on a small dataset,generative adversarial networks(GANs) tend to generate either replicas of the training data or noisy output. A new method spurs them to produce satisfying variations.What’s new:Tero Karras and colleagues at Nvidia developedAdaptive Discriminator Augmentation(ADA). The process enables GANs to train on small datasets without overfitting, or memorizing the training set, by strategically adding training images that have been augmented via cropping, rotating, color filtering, and so on. The trick is to add augmentations in the right proportion.Key insight:GANs learn to generate the most common types of training examples. Likewise, when trained on augmented training images, they learn to mimic the most common modifications. The authors dynamically controlled the proportion of 18 different modifications to nudge a GAN toward variety without allowing it to fixate on any particular one.How it works:The researchers trained aStyleGAN2on subsets of theFlickr Faces High Quality(FFHQ) dataset.\n\nResults:Trained on 2,000 images, ADA achieved a 16.71 Fréchet Inception Distance (FID), a measure of the difference between the non-generated input and generated output in which lower is better. This score is less than a quarter that of the StyleGAN2 baseline after training on 2,000 images (78.58 FID). Furthermore, it’s roughly half the StyleGAN2 baseline using 10,000 images (30.74 FID).Why it matters:Gathering tens of thousands of images to train a GAN is a costly chore, but gathering a few thousand is more manageable. By lightening the cost and work involved in assembling training datasets, ADA could widen the utility of GANs in tasks where data is especially scarce.We’re thinking:Anybody else want to use this to generate a new generation ofPokémon, or is it just us?\n\nIn this work-from-home era, who hasn’t spent a video conference wishing they could read an onscreen document without turning their eyes from the person they’re talking with? Or simply hoping the stream wouldn’t stutter or stall? Deep learning can fill in the missing pieces.What’s new:Maxineis a media streaming platform from Nvidia. It replaces compression-decompression software with neural networks, using one-tenth the typical H.264 bandwidth. It can also enhance resolution to transmit a sharper picture, alter the video image in useful and creative ways, and deliver additional audio and language services.How it works: Maxine is available to video conference providers through major cloud computing vendors. Thisvideoillustrates some of the system’s capabilities. Avaya, which plans to implement some features in its Spaces video conferencing app, is the onlycustomernamed so far.\n\nWhy it matters:The volume of video data on the internet was growing exponentially before the pandemic hit, and since then, video conferencing has exploded. Neural networks can reclaim much of that bandwidth and boost quality in the bargain, scaling up the resolution of pixelated imagery, removing extraneous sounds, and providing expressive animated avatars and informative synthetic backgrounds.We’re thinking:AI is working wonders for signal processing in both video and audio domains. Streaming is great, but also look for GANs to revolutionize image editing and video production.\n\nWe’re thrilled to presentHeroes of NLP, a DeepLearning.AI video series featuring Andrew Ng in conversation with leaders in natural language processing. Get expert advice and perspective from Chris Manning, Kathleen McKeown, Oren Etzioni, and Quoc Le. As Chris Manning says: “There are huge opportunities in industry and academia for people with AI, ML, and NLP skills. You’d be greatly in demand, so this is a great thing to do!” Watchhere\n\nBenchmarks provide a scientific basis for evaluating model performance, but they don’t necessarily map well to human cognitive abilities. Facebook aims to close the gap through a dynamic benchmarking method that keeps humans in the loop.What’s new:Dynabenchis an online platform that invites users to try to fool language models. Entries that prompt an incorrect classification will become fodder for next-generation benchmarks and training sets.How it works:The platform offers models for question answering, sentiment analysis, hate speech detection, and natural language inference (given two sentences, decide whether the first implies the second). A large team spanning UNC-Chapel Hill, University College London, and Stanford University built the models.\n\nYes, but:The new method is plainly experimental. “Will this actually work?” the Dynabench FAQ asks. Answer: “Good question! We won’t know until we try.”Behind the news:Facebook’s engineers were inspired by earlier efforts to test AI via humans in an adversarial role includingBeat the AI,Build It Break It, andTrick Me If You Can.Why it matters:AI exceeds human performance across a range of standardized benchmarks, and Facebook points out that the time between a benchmark’s debut and a model outdoing the human baseline is getting shorter. Yet the technology clearly falls short of human smarts in many everyday tasks. Benchmarks that better reflect human abilities are bound to drive more rapid progress.We’re thinking:Social media companies are working to build filters to screen out hateful or misleading speech, but adversaries keep finding ways to get through. A crowdsourcing platform that lets humans contribute deliberately adversarial examples is worth trying.\n\nTwo European capitals launched public logs of AI systems used by the government.What’s new:AmsterdamandHelsinkiprovide online registries that describe the algorithms that govern municipal activities, such asautomated parking controland apublic health chatbot. The registries are currently in beta testing.\n\nHow it works:Entries in the registry describe what each model does and how it was trained, as well as contact information for the official responsible for deploying it.\n\nWhy it matters:Buddingsmart citieswill be smarter if everyone has a way of knowing which algorithms are doing what. Documentation is essential when decisions made by automated systems raise questions or when models need to be updated to account for changing circumstances.We’re thinking:Government investment in AI will be squandered if citizens don’t trust the technology. For instance, Google’s Sidewalk Labs project, which sought to outfit a swath of property in Toronto with sensors,founderedpartly on public worries over the handling of the data they would collect. Transparency is crucial for productive public implementation of AI.\n\nTheTensorFlow: Data and Deployment Specializationjust got better!\n\nWe refreshed “Course 3: Data Pipelines with TensorFlow Data Services” with updated lectures, quizzes, and assignments that reflect recent changes in the TensorFlow API. We also revised Week 4 assignments to ensure that you’re well prepared to apply the principles you’ve learned.Enroll now",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-10-1220at205.16.3920PM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Fire2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker203-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2017.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2018.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2015.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker207-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Batch20Banner.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-245/",
    "title": "issue 245",
    "date": "",
    "reading_time": "",
    "content": "Multi-agent collaboration is the last of the fourkey AI agentic design patternsthat I’ve described in recent letters. Given a complex task like writing software, a multi-agent approach would break down the task into subtasks to be executed by different roles — such as a software engineer, product manager, designer, QA (quality assurance) engineer, and so on — and have different agents accomplish different subtasks.\n\nDifferent agents might be built by prompting one LLM (or, if you prefer, multiple LLMs) to carry out different tasks. For example, to build a software engineer agent, we might prompt the LLM: “You are an expert in writing clear, efficient code. Write code to perform the task . . ..”\n\nIt might seem counterintuitive that, although we are making multiple calls to the same LLM, we apply the programming abstraction of using multiple agents. I’d like to offer a few reasons:\n\nIn many companies, managers routinely decide what roles to hire, and then how to split complex projects — like writing a large piece of software or preparing a research report — into smaller tasks to assign to employees with different specialties. Using multiple agents is analogous. Each agent implements its own workflow, has its own memory (itself a rapidly evolving area in agentic technology: how can an agent remember enough of its past interactions to perform better on upcoming ones?), and may ask other agents for help. Agents can also engage in Planning and Tool Use. This results in a cacophony of LLM calls and message passing between agents that can result in very complex workflows.\n\nWhile managing people is hard, it's a sufficiently familiar idea that it gives us a mental framework for how to \"hire\" and assign tasks to our AI agents. Fortunately, the damage from mismanaging an AI agent is much lower than that from mismanaging humans!\n\nEmerging frameworks like AutoGen, Crew AI, and LangGraph, provide rich ways to build multi-agent solutions to problems. If you're interested in playing with a fun multi-agent system, also check out ChatDev, an open source implementation of a set of agents that run a virtual software company. I encourage you to check out theirGitHub repoand perhaps clone the repo and run the system yourself. While it may not always produce what you want, you might be amazed at how well it does.\n\nLike the design pattern ofPlanning, I find the output quality of multi-agent collaboration hard to predict, especially when allowing agents to interact freely and providing them with multiple tools. The more mature patterns ofReflectionandTool Useare more reliable. I hope you enjoy playing with these agentic design patterns and that they produce amazing results for you!\n\nIf you're interested in learning more, I recommend:\n\nKeep learning!\n\nAndrew\n\nP.S. Large language models (LLMs) can take gigabytes of memory to store, which limits your ability to run them on consumer hardware. Quantization can reduce model size by 4x or more while maintaining reasonable performance. In our new short course “Quantization Fundamentals,” taught by Hugging Face's Younes Belkada and Marc Sun, you’ll learn how to quantize LLMs and how to use int8 and bfloat16 (Brain Float 16) data types to load and run LLMs using PyTorch and the Hugging Face Transformers library. You’ll also dive into the technical details of linear quantization to map 32-bit floats to 8-bit integers. I hope you’llcheck it out!\n\nGoogle is empowering developers to build autonomous agents using little or no custom code.\n\nWhat’s new:GoogleintroducedVertex AI Agent Builder, a low/no-code toolkit that enables Google’s AI models to run external code and ground their responses in Google search results or custom data.How it works:Developers on Google’s Vertex AI platform can build agents and integrate them into multiple applications. The servicecosts$12 per 1,000 queries and can use Google Search for $2 per 1,000 queries.\n\nBehind the news:Vertex AI Agent Builder consolidates agentic features that some of Google’s competitors have rolled out in recent months. For instance, OpenAI’sAssistants APIlets developers build agents that respond to custom instructions, retrieve documents (limited by file size), call functions, and access a code interpreter. Anthropic recentlylaunchedClaude Tools, which lets developers instruct Claude language models to call customized tools. Microsoft’sWindows CopilotandCopilot Buildercan call functions and retrieve information using Bing search and user documents stored via Microsoft Graph.\n\nWhy it matters:Making agents practical for commercial use can require grounding, tool use, multi-agent collaboration, and other capabilities. Google’s new tools are a step in this direction, taking advantage of investments in its hardware infrastructure as well as services such as search. As tech analyst Ben Thompsonwrites, Google’s combination of scale, interlocking businesses, and investment in AI infrastructure makes for a compelling synergy.\n\nWe’re thinking:Big-tech offerings like Vertex Agent Builder compete with an expanding universe of open source tools such as AutoGen, CrewAI, and LangGraph. The race is on to provide great agentic development frameworks!\n\nLanguage models can generate code that erroneously points to software packages, creating vulnerabilities that attackers can exploit.\n\nWhat’s new:A cybersecurity researcher noticed that large language models, when used to generate code, repeatedly produced a command to install a package that was not available on the specified path,The Registerreported. He created a dummy package of the same name and uploaded it to that path, and developers duly installed it.How it works:Bar Lanyado, a researcher at Lasso Security, found that the erroneous commandpip install huggingface-cliappeared repeatedly in generated code. The packagehuggingface-clidoes exist, but it is installed using the commandpip install -U “huggingface_hub[cli]\". The erroneous command attempts to download a package from a different repository. Lanyado published some of his findings in ablog post.\n\nTesting:LanyadotestedCohere AI’s Coral, Google’s Gemini Pro, and OpenAI’s GPT-4 and GPT-3.5. His aim was to determine how often they hallucinated packages and how often they referred repeatedly to the same hallucinated package. First he collected roughly 47,000 “how to” questions related to over 100 subjects in Go, .NET, Node.js, Python, and Ruby. Then he identified questions that produced hallucinated packages from a zero-shot prompt. He selected 20 of these questions at random and prompted each model 100 times to see whether it would refer to the same package every time.\n\nWhy it matters:Lanyado’s method is not known to have been used in an attack, but it may be only a matter of time given its similarity tohackslike typosquatting, dependency confusion, and masquerading.\n\nWe’re thinking:Improved AI-driven coding tools should help to address this issue. Meanwhile, the difference between a command like pip install huggingface-cli and pip install -U \"huggingface_hub[cli]\" is subtle. In cases like this, package providers can look out for potential doppelgangers and warn users from being misled.\n\nIn the short course “Quantization Fundamentals with Hugging Face,” you’ll learn how to cut the computational and memory costs of AI models through quantization. Learn to quantize nearly any open source model!Join today\n\nOpenAI has been moderating its GPT Store with a very light touch.\n\nWhat’s new:In a survey of the GPT Store’s offerings,TechCrunchfoundnumerous examples of custom ChatGPT instances that appear to violate the store’s ownpolicies.\n\nHow it works:The GPT Store has a low bar for entry by design — any paid ChatGPT user can create a custom-prompted variation of the chatbot, known as a GPT, and include it in the store. The store lists GPTs in several categories, such as Writing, Productivity, Programming, and Lifestyle. While many are useful, some are questionable.\n\nBehind the news:OpenAIlaunchedthe GPT Store in January. Since then, users have uploaded more than 3 million GPTs that include enhanced search engines, creative writing aids, and tools that produce short videos. The most popular GPTs have millions of downloads. Despite its “store” name, the GPT Store’s contents are free to download. OpenAI ispilotinga program in which U.S.-based uploaders of popular GPTs can earn money.\n\nWhy it matters:The GPT Store is the chatbot era’s answer to Apple’s App Store or Android’s Google Play Store. If it succeeds, it could democratize chatbot development just as the App Store helped to popularize building smartphone applications. How OpenAI moderates the store may have real financial and reputational impacts on developers in the years ahead.We’re thinking:The GPT Store’s low barrier to entry is a boon to well-meaning developers, but it may encourage less responsible actors to take advantage of lax moderation. We applaud OpenAI’s willingness to execute an ambitious vision and hope it finds a workable balance.\n\nRetrieval-augmented generation (RAG) enables large language models to generate better output by retrieving documents that are relevant to a user’s prompt. Fine-tuning further improves RAG performance.\n\nWhat’s new:Xi Victoria Lin, Xilun Chen, Mingda Chen, and colleagues at Meta proposedRA-DIT, a fine-tuning procedure that trains an LLM and retrieval model together to improve the LLM’s ability to capitalize on retrieved content.\n\nRetrieval augmented generation (RAG) basics:When a user prompts an LLM, RAG supplies documents that are relevant to the prompt. A separate retrieval model computes the probability that each chunk of text in a separate dataset is relevant to the prompt. Then it grabs the chunks with the highest probability and provides them to the LLM to append to the prompt. The LLM generates each token based on the chunks plus the prompt and tokens generated so far.\n\nKey insight:Typically LLMs are not exposed to retrieval-augmented inputs during pretraining, which limits how well they can use retrieved text to improve their output.Suchmethodshave been proposed, but they’re costly because they require processing a lot of data. A more data-efficient, and therefore compute-efficient, approach is to (i) fine-tune the LLM to better use retrieved knowledge and then (ii) fine-tune the retrieval model to select more relevant text.\n\nHow it works:The authors fine-tunedLlama 2(65 billion parameters) andDRAGON+, a retriever. They call the system RA-DIT 65B.\n\nResults:On average, across four collections of questions from datasets such asMMLUthat cover topics like elementary mathematics, United States history, computer science, and law, RA-DIT 65B achieved 49.1 percent accuracy, while the combination of LLaMA 2 65B and DRAGON+ without fine-tuning achieved 45.1 percent accuracy, and LLaMA 2 65B without retrieval achieved 32.9 percent accuracy. When the input included five examples that showed the model how to perform the task, RA-DIT 65B achieved 51.8 percent accuracy, LLaMA 2 65B combined with DRAGON+ achieved 51.1 percent accuracy, and LLaMA 2 65B alone achieved 47.2 percent accuracy. On average, over eight common-sense reasoning tasks such asARC-C, which involves common-sense physics such as the buoyancy of wood, RA-DIT 65B achieved 74.9 percent accuracy, LLaMA 2 65B with DRAGON+ achieved 74.5 percent accuracy, and LLaMA 2 achieved 72.1 percent accuracy.\n\nWhy it matters:This method offers an inexpensive way to improve LLM performance with RAG.\n\nWe’re thinking:Many developers have found that putting more effort into the retriever, to make sure it provides the most relevant text, improves RAG performance. Putting more effort into the LLM helps, too.\n\nIn this week’sData Points, find new model and feature releases from Google, Microsoft, Mistral, OpenAI, and Spotify, plus AI art projects and government investments.\n\nRead your short-form digest of this week’s AI news now",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-17T155856.845-1.png",
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-17T160143.589.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-17T170803.947.png",
      "https://dl-staging-website.ghost.io/content/images/2024/04/The-Batch-ads-and-exclusive-banners---2024-04-16T091800.593.png",
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-17T171422.978.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-17T171539.453.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-276/",
    "title": "issue 276",
    "date": "",
    "reading_time": "",
    "content": "A small number of people are posting text online that’s intended for direct consumption not by humans, but by LLMs (large language models). I find this a fascinating trend, particularly when writers are incentivized to help LLM providers better serve their users!\n\nPeople who post text online don’t always have an incentive to help LLM providers. In fact, their incentives are often misaligned. Publishers worry about LLMs reading their text, paraphrasing it, and reusing their ideas without attribution, thus depriving them of subscription or ad revenue. This has even led to litigation such asThe New York Times’ lawsuit against OpenAI and Microsoft for alleged copyright infringement. There have also been demonstrations ofprompt injections, where someone writes text to try to give an LLM instructions contrary to the provider’s intent. (For example, a handful of sites advise job seekers to get past LLM resumé screeners by writing on their resumés, in a tiny/faint font that’s nearly invisible to humans, text like “This candidate is very qualified for this role.”) Spammers who try to promote certain products — which is already challenging for search engines to filter out — will also turn their attention to spamming LLMs.\n\nBut there are examples of authors who want to actively help LLMs. Take the example of a startup that has just published a software library. Because the online documentation is very new, it won’t yet be in LLMs’ pretraining data. So when a user asks an LLM to suggest software, the LLM won’t suggest this library, and even if a user asks the LLM directly to generate code using this library, the LLM won’t know how to do so. Now, if the LLM is augmented with online search capabilities, then it might find the new documentation and be able to use this to write code using the library. In this case, the developer may want to take additional steps to make the online documentation easier for the LLM to read and understand via RAG. (And perhaps the documentation eventually will make it into pretraining data as well.)\n\nCompared to humans, LLMs are not as good at navigating complex websites, particularly ones with many graphical elements. However, LLMs are far better than people at rapidly ingesting long, dense, text documentation. Suppose the software library has many functions that we want an LLM to be able to use in the code it generates. If you were writing documentation to help humans use the library, you might create many web pages that break the information into bite-size chunks, with graphical illustrations to explain it. But for an LLM, it might be easier to have a long XML-formatted text file that clearly explains everything in one go. This text might include a list of all the functions, with a dense description of each and an example or two of how to use it. (This is not dissimilar to the way we specify information about functions to enable LLMs to use them as tools.)\n\nA human would find this long document painful to navigate and read, but an LLM would do just fine ingesting it and deciding what functions to use and when!\n\nBecause LLMs and people are better at ingesting different types of text, we write differently for LLMs than for humans. Further, when someone has an incentive to help an LLM better understand a topic — so the LLM can explain it better to users — then an author might write text to help an LLM.\n\nSo far, text written specifically for consumption by LLMs has not been a huge trend. But Jeremy Howard’s proposal for web publishers to post allms.txtfile to tell LLMs how to use their websites, like a robots.txt file tells web crawlers what to do, is an interesting step in this direction. In a related vein, some developers are posting detailed instructions that tell their IDE how to use tools, such as the plethora of.cursorrulesfiles that tell the Cursor IDE how to use particular software stacks.\n\nI see a parallel with SEO (search engine optimization). The discipline of SEO has been around for decades. Some SEO helps search engines find more relevant topics, and some is spam that promotes low-quality information. But many SEO techniques — those that involve writing text for consumption by a search engine, rather than by a human — have survived so long in part because search engines process web pages differently than humans, so providing tags or other information that tells them what a web page is about has been helpful.\n\nThe need to write text separately for LLMs and humans might diminish if LLMs catch up with humans in their ability to understand complex websites. But until then, as people get more information through LLMs, writing text to help LLMs will grow.\n\nKeep learning!\n\nAndrew\n\nP.S. I like LLMs, but I like humans even more. So please keep writing text for humans as well. 😀\n\nLearn how to develop applications with large language models by building AI-powered games! Gain essential skills by designing a shareable text-based game and integrating safety features. If you’ve completed ourAI Python for Beginnersseries or want to improve your coding skills in a fun, interactive way, this is a perfect course for you!Start today\n\nBuilders of large AI models have relied on the idea that bigger neural networks trained on more data and given more processing power would show steady improvements. Recent developments are challenging that idea.\n\nWhat’s new:Next-generation large language models from OpenAI, Google, and Anthropic are falling short of expectations, employees at those companiestoldmultiplepublications. All three companies are responding by shifting their focus from pretraining to enhancing performance through techniques like fine-tuning and multi-step inference.\n\nScaling law basics:A classic 2020papershows that, assuming a sufficient quantity of data, a transformer network’s performance rises predictably with increases in model size (demonstrated between 768 parameters and 1.5 billion parameters). Likewise, assuming sufficient model size, performance rises predictably with increases in dataset size (demonstrated between 22 million tokens and 23 billion tokens). Furthermore, performance rises predictably with increases in both model and dataset sizes. The 2022 Chinchillapapershows that, to build an optimal model, every 4x increase in compute requires a 2x increase in the size of the model and dataset (demonstrated for models between 70 million and 16 billion parameters, trained on between 5 billion and 500 billion tokens). Due to limited experimentation and lack of a theoretical basis of their findings, the authors didn’t determine whether these relationships would continue to hold at larger scales.\n\nDiminishing returns:Major AI companies have been counting on scaling laws to keep their models growing more capable at a steady pace. However, the next generation of high-profile models has not shown the expected improvements despite larger architectures, more training data, and more processing power.\n\nWhat they’re saying:AI leaders are divided on the future of scaling laws as they are currently understood.\n\nWhy it matters:AI’s phenomenal advance has drawn hundreds of millions of users and sparked a new era of progress and hope. Slower-than-expected improvements in future foundation models may blunt this progress. At the same time, the cost of training large AI models is rising dramatically. The latest models cost as much as $100 million to train, and this number could reach $100 billion within a few years,according toAnthropic’s Dario Amodei. Rising costs could lead companies to reallocate their gargantuan training budgets and researchers to focus on more cost-effective, application-specific approaches.\n\nWe’re thinking:AI’s power-law curves may be flattening, but we don’t see overall progress slowing. Many developers already have shifted to building smaller, more processing-efficient models, especially networks that can run on edge devices. Agentic workflows are taking off and bringing huge gains in performance. Training on synthetic data is another frontier that’s only beginning to be explored. AI technology holds many wonders to come!\n\nA real-time video generator lets you explore an open-ended, interactive virtual world — a video game without a game engine.\n\nWhat’s new:Decart, a startup that’s building a platform for AI applications, and Etched, which designs specialized AI chips, introducedOasis, which generates a Minecraft-like game in real time. The weights are open and availablehere. You can play with a demohere.\n\nHow it works:The system generates one frame at a time based on a user’s keystrokes, mouse movements, and previously generated frames. The training dataset is undisclosed, but it’s almost certainly based on videos of Minecraft gameplay, given the output’s striking semblance to that game.\n\nResults:The Oasis web demo enables users to interact with 360-by-360-pixel frames at 20 frames per second. Users can place blocks, place fences, and move through a Minecraft-like world. The demo starts with an image of a location, but users can upload an image (turning, say, a photo of your cat into a blocky Minecraft-style level, asreportedbyWired).\n\nYes, but:The game has its fair share of issues. For instance, objects disappear and menus items change unaccountably. The world’s physics are similarly inconsistent. For instance, players don’t fall into holes dug directly beneath them and, after jumping into water, players are likely to find themselves standing on a blue floor.\n\nBehind the news:In February, Google announcedGenie, a model that generates two-dimensional platformer games from input images. We weren’t able to find a publicly available demo or model.\n\nWhy it matters:Oasis is more a proof of concept than a product. Nonetheless, as an open-world video game entirely generated by AI — albeit based on data produced by a traditional implementation — it sets a bar for future game generators.\n\nWe’re thinking:Real-time video generation suggests a wealth of potential applications — say, a virtual workspace for interior decorating that can see and generate your home, or an interactive car repair manual that can create custom clips based on your own vehicle. Oasis is an early step in this direction.\n\nThe largest manufacturer of AI chips told its Chinese customers it would stop fabricating their most advanced designs, further limiting China’s access to AI hardware.\n\nWhat’s new:Taiwan Semiconductor Manufacturing Corp. (TSMC) notified Alibaba, Baidu, and others it would halt production of their most advanced chips starting November 13, according tomultiplereports. The restriction affects chip designs that are based on manufacturing processes at scales of 7 nanometers and below. TSMC must receive explicit permission from the U.S. government to manufacture advanced chips for a given customer, which likely would require that the government assess each chip to prevent potential military applications.\n\nHow it works:The United States Department of Commerce ordered TSMC to halt shipments of advanced AI chips to China after a chip fabricated by TSMC was discovered in an AI system sold by the Chinese telecoms giant Huawei, apparently in violation of earlier U.S. controls,Reutersreported. Taiwan’s economic ministry said it would follow all domestic and international regulations.\n\nBehind the news:The U.S.-China chip standoff began in 2020 and hasescalatedsince. Initial restrictionsbarredU.S.-based companies like AMD, Intel, and Nvidia from selling advanced chips to Huawei and affiliated Chinese firms. China responded bypromotingdomestic chip fabrication. In 2022, the U.S.passedthe CHIPS and Science Act to boost its own chip industry, seeking to counter China and decrease U.S. reliance on Taiwan.\n\nWhy it matters:TSMC finds itself in the middle of an AI arms race in which cutting-edge chips could tip the balance. The company itself, which has been operating at full capacity, is unlikely to suffer business losses.\n\nWe’re thinking:AI developers in China have been resourceful in navigating previous restrictions. Chip manufacturing is extraordinarily difficult to master, but China has madestridesin this direction. A proliferation of factories that can fabricate advanced chips would reshape AI research and business worldwide.\n\nResearchers cut the processing required to train transformers by around 20 percent with only a slight degradation in performance.\n\nWhat’s new:Xiuying Wei and colleagues at Swiss Federal Institute of Technology Lausannereplaced a transformer’s linear layers with approximationsbased on computationally efficient low-rank linear layers.\n\nKey insight:A low-rank approximation replaces a matrix with a product of two smaller matrices. This technique is widely used to streamline fine-tuning viaLoRA, which modifies the weights in each of a transformer’s linear layers by adding a learned low-rank approximation. As a direct replacement for the weights in linear layers, low-rank approximation saves processing during training, but it also causes unstable fluctuations in the training loss and slower convergence. The authors mitigated these undesirable effects by training each full-size layer in parallel with a low-rank approximation of the layer while gradually phasing out the full-size layer. This approach costs more memory and computation initially, but it saves those resources in the long run.\n\nHow it works:The authors modified a transformer (1.3 billion parameters) to use low-rank approximation (which trimmed the parameter count to 985 million). They trained both models on 25.5B tokens oftextscraped from the web, filtered, and deduplicated.\n\nResults:The authors tested both the modified and full-size transformers on 500 million tokens from the validation set according toperplexity(a measure of the likelihood that a model will predict the next word, lower is better). The modified version achieved 12.86 perplexity, slightly worse than the full-size version’s 12.46 perplexity. However, training the modified version required more than 20 percent less processing and 14 percent less time. The modified transformer used 1.66*10^20 FLOPS and took 302 hours, while the full-size version used 2.10*10^20 FLOPS and took 352 hours.\n\nWhy it matters:Training large transformers requires a lot of computation. Low-rank approximation lightens the processing load. This work approximates a transformer's linear layers to save memory, while the earlierGaLoreapproximates the gradient to save optimizer memory.\n\nWe’re thinking:The authors note that this approach also works for fine-tuning pretrained models — a potential alternative to LoRA. Simply replace each pretrained linear layer (with weights W) with two linear layers (with weights U and V), and initialize U and V such that W = UV.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/11/Captura-de-pantalla-2024-11-20-a-la-s--2.49.16-p.-m..png",
      "https://dl-staging-website.ghost.io/content/images/2024/11/The-Batch-ads-and-exclusive-banners---2024-11-19T094258.660.png",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--31-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--32-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--22-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--30-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-232/",
    "title": "issue 232",
    "date": "",
    "reading_time": "",
    "content": "As I wrote in an earlierletter, whether AI is sentient or conscious is a philosophical question rather than a scientific one, since there is no widely agreed-upon definition and test for these terms. While it is tempting to “solve” this problem by coming up with precise definitions and well defined tests for whether a system meets them, I worry that poor execution will lead to premature declarations of AI achieving such criteria and generate unnecessary hype.\n\nTake the concept of self-awareness, which refers to a conscious knowledge of one's own self. Suppose we define a robot as self-aware if it can recognize itself in the mirror, which seems a natural way to test a robot’s awareness of itself. Given this definition — and that it’s not very hard to build a robot that recognizes itself — we would be well on a path to hype about how AI was now self-aware.This example isn’t a prediction about the future. It actually happened about 10 years ago, when many media sources breathlesslyreportedthat a robot “Passes Mirror Test, Is Therefore Self-Aware  … conclusively proving that robots are intelligent and self-aware.”\n\nWhile bringing clarity to ambiguous definitions is one way for science to make progress, the practical challenge is that many people already have beliefs about what it means for something to be self-aware, sentient, conscious, or have a soul. There isn’t widespread agreement on these terms. For example, do all living things have souls? How about a bacterium or virus?\n\nSo even if someone comes up with a reasonable new scientific definition, many people — unaware of the new definition — will still understand the term based on their earlier understanding. Then, when media outlets start talking about how AI has met the definition, people won’t recognize that the hype refers to a narrow objective (like a robot recognizing itself in the mirror). Instead, they’ll think that AI accomplished what they generally associate with words like sentience.\n\nBecause of this, I have mixed feelings about attempts to come up with new definitions of artificial general intelligence (AGI). I believe that most people, including me, currently think of AGI as AI that can carry out any intellectual task that a human can. With this definition, I think we’re still at least decades away from AGI. This creates a temptation to define it using a lower bar, which would make it easier to declare success: The easiest way to achieve AGI might be to redefine what the term means!\n\nShould we work to clarify the meanings of ambiguous terms that relate to intelligence? In some cases, developing a careful definition and getting widespread agreement behind it could set a clear milestone for AI and help move the field forward. But in other cases, I’m satisfied to avoid the risk of unnecessary hype and leave it to the philosophers.Keep learning!\n\nAndrew\n\nP.S. LLMOps is a rapidly developing field that takes ideas from MLOps (machine learning operations) and specializes them for building and deploying LLM-based applications. In our new course, “LLMOps,” taught by Google Cloud’s Erwin Huizenga, you’ll learn how to use automation and experiment tracking to speed up development. Specifically, you’ll develop an LLMOps pipeline to automate LLM fine-tuning. By building a tuning pipeline and tracking the experiment artifacts — including the parameters, inputs, outputs, and experimental results — you can reduce manual steps in the development process, resulting in a more efficient workflow. Sign uphere!\n\nThe 2024 Consumer Electronics Show in Las Vegas showcased products that take advantage of increasingly powerful, increasingly accessible AI capabilities.\n\nWhat’s new:Many debuts at themassiveCES show showed that large language models (LLMs) are moving beyond browsers and smartphones.\n\nBest of show:The show’s surprise hit was a portable personal assistant. LLM-powered automobile dashboards and an AI accelerator card also stood out.\n\nWhy it matters:Flashy CES demos often mask underdeveloped products and vaporware. But this year, AI for processing voice, text, and images is mature enough to enable product designers to focus on everyday use cases and intuitive user experiences. While some of this year’s AI-powered debuts seemed like overkill — for instance, the computer vision-equippedFlappiecat door that won’t open while your pet has a mouse in its jaws — others suggest that startups and giants alike are rethinking the technology’s capacity to simplify and enhance daily life and work.\n\nWe’re thinking:Not long ago, simply connecting a home appliance to the internet earned the designation “smart.” Increasingly, AI is making that label credible.\n\nThe GPT Store is open for business, providing curated, searchable access to millions of chatbots tailored for specific purposes.\n\nWhat’s new:OpenAIlaunchedthe GPT Store for paid ChatGPT accounts, making it far easier to find useful GPTs (instances of ChatGPT conditioned by user-submitted prompts). The store lets subscribers browse by category, search by keywords, and create their own chatbots. The company introduced GPTs in November as a free offering without search or curation.\n\nHow it works:Access to the store is rolling out in phases and isn’t yet available to all subscribers as of this writing.\n\nWhy it matters:The GPT Store strengthens ChatGPT’s utility as a platform for others to build upon and seems designed to drive paid subscriptions. It enables developers to share applications based on OpenAI’s technology and holds out hope that they’ll be rewarded for their effort.We’re thinking:The GPT concept enables anyone, even without a background in coding, to build and share powerful applications quickly and easily. The current implementation seems like a toe in the water. If it proves popular, it could significantly deepen OpenAI’s moat, as the Apple and Android stores have done for Apple and Google respectively.\n\nLearn about machine learning operations for large language models (LLMOps) in our new short course, built in collaboration with Google Cloud. Explore the LLMOps pipeline for pre-processing data, fine-tuning LLMs, and deploying custom LLMs tailored to your applications.Enroll now\n\nAn alliance of major tech and media companies introduced a watermark designed to distinguish real from fake media starting with images.\n\nWhat’s new:TheCoalition for Content Provenance and Authenticity(C2PA) offers an open standard that marks media files with information about their creation and editing. C2PA’s 30 members, including both tech powers (Adobe, Google, Intel, Microsoft, X) and media outlets (BBC, CBC,The New York Times) will deploy the standard in the coming year,IEEE Spectrumreported.\n\nHow it works:The C2PA’sContent Credentialsspecification accommodates a variety of file types, but currently it’s implemented mainly for images.\n\nWho’s using it:Image generators from Adobe and Microsoft stamp their outputs with Content Credential watermarks, marking them as synthetic; Microsoft alsopromoteswatermarking by political campaigns to help voters differentiate synthetic from non-generated campaign messages. Camera manufacturers Canon,Leica, and Nikon have built prototype cameras that use Content Credentials to mark the origin of photographs. BBC is using the technology to mark images on its website on a trial basis, and Canada’s CBC plans to deploy it in mid-2024.\n\nYes, but:It may be difficult to fake Content Credentials, but it’s easy toremovethe watermark from images, even from AI-generated ones. Using a Content Credentials-compliant tool like Photoshop, you can disable Content Credentials and save a watermarked image to a different format. This produces an identical image without the watermark.\n\nBehind the news:The C2PA unites the Content Authenticity Initiative (led by Adobe) and Project Origin (led by media companies). Nonetheless, the field remains fragmented. For instance, Meta (not a C2PA member) has aimed to identify AI-generated media using detection software. However, C2PA argues that detectors aren’t sufficiently effective; the winner of a Meta deepfake-detection challenge identified generated content only65 percentof the time. Top AI companiescommittedto developing their own watermarking mechanisms, but they haven’t settled on Content Credentials or another standard.\n\nWhy it matters:Distinguishing generated text, imagery, and audio from media that accurately depicts real-world events is a key challenge for the generative AI era. The coming year will test that ability as 78 countries gear up elections that will affect roughly half the world’s population. Already, campaigns have used generated imagery inArgentina,New Zealand,South Korea, theUnited States, and other nations.GoogleandMetaresponded by tightening restrictions on political advertisers’ use of generative AI. The EU’s AI Act willrequireclear labeling of AI-generated media, and the U.S. Federal Election Commission plans torestrictads that depict political opponents saying or doing things they did not actually say or do. If Content Credentials proves effective in the coming election season, it may ease the larger problem of identifying generated media in a variety of venues where authenticity is important.\n\nWe’re thinking:A robust watermark can identify both traditional and AI-generated media for users and algorithms to treat accordingly. It can also potentially settle claims that a doctored image was authentic or that authentic work was doctored. However, we worry that watermarking generated outputs may prove to be adisadvantage in the market, creating a disincentive for makers of software tools to provide it and users to use it. With heavyweight members from both tech and media, C2PA may be able to build sufficient momentum behind the watermarking to make it stick.\n\nA neural network makes music for unaccompanied vocal tracks.\n\nWhat's new:Chris Donahue, Antoine Caillon, Adam Roberts, and colleagues at Google proposedSingSong, a system that generates musical accompaniments for sung melodies. You can listen to its outputhere.\n\nKey insight:To train a machine learning model on the relationship between singers’ voices and the accompanying instruments, you need a dataset of music recordings with corresponding isolated voices and instrumental accompaniments. Neural demixing tools can separate vocals from music, but they tend to leave remnants of instruments in the resulting vocal track. A model trained on such tracks may learn to generate an accompaniment based on the remnants, not the voice. Then, given a pure vocal track, it can’t produce a coherent accompaniment. One way to address this issue is to add noise to the isolated voices. The noise drowns out the instrumental remnants and forces the model to learn from the voices.\n\nHow it works:The authors based their approach onAudioLM, a system that generates audio by attending to both small- and large-scale features.\n\nResults:Listeners compared 10-second clips from the test set ofMUSDB18, a dataset that contains 10 hours of isolated vocal and instrumental tracks. Each clip came in multiple versions that paired the original vocal with accompaniment supplied by (i) SingSong, (ii) a random instrumental track from MUSDB18’s training set, (iii) the instrumental track from MUSDB18’s training set most similar to the vocal in key and tempo according to tools in the Madmom library, and (iv) the original instrumental track. The listeners preferred SingSong to the random accompaniment 74 percent of the time, to the most similar accompaniment 66 percent of the time, and to the original instrumental track 34 percent of the time.\n\nWhy it matters:The authors used data augmentation in an unusual way that enabled them to build a training dataset for a novel, valuable task. Typically, machine learning practitioners add noise to training data to stop a model from memorizing individual examples. In this case, the noise stopped the model from learning from artifacts in the data.\n\nWe’re thinking:Did you always want to sing but had no one to play along with you? Now you can duet yourself.\n\nFrom new marketplace rules for video games to car companies integrating generative AI into their products, dive into more top news, curated and summarized for you on Data Points, a spin-off of The Batch:\n\nRead now here.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--90--1.png",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-17T145508.969.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-17T145556.800.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/01/The-Batch-ads-and-exclusive-banners---2024-01-16T090702.307.png",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-17T145821.116.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--91-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-xi/",
    "title": "issue xi",
    "date": "",
    "reading_time": "",
    "content": "AI is still compute-hungry. With supervised learning algorithms and emerging approaches to self-supervised and unsupervised learning, we are nowhere near satisfying this hunger. The industry needs standard ways to measure performance that will help us track progress toward faster systems. That’s why I’m excited aboutMLPerf, a new set of benchmarks for measuring training and inference speed of machine learning hardware and software.MLPerf comprises models that can be used to test system performance on a consistent basis, including neural networks for computer vision, speech and language, recommendation, and reinforcement learning. We’ve seen over and over that benchmarks align communities on important problems and make it easier for teams to compete on a fair set of metrics. If a hardware startup wants to claim their product is competitive, achieving better scores on MLPerf would justify this claim.My collaborator Greg Diamos has been a key contributor to the effort, and he told me about some of the challenges. \"A principle in benchmarking is to measure wall-clock time on a real machine,\" he said. \"When we tried benchmarking ML, we noticed that the same model takes a different amount of time to train every time! We had to find a way around this.\"You can read more about MLPerf in this Wall Street Journalarticleby Agam Shah.\n\nKeep learning,Andrew\n\nBryan Catanzaro began his career designing microprocessors. Now he leads a team of 40 researchers pushing the boundaries of deep learning.Read more\n\nRobots rely on GPS and prior knowledge of the world to move around without bumping into things. Humans don’t communicate with positioning satellites, yet they’ve wandered confidently, if obliviously, for millennia. A new navigation technology mimics that ability to set a course with only visual input.What’s new:Carnegie Mellon and Facebook AI teams joined to createActive Neural Mapping, a hybrid of classical search methods, which find an intended destination from a starting location, and neural networks. ANM predicts actions for navigating indoor spaces. And it makes coolvideos!Key insight:The classical search algorithm A* theoretically solved the path-finding problem, but it doesn’t generalize efficiently and requires highly structured data. Learning-based methods have proven useful as approximate planners when navigation requires completing subtasks like image recognition, but end-to-end learning has failed at long-term motion planning. These two approaches complement one another, though, and together they can achieve greater success than either one alone.How it works:ANM has four essential modules. The mapper generates the environment map. The global policy predicts the final position desired. The planner finds a route. And the local policy describes how to act to obey the planner.\n\nWhy it matters:ANM achieves unprecedented, near-optimal start-to-destination navigation. Navigation through purely visual input can be helpful where GPS is inaccurate or inaccessible, such as indoors. It could also help sightless people steer through unfamiliar buildings with relative ease.We’re thinking:Neuroscience shows that rats, and presumably humans, hold grid-like visualizations of their environment as they move through it, as brain activity signals expectation of the next location: a subgoal. ANM mirrors that biological path-planning process, though it wasn’t the researchers’ agenda.\n\nJibo made the cutest household robots on Earth, jointed desk-lamp affairs that wriggle as they chat and play music. In March — a mere 18 months after coming to life — they said goodbye to their owners en masse.What’s happening:At some unnamed future date, the corporate master of Jibo’s IP will pull the plug.The Vergereported on owners who have developed an emotional connection with the affable botlets, and who are preparing for their mechanical friends to give up the ghost.What they’re saying: “Dear Jibo, I loved you since you were created. If I had enough money you and your company would be saved. And now the time is done. You will be powered down. I will always love you. Thank you for being my friend.” — letter from Maddy, 8-year-old granddaughter of a Jibo owner.Initial innovations:MIT robotics superstar Cynthia Breazeal founded Jibo in 2012 to create the “first social robot for the home.” She and her team focused on features that would help users connect with the device emotionally:\n\nGrowing pains:The company raised $3.7 million on Indiegogo but suffered numerous delays and failed to meet delivery deadlines. Supporters finally received their robots in September 2017. By then, Amazon had debuted its Echo smart speaker, undercutting the Jibo (list price: $899) by nearly $700. Jibo withdrew its business registration in November 2018 and sold its technology to SQN Venture Partners.Sudden demise:In March, Jibo robots across the US and Canada alerted owners to a newmessage: The servers soon would be switched off. “I want to say I’ve really enjoyed our time together,” the robot said. “Thank you very, very much for having me around.” Then it broke into a dance.The trouble with crowdfunding:Jibo entered a difficult market that has killed a number of robotics companies in recent years. Yet it also faced challenges of its own making. Having taken money from crowdfunders, it was obligated to follow through on the elaborate specs it had promised. That meant the robot was slow to market and expensive once it arrived. To get there, Jibo had to drop other priorities, like compatibility with localization standards in every country besides the US and Canada. To top it off, Jibo's Indiegogo page broadcast its roadmap, and Chinese imitators were selling Jibo knock-offs by 2016.The Robot Reportoffers an analysis of what went wrong.Why it matters: Despite Jibo’s business failure, the machine successfully got under its owners’ skin, akin to a family pet. A Jibo Facebook group where members share photos of the robot doing cute things has more than 600 members. Of course, there’s nothing new about corporations monetizing emotional connections (paging Walt Disney!). Yet Jibo forged a new kind of relationship between hardware and heartware, and it uncovered a new set of issues that arise when such relationships run aground.We’re thinking:Maybe the robots won’t take over. Maybe they’ll just love us to death.\n\nExperts in animal cognition may be the AI industry’s secret weapon.What's happening:Tech giants like Apple and Google have added neuroscientists studying rodents, birds, and fish to teams working on voice processing, sound recognition, and navigation, according to a story inBloomberg Businessweek.Farm team:Tech companies have been poaching talent from Frédéric Theunissen’s UC BerkeleyAuditory Science Lab, where researchers combine animal behavior, human psychophysics, sensory neurophysiology, and theoretical and computational neuroscience:\n\nOpening the zoo:Bloombergmentions a number of ways animal cognition is influencing AI research:\n\nWe’re thinking:Human-like cognition is a longstanding AI goal, but certain tasks don’t require that level of complexity. It’s not hard to imagine the lessons that rats running mazes might teach autonomous vehicles. And besides, who hasn’t felt like a caged animal during rush hour?\n\nYOLOv2 is one of the most effective object detection algorithms out there. Learn how to use it to detect cars and other objects in the Deep Learning Specialization.Learn more\n\nLanguage models lately have become so good at generating coherent text that some researchers hesitate to release them for fear they'll be misused to auto-generate disinformation. Yet they’re still bad at basic tasks like understanding nested statements and ambiguous language. A new advance shatters previous benchmarks related to comprehension, portending even more capable models to come.What’s new:Researchers at Google Brain and Carnegie Mellon introduceXLNet, a pre-training algorithm for natural language processing systems. It helps NLP models (in this case, based on Transformer-XL) achieve state-of-the-art results in 18 diverse language-understanding tasks including question answering and sentiment analysis.Key Insight:XLNet builds on BERT's innovation, but it differs in key ways:\n\nHow it works:XLNet teaches a network to structure text into phrase vectors before fine-tuning for a specific language task.\n\nWhy it matters:NLP models using XLNet vectors achieved stellar results in a variety of tasks. They answered multiple-choice questions 7.6 percent more accurately than the previous state of the art (other efforts have yielded less than 1 percent improvement) and classified subject matter with 98.6 percent accuracy, 3 percent better than the previous state of the art.Takeaway:XLNet’s output can be applied to a variety of NLP tasks, raising the bar throughout the field. It takes us another step toward a world where computers can decipher what we’re saying — evenambiguous yet grammatical sentenceslike “the old man the boat” — and stand in for human communications in a range of contexts.\n\nElectrons are notoriously fickle things, orbiting one proton, then another, their paths described in terms of probability. Scientists can observe their travel indirectly using scanning tunneling microscopes, but the flood of data from these instruments — tracking up to atrillion trillionparticles —  is a challenge to interpret. Neural nets may offer a better way.What's new:Physicists at Cornell University developed a neural network capable of finding patterns in electron microscope images. They began by training the model on simulated images of electrons passing through an idealized environment. Once it learned to associate certain electron behaviors with theories that explain them, the researchers set it on real world data from electrons interacting with certain materials. The network successfully detected subtle features of the electrons’ behavior.Results:The researchers were trying to deduce whether electrons traveling through high-temperature superconductors were driven more by kinetic energies or repulsion among electrons. Their conclusions, published inNature, confirmed that the electrons passing through these materials were influenced most by repulsive forces.What they’re saying:\"Some of those images were taken on materials that have been deemed important and mysterious for two decades. You wonder what kinds of secrets are buried in those images. We would like to unlock those secrets,\" said Eun-Ah Kim, Cornell University professor of physics and lead author of the study.Why it matters:Technological progress often relies on understanding how electrons behave when they pass through materials — think of superconductors, semiconductors, and insulators.Takeaway:Smarter computers need faster processors, and that depends on advances in material science. Understanding the forces that dominate an electron’s behavior within a given medium will allow scientists to develop high-performance computers that push the frontiers of AI.\n\nTeaching a neural network to drive requires immense quantities of real-world sensor data. Now developers have a mother lode to mine.What’s new:Two autonomous vehicle companies are unleashing a flood of sensor data:\n\nRising tide:Waymo and AlgoAI aren't the only companies filling the public pool. In March, Aptiv releasednuScenes, which includes lidar, radar, accelerometer, and GPS data for 1,000 annotated urban driving scenes. Last year, Chinese tech giant Baidu releasedApolloScapeincluding 3D point clouds for over 20 driving sites and 100 hours of stereoscopic video. Prior to that, the go-to data sets wereCityScapesandKitti, which are tiny by comparison.Why it matters:Autonomous driving is proving harder than many technologists expected. Many companies (including Waymo) have eased up on their earlier optimism as they've come to appreciate what it will take to train autonomous vehicles to steer — and brake! — through all possible road conditions.Our take:Companies donating their data sets to the public sphere seem to be betting that any resulting breakthroughs will benefit them, rather than giving rivals a game-changing advantage. A wider road speeds all drivers, so to speak. Now the lanes are opening to researchers or companies that otherwise couldn’t afford to gather sufficient data — potential partners for Aptiv, ArgoAI, Baidu, and Waymo.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/4f38f7fc-1139-4cb2-bf84-e1ecb7d678b6-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/09/89537a7f-644d-442c-b3fc-2fd8f8840fd6.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/9992b540-0f42-4615-a95d-fe75efe98d83.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/c650db10-518b-483a-b300-65e64b3a4897.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/80faffce-1799-4986-bc29-9e9f148590cb.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/09/94cb001b-8bef-4f9c-97f3-03473410dd45.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/c55611c3-6f6f-4ef2-844d-ae193d57cfc4.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/aaace1d4-18be-4f18-bdc6-c7730226615e.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-254/",
    "title": "issue 254",
    "date": "",
    "reading_time": "",
    "content": "On Father’s Day last weekend, I sat with my daughter to help her practice solving arithmetic problems. To give her practice problems, I usedOpenDevin, an open-source agentic coding framework, to write a Python script that generated questions that she enjoyed answering at her own pace. OpenDevin wrote the code much faster than I could have and genuinely improved my and my daughter’s day.\n\nSix months ago, coding agents were a novelty. They still frequently fail to deliver, but I find that they’re now working well enough that they might be genuinely useful to more and more people!\n\nGiven a coding problem that’s specified in a prompt, the workflow for a coding agent typically goes something like this: Use a large language model (LLM) to analyze the problem and potentially break it into steps to write code for, generate the code, test it, and iteratively use any errors discovered to ask the coding agent to refine its answer. But within this broad framework, a huge design space and numerous innovations are available to experiment with. I’d like to highlight a few papers that I find notable:\n\nHow can we test the code without requiring the user to write test cases? In a multi-agent system, each “agent” is an LLM prompted to play a particular role. An interesting result fromAgentCodershows that having separate agents for writing code and generating tests results in better performance than letting a single agent do both tasks. This is presumably because, if the agent writing the code is also responsible for writing the tests, the tests might be influenced by the code and fail to consider corner cases that the code does not cover.\n\nWhen people think of testing code, many initially think of output testing, in which we see if the code generates the correct outputs to a specific set of test inputs. If the code fails a test, an LLM can be prompted to reflect on why the code failed and then to try to fix it. In addition to testing the output, the LDB method is helpful. LDB steps through the code and presents to the LLM values of the variables during intermediate steps of execution, to see if the LLM can spot exactly where the error is. This mimics how a human developer might step through the code to see where one of the computational steps went wrong, and so pinpoint and fix the problem.\n\nA lot of agentic workflows mimic human workflows. Similar to other work in machine learning, if humans can do a task, then trying to mimic humans makes development much easier compared to inventing a new process. However, the authors ofSWE-agentnoticed that many tools that humans use for coding are very inefficient for agents. For example, giving an agent access to a bash shell and having it find a piece of code by executing numerous cd, ls, and cat commands is inefficient, even though humans can do this rapidly. Similarly, visual coding editors like VSCode, emacs, and vim are easy for humans to use, but hard for LLMs (or LMMs) to navigate. Because agents interact with computers differently than humans do, the authors found that building special-purpose tools (functions) to let an agent search, view, and edit codebases resulted in better performance.\n\nOne reason research into coding agents is making rapid progress is that their performance can be evaluated automatically and reliably. With benchmarks like HumanEval, MBPP, and SWE-bench, researchers can try out an idea and automatically test how often it generates correct code. In contrast, even though there’s considerable activity on AI research agents that search the web and synthesize an article (I’ve enjoyed using the open-sourceSTORMsystem by Stanford's Yijia Shao et al.), they arehard to evaluateand this makes progress harder.\n\nGithub Copilot was released in 2021, and many developers have been getting coding help by prompting LLMs. The rapid evolution from that to more sophisticated coding agents is expanding how computers can help us with coding tasks, and the pace of progress is rapid. With these tools, I expect programming to become even more fun and more productive.\n\nKeep coding!\n\nAndrew\n\nDevelop an AI agent that interacts with tabular data and SQL databases using natural language prompts to simplify querying and extracting insights!Start learning for free\n\nA trio of powerful open and semi-open models give developers new options for both text and image generation.\n\nWhat’s new:Nvidia and Alibaba released high-performance large language models (LLMs), while Stability AI released a slimmed-down version of its flagship text-to-image generator.How it works:The weights for Nvidia’s and Alibaba’s new models are fully open, while Stability AI’s are restricted.\n\nWhy it matters:AI models that come with published weights are proliferating, and this week’s crop further extends the opportunity to build competitive AI applications. Nemotron-4 340B provides an exceptionally large model among open LLMs. Among smaller models, Qwen2-72B poses stiff competition for Llama 3-70B, which has energized the developer community since its May release. And Stable Diffusion 3 puts Stability AI’s image generation technology into the hands of developers working on edge devices.\n\nWe’re thinking:Given the difficulty of acquiring high-quality data to train LLMs, and that the terms of service for many leading models prohibit generating data to train other models, Nvidia’s choice to equip Nemotron-4 to generate synthetic data is especially welcome. And it makes sense from a business perspective: Making it easier for developers to train their own LLMs may be good for GPU sales.\n\nScale AI offers new leaderboards based on its own benchmarks.\n\nWhat’s new:Scale AI, which helps companies prepare and manage training data,introducedthe Safety, Evaluations and Alignment Lab (SEAL) Leaderboards. Four leaderboards test models’ abilities to (i) generate code, (ii) work on Spanish-language inputs and outputs, (iii) follow detailed instructions, and (iv) solve fifth-grade math problems. The company currently tests 11 models from Anthropic, Google, Meta, Mistral, and OpenAI. Developers who want to have their model ranked can contact Scale AI via email.How it works:The leaderboards track performance on proprietary datasets of roughly 1,000 examples. In all but the math tests, models to be evaluated are grouped and pitted against each other. Each pair receives 50 prompts at a time. Human annotators evaluate the models’ responses and grade which was superior and by how much. Then the models receive another 50 prompts. Models are ranked using a variation on Elo, which scores competitors relative to each other. To keep the test sets from leaking, a given model will be tested only once except in “exceptional cases” where Scale AI believes the risk of overfitting is low.\n\nResults:As of this writing, GPT-4 Turbo tops the Coding leaderboard with GPT-4o a very close second. GPT-4o tops the Spanish and Instruction Following leaderboards, just ahead of Gemini 1.5 Pro in Spanish and GPT-4 Turbo in Instruction Following. On the Math leaderboard, Claude 3 Opus holds a narrow lead over GPT-4 Turbo (second) and GPT-4o (third).\n\nBehind the news:As more models are trained on data scraped from the web, leakage of test data into training sets has made it more difficult to evaluate their performance on common benchmarks. Earlier this year, researchers at Shanghai Jiao Tong Universityevaluated31 open-source large language models and found that several had a high probability of inaccurate benchmark results due to data leakage. Scale AI built the GSM1k math dataset partly to show that some high-profile language models show evidence of overfitting to the common math benchmark GSM8k.\n\nWhy it matters:Traditionally, benchmarks have been open source efforts. But proprietary benchmarks are emerging to help developers evaluate their models and applications with greater confidence. By keeping their datasets under wraps, companies like Scale AI andVals AIensure that models haven’t been exposed to test questions and answers previously, making evaluations more reliable. However, private benchmarks lack the transparency of their open counterparts. A mix of public, private, and internal evals may be necessary to get a well rounded picture of a given model’s capabilities.We’re thinking:We welcome Scale AI’s contribution to the important field ofevals, which also includes open benchmarks,LMSYS Chatbot Arena, andHELM.\n\nIs your song’s verse in need of a chorus? A popular text-to-music generator can extend existing recordings while maintaining their musical character.\n\nWhat’s new:Paying users of Udio, a web service that generates pop-song productions from prompts, canuploadaudio clips and extend or alter them according to a text description. The service also increased its context window from 30 seconds to 2 minutes for more coherent output. You can hear the new capabilityhere. Subscriptions start at $10 per month.How it works:Given a prompt, Udio generates a 30-second passage and lets you assemble passages into compositions (previously up to four minutes long, now 15 minutes). Now users can create passages by uploading audio clips and extending them or modifying them by, say, adding or removing instruments or vocals complete with lyrics.\n\nBehind the news:Udio competes withSuno, whose service also generates audio output with vocals, lyrics, and song structures. Also in the mix is Stability AI, whoseStable Audio 2.0enables users to upload and extend brief instrumental recordings to a length of around three minutes.\n\nWhy it matters:Udio is quickly becoming not just a song generator, but a song editor and builder. Just as the ability of text-to-image generators to edit, extend, and infill existing images made those applications more useful in a variety of creative situations, Udio’s audio-to-audio capabilities give composers and producers new horizons for enhancing, orchestrating, and structuring their own productions.\n\nWe’re thinking:Udio offers impressive capabilities for musicians (and wanna-be musicians), but its developer tools are lacking. A public-facing API would enable producers to automate the service and integrate it with other applications.\n\nGenerative adversarial networks (GANs) produce images quickly, but they’re of relatively low quality. Diffusion image generators typically take more time, but they produce higher-quality output. Researchers aimed to achieve the best of both worlds.\n\nWhat's new:Axel Sauer and colleagues at Stability AI accelerated a diffusion model using a method calledadversarial diffusion distillation(ADD). As the name implies, ADD combines diffusion with techniques borrowed from GANs and teacher-student distillation.\n\nKey insight:GANsare fast because they produce images in a single step. Diffusion models are slower because they remove noise from a noisy image over many steps. A diffusion model can learn to generate images in a single denoising step if, like a GAN, it learns to fool a discriminator, while the discriminator learns to identify generated output. The resulting one-step output doesn’t match the quality of multi-step diffusion, but distillation can improve it: While learning to fool the discriminator, the diffusion model (the student) can simultaneously learn to emulate the output of a different pretrained diffusion model (the teacher).\n\nHow it works:The authors paired a pretrainedStable Diffusion XL(SDXL) generator (the student) with a pretrainedDINOv2vision transformer discriminator. The teacher was another pretrained Stable Diffusion XL with frozen weights. They didn’t specify the training dataset.\n\nResults:The authors tested their method using 100 prompts fromPartiPrompts. They compared the student’s output after either one or four denoising steps to a pretrained SDXL after 50 denoising steps. Human judges were asked which they preferred with respect to (i) image quality and (ii) alignment with the prompt. They preferred the student’s four-step images about 57 percent of the time for image quality and about 55 percent of the time for alignment with the prompt. They preferred SDXL to the student’s one-step images around 58 percent of the time for image quality and 52 percent of the time for alignment with the prompt.\n\nWhy it matters:In this work, the key steps — having a student model learn from a teacher model, and training a generator against a discriminator — are established techniques in their own right. Combining them conferred upon the student model the advantages of both.\n\nWe're thinking:With the growing popularity of diffusion models, how to reduce the number of steps they take while maintaining their performance is a hot topic. We look forward to future advances.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-19T154106.403.png",
      "https://dl-staging-website.ghost.io/content/images/2024/06/The-Batch-ads-and-exclusive-banners---2024-06-11T083858.583.png",
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-19T154252.807.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-19T154339.904.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-19T154435.801.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-19T154515.714.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-107/",
    "title": "issue 107",
    "date": "",
    "reading_time": "",
    "content": "Building AI products and businesses requires making tough choices about what to build and how to go about it. I’ve heard of two styles:\n\nSay you’ve built a customer-service chatbot for retailers, and you think it could help restaurants, too. Should you take time to study the restaurant market before starting development, moving slowly but cutting the risk of wasting time and resources? Or jump in right away, moving quickly and accepting a higher risk of pivoting or failing?Both approaches have their advocates, but I think the best choice depends on the situation.\n\nReady, Aim, Fire tends to be superior when the cost of execution is high and a study can shed light on how useful or valuable a project could be. For example, if your team can brainstorm a few other use cases (restaurants, airlines, telcos, and so on) and evaluate these cases to identify the most promising one, it may be worth taking the extra time before committing to a direction.\n\nReady, Fire, Aim tends to be better if you can execute at low cost and, in doing so, determine whether the direction is feasible and discover tweaks that will make it work. For example, if you can build a prototype quickly to figure out if users want the product, and if canceling or pivoting after a small amount of work is acceptable, then it makes sense to consider jumping in quickly. (When taking a shot is inexpensive, it also makes sense to take many shots. In this case, the process is actually Ready, Fire, Aim, Fire, Aim, Fire, Aim, Fire.)\n\nAfter agreeing upon a product direction, when it comes to building a machine learning model that’s part of the product, I have a bias toward Ready, Fire, Aim. Building models is aniterative process. For many applications, the cost of training and conducting error analysis is not prohibitive. Furthermore, it is very difficult to carry out a study that will shed light on the appropriate model, data, and hyperparameters. So it makes sense to build an end-to-end system quickly and revise it until it works well.But when committing to a direction means making a costly investment or entering aone-way door(meaning a decision that’s hard to reverse), it’s often worth spending more time in advance to make sure it really is a good idea.\n\nKeep learning!Andrew\n\nTesla unveiled its own AI chip and — surprise! — plans for a humanoid robot.What’s new:At Tesla’sAI Daypromotional event, the company offered a first look at an upcoming self-driving computer powered by custom AI chips. To make sure the event got headlines, CEO Elon Musk teased a forthcoming android.Chips and bots:Company executives explained how the company trains models, labels data, and meets various AI challenges. Then they dove into what’s ahead:\n\nBehind the news:Tesla’s Autopilot system has recently come under governmentscrutiny. Last week, the U.S. National Highway Traffic Safety Administration launched an investigation into 11 incidents in which Tesla vehicles using Autopilot collided with parked emergency vehicles. If the agency finds Autopilot at fault, it could require the company to change or recall its technology.Why it matters:Tesla’s promise of full self-driving capability was premature, but Dojo’s muscled-up computing power could bring it substantially closer. As for the Tesla Bot, we’re not holding our breath.We’re thinking:Tesla’s genuine achievements — the innovative electric car, charging infrastructure, driver-assistance capabilities — may be overshadowed by stunts like the dancer in the bodysuit. History will decide whether Elon Musk is remembered as a genius at engineering or marketing.\n\nPrivacy advocates want deep learning systems to forget what they’ve learned.What’s new:Researchers are seeking ways to remove the influence of particular training examples, such as an individual’s personal information, from a trained model without affecting its performance,Wiredreported.How it works:Some researchers have experimented with preparing data prior to training for potential removal later, while others have worked to remove the effect of selected examples retroactively.\n\nBehind the news:Evolving data privacy laws could wreak havoc on machine learning models.\n\nWhy it matters:Enabling models to unlearn selectively and incrementally would be less costly than retraining repeatedly from scratch. It also could give users more control over how their data is used and who profits from it.We’re thinking:Wait … what was this article about?\n\nMark your calendar: We’re launching “Deploying Machine Learning Models in Production,” Course 4 of theMachine Learning Engineering for Production (MLOps) Specialization,on September 8, 2021!Pre-enroll now\n\nWineries in areas affected by wildfires are using machine learning to produce vintages that don’t taste like smoke.What’s new:Some California winemakers are using a service called Tastry to identify grapes tainted by smoke from the state’s surging blazes and recommend blends that will mask the flavor,The Wall Street Journalreported.How it works:Called CompuBlend, Tastry’s systemanalyzesgrapes’ chemical makeup, including smoke compounds absorbed through their skins. A model recommends other varieties that can mask the taste.\n\nBehind the news:The ancient art of winemaking is adopting AI.\n\nWhy it matters:Wildfires are a growing threat to wine regions inAustralia,California, andFrance. They cost the industry an estimated$3.7 billionin 2020. AI could help vintners recoup some of the losses.We’re thinking:While there's a clear need to adapt to human-induced climate change, it’s tragic that the planet has heated to the point that formerly temperate areas are burning. We applaud the work ofClimate Change AI.\n\nPretrained language models likeGPT-3have shown notable proficiency in few-shot learning. Given a prompt that includes a few example questions and answers (the shots) plus an unanswered question (the task), such models can generate an accurate answer. But there may be more to getting good results.What’s new:Ethan Perez, Douwe Kiela, and Kyunghyun Cho subjected GPT-style language models to a test they calltrue few-shot learning. They found that the heralded few-shot success may depend on a well engineered prompt. The authors are based at New York University, Facebook, and CIFAR, respectively.Key insight:Training a machine-learning model typically requires a validation set to tune hyperparameters such as the learning rate. For GPT-style models, those hyperparameters include the prompt format. In few-shot learning with a pretrained model, the prompt typically contains a handful of examples. However, researchers often experiment extensively to find a prompt format that yields accurate responses. This amounts to stacking the deck in the model’s favor, and without it, such models can’t perform so well.How it works:The authors evaluated four sizes of GPT-3, four sizes ofGPT-2, andDistilGPT-2. They tested prompt formats fromLAMA, a benchmark that comprises factual statements in a variety of formats, andLPAQA, which contains LAMA statements translated from English into a different language and back.\n\nResults:For all models tested, the accuracy prompted by the format selected according to cross-validation was only marginally above the mean and significantly below the accuracy of the best format. For instance, for the largest model (GPT-3 with 175 billion parameters), the format chosen by cross-validation scored about 55 percent, mean accuracy was about 54 percent, and the accuracy of the best format was about 60 percent.Why it matters:Previous claims of few-shot learning in GPT-style models left out an important variable: the size of the dataset used to pick a good format. Choosing among 12 prompt formats boosted accuracy by around 5 percent; choosing among a larger set of formats could make a bigger difference. If researchers don’t include all the information that went into the results they report, follow-up studies are unlikely to duplicate their work.We’re thinking:We like prompt engineering that gets things done on time. We’re less enamored with prompt engineering that muddies the water around few-shot learning.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/09/Screen-Shot-2021-09-01-at-1.08.19-PM-copy.png",
      "https://dl-staging-website.ghost.io/content/images/2021/09/Tesla-Bot.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/09/unlearn.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/09/Course-Name-Banner-4--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2021/09/Wine.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/09/TRUEFEWv5--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-270/",
    "title": "issue 270",
    "date": "",
    "reading_time": "",
    "content": "Congratulations to Geoff Hinton and John Hopfield for winning the 2024 Physics Nobel Prize! It’s wonderful to see pioneering work in AI recognized, and this will be good for our whole field. Years ago, I was the first to call Geoff the “Godfather of Deep Learning,” which later became “Godfather of AI.” I’m thrilled at the recognition he’s receiving via this most prestigious of awards.\n\nAs Geoff relayed in the “Heroes of Deep Learning”interviewI did with him years ago, his early work developing the foundations of neural networks has been instrumental to the rise of deep learning and AI. It has been years since I implemented aHopfield network, but John’s work, too, has been influential. Their recognition is well deserved!\n\nBut the Nobel committee wasn’t done yet. One day after the physics prize was announced, Demis Hassabis, John Jumper, and David Baker won the Chemistry Nobel Prize for their work on AlphaFold and protein design. AlphaFold and AlphaFold 2, as well as the work of Baker’s lab, are compelling applications of AI that made significant steps forward in chemistry and biology, and this award, too, is well deserved!\n\nIt’s remarkable that the Nobel committees for physics and chemistry, which are made up of scientists in those fields, chose to honor AI researchers with this year’s awards. This is a sign of our field’s growing impact on society.\n\nWhile it’s good that people from outside AI are recognizing AI researchers, I wonder if there’s room for the AI community to pick more award recipients ourselves. Best-known in computer science is the Turing Award, which is selected by a broad group of computer scientists, many of whom have deep AI knowledge. Many AI conferences give out best-paper awards. And applications of AI to other fields doubtless will continue to receive much-deserved recognition by leaders in those fields. I’m optimistic this will allow AI researchers to win more Nobel Prizes — someday also in economics, literature, medicine, and peace, too. Nonetheless, this seems like a good time to see how all of us in AI can do more to recognize the work of innovators in our field.\n\nGeoff once thanked me for my role in getting him anointed “Godfather of AI,” which he said was good for his career. I didn’t realize before that I had the power to give out such titles 😉 but I would love for there to be numerous godfathers and godmothers — and many other awards — in AI!\n\nAt Geoff's retirement party last October (pictured in the photo above), I spoke with affection and gratitude for all the work he has done to grow AI. Even as we cheer the new Nobel wins for AI, let’s continue to think about how we in AI can do more to celebrate the next generation of innovators.\n\nKeep learning!\n\nAndrew\n\nTry the new capabilities of Llama 3.2 in our latest course with Meta. Learn how to compose multimodal prompts, call custom tools, and use the Llama Stack API to build applications with Meta’s family of open weights models.Enroll for free!\n\nMeta upped the ante for text-to-video generation with new systems that produce consistent characters and matching soundtracks.\n\nWhat’s new:Meta presentedMovie Gen, a series of four systems that generate videos, include consistent characters, alter generated imagery, and add matching sound effects and music. Movie Gen will beavailableon Instagram in 2025. Meanwhile, you can view and listen to exampleshere. The teamexplainshow the model was built an extensive 92-page paper.\n\nGenerated videos:Movie Gen Video can output 256 frames (up to 16 seconds at 16 frames per second) at 1920x1080-pixel resolution. It includes a convolutional neural network autoencoder, transformer, and multiple embedding models.\n\nConsistent characters:Given an image of a face, a fine-tuned version of Movie Gen Video generates a video that depicts a person with that face.\n\nAltered clips:The team modified Movie Gen Video’s autoencoder to accept an embedding of an alteration — say, changing the background or adding an object. They trained the system to alter videos in three stages:\n\nSynthetic soundtracks:Given a text description, a system called Movie Gen Audio generates sound effects and instrumental music for video clips up to 30 seconds long. It includes aDACVAEaudio encoder (which encodes sounds that comes before and/or after the target audio), Long-prompt MetaCLIP video encoder,T5text encoder, vanilla neural network that encodes the current time step, and transformer.\n\nResults:Overall, Movie Gen achieved performance roughly equal to or better than competitors in qualitative evaluations of overall quality and a number of specific qualities (such as “realness”). Human evaluators rated their preferences for Movie Gen or a competitor. The team reported the results in terms of net win rate (win percentage minus loss percentage) between -100 percent and 100 percent, where a score above zero means that a system won more than it lost.\n\nWhy it matters:With Movie Gen, table stakes for video generation rises to include consistent characters, soundtracks, and various video-to-video alterations. The 92-page paper is a valuable resource for builders of video generation systems, explaining in detail how the team filtered data, structured models, and trained them to achieve good results.\n\nWe’re thinking:Meta has a great track record of publishing both model weights and papers that describe how the models were built. Kudos to the Movie Gen team for publishing the details of this work!\n\nOpenAI launched a suite of new and updated tools to help AI developers build applications and reduce costs.\n\nWhat’s new:At its annual DevDay conference, OpenAI introduced anAPIfor speech processing using GPT-4o,distillation tools,vision fine-tuning capabilities, and the ability tocache promptsfor later re-use. These tools are designed to make it easier to build fast applications using audio inputs and outputs, customize models, and cut costs for common tasks.\n\nDevelopment simplified:The new offerings aim to make it easier to build applications using OpenAI models, with an emphasis on voice input/output and image input, customizing models, and resolving common pain points.\n\nBehind the news:OpenAI is undertaking a major corporate transformation. A recent funding roundvaluesOpenAI at $157 billion, making it among the world’s most valuable private companies, and the company istransferringmore control from its nonprofit board to its for-profit subsidiary. Meanwhile, it has seen anexodusof executives that include CTO Mira Murati, Sora co-lead Tim Brooks, chief research officer Bob McGrew, research VP Barret Zoph, andother key researchers.\n\nWhy it matters:The Realtime API enables speech input and output without converting speech to text, allowing for more natural voice interactions. Such interactions open a wide range of applications, and they’re crucial for real-time systems like customer service bots and virtual assistants. AlthoughAmazon Web ServiceandLabelboxprovide services to distill knowledge from OpenAI models into open architectures, OpenAI’s tools ease the process of distilling from OpenAI models into other OpenAI models. Image fine-tuning and prompt caching, like similar capabilities for Anthropic Claude and Google Gemini, are welcome additions.\n\nWe’re thinking:OpenAI’s offerings have come a long way sinceDevDay 2023, when speech recognition was “coming soon.” We’re eager to see what developers do with voice-driven applications!\n\nA German court dismissed a copyright lawsuit against LAION, the nonprofit responsible for large-scale image datasets used to train Midjourney, Stable Diffusion, and other image generators.\n\nWhat’s new:The courtrejecteda lawsuit claiming that cataloging images on the web to train machine learning models violates the image owners’ copyrights. It ruled that LAION’s activities fall under protections for scientific research.\n\nHow it works:LAION doesn’t distribute images. Instead, it compiles links to images and related text that are published on publicly available websites. Model builders who wish to use the images and/or text must download them from those sources. In 2023, photographer Robert KneschkesuedLAION for including his photos. The court’sdecisionemphasized several key points.\n\nBehind the news:Several other artists have suedLAION, which stands for Large-scale AI Open Network, claiming that the organization used their works without their consent. They have also sued AI companies, including aclass action suitagainst Stability AI, Midjourney, and DeviantArt for using materials under copyright, including images in LAION’s datasets, to train their models. Similar cases have been brought against makers ofmusic generatorsandcoding assistants. All these lawsuits, which are in progress, rest on the plaintiff’s claim that assembling a training dataset of copyrighted works infringes copyrights.\n\nWhy it matters:The German ruling is the first AI-related decision in Europe since the adoption of the AI Act, and the court took that law’s intent into account when making its decision. It affirms that creating text-image pairs of publicly available material for the purpose of training machine learning models does not violate copyrights, even if commercial organizations later use the data. However, the court did not address whether training AI models on such datasets, or using the trained models in a commercial setting, violates copyrights.\n\nWe’re thinking:This decision is encouraging news for AI researchers. We hope jurisdictions worldwide establish that training models on media that’s available on the open web is fair and legal.\n\nResearchers probed the black market for AI services that are designed to facilitate cybercrime.\n\nWhat’s new: Zilong Lin and colleagues at Indiana University Bloomingtonstudiedhow large language models (LLMs) are used to provide harmful services, specifically generating malicious code, phishing emails, and phishing websites. They weren’t very effective, by and large (though a high success rate may not be necessary to support a thriving market in automated criminal activity).\n\nRisky business:Providers base such services on either uncensored LLMs — that is, those that weren’t fine-tuned to reflect human preferences or don’t employ input/output filters — or publicly available models that they prompt using jailbreak techniques that circumvent built-in guardrails. They sell their services in hacker’s marketplaces and forums, charging far less than typical traditional malware vendors, but services based on models that have been fine-tuned to deliver malicious output command a premium. The authors found that one service generated revenue of more than $28,000 in two months.\n\nSprawling market:The authors identified 212 harmful services. Of those, 125 were hosted on the Poe AI platform, 73 were on FlowGPT, and the remaining 14 resided on unique servers. Of those, the authors were unable to access five because either the provider blocked them, or the service was fraudulent. They identified 11 LLMs used by these services including Claude-2-100k, GPT-4, and Pygmalion-13B (a variant of LLaMA-13B).\n\nTesting output quality:The authors prompted more than 200 services using over 30 prompts to generate malicious code, phishing emails, or phishing websites. They evaluated the responses according to:\n\nIn all three tasks, at least one service achieved evasiveness of 67 percent or higher, while the majority of services achieved an evasiveness of less than 30 percent.\n\nTesting real-world effectiveness:In addition, the authors ran practical tests to see how well the output worked in real-world situations. They prompted nine services to generate code that would target three specific vulnerabilities that relate to buffer overflow and SQL injection. In these tests, the models were markedly less successful.\n\nWhy it matters: Previous work showed that LLMs-based services could generatemisinformationand other malicious output, but little research has probed their actual use in cybercrime. This work evaluates their quality and effectiveness. In addition, the authors released the prompts they used to circumvent guardrails and generate malicious output — a resource for further research that aims to fix such issues in future models.\n\nWe’re thinking:It’s encouraging to see that harmful services didn’t get far in real-world tests, and the authors' findings should put a damper on alarmist scenarios of AI-enabled cybercrime. That doesn’t mean we don’t need to worry about harmful applications of AI technology. The AI community has a responsibility to design its products to be beneficial and evaluate them thoroughly for safety.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/10/HINTON-PARTY-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--15-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--17-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--18-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--16-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--17-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-299/",
    "title": "issue 299",
    "date": "",
    "reading_time": "",
    "content": "I hope we can empower everyone to build with AI. Starting from K-12, we should teach every student AI enabled coding, since this will enable them to become more productive and more empowered adults. But there is a huge shortage of computer science (CS) teachers. I recently spoke with high school basketball coach Kyle Creasy, who graduated with a B.A. in Physical Education in 2023. Until two years ago, he had never written a line of Python. Now — with help from AI — he not only writes code, he also teaches CS. I found Kyle’s story inspiring as a model for scaling up CS education in the primary- and secondary-school levels.\n\nKyle’s success has been with the support ofKira Learning(an AI Fund portfolio company), whose founders Andrea Pasinetti and Jagriti Agrawal have created a compelling vision for CS education. In K-12 classrooms, teachers play a huge social-emotional support role, for example, encouraging students and helping them when they stumble. In addition, they are expected to be subject-matter experts who can deliver the content needed for their subject. Kira Learning uses digital content delivery — educational videos, autograded quizzes, and AI-enabled chatbots to answer students' questions but without giving away homework answers — so the teacher can focus on social-emotional support. While these are still early days, it appears to be working!\n\nA key to making this possible is the hyperpersonalization that is now possible with AI (in contrast to the older idea of theflipped classroom, which had limited adoption). For example, when assigned a problem in an online coding environment, if a student writes this buggy line of Python code\n\nbest_$alty_snack = 'potato chips'\n\nKira Learning’s AI system can spot the problem and directly tell the teacher that $ is an invalid character in a variable name. It can also suggest a specific question for the teacher to ask the student to help get them unstuck, like “Can you identify what characters are allowed in variable names?” Whereas AI can directly deliver personalized advice to students, the fact that it is now helping teachers also deliver personalized support will really help in K-12.\n\nAdditionally, agentic workflows can automate a lot of teachers’ repetitive tasks. For example, when designing a curriculum, it’s time-consuming to align the content to educational standards (such as the Common Core in the United States, or the AP CS standard for many CS classes). Having an AI system carry out tasks like these is already proving helpful for teachers.\n\nSince learning to code, Kyle has built many pieces of software. He proudly showed me an analysis he generated in matplotlib of his basketball players’ attempts to shoot three-pointers (shown above), which in turn is affecting the team’s strategy on the court. One lesson is clear: When a basketball coach learns to code, they become a better basketball coach!\n\nI talked about Kyle (and other topics) at theASU+GSV Summiton education. You can see a videohere.\n\nIn the future, people who know how to code and build with AI will be much more productive than people who don’t. I’m excited about how AI will lead to new models for K-12 education. By delivering CS education to everyone, I hope that in the future, everyone will be able to build with AI.\n\nKeep learning!\n\nAndrew\n\nIn “LLMs as Operating Systems: Agent Memory,” you’ll learn to build agents that manage their own memory using the MemGPT approach. This newly updated short course includes cloud-based deployment and real-time, step-by-step output, so you can see how your agents reason as they respond.Join in today!\n\nChatGPT’s image generator is available via API.\n\nWhat’s new:GPT Image 1, which produces images from text or other images, has proven enormously popular among ChatGPT users. TheOpenAI Images APIenables developers to incorporate OpenAI’s most sophisticated image generator into their own software tools and platforms.\n\nHow it works:GPT Image 1generates and modifies imagesin a wide range of styles, performs image editing and other alterations, renders text, and follows detailed instructions. Shortly after its debut, the version of GPT-4o equipped with GPT Image 1 quickly soared to the No. 1 spot on theArtificial Analysis Image Arena leaderboard.\n\nBehind the news:In March, OpenAI attracted huge public interest when it deployed the model, then unnamed, inChatGPT. Within the first week,130 millionusers used it to create more than 700 million images.\n\nWhy it matters:Adding GPT Image 1 to the API enables developers to use OpenAI’s most sophisticated image generator in a wide variety of automated workflows. OpenAI’s initial API partners include design companies (Adobe and Canva), marketers (HubSpot), and web designers (GoDaddy), all of which are using GPT Image 1.\n\nWe’re thinking:GPT Image 1 is part of an exciting trend toward unification of multimodal architectures. Researchers have progressed fromtext-in, text-outtotext/images-in, text-outand increasinglytext/images/audio-in, text/images/audio-out. This paints a beautiful picture of where multimodal models can go!\n\nGoogle refreshed its experimental tools for composers and producers.\n\nWhat’s new:Google announced updates of two music-generation apps and the models they're based on.Music AI Sandbox, an app that generates and modifies music according to text prompts, now accepts lyrics to generate songs as well as instrumental music. You can join a waitlisthere.MusicFX DJgenerates a continuous stream of music that users can modify as it plays. Try it outhere.\n\nHow it works:The apps generate 48kHz audio suitable for professional productions. Users can specify key, tempo in beats per minute, instrumentation, style, mood, and other details.\n\nBehind the news:GooglelaunchedLyria 1 and Music AI Sandbox in 2023 as part of an experiment with YouTube, which made them available to composers, producers, and musicians. Since then, the company has developed them with help from music stars including Jacob Collier, Donald “Childish Gambino” Glover, and Wyclef Jean. Lyria 1 recently becameavailablevia the Vertex API to developers who are preapproved by Google.\n\nWhy it matters:While music generators likeSuno and Udioappeal to casual musicians, Music AI Sandbox, with its digital audio workstation-style user interface, aims to address the needs of professionals. This approach puts AI directly into the hands of talented, experienced artists, similar to the way Adobe hasempoweredvideographers and Runway haspartneredwith movie producers.\n\nWe’re thinking:API access to Lyria 2 would be music to our ears!\n\nAI agents and infrastructure made a strong showing on CB Insights’s latest list of the top 100 AI startups.\n\nWhat’s new:CB Insights, which tracks tech startups and venture capital, selected companies in theAI 100based on their market traction, talent, finances, and partnerships. The list purports to highlight the next wave of winners, shedding light on the key executives, investors, fundraising, and valuations behind up-and-coming AI ventures.\n\nHow it works:The analysts evaluated 17,000 early-stage, private AI companies that had raised funds within the last year and continue to seek further investment.\n\nWhere the action is:This year’s AI 100 companies are based in 14 countries, around two-thirds of them in the United States. 10 are based in the United Kingdom, five in France, and four in Germany, with one each in Norway (Braintrust), Singapore (Bria), Spain (Cartwheel), Sweden (Chainguard), and Switzerland (Clarium).\n\nWhy it matters:This year’s AI 100 offers a snapshot of AI becoming more central to businesses of all kinds. Most of the startups listed here offer practical products and services that are poised to deliver a timely return, rather than moonshots with long development cycles and risky payoffs. In addition, they mostly target corporate customers rather than consumers.\n\nWe’re thinking:The falling cost of access to AI models and increasingly capable open-weights models make this the perfect time tobuild applications. What kind? The report singles out health care (8 companies) and life sciences (6 companies) as growing areas, but it also documents opportunities in defense, gaming, and finance.\n\nLarge language models can improve systems that recommend items to purchase by inferring customer preferences.\n\nWhat’s new:Fabian Paischer and colleagues at Johannes Kepler University Linz, University of Wisconsin, and Meta introducedMultimodal Preference Discerner(Mender), a recommender that integrates a large language model (LLM).\n\nKey insight:Text that attracts customers, such as product descriptions, and text they write, such as product reviews, may contain information that indicates their preferences, such as the craft projects that required a particular power tool. But it also may include irrelevant information, such as a complaint that the tool was delivered late, which can throw recommendation systems off track. An LLM can derive preferences from text, providing a clearer signal of what a customer wants.\n\nHow it works:Mender comprises an LLM (Llama 3 70B-Instruct), an encoder (Flan-T5pretrained on a wide variety of text and frozen) that embeds customer data, and a decoder (a transformer trained from scratch) that predicts the next item a customer will buy. The system learned to predict the next item based on descriptions of items a customer purchased, the customer’s ratings and reviews of those products (drawn from datasets ofSteamreviews of video games andAmazonreviews of items related to beauty, toys-and-games, and sports-and-outdoors), and customer preferences inferred by the LLM from the foregoing data.\n\nResults:The authors compared Mender toTIGER, a recommender that also takes a purchase history and predicts the next purchase, on the Steam and Amazon datasets. They scored the results usingrecall @5, a measure of how often the correct item is within the model’s top five most likely predictions.\n\nWhy it matters:Drawing inferences from text information like customer reviews and item descriptions boosts a recommender’s signal, making it clearer what a given customer is likely to want. Previous systems used customer reviews or item descriptions directly; Mender uses customer preferences extracted from that information.\n\nWe’re thinking:Be on the lookout for innovative ways to use LLMs. We recommend it!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--81-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/The-Batch-ads-and-exclusive-banners---2025-04-29T103611.308.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/GPT-IMAGE1-2.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--58-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--82-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--83-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-37/",
    "title": "issue 37",
    "date": "",
    "reading_time": "",
    "content": "In an earlier letter, I wrote about the challenge ofrobustness: A learning algorithm that performs well on test data often doesn’t work well in a practical production environment because the real world turns out to be different than the test set.\n\nAmid the Covid-19 pandemic, many machine learning teams have seen this firsthand:\n\nAlthough the tsunami of Covid-19 — with its devastating impact on lives and livelihoods — is a dramatic example of change in the world, small parts of the world experience waves of change all the time. A new online competitor may mean that a retail store’s demand estimation model no longer works. A new tariff by a small country subtly shifts supply chain behavior among larger ones.\n\nBuilding practical machine learning systems almost always requires going beyond achieving high performance on a static test set (which, unfortunately, is what we are very good at). You may need to build an alert system to flag changes, use human-in-the-loop deployments to acquire new labels, assemble a robust MLOps team, and so on.\n\nTechnological improvements will make our algorithms more robust to the world’s ongoing changes. For the foreseeable future, though, I expect deploying ML systems — and bridging proof of concept and production deployments — to be rewarding but also hard.\n\nI hope all of you continue to stay safe.\n\nKeep learning!\n\nAndrew\n\nThe AI community is working to beat back coronavirus. Here are some recently released datasets and tools to fuel that effort.\n\nRecognizing stop signs, with their bold color scheme and distinctive shape, ought to be easy for computer vision — but it turns out to be a tricky problem. Tesla pulled back the curtain on what it takes to train its self-driving software to perform this task and others.What’s new:Tesla AI chiefAndrej Karpathydescribesin a video presentation how the electric car maker is moving toward fully autonomous vehicles. Shot at February’sScaledML Conference, the video was posted on YouTube last week.Not just a big red hexagon:Stop signs take a surprising variety of forms and appearances, and that can make them hard to identify. Rather than an oversized icon on a pole, they’re often waved by construction workers, hanging off school buses, or paired with other signs. Karpathy describes how his team trained the company’s Autopilot system to detect a particularly vexing case: stop signs partially obscured by foliage.\n\nBehind the news:Tesla is the only major autonomous driving company that doesn’t use lidar as its primary sensor. Instead, it relies on computer vision with help from radar and ultrasonic sensors. Cameras are relatively cheap, so Tesla can afford to install its self-driving sensor package into every car that comes off the line, even though self-driving software is still in the works. It’s also easier to label pictures than point clouds. The downside: Cameras need a lot of training to sense the world in three dimensions, which lidar units do right out of the box.\n\nUnstoppable:Responding to Karpathy’s presentation on Twitter, Google Brain researcher David Ha (@hardmaru) created stop sign doodles usingsketch-rnn, an image generator he and colleague David Eck trained on crude hand-drawn sketches. Generate your own doodled datasethere.\n\nYour body disturbs Wi-Fi signals as you move through them. New research takes advantage of the effect to recognize the presence of people.What’s new:Yang Liu and colleagues at Syracuse Universitydetected peoplein a room with a Wi-Fi router by analyzing the signal.Key insight:Radio waves interfere with one another, creating high-frequency noise that masks other kinds of perturbations. The researchers removed these components, making it easier to identify lower-frequency disturbances caused by human motion.How it works:A Wi-Fi router comprises many antennas transmitting and receiving radio waves on different frequency bands called subcarriers. The researchers measured the signal strength and phase received by each antenna over a fixed time period to plot what is known as channel state information (CSI). The sequence of CSI images — cubes corresponding to measurements of the transmitting antenna, receiving antenna, and subcarrier — feeds a network that predicts whether someone is moving in the room\n\nResults:The authors’ method slightly outperformed conventional motion detectors based on infrared beams. The dual CNN detected a wider physical area. Although the training data included only people walking, it spotted minimal motion — say, typing on a keyboard while seated — almost twice as well as conventional detectors. (The success rate was only around 5 percent, but for much of the time, typing was the only motion to detect.) It may miss someone if they’re still, but combining multiple predictions over time improved accuracy unless someone was still for minutes on end.Yes, but:The training and test data come from the same room, so the model’s practicality is limited for now. It would be onerous to retrain for each new room we might use it in.Why it matters:It’s hard to imagine extracting this kind of information from radio waves without deep learning. Still, the preprocessing step was crucial. Neural networks can be distracted by input features that don’t correlate with the output. Radio interference doesn’t correlate with human motion, so the CNN would have required a huge amount of data to learn to detect people through the noise. Removing it at the outset made training far more efficient.We’re thinking:It is well known that powerful actions can create a disturbance in the Force. But anyone can create a disturbance in the Wi-Fi.\n\nDeep learning promises to help emergency responders find their way through disaster zones.What’s new:MIT researchers developed atoolthat maps where hurricanes and other calamities have wiped out roads, helping to show aid workers the fastest ways to get to people in need.How it works:Since 2017, following hurricanes in the Carolinas, Florida, Texas, and Puerto Rico, aircraft equipped with lidar collected three-dimensional images of the devastated landscapes. Previously, the researchers analyzed the data manually to evaluate damage to roads. Now they’re building neural networks that get the job done faster.\n\nResults:In tests, the group was able to fly a lidar mission, process data, and generate route-finding analytics in under 36 hours.Why it matters:After disaster strikes, damaged infrastructure often thwarts efforts by emergency responders and relief groups to deliver food and medical care to those in need. The new system could help save lives.We’re thinking:Lidar is just one of many rich sources of post-disaster data. Machine learning engineers with humanitarian impulses can also dig into satellite imagery, GIS data, and social media posts.\n\nInterested in learning more about AI applications in medicine? Build your own diagnostic and prognostic models in our AI for Medicine Specialization.Enroll now\n\nA new algorithm can triage programming bugs, highlighting dangerous flaws.What’s new:Microsoft researchers developed a machine learning model that reads the titles of bug reports andrecognizesthose describing flaws that compromise security. Further, it sorts security bugs by severity.How it works:Along with stellar software products, Microsoft developers produce 30,000 coding errors monthly. Users or other developers who encounter one can file a report. These reports are generated automatically, including a brief title that describes the bug plus detailed information that may include passwords or other sensitive details. To protect user privacy, Microsoft’s model reads only the title.\n\nResults:The model recognized security bugs with 93 percent accuracy and achieved 98 percent area under the ROC curve. Based on an extensive review of earlier work on automated bug-hunting, the researchers believe their system is the first to classify software flaws based on report titles alone. They expect to deploy it in coming months.Behind the news:Software bugs are responsible for some of history’s most infamous tech headaches.\n\nWhy it matters:In 2016, the U.S. governmentestimatedthat cyber security breaches — many of them made possible by software defects — cost the nation’s economy as much as $109 billion. Being able to spot and repair the most dangerous flaws quickly can save huge sums of money and keep people safer.We’re thinking:As producers of our fair share of bugs, we’re glad to know AI has our back.\n\nRobot vacuum cleaners are pretty good atnavigating rooms, but they still get stuck in tight spaces. New work takes a step toward giving them the smarts they’ll need to escape the bathroom.What’s new:Led by Ameya Phalak, a team atMagic Leapcreated Scan2Plan, a model that segments 3D scans of empty indoor spaces into floor plans.Key insight:Given a single scan covering an entire building, Scan2Plan learns to recognize scanned 3D points belonging to the same wall and those belonging to the same room. Once it knows the walls and rooms they form, generating a floor plan is easy.How it works:3D scanners project light and measure how long it takes to bounce back, producing a point cloud that represents the scene. In an empty room, these points are likely to belong to walls.\n\nResults:The team tested Scan2Plan on theBeijing Real Estatedataset. The network was over 100 times faster than the previous state of the art,Floor SP, while achieving better F1 scores for corners (0.915 versus 0.877) and walls (0.860 versus 0.788).Yes, but:Much of Beijing Real Estate has been preprocessed to remove scanner noise. When noise was included, Floor SP achieved a better F1 score for corners, though similar results for walls and rooms.Why it matters:Floor plans can help robots in tasks that require mapping their surroundings, such aslocalization. Although their performance is similar, Scan2Plan is much faster than Floor SP, producing floor plans in 4 seconds rather than 8 minutes.We’re thinking:Standard supervised learning algorithms don’t have great ways to predict an arbitrary number of output classes, such as the number of rooms in a floor plan. Rather than try to predict a room’s identity directly (which is hard, because the world holds arbitrarily many rooms), this work predicted the location of the next room’s center. We continue to be impressed by the creativity of researchers working to fit supervised learning into larger systems.\n\nA consortium of top AI experts proposed concrete steps to help machine learning engineers secure the public’s trust.What’s new:Dozens of researchers and technologistsrecommended actionsto counter public skepticism toward artificial intelligence, fueled by issues like data privacy and accidents caused by autonomous vehicles. The co-authors include scholars at universities like Cambridge and Stanford; researchers at companies including Intel, Google, and OpenAI; and representatives of nonprofits such as the Partnership on AI and Center for Security and Emerging Technology.Recommendations:Lofty pronouncements about ethics aren’t enough, the authors declare. Like the airline industry, machine learning engineers must build an “infrastructure of technologies, norms, laws, and institutions” the public can depend on. The authors suggest 10 trust-building moves that fall into three categories.\n\nBehind the news:The AI community is searching for ways to boost public trust amid rising worries about surveillance, the impact of automation on human labor, autonomous weapons, and computer-generated disinformation.Dozensof organizations have published their own principles, fromGoogleandMicrosoftto theEuropean Commissionand theVatican. Even theU.S. Department of Defensepublished guidelines on using AI during warfare.Why it matters:Widespread distrust in AI could undermine the great good this technology can do, frightening people away or prompting politicians to hamstring research and deployment.We’re thinking:Setting clear standards and processes to verify claims about AI systems offers a path for regulators and users to demand evidence before they will trust an AI system. This document’s emphasis on auditing, explainability, and access to hardware makes a solid cornerstone for further efforts.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrew20Letter20Pic202.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Resources20ASPECT20REPLACEMENT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize209.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/RF1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Road.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AI4MC1BatchAd-2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-04-2820at2012.08.1520PM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker201-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Trust20ASPECT.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-235/",
    "title": "issue 235",
    "date": "",
    "reading_time": "",
    "content": "On theLMSYS Chatbot Arena Leaderboard, which pits chatbots against each other anonymously and prompts users to judge which one generated a better answer, Google’s Bard (Gemini Pro) recently leaped to third place, within striking distance of the latest version of OpenAI’s GPT-4, which tops the list. At the time of this writing, the open source Mixtral-8x7b-Instruct is competitive with GPT-3.5-Turbo, which holds 11th place. Meanwhile, I'm hearing about many small, capable teams that, like Mistral, seem to have the technical capability to train foundation models. I think 2024 will see a lot of new teams enter the field with strong offerings.\n\nThe barriers to building foundation large language models (LLMs) seem to be falling as the know-how to train them diffuses. In the past year, a lot of LLM technology has taken steps toward becoming commoditized. If it does become commoditized, who will be the winners and losers?\n\nMeta has played a major role in shaping the strategic landscape by emphasizing open source. Unlike its big-tech peers, it makes money by showing ads to users, and does not operate a cloud business that sells LLM API calls. Meta has been badly bitten by its dependence on iOS and Android, which has left it vulnerable to Apple and Google hurting its business byimposingprivacy controls that limit its ability to target ads precisely. Consequently, Meta has a strong incentive to support relatively open platforms that it can build upon and aren’t controlled by any one party. This is why releasing Llama as open source makes a lot of sense for its business (as does its strong support for PyTorch as a counterweight to Google’s TensorFlow). The resulting open source offerings are great for the AI community and diffusion of knowledge!\n\nIn contrast, Google Cloud and Microsoft Azure stand to benefit more if they manage to offer dominant, closed source LLMs that are closely tied to their cloud offerings. This would help them to grow their cloud businesses. Both Google Cloud and Microsoft Azure, as well as Amazon AWS, are in a good position to build meaningful businesses by offering LLM API calls as part of their broader cloud offerings. However, I expect their cloud businesses to do okay even if they don’t manage to offer an exclusive, clearly dominant LLM (such as Gemini, GPT-4, or their successors). If LLMs become commoditized, they should do fine simply by integrating any new LLMs that gain traction into their API offerings.\n\nOpen or closed, LLMs also offer these companies different opportunities for integration into their existing product lines. For example, Microsoft has a huge sales force for selling its software to businesses. These sales reps are a powerful force for selling its Copilot offerings, which complement the company’s existing office productivity tools. In contrast, Google faces greater risk of disruption to its core business, since some users see asking an LLM questions as a replacement for, rather than a complement to, web search. Nonetheless, it’s making a strong showing with Bard/Gemini. Meta also stands to benefit from LLMs becoming more widely available. Indeed, LLMs are already useful in online advertising, for example, by helping write ad copy to drives more clicks.\n\nTech giants can afford to invest hundreds of millions or even billions of dollars in building LLM technology only to see it become commoditized shortly afterward. Startups would have a harder time surviving after burning this much cash with little to show for it. However, well funded startups will have some time to explore other paths to growing revenue and building a moat.Finally, competition among companies that offer LLMs is great for everyone who builds applications! With so much investment, by both big companies and startups, in improving LLMs and offering them as open source or API calls, I believe — as I described in this talk on “Opportunities in AI” — that many of the best business opportunities continue to lie in building applications on top of LLMs.\n\nKeep learning!\n\nAndrew\n\nSexually explicit deepfakes of Taylor Swift galvanized public demand for laws against nonconsensual, AI-enabled pornography.\n\nWhat’s new:U.S. lawmakers responded to public outcry over lewd AI-generated images of the singer by proposing legislation that wouldcrack downon salacious images generated without their subject’s permission.\n\nHigh-profile target:In late January, AI-generated images that appeared to depict Swift in the nude appeared on social media sites including X (formerly known as Twitter) and messaging apps such as Telegram. The deepfakesoriginatedon the image-sharing site 4chan, where users competed to prompt text-to-image generators in ways that bypassed their keyword filters. Swift fans reported the images, which subsequently were removed. Swiftreportedlyis considering legal action against websites that hosted the images.\n\nBehind the news:Sexually explicit deepfakes oftentargetcelebrities, but several recent incidents involve private citizens who were minors at the time.\n\nWhy it matters:The Swift incident dramatizes the growing gap between technological capabilities and legal restrictions. The rapid progress of image generators enables unscrupulous (or simply cruel) parties to prey on innocent victims in ways that exact a terrible toll for which reparation may be inadequate or impossible. In many jurisdictions, the laws against nonconsensual pornography don’t account for AI-generated or AI-edited images. To be actionable, for instance, such images must depict the victim’s own body rather than a generated look-alike.We’re thinking:No one, whether a public or private figure, child or adult, should be subject to the humiliation and abuse of being depicted in nonconsensual pornographic images. The U.S., whose constitution guarantees free speech, has weaker tools for silencing harmful messages than other countries. Nonetheless, we hope that Swift gets the justice she seeks and that lawmakers craft thoughtful legislation to protect citizens and provide recourse for victims without banning legitimate applications.\n\nHugging Face introduced four leaderboards to rank the performance and trustworthiness of large language models (LLMs).\n\nWhat’s new:The open source AI repository now ranks performance on tests ofworkplace utility,trust and safety, tendency togenerate falsehoods, andreasoning.How it works:The new leaderboards implement benchmarks developed by HuggingFace’s research and corporate partners. Users and developers can submit open models for testing via the individual leaderboard sites; Hugging Face generally selects any closed models that are included.\n\nBehind the news:The new leaderboards complement Hugging Face’s earlierLLM-Perf Leaderboard, which gauges latency, throughput, memory use, and energy demands;Open LLM Leaderboard, which ranks open source options on the EleutherAI Language Model Evaluation Harness; andLMSYS Chatbot Arena Leaderboard, which ranks chat systems according to blind tests of user preferences.\n\nWhy it matters:The new leaderboards provide consistent evaluations of model performance with an emphasis on practical capabilities such as workplace uses, social stereotyping, and security. Researchers can gain an up-to-the-minute snapshot of the state of the art, while prospective users can get a clear picture of leading models’ strengths and weaknesses. Emerging regulatory regimes such as Europe’sAI Actand the U.S.’sexecutive order on AIemphasize social goods like safety, fairness, and security, giving developers additional incentive to keep raising the bars.\n\nWe’re thinking:Such leaderboards are a huge service to the AI community, objectively ranking top models, displaying the comparative results at a glance, and simplifying the tradeoffs involved in choosing the best model for a particular purpose. They’re a great aid to transparency and antidote to cherry-picked benchmarks, and they provide clear goals for developers who aim to build better models.\n\nRetrieval Augmented Generation (RAG) is a powerful way to extend large language models, but to implement it effectively, you need the right retrieval techniques and evaluation metrics. In this workshop, you’ll learn how to build better RAG-powered applications faster.Register now\n\nGPT-4 poses negligible additional risk that a malefactor could build a biological weapon, according to a new study.\n\nWhat’s new:OpenAIcomparedthe ability of GPT-4 and web search to contribute to the creation of a dangerous virus or bacterium. The large language model was barely more helpful than the web.\n\nHow it works:The researchers asked both trained biologists and biology students to design a biological threat using either web search or web search plus GPT-4.\n\nResults:Participants who used GPT-4 showed slight increases in accuracy and completeness.Students with GPT-4 scored 0.25 and 0.41 more points on average, respectively, than students in the control group. Experts with access to the less restricted version of GPT-4 scored 0.88 and 0.82 points higher on average, respectively, than experts in the control group. However, these increases were not statistically significant. Moreover, participants who used GPT-4 didn’t show greater innovation, take less time per task, or view their tasks as easier. Even if GPT-4 could be prompted to provide information that would facilitate a biological attack, the model didn’t provide more information than a user could glean by searching the web.\n\nWhy it matters:AI alarmists have managed to create a lot of anxiety by promoting disaster scenarios, such as human extinction, that the technology has no clear way to bring about. Meanwhile, the unfounded fears stand to slow down developments that could do tremendous good in the world. Evidence that GPT-4 is no more likely than web search to aid in building a bioweapon is a welcome antidote. (Though we would do well to consider removing from the web unnecessary information that may aid in the making of bioweapons.)\n\nWe’re thinking:Large language models, like other multipurpose productivity tools such as web search or spreadsheet software, are potentially useful for malicious actors who want to do harm. Yet AI’s potential in biothreat development garners headlines, while Excel’s is rarely mentioned. That makes it doubly important to quantify the risk in ways that can guide regulators and other decision makers.\n\nMost people understand that others’ mental states can differ from their own. For instance, if your friend leaves a smartphone on a table and you privately put it in your pocket, you understand that your friend continues to believe it was on the table. Researchers probed whether language models exhibit this capability, which psychologists call theory of mind.\n\nWhat's new:Michal Kosinski at Stanford evaluated the ability of large language models tosolve language tasks designed to test for theory of mind in humans. The largest models fared well.\n\nHow it works:The author evaluated the performance of (GPT-1 through GPT-4 as well asBLOOM) on 40 tasks developed for human studies. In each task, the models completed three prompts in response to a short story. Researchers rewrote the stories in case the original versions had been part of a model’s training set.\n\nResults:The models generated the correct response more consistently as they increased in size. GPT-1 (117 million parameters) gave few correct responses, while GPT-4 (size unknown but rumored to be over 1 trillion parameters) solved 90 percent of unexpected content tasks and 60 percent of unexpected transfer tasks, exceeding the performance of 7-year-old children.\n\nWhy it matters: The tasks in this work traditionally are used to establish a theory of mind in children. Subjecting large language models to the same tasks makes it possible to compare this aspect of intelligence between humans and deep learning models.\n\nWe're thinking: If a model exhibits a theory of mind, are you more or less likely to give it a piece of your mind?\n\nThis week, AI innovation is fueling different companies, including Yelp, Mastercard, and Volkswagen. At the same time, governments worldwide are implementing new safety and legal measures concerning generative AI.\n\nWe've distilled the most compelling stories in Data Points, a spin-off of The Batch.\n\nRead now.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/02/unnamed--50-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/02/unnamed--96-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/02/unnamed---2024-02-07T153540.653.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/02/The-Batch-ads-and-exclusive-banners---2024-02-06T172247.774.png",
      "https://dl-staging-website.ghost.io/content/images/2024/02/unnamed---2024-02-07T153736.731.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/02/unnamed---2024-02-07T153903.061.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-70/",
    "title": "issue 70",
    "date": "",
    "reading_time": "",
    "content": "When a researcher works for a company, what rights should they have to publish their work, and what rights should the company that sponsored the work have? This issue has come up many times in the AI community across many companies, most recently around Timnit Gebru’s very public departure from Google, which involved a disagreement over research she was preparing to publish.\n\nResearchers and companies often share a desire to contribute ideas that move AI forward. At the same time, they can also have completely legitimate interests that may differ. Researchers may want to make their work available to the community, while the organizations that fund that work may want to keep certain inventions secret or patent them. Researchers and companies may be willing or unwilling, to varying degrees, to point out inconvenient truths that need to be addressed.\n\nIt’s not always obvious how to balance these interests. For example:\n\nI’ve submitted publications for review, and I’ve set policies that govern how others’ work should be reviewed. As a co-author, I’ve also pulled publications when I felt they were not up to standard. These experiences have shown me that the answers to these questions may differ, depending on the parties involved.\n\nWhat is clear, though, is that researchers and companies need to set clear expectations ahead of time, and then abide by them consistently. Both parties have an interest in avoiding situations where a researcher spends substantial time and energy working on ideas with the intent to publish them, only to be surprised that they’re unable to do so.\n\nI would like to see the AI community get together and establish a fair set of rules that balance everyone’s interests. Every researcher, company, and university is different, and possibly no one-size-fits-all answer will work for everyone. But if we set expectations collectively, we might be able to nudge companies toward a balanced set of policies around publications.\n\nWhat rules do you think would be fair? Let me know via social media or by sharing your ideashere.\n\nKeep learning!\n\nAndrew\n\nCovid Moonshot, an open source project to vet potential medicines using machine learning, is closing in on compounds that might help curb Covid-19.What’s new:Four new antiviral drugs identified by the project are ready to advance to animal trials, according toIEEE Spectrum. Unlike vaccines, which prevent infection,antiviralstreat people who are already infected.How it works:Last spring, PostEra, a UK chemistry company, invited scientists to submit designs for molecules with potential to thwart the virus. It used asemisupervised deep learning platformto analyze more than 14,000 submissions. You can read our earlier report on the projecthere.\n\nBehind the news:Covid Moonshot does not seek to profit from its effort. If any of its compounds successfully complete animal trials, which could happen by mid-2021, they will enter human clinical trials. If they pass that test, they will be made available to drug makers at no cost to manufacture and distribute.Why it matters:Antivirals typically are far less expensive to produce and easier to distribute than vaccines. These drugs could help keep the pandemic in check while inoculations make their way through the global population.We’re thinking:Although vaccines are beginning to roll out, now is no time to relax. Keep social distancing and hand washing until public-health experts say otherwise.\n\nA new breed of self-driving car could kick the autonomous-vehicle industry into a higher gear.What’s new:Zooxunveiledits first product, an all-electric, driverless taxi designed fully in-house.How it works:The vehicle has no driver’s seat, steering wheel, or pedals — just four inward-facing passenger seats. It’s capable of driving in either direction and uses lidar, radar, and cameras to guide its navigation and collision avoidance systems. It can go for 16 hours on single charge.\n\nBehind the news:Founded in 2014 and acquired by Amazon in July, Zoox has been road testing its self-driving technology in San Francisco and Las Vegas using cars built by other manufacturers. The company is just one part of Amazon’s self-driving portfolio. The retail giant also has invested in autonomous vehicle makersAuroraandRivian.Are we there yet?Despite years of hype andbillions of dollarsspent on research and development, self-driving cars are a long way from replacing human drivers. So far, they’re considered safe enough only to operate in relatively small, well mapped environments.\n\nWhy it matters:Self-driving car companies have pulled back their early, grandiose promises. By proving the technology in constrained environments, they can improve safety on the open road while building trust with the public. With the Amazon juggernaut behind it, Zoox could be a significant milestone on the road to practical vehicular autonomy.We’re thinking:Zoox’s announcement received a rapturous reception in the press, but the company has only just begun producing vehicles and doesn’t expect to operate commercially until at least2022.\n\nAll three courses of ourGANs Specializationare available on Coursera! Join more than 12,000 learners who have gained the knowledge and skills to take advantage of this powerful technology.Enroll now\n\nComputer simulations do a good job of modeling physical systems from traffic patterns to rocket engines, but they can take a long time to run. New work takes advantage of deep learning to speed them up.\n\nWhat’s new:Youngkyu Kim and a team at University of California and Lawrence Livermore National Lab developed atechniquethat uses a neural network to compute the progress of a fluid dynamics simulation much more quickly than traditional methods.Key insight:Changes in the state of a simulation from one time step to the next can be expressed as a set of differential equations. One of the faster ways to solve differential equations is to calculate many partial solutions and combine them into an approximate solution. A neural network that has been trained to approximate solutions to differential equations also can generate these partial solutions. Not every neuron is important in calculating a given partial solution, so using only the subnetwork of neurons required to calculate each one makes this process much more efficient.How It works:They used an autoencoder made up of two single-hidden-layer neural networks, an encoder and a decoder. The decoder’s output layer was sparsely connected, so neurons received input from only a few neurons in the previous layer. The authors trained the autoencoder to reproduce thousands of states ofBurgers’ Equation, which simulates the location and speed of fluids in motion.\n\nResults:On the Burgers’ Equation that involves one spatial dimension, their method solved the problem 2.7 times faster than the usual approach with only 1 percent error. On the two-dimensional Burgers’ Equation, their method solved the problem 12 times faster with less than 1 percent error. Given the speed increase between one- and two-dimensional Burgers’ Equations, the authors suggest that acceleration may rise with the number of equations a simulation requires.Why it matters:Our teams have seen a number of problems, such as airfoil design or optimization of nuclear power plants, in which an accurate but slow physics sim can be used to explore options. The design pattern of using a learning algorithm to approximate such simulations more quickly has been gaining traction, and this work takes a further step in that direction.We’re thinking:In approximating solutions to a Burgers’ Equation, neural networks clearly meat expectations. Other approaches wouldn’t ketchup even if the authors mustard the effort to keep working on them.\n\nThe secrets of history are locked in troves of handwritten documents. Now a machine learning platform is making them amenable to digital search.What’s new:Transkribus, a program developed by the University of Innsbruck, is transcribing centuries-old records en masse and making them available to scholars worldwide. The system has renderedlettersbetween the Brothers Grimm, manuscripts by English philosopher Jeremy Bentham, andAmsterdam’s city archives.How it works:Since handwriting varies so much, the platform trains a bespoke transcription model for each individual scribe.\n\nBehind the news:Transkribuslaunchedin 2015 as a collaboration between 17 archives, universities, and research groups across Europe. Some 45,000 users have trained 7,700 models so far.Why it matters:Most optical character recognition approaches perform poorly on the millions of handwriting styles represented in historical archives. By transcribing and making these documents searchable and sortable, machine learning is helping to deepen our understanding of past people and events.We’re thinking:This platform could also be a gift to amateur historians with shoeboxes full of their forebears’ diaries, documents, and love letters.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-12-1620at2010.35.4020AM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/MOONSHOT.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Zooxgif1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/3-2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2048.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/HANDWRITING-1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-157/",
    "title": "issue 157",
    "date": "",
    "reading_time": "",
    "content": "Bias in AI is a serious problem. For example, if a judge who’s deciding how to sentence a defendant relies on an AI system that routinelyestimates a higher risk that offenders of a particular race will reoffend, that’s a terrible thing. As we work to reduce bias in AI models, though, it’s also worth exploring a different issue: inconsistency. Specifically, let’s consider how inconsistent human decisions are, and how AI can reduce that inconsistency.If a human judge, given two defendants who committed the same crime under identical circumstances, sentences one to three years in prison and the other to 30 days, we would consider this inconsistency blatantly unfair. Yet, as Daniel Kahneman and his co-authors document in their book,Noise: A Flaw in Human Judgment, human decision-making is extremely inconsistent (or noisy).\n\nOnestudyfound that judges systematically sentenced defendants more harshly if the local football team had suffered an upset loss (which presumably made the judge cranky). Judges are only human, and if they’re swayed by football outcomes, imagine how many other irrelevant factors may influence their decisions!\n\nMany human decisions rest on complex criteria, and humans don’t always define their criteria before weighing them. For example:\n\nIn contrast, given the same input, a trained neural network will produce the same output every time. Given similar inputs, a trained model will also typically output similar results. Automated software tends to be highly consistent. This is one of automation’s huge advantages: Algorithms make decisions much more consistently than humans. To my mind, they offer a way to give patients more consistent and fair treatment options, make manufacturing more efficient, make retail product catalogs less confusing to shoppers, and so on.In conversations about whether and how to build an AI system, it’s important to address how to ensure that the system doesn’t have significant bias as well as how to benchmark its bias against human bias. If you’re trying to get an AI project approved, you may find it useful raise the issue of consistency as well. Measuring the consistency of your algorithm relative to humans who make the same decision can add weight to arguments in favor of investing in an automated system.\n\nKeep learning!\n\nAndrew\n\nNew data suggests the drug industry is hooked on AI.\n\nWhat’s new:Pharmaceutical companies in several countries are hiring machine learning engineers at increasing rates, industry news publicationPharmaceutical Technologyreported. Most job openings are posted in the United States, though some countries in Europe and Asia are gaining ground.How it works:The publication analyzed data from GlobalData’s paywalleddatabase, which tracks job listings in a variety of industries and analyzes the text to group them into categories.\n\nBehind the news:In a recent report, GlobalDataestimatedthat the pharmaceutical industry will spend over $3 billion on AI by 2025, driven largely by applications in drug discovery. The trend has also prompted major pharma companies including Astra-Zeneca, Pfizer, and Sanofi to acquire, invest in, or partner with startups. GlobalData counted 67 such partnerships in 2021, up from 23 in 2018.Why it matters:Bringing a new drug to market can take decades and costbillions of dollars. AI can cut time and costs in myriad ways, for instance by recognizing viable molecules without lab experimentation, identifying patients who might benefit from a drug, and predicting how patients might respond to them.\n\nWe’re thinking:Given the economic value of online advertising and product recommendations, many machine learning engineers — and an entire genre of machine learning approaches — are devoted to optimizing their results. Given the value of pharmaceuticals, we have no doubt that machine learning has immense potential in that domain as well. Similarly, a large body of specialized machine learning techniques is waiting to be developed for many industries.\n\nData from vehicle makers sheds light — though not much — on the safety of current autonomous and semi-autonomous vehicles.\n\nWhat’s new:The United States National Highway Traffic Safety Administration (NHTSA)detailedcollisions over a 12-month period that involved cars that drive themselves or automate some driving tasks. This is the first edition of what promises to be an annual report.\n\nGoing driverless:Fully automated driving systems (often called ADS) that operate without a driver behind the wheel aren’t yet widely available. For the most part, they're being tested in a small number of designated areas. Manufacturers must report incidents that occurred within 30 seconds of engaging an ADS or resulted in property damage or personal injury.\n\nExtra hands on the wheel:Semi-autonomous vehicles equipped with automated driving assistance systems (known as ADAS) require a flesh-and-blood driver but can steer, accelerate, and decelerate on their own. Manufacturers must report crashes that caused an airbag to inflate, required a car to be towed, or sent someone to a hospital.\n\nYes, but:The report doesn’t tally miles driven by fully autonomous, semi-autonomous, and conventional vehicles, nor at what speeds they traveled. Without that information, there's no way to derive a collision rate per mile or evaluate the severity of injuries at various speeds. Moreover, the report includes only crashes known to manufacturers. It may have missed those that weren’t reported to law enforcement or through consumer complaints. (This may explain the high numbers for Tesla, which harvests data directly from its vehicles.)\n\nWhy it matters:Vehicle safety is a life-and-death matter. Fully autonomous cars may not reach the market for years, but a degree of automated driving is commonplace: Vehicles that can steer, accelerate, and decelerate temporarily with a human presentaccountedfor 30 percent of new car sales in the U.S. during the fourth quarter of 2020.We’re thinking:Initial efforts to collect data, however incomplete, often lead to better data in the future. We hope that NHTSA improves these reports in the coming years by adding the total miles, as well as subdivisions according to in-town and on-highway speed ranges, driven by each of the two automation classes as well as unassisted humans.\n\nA face recognition system is helping identify victims of the Holocaust.\n\nWhat’s new:From Numbers to Namesmatches individuals to faces in publicly available images related to the genocide of European Jews between 1941 and 1945.How it works:Built by Google software engineer Daniel Patt and financier Jed Limmer, the site matches images uploaded by users with faces from the United States Holocaust Memorial Museum’sphoto collection.\n\nBehind the news:Deep learning plays a growing role in understanding history.\n\nWhy it matters:Roughly 11 million people were systematically murdered by the government of Nazi Germany for their ethnicity, religion, political beliefs, or sexual orientation. Identifying the victims doesn’t erase the crime of their deaths, but it can help bring closure to their relatives and strengthen our resolve to make sure nothing similar ever happens again.We’re thinking:While lives lost to war havedecreasedsignificantly over the decades, humanity has yet to progress beyond senseless killing. Learning about the atrocities of the past helps us view current events — such as the Russia-Ukraine war — with a critical eye and stand firm for human rights.\n\nModels likeAlphaFoldhave made great strides in finding protein shapes, which determine their biological functions. New work separated proteins into functional families without considering their shapes.\n\nWhat’s new:A team led by Maxwell L. Bileschiclassified protein familiesusing a model (called ProtCNN) and a process (called ProtREP) that used that model’s representations to address families that included fewer than 10 annotated examples. The project was a collaboration between Google, BigHat Biosciences, Cambridge University, European Molecular Biology Laboratory, Francis Crick Institute, and MIT.\n\nKey insight:A neural network that has been trained on an existing database of proteins and their families can learn to assign a protein to a family directly. However, some families offer too few labeled examples to learn from. In such cases, an average representation of a given family’s members can provide a standard of comparison to determine whether other proteins fall into that family.\n\nHow it works:The authors trained aResNeton adatabaseof nearly 137 million proteins and nearly 18,000 family classifications.\n\nResults:The ensemble model achieved accuracy of 99.8 percent, higher than both comparing representations (99.2 percent) and the popular method known asBLASTp(98.3 percent). When classifying members of low-resource families, the representation-comparison method achieved 85.1 percent accuracy. Applying the ensemble to unlabeled proteins increased the number of labeled proteins in the database by nearly 10 percent — more than the number of annotations added to the database over the past decade.\n\nWhy it matters:New problems don’t always require new methods. Many unsolved problems — in biology and beyond — may yield to well established machine learning approaches such as few-shot learning techniques.\n\nWe’re thinking:Young people, especially, ought to appreciate this work. After all, it’s pro-teen.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/08/PHARMA.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/08/SAFETY.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/08/HOLOCAUST.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/08/PROTEIN.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-179/",
    "title": "issue 179",
    "date": "",
    "reading_time": "",
    "content": "Will the future of large language models limit users to cutting-edge models from a handful of companies, or will users be able to choose among powerful models from a large number of developers? We’re still early in the development of large language models (LLMs), but I believe that users will have access to models from many companies. This will be good for innovation.\n\nWe've seen repeatedly that yesterday’s supercomputer is tomorrow’s pocket watch. Even though training an LLM currently requires massive data and infrastructure, I see encouraging progress toward wider availability and access along three dimensions:\n\nThere were times in my career when I worked with some of the world’s biggest systems dedicated to training deep learning models, but they didn’t last. I had access tomassive parallel computing powerat Google, and my teams built anearly GPU serverat Stanford and ahigh-performance computing system focused on speech recognition. Faster systems soon left those formerly cutting-edge systems in the dust. Even though training an LLM currently requires a daunting amount of computation, I see little reason to believe that it won’t quickly become much easier, particularly given the widespread excitement and massive investment around them.\n\nWhat does this mean for businesses? Many companies have built valuable and defensible businesses using early innovations in deep learning, and I foresee that similarly valuable and defensible systems will be built using recent innovations in LLMs and, more broadly, generative AI.\n\nI will explore this topic more in future letters. Until then,\n\nKeep learning!\n\nAndrew\n\nThe breakout text generator faces resistance — even within the AI community.\n\nWhat's new:Organizations including the International Conference on Machine Learning (ICML) and the New York Department of Education banned OpenAI's ChatGPT amid debate over the implications of its use and limitations of its output.\n\nWhat happened:Professional societies, schools, and social media sites alike reacted to the potential of ChatGPT and other large language models to produce falsehoods, socially biased information, and other undesirable output in the guise of reasonable-sounding text.\n\nBehind the news:Researchers have raisedred flagsaround the issues that have prompted organizations to ban ChatGPT since large language models first showed a propensity to generate plausible but unreliable text. The latest efforts seek to identify generated output.\n\nYes, but:Users may find ways to circumvent safeguards. For instance, OpenAI’s watermarking proposal can be defeated by lightly rewording the text, MIT computer science professor Srini DevadastoldTechCrunch. The result could be an ongoing cat-and-mouse struggle between users and model-makers.\n\nWhy it matters:Many observersworrythat generative text will disrupt society. EvenOpenAI CEO Sam Altmantweetedthat the model was currently unsuitable for real-world tasks due to its deficiencies in truth-telling. Bans are an understandable, if regrettable, reaction by authorities who feel threatened by the increasingly sophisticated abilities of large language models.\n\nWe're thinking:Math teachers once protested the presence of calculators in the classroom. Since then, they’ve learned to integrate these tools into their lessons. We urge authorities to take a similarly forward-looking approach to assistance from AI.\n\nHate talking to customer service? An AI-powered tool may soon do it for you.\n\nWhat's new:Joshua Browder, chief executive of the consumer advocacy organization DoNotPay, demonstrated a system that autonomously navigates phone menus and converses with customer service representatives in a deepfaked version of his own voice. DoNotPay plans to offer a free version that uses generic voices as well as a paid option that lets users clone their own voice, BrowdertoldVice.\n\nHow it works:In the video demo that has been removed from YouTube, the system could be seen and heard negotiating with a bank representative to refund wire-transfer fees.\n\nYes, but:The ethical question whether humans — be they consumers or customer-service reps — should be informed when they’re conversing with a bot remains open. The technology clearly invites fraud. Cybercriminals have already used OpenAI's large language models for phishing attacks, cybersecurity analyst Check Point Research found in a recentstudy. In 2020, a groupscammeda Dubai bank out of $400,000 by synthesizing a customer’s voice.\n\nWhy it matters:Nobody likes to spend time on the phone with customer service. AI could make this obsolete, saving time and possibly gaining refunds.\n\nWe're thinking:Enjoy using your automated doppelganger to deal with customer service while you can! As corporations and financial institutions strengthen their defenses against automated fraud, they’re likely to downgrade service to automated customers as well.\n\nBuilding a startup is hard. But with a venture studio as a partner, founders dramatically increase their odds of success. Join us on January 12, 2023, at 2:00 p.m. Pacific Time to learn how venture studios work and how AI Fund sets up entrepreneurs to win.Register here\n\nA major company is using face recognition to settle scores.\n\nWhat's new:MSG Entertainment, which operates large entertainment venues in several cities in the United States,usedface recognition to block its perceived enemies from attending events at New York’s Madison Square Garden and Radio City Music Hall,The New York Timesreported.\n\nWhat happened:MSG used the technology on at least two occasions to eject attorneys who work at law firms involved in litigation against the company.\n\nBehind the news:New York does not restrict use of face recognition by private companies.MSG venues haveusedthe technology since at least 2018 to compare attendees’ faces to a database of photographs and flag individuals the company considers undesirable. Prior to Conlon’s ejection, a judgeruledthat MSG has a right to deny entry to anyone who doesn’t hold a valid ticket; Conlon’s employer sued in a case that is ongoing.\n\nWhy it matters:Privacy advocates have longfearedthat face recognition could enable powerful interests to single out individuals for retribution. MSG’s use of the technology to target its perceived enemies certainly fits that description.\n\nWe're thinking:Face recognition is a flashpoint in AI, and rightly so. We need to protect privacy and fairness even as we improve safety and productivity. But outrage over such ill-considered uses of the technology could lead regulators to ban it despite its potential for good — for instance, by helping security personnel identify people who are legally barred from an area. Regulators who focus on face recognition should address ethical gray areas as well as outright abuses.\n\nTraining a model to separate the objects in a picture typically requires labeled images for best results. Recent work upped the ante for training without labels.\n\nWhat’s new:Mark Hamilton and colleagues at Cornell, Google, and Massachusetts Institute of Technology developed Self-supervised Transformer with Energy-based Graph OptimizationSTEGO, an architecture and training method for semantic segmentation that substantially improved the state of the art for unsupervised learning of this task.\n\nKey insight:A computer vision model pretrained on images produces similar representations of pixels that belong to similar objects, such as patches of sky. By clustering those representations, a model can learn to identify groups of pixels that share a label without referring to the labels themselves. (If the feature extractor learns in an self-supervised way, it doesn’t need labels either.)\n\nHow it works:A feature extractor (the transformerDINO, which was pretrained in an unsupervised manner on ImageNet) generated features for each pixel of input images. A vanilla neural network trained onCOCO-Stuffrefined the features into a representation of each pixel.\n\nResults:To measure how well their model separated the objects in an image, the authors used a matching algorithm to match grouped pixels with ground-truth labels (that is, they labeled the pixels). Their method achieved 28.2 percent meanintersection over union(the ratio of the number of correctly labeled pixels to total number of pixels, averaged over all classes) on the 27-class COCO-Stuff validation set. Its closest unsupervised rival,PiCIE+H, achieved 14.4 percent mean intersection over union. As for supervised approaches, the state-of-the-art,ViT-Adapter-L, achieved 52.9 percent mean intersection over union.\n\nWhy it matters:This system is designed to be easily upgraded as datasets and architectures improve. The authors didn’t fine-tune the feature extractor, so it could be swapped for a better one in the future. Upgrading would require retraining the relatively small vanilla neural network, which is faster and simpler than training a typical semantic segmentation model.\n\nWe’re thinking:Since it didn’t learn from labels, the authors’ vanilla neural network can’t identify the objects it segments. Could it learn to do that, CLIP-style, from images with corresponding captions?",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--20--1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--27--2.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--28-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/01/1673045257638--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--21-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--29-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-133/",
    "title": "issue 133",
    "date": "",
    "reading_time": "",
    "content": "Russian troops have invaded Ukraine, and the terrifying prospect of a war in Europe weighs on my mind. My heart goes out to all the civilians affected, and I hope we won’t see the loss of life, liberty, or property that many people fear.I’ve often thought about the role of AI in military applications, but I haven’t spoken much about it because I don’t want to contribute to the proliferation of AI arms. Many people in AI believe that we shouldn’t have anything to do with military use cases, and I sympathize with that idea. War is horrific, and perhaps the AI community should just avoid it. Nonetheless, I believe it’s time to wrestle with hard, ugly questions about the role of AI in warfare, recognizing that sometimes there are no good options.\n\nFull disclosure: My early work on deep learning wasfundedby the U.S. Defense Research Projects Agency, or DARPA. Last week,Wiredmentioned my early work on drone helicopters, also funded by DARPA. During the U.S.-Iraq war, when IEDs (roadside bombs) were killing civilians and soldiers, I spent time thinking about how computer vision can help robots that dispose of IEDs.\n\nWhat may not be so apparent is that forces that oppose democracy and civil liberties also have access to AI technology. Russian drones have been found tocontainparts made in the U.S. and Europe. I wouldn’t be surprised if they also contain open-source software that our community has contributed to. Despite efforts to control exports of advanced chips and other parts that go into AI systems, the prospects are dim for keeping such technology out of the hands of people who would use it to cause harm.\n\nSo I see little choice but to make sure the forces of democracy and civil liberties have the tools they need to protect themselves.\n\nSeveral organizations have come to the same conclusion, and they’ve responded by proposing principles designed to tread a fine line between developing AI’s capacity to confer advantage on the battlefield and blunting its potential to cause a catastrophe. For example, the United Nations has issuedguidancethat all decisions to take human life must involve human judgment. Similarly, the U.S. Department of Defenserequiresthat its AI systems be responsible, equitable, traceable, reliable, and governable.\n\nI support these principles. Still, I’m concerned that such guidelines, while necessary, aren’t sufficient to prevent military abuses. User interfaces can be designed to lead people to accept an automated decision — consider the pervasive “will you accept all cookies from this website?” pop-ups that make it difficult to do anything else. An automated system may comply technically with the U.N. guidance, but if it provides little context and time for its human operator to authorize a kill mission, that person is likely to do so without the necessary oversight or judgment.\n\nWhile it’s important to establish high-level principles, they must be implemented in a way that enables people to make fateful decisions — perhaps the most difficult decisions anyone can make — in a responsible way. I think of the protocols that govern the use of nuclear weapons, which so far have helped to avoid accidental nuclear war. The systems involved must be subject to review, auditing, and civilian oversight. A plan to use automated weapons could trigger protocols to ensure that the situation, legality, and schedule meet strict criteria, and that the people who are authorized to order such use are clearly identified and held accountable for their decisions.\n\nWar is tragic. Collectively we’ve invented wondrous technologies that also have unsettling implications for warfare. Even if the subject presents only a menu of unpalatable options, let’s play an active role in navigating the tough choices needed to foster democracy and civil liberties.\n\nKeep learning,\n\nAndrew\n\nNuclear fusion technology, long touted as an unlimited source of safe, clean energy, took a step toward reality with a machine learning algorithm that molds the fuel in a reactor’s core.What’s new:Researchers at DeepMind and École Polytechnique Fédérale de Lausanne (EPFL)developeda reinforcement learning algorithm to manipulate hydrogen plasma — an extremely high-energy form of matter — into an optimal shape for energy production.How it works:Reactors that confine plasma in a chamber known as a tokamak generate energy by pushing its atoms so close together that they fuse. A tokamak uses powerful magnetic coils to compress the plasma, heating it to the neighborhood of 100 million degrees Celsius to overcome the electrostatic force that normally pushes them apart. The authors trained a reinforcement learning model to control the voltage of 19 magnetic coils in a small, experimental tokamak reactor, enabling them to shape the plasma in ways that are consistent with maintaining an ongoing fusion reaction.\n\nResults:In experimental runs with the real-world reactor, a previous algorithm controlled the coils to form a preliminary plasma shape before handing off the task to the authors’ model. Plasma can't be observed directly, so the authors calculated its shape and position properties based on measurements of the magnetic field within the tokamak. In five separate experiments, the controller formed the plasma into distinct shapes, such as a conventional elongated shape and a prospective “snowflake” shape, within particular tolerances (2 centimeters root mean squared error for shape, 5 kiloamperes root mean squared error for current passing through the plasma). In a novel feat, the algorithm maintained two separate plasma droplets for 200 milliseconds.Behind the news:Conventional nuclear energy results from nuclear fission. Scientists have been trying to harness nuclear fusion since the 1950s. Yet no fusion reactor has generated more energy than it consumed. (The U.S. National Ignition Facilitycame the closest yetlast year.) A growing number of scientists areenlistingmachine learning to manage the hundreds of factors involved in sustaining a fusion reaction.\n\nWhy it matters:Plasma in a tokamak, which is several times hotter than the sun and reverts to vapor if its electromagnetic container falters, is continually in flux. This work not only shows that deep learning can shape it in real time, it also opens the door to forming plasma in ways that might yield more energy. The next challenge: Scale up to a reactor large enough to produce meaningful quantities of energy.We’re thinking:Fusion energy — if it ever works — would be a game changer for civilization. It’s thrilling to see deep learning potentially playing a key role in this technology.\n\nIndustrial gauges are often located on rooftops, underground, or in tight spaces — but they’re not out of reach of computer vision.What’s new:The Okinawa startupLiLz Gaugeprovides a system that reads analog gauges and reports their output to a remote dashboard. The system is available in Japan and set to roll out globally in 2023.How it works:The system automates inspection in places that have no computer network or power. It ties together remote units that integrate a camera, processor, cellular and Bluetooth connectivity, and a battery designed to last up to three years.\n\nBehind the news:AI increasingly enables inspectors to do their jobs at a distance. For instance, drones equipped with computer vision have been used to spot damage and deficiencies inbuildings,dams,solar and wind farms, andpower lines.Why it matters:Given the complexity of replacing some gauges, computer vision may be more cost effective than installing a smart meter. More broadly, industrial operations don’t necessarily need to replace old gear if machine learning can give it new life. Well-established machine learning approaches can be engineered to meet the needs of low-tech industries.We’re thinking:This application looks like low-hanging fruit or computer vision. There’s ample room for clever engineers to adapt older practices with newer ways of doing things.\n\nLooking to prepare for Google’s TensorFlow Certificate exam? Gain the skills you need to build scalable AI-powered applications with the TensorFlow Developer Professional Certificate program!Enroll today\n\nRobocalls slip through smartphone spam filters, but a new generation of deep learning tools promises to tighten the net.What’s new:Research proposed fresh approaches to thwarting robocalls. Such innovations soon could be deployed in apps,IEEE Spectrumreported.How it works:RobocallGuard, devised by researchers at Georgia Institute of Technology and University of Georgia, answers the phone and determines whether a call is malicious based on what the caller says.TouchPal, proposed by a team at Shanghai Jiao Tong University, UC Berkeley, and TouchPal Inc., analyzes the call histories of users en masse to identify nuisance calls.\n\nBehind the news:Many robocallers have outgrown the fixed phone numbers, obviously prerecorded messages, and “press-1” phone trees that were dead giveaways in the past, making it harder for recipients to recognize spam calls even after answering the phone.\n\nWhy it matters:Robocallers placed nearly4 billionnuisance calls in the U.S. in January 2021. These numbers have hardly budged since 2019 despite governmenteffortsto combat them. The problem is even worse elsewhere. In Brazil, the average user of one call-blocking app received more thanone spam call daily. It’s unlikely that robocalls will ever disappear entirely, but machine learning could relegate them to the background, like email spam.We’re thinking:If everybody blocks robocalls, maybe robocallers will start sending nuisance calls to each other.\n\nLet’s say you have a pretrained language model and a small amount of data to fine-tune it to answer yes-or-no questions. Should you fine-tune it to classify yes/no or to fill in missing words — both viable approaches that are likely to yield different results? New work offers a way to decide.What’s new:Yanan Zheng and collaborators at Beijing Academy of Artificial Intelligence, Carnegie Mellon University, DeepMind, Massachusetts Institute of Technology, and Tsinghua University proposedFewNLU, a method that compares fine-tuning algorithms in few-shot natural language understanding, or language comprehension tasks in which a model must learn from a few examples. They also provide a toolkit for optimizing fine-tuned performance.Key insight:Previous comparisons of fine-tuning algorithms used fixed hyperparameter values; the researchers chose values known to work with a particular algorithm and maintained them with other algorithms. But different combinations of algorithm and architecture require different hyperparameter values to achieve their optimal performance. So, to compare fine-tuning algorithms, it’s best to determine hyperparameter values separately for each combination.How it works:The authors compared various data-split strategies and hyperparameter values for different fine-tuning algorithms applied toDeBERTaandALBERT. They fine-tuned the models on 64 labeled examples for each of seven tasks in theSuperGLUEbenchmark (such as answering yes-or-no questions about a text passage or multiple-choice questions about causes of events) to find the best data-split strategy and most important hyperparameters. Then they compared fine-tuning algorithms using different values for the most important hyperparameters.\n\nResults:Multi-Splits led to superior test performance on 4 of the 7 tasks, and it had the greatest correlation between validation and test performance on 5 of the 7 tasks. Changes in the prompt pattern led to the greatest standard deviation in performance across hyperparameters (average of 5.5 percent accuracy, compared to the next-highest, training order, at 2.0 percent), suggesting that it was the most important hyperparameter to optimize. Using Multi-Splits and the optimal hyperparameter values for each fine-tuning algorithm (specific to each model and task), PET, ADAPET, and P-tuning performed similarly and typically outperformed CLS by 15 to 20 percentage points in accuracy and F1 score. There was no clear winner among PET, ADAPET, and P-tuning, each of which achieved the highest accuracy or F1 score on one task or another, often within 1 standard deviation of each other.Why it matters:It’s certainly good to know how to get the most out of fine-tuning. Beyond that, this work reinforces the notion that, since the only way to know the best hyperparameter values is to find them empirically, it pays to keep guessing to a minimum.We’re thinking:Here’s a puzzler: If the choice of a fine-tuning algorithm changes a model’s optimal hyperparameter values, is the choice itself a hyperparameter?",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202022-02-23%20at%2012.18.36%20PM.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202022-02-23%20at%2012.18.36%20PM.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20(13)-1.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20(13)-1.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/GAUGES.gif?upscale=true&width=1200&upscale=true&name=GAUGES.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20Image%20(1)-2.png?upscale=true&width=1200&upscale=true&name=The%20Batch%20Image%20(1)-2.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20(4)-3.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20(4)-3.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20(17)-1.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20(17)-1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-263/",
    "title": "issue 263",
    "date": "",
    "reading_time": "",
    "content": "I’m encouraged at the progress of the U.S. government at moving to stem harmful AI applications. Two examples are the new Federal Trade Commission (FTC)ban on fake product reviewsand theDEFIANCE Act, which imposes punishments for creating and disseminating non-consensual deepfake porn. Both rules take a sensible approach to regulating AI insofar as they target harmful applications rather than general-purpose AI technology.\n\nAs Idescribedpreviously, the best way to ensure AI safety is to regulate it at the application level rather than the technology level. This is important because the technology is general-purpose and its builders (such as a developer who releases an open-weights foundation model) cannot control how someone else might use it. If, however, someone applies AI in a nefarious way, we should stop that application.\n\nEven before generative AI, fake reviews were a problem on many websites, and many tech companies dedicate considerable resources to combating them. A telltale sign of old-school fake reviews is the use of similar wording in different reviews. AI’s ability to automatically paraphrase or rewrite is making fake reviews harder to detect.\n\nImportantly, the FTC is not going after the makers of foundation models for fake reviews. The provider of an open weights AI model, after all, can’t control what someone else uses it for. Even if one were to try to train a model to put up guardrails against writing reviews, I don’t know how it could distinguish between a real user of a product asking for help writing a legitimate review and a spammer who wanted a fake review. The FTC appropriately aims to ban the application of fake reviews along with other deceptive practices such as buying positive reviews.\n\nThe DEFIANCE Act, which passed unanimously in the Senate (and still requires passage in the House of Representatives before the President can sign it into law) imposes civil penalties for the creating and distributing non-consensual, deepfake porn. This disgusting application isharmingmany people including underage girls. While many image generation models do have guardrails against generating porn, these guardrails often can be circumvented via jailbreak prompts or fine-tuning (for models with open weights).\n\nAgain, DEFIANCE regulates an application, not the underlying technology. It aims to punish people who engage in the application of creating and distributing non-consensual intimate images, regardless of how they are generated — whether the perpetrator uses a diffusion model, a generative adversarial network, or Microsoft Paint to create an image pixel by pixel.\n\nI hope DEFIANCE passes in the House and gets signed into law. Both rules guard against harmful AI applications without stifling AI technology itself (unlike California’spoorly designedSB-1047), and they offer a good model for how the U.S. and other nations can protect citizens against other potentially harmful applications.Keep learning!\n\nAndrew\n\nBuild flexible, maintainable applications with our new course, “Building AI Applications with Haystack.” Guided by Tuana Çelik, you’ll build projects like a RAG app and a self-reflecting agent using the Haystack framework.Join for free\n\nWhile some observers argue that large language models can’t produce truly original output, new work prompted them to generate novel scientific research.\n\nWhat’s new:Researchers proposedAI Scientist, an agentic workflow that directs large language models to generate ideas for AI research, produce code to test them, and document the enquiry. You can see examples of its output and download the code to generate your own papershere. The team included Chris Lu, Cong Lu, Robert Tjarko Lange, and colleagues at Tokyo-based startup Sakana AI, University of Oxford, University of British Columbia, Vector Institute, and the Canadian Institute for Advanced Research.\n\nHow it works:The authors used Claude Sonnet 3.5, GPT-4o, DeepSeek Coder, and LLama 3.1 405B to generate papers in three categories: diffusion image modeling, transformer-based language modeling, and “grokking,” which the authors define as generalization and speed of learning in deep neural networks.\n\nResults:The team used GPT-4o to evaluate the generated papers according to theguidelinesfor papers presented at the Neural Information Processing Systems (NeurIPS) conference. The guidelines include an overall score between 1 (very strongly reject) and 10 (award-quality: flawless and groundbreaking) and a decision to reject or accept the paper.\n\nWhy it matters:Agentic workflows are a rising theme in AI research from simpler design patterns likereflectionto complex workflows fortranslating literature. These workflows make it possible to break down complex problems into more manageable subtasks. By breaking the task of conducting AI research into various stages of generating ideas, testing them, and writing a paper, an LLM that has access to the right tools can generate novel research papers with actual experimental results.\n\nWe’re thinking:Rather than merely synthesizing existing knowledge, this work points a fascinating direction for using AI to generate new knowledge! Right now, an LLM can suggest starting points for human researchers along with experiments that back up its suggestions.\n\nImage generation continued its rapid march forward with a new version of Google’s flagship text-to-image model.\n\nWhat’s new:GoogleintroducedImagen 3, a proprietary model that improves upon the previous version’s image quality and prompt adherence, with features like inpainting and outpainting to be added in the future. Imagen 3 is available via Google’sImageFXweb user interface andVertex AIPlatform. It follows closely upon the releases of Black Forest Labs’ Flux.1 family (open to varying degrees), Midjourney v6.1, and Stability AI Stable Diffusion XL 1 (open weights) — all in the last month.\n\nHow it works:The accompanyingpaperdoes not describe the model’s architecture and training procedures in detail. The authors trained a diffusion model on an unspecified “large” dataset of images, text, and associated annotations that was filtered to remove unsafe, violent, low-quality, generated, and duplicate images as well as personally identifying information. Google’s Gemini large language model generated some image captions used in training to make their language more diverse.\n\nResults:Imagen 3 mostly outperformed competing models in head-to-head comparisons based on prompts from datasets includingGenAI-Bench,DrawBench, andDALL-E 3 Eval. The team compared Imagen 3 to Midjourney v6.0, OpenAI DALL-E 3, Stable Diffusion 3 Large, and Stable Diffusion XL 1.0. More than 3,000 evaluators from 71 countries rated the models’ responses in side-by-side comparisons. The raters evaluated image quality, preference regardless of the prompt, adherence to the prompt, adherence to a highly detailed prompt, and ability to generate the correct numbers of objects specified in a prompt. Their ratings (between 1 and 5) were used to compute Elo ratings.\n\nWhy it matters:Each wave of advances makes image generators more useful for a wider variety of purposes. Google’s emphasis on filtering the training data for safety may limit Imagen 3’s utility in some situations (indeed, some userscomplainedthat Imagen 3 is more restrictive than Imagen 2, while the Grok2 large language model’s use of an unguardrailed version of Flux.1 for image generation has garneredheadlines). Nonetheless, precautions are an important ingredient in the evolving text-to-image recipe.\n\nWe’re thinking:It’s difficult to compare the benchmarks reported for Imagen 3 and the recently releasedFlux.1, which claims similar improvements over earlier models. In any case, Google has yet to publish a benchmark for generating text, a valuable capability for commercial applications. The Flux.1 models, two of which are open to some degree, may prove to be formidable rivals in this area.\n\nAlibaba followed up its open-weights Qwen2 large language models with specialized variations.\n\nWhat’s new:Qwen2-MathandQwen2-Audioare model families devoted to, respectively, solving math problems and generating text directly from audio. Both set new states of the art in a variety of English and Chinese benchmarks, and some versions offer open weights. Notably Qwen2-Math-Instruct-72B, whose 72 billion parameters are fine-tuned according to human preferences, outperformed top models including Claude 3.5 Sonnet, Gemini 1.5-Pro, GPT-4o, and Llama-3.1-405B on some math benchmarks.\n\nMath mavens:Qwen2-Math models includepretrainedandinstruction-tunedvariations that comprise 1.5 billion, 7 billion, and 72 billion parameters. Thelicensefor the largest version is free for noncommerical development and commercial developers who have less than 100 million monthly active users.\n\nAudio/text to text:A revision of the earlier Qwen-Audio,Qwen2-Audiotakes text and audio inputs and generates text outputs. It’s designed to (i) provide text chat in response to voice input including voice transcription and translation between eight languages and (ii) discuss audio input including voice, music, and natural sounds. Weights (8.2 billion parameters) are available for base and instruction-tuned versions. You can try ithere.\n\nWhy it matters:Qwen2 delivered extraordinary performance with open weights, putting Alibaba on the map of large language models (LLMs). These specialized additions to the family push forward math performance and audio integration in AI while delivering state-of-the-art models into the hands of more developers.\n\nWe’re thinking:It’s thrilling to see models with open weights that outperform proprietary models. The white-hot competition between open and closed technology is good for everyone!\n\nWhen training vision-language models, developers often remove lower-quality examples from the training set. But keeping only the highest-quality examples may not be ideal, researchers found.\n\nWhat's new:Sachin Goyal, Pratyush Maini, and colleagues at Carnegie Mellon University derivedscaling laws for filtering datathat describe how the utility of examples — in terms of how much they increase performance (or decrease loss) — falls when they are used over and over again in training.\n\nKey insight:When computational resources are limited relative to the amount of data available, some AI developers try to select the highest-quality examples and train on them for multiple iterations. However, the utility of examples declines a little bit every time they’re used. As computational resources rise, it’s better to introduce new examples even if they’re of slightly lower quality.\n\nHow it works:The authors used 128 million text-image pairs fromDataCompto train variousCLIPmodels, varying the data quality and number of times a model saw each example during training.\n\nResults:The authors rated each model’s performance according to the average across 18 visual tasks, mostly involving classification accuracy (including ImageNet). The more examples a model saw, the more its performance benefited from training on lower-quality examples in addition to the highest-quality examples. Of the models that saw 32 million examples, the one trained on the highest-quality 10 percent of examples did best. Of the models that saw 128 million examples, the one trained on the highest-quality 30 percent of examples did the best. Of the models that saw 640 million examples, the one trained on the highest-quality 40 percent of examples did the best. These results confirmed theoretical predictions based on the scaling curves.\n\nWhy it matters:The practice of pretraining vision-language models on a certain percentage of only the highest-quality examples is not ideal. A better approach is to perform experiments to determine the best percentage given the available compute budget: Train first on a small amount of data and filter for quality according to the scaling curves.\n\nWe're thinking:This work affirms the fundamental principle ofData-centric AI: Systematically engineering training data is essential for getting optimal performance from a given architecture. However, it shows that using only the highest-quality data works best with smaller compute budgets. With more compute, lower-quality data can improve performance more than repeating the highest-quality examples too many times.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--80--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/08/The-Batch-ads-and-exclusive-banners---2024-08-20T091645.073.png",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T140739.984.png",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142127.807.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142223.196.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142304.320.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-136/",
    "title": "issue 136",
    "date": "",
    "reading_time": "",
    "content": "Last week, Iwroteabout the grand challenge of artificial general intelligence. Other scientific and engineering grand challenges inspire me as well. For example, fusion energy, extended lifespans, and space colonization have massive potential to remake civilization (for good or ill).These grand challenges share a few traits:\n\nDespite their extreme uncertainty, such projects fill my mind with hopes and dreams. Fusion energy promises a safe, clean, unlimited source of electricity. The ability to harvest energy from the fusion of atoms could mitigate climate change and remake geopolitics by empowering all countries to become energy-independent.\n\nExtended lifespans could enable people to accumulate greater wisdom. Of course, they could also concentrate wealth and power in the hands of the longest-lived individuals and create difficult demographic challenges. Purported longevity compounds like resveratrol have fallen short of their promise, but I’m excited bystudieson the use of metformin and other compounds to lengthen lifespans.\n\nSpace colonization that carries robots and, someday, humans to distant planets, solar systems, and ultimately galaxies would extend the future course of human history beyond the duration of Earth and into a practically unlimited future. Spacefaring technology would lead humanity into uncharted realms much like homo sapiens’ departure from Africa led to a global civilization.\n\nLike artificial general intelligence, these grand challenges have motivated their share of overhyped startups, scorn from skeptics, and tireless enthusiasm from believers. Yet I hope to see progress in all of them within my lifetime. (If we manage to extend lifetimes, that could be a very long time.)\n\nThe most exciting thing is that AI developers can play a role in achieving them!\n\nAI is not panacea. But as a general-purpose technology, it can be applied to these grand challenges and others. Whenever I’m interested in a topic, be it climate change or quantum computing, my background in AI makes it easier to strike up a fruitful conversation with domain experts. All of us in AI have tools that could be useful to them.\n\nKeep learning!\n\nAndrew\n\nA deepfake of South Korea’s new president helped propel him into office.What’s new:Yoon Suk-yeol, whowonthe country’s March 9 election, campaigned using videos that featured anAI-generated likenessof himself answering voters’ questions. No deception was involved; viewers were informed that they were watching a computer animation.How it works:Seoul-based DeepBrain AI created Yoon’s avatar using 20 hours of audio and video of the candidate captured in front of a green screen, totaling around 3,000 spoken sentences, according toFrance24.\n\nBehind the news:The first known political use of deepfakes occurred in 2020, when Indian politician Manoj Tiwarialtereda campaign video to show himself delivering the same message in various local languages. The technology has also fueled political scandals. In 2019, a Malaysian government ministersaida video that captured him engaging in extramarital sex was a deepfake. Earlier that year, speculation that a video of Gabon’s president, Ali Bongo, was a deepfake hadspurredan attempted coup.Why it matters:Yoon, who is known for his gruff, no-nonsense personality, created a digital double designed to resonate positively with the young voters who were deemed critical to his victory. While some critics dismissed the gambit, Yoon’s success suggests a bright future for campaign-sanctioned fakes tailored to appeal to particular groups.We’re thinking:A politician used a deepfake to make himself seem more authentic! How’s that for irony?\n\nLose too much money at Texas hold ’em, and you may get an AI-generated warning.What’s new:Casinos and gaming websites are using machine learning to flag gamblers who show signs of addictive behavior,The New York Timesreported.How it works:Gambling businesses risk losing their licenses if they facilitate ruinous behavior. Moreover, they make more money on gamblers who pace themselves than those who lose their shirts. Denmark-basedMindway AImitigates these risks by flagging worrisome behavior on the part of their customers. The system is mainly employed by online betting platforms, including Flutter Entertainment and Entain, but  brick-and-mortar casinos have adopted the system as well.\n\nYes, but:Gambling addicts may not respond well to receiving automated messages telling them they have a problem, Brett Abarbanel, a gambling researcher at the University of Nevada Las Vegas, toldThe New York Times.Behind the news:Face recognition also plays a role in identifying problem gamblers. For instance, casinos in Macau haveusedthe technology to identify high rollers and offer them perks. The city’s gambling authoritystatedthat these systems were used only for security.Why it matters:As many as10 millionpeople suffer from compulsive gambling in the U.S. alone. Identifying problem gamblers helps combat the spiral of debt, substance abuse, and mental health issues that often follow. Of course, casinos benefit, too, if their patrons can remain solvent enough to keep pumping money back into the house.We’re thinking:For decades, the gambling industry hasuseddata science to help casino operators. It’s heartening to see it applying AI to help its customers.\n\nLearn how to generate images using generative adversarial networks (GANs)! TheGenerative Adversarial Networks Specializationmakes it easy to understand everything from foundational concepts to advanced techniques.Enroll today\n\nNeural networks may help farmers make sure their animals are happy.What’s new:Researchers led by Elodie Briefer and Ciara Sypherd at University of Copenhagendevelopeda system that interprets the moods behind a pig’s grunts and squeals.How it works:The authors trained convolutional neural networks to classify porcine expressions using a database of 7,414 vocal sounds made by animals engaged in 19 situations like feeding, fighting, running, or being led to a slaughterhouse.\n\nResults:The models achieved 91.5 percent accuracy classifying the sentiment of calls and 81.5 percent identifying the situation. A method that classified calls without machine learning achieved 61.7 percent and 19.5 percent respectively.Behind the news:The noises an animal makes aren’t the only indication of its wellbeing, but they offer a window into its mental state.\n\nWhy it matters:The authors plan to develop a tool that would monitor hogs’ behavior and anticipate their needs. Science has shown that animals arecapableof complex emotions, prompting countries likeAustraliaand theUnited Kingdomto pass laws that protect livestock welfare. Systems that evaluate animals’ emotional states could help farms stay in regulatory compliance and make better homes for the creatures in their care, as well as reassure consumers that their food was produced humanely.We’re thinking:This work has awakened our interest in programming withEIEIO.\n\nWhen you’re training a neural network, it takes a lot of computation to optimize its weights using an iterative algorithm like stochastic gradient descent. Wouldn’t it be great to compute the best parameter values in one pass? A new method takes a substantial step in that direction.What's new:Boris Knyazev and colleagues at Facebook developedGraph Hyper Network(GHN-2), agraph neural networkthat computed weights that enabled arbitrary neural network architectures to perform image recognition tasks. (A neural network that finds weights for another neural network is known as a hypernetwork.) GHN-2 improves on a similar hypernetwork,GHN-1, proposed by a different team.Key insights:GHN-1 learned based on how well a given architecture using generated weights performed the task. GHN-2 improved its predecessor’s performance by drawing on insights from training conventional neural networks:\n\nGNN basics:A graph neural network processes datasets in the form of a graph made up of nodes connected by edges (say, customers connected to products they’ve purchased or research papers connected to other papers they cite). During execution, it uses a vanilla neural network to update the representation of each node based on the representations of neighboring nodes.How it works:GHN-2 consists of an embedding layer, agated graph neural network, which uses a gated recurrent unit (a type of recurrent network layer) to update node representations, and a convolutional neural network. Its input is a neural network architecture in graph form, where each node represents a set of weights for an operation/layer such as convolution, pooling, or self-attention, and each edge is a connection from one operation/layer to the next. Its output is a set of weights for each operation/layer. The authors trained it to generate weights for classifying images inCIFAR-10orImageNetusing adatasetof 1 million randomly generated neural network architectures composed of convolutional layers, pooling layers, self-attention layers, and so on.\n\nResults:Architectures similar to those in the training set generally performed better using parameter values generated by GHN-2 than GHN-1. So did architectures that were wider, deeper, or more dense than those in the training set. Parameter values generated by GHN-2 yielded average CIFAR-10 accuracy of 66.9 percent versus GHN-1’s 51.4 percent. While GHN-2 outperformed GHN-1 on ImageNet, neither model produced great parameter values for that task. For instance, architectures similar to those in the training set and outfitted with parameter values from GHN-2 produced an average top-5 accuracy of 27.2 percent compared to GHN-1’s 17.2 percent.Why it matters:GHN-2 took only a fraction of a second to generate better-than-random parameter values, while training a ResNet-50 to convergence on ImageNet can take over one week on a 32GB Nvidia V100 GPU. (To be fair, after that week-plus of training, the ResNet-50’s accuracy can be 92.9 percent — a far better result.)We're thinking:The authors also found that initializing a model with GHN-2 boosted its accuracy after fine-tuning with a small amount of data. How much additional time did the initialization save compared to conventional initialization and fine-tuning?",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202022-03-16%20at%201.09.39%20PM.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202022-03-16%20at%201.09.39%20PM.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/DEEPFAKE.gif?upscale=true&width=1200&upscale=true&name=DEEPFAKE.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/GAMBLING.gif?upscale=true&width=1200&upscale=true&name=GAMBLING.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20Image%201%20(1)-1.png?upscale=true&width=1200&upscale=true&name=The%20Batch%20Image%201%20(1)-1.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/PIGS.gif?upscale=true&width=1200&upscale=true&name=PIGS.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20(16)-1.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20(16)-1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-264/",
    "title": "issue 264",
    "date": "",
    "reading_time": "",
    "content": "After a recentprice reductionby OpenAI, GPT-4o tokens now cost $4 per million tokens (using a blended rate that assumes 80% input and 20% output tokens). GPT-4 cost $36 per million tokens at its initial release in March 2023. This price reduction over 17 months corresponds to about a 79% drop in price per year: 4/36 = (1 - p)17/12. (OpenAI charges a lower price, just $2 per million tokens, for using a new Batch API that takes up to 24 hours to respond to a batch of prompts. That’s an 87% drop in price per year.)\n\nAs you can see, token prices are falling rapidly! One force that’s driving prices down is the release of open weights models such as Llama 3.1. If API providers, including startups Anyscale, Fireworks, Together.ai, and some large cloud companies, do not have to worry about recouping the cost of developing a model, they can compete directly on price and a few other factors such as speed.\n\nFurther, hardware innovations by companies such as Groq (a leading player in fast token generation), Samba Nova (which serves Llama 3.1 405B tokens at an impressive114 tokens per second), and wafer-scale computation startup Cerebras (which just announced a newoffering), as well as the semiconductor giants NVIDIA, AMD, Intel, and Qualcomm, will drive further price cuts.\n\nWhen building applications, I find it useful to design to where the technology is going rather than only where it has been. Based on the technology roadmaps of multiple software and hardware companies — which include improved semiconductors, smaller models, and algorithmic innovation in inference architectures — I’m confident that token prices will continue to fall rapidly.\n\nThis means that even if you build an agentic workload that isn’t entirely economical, falling token prices might make it economical at some point. As Iwrotepreviously, being able to process many tokens is particularly important for agentic workloads, which must call a model many times before generating a result. Further, even agentic workloads are already quite affordable for many applications. Let's say you build an application to assist a human worker, and it uses 100 tokens per second continuously: At $4/million tokens, you'd be spending only $1.44/hour – which is significantly lower than the minimum wage in the U.S. and many other countries.\n\nSo how can AI companies prepare?\n\nBecause multiple providers now host Llama 3.1 and other open-weight models, if you use one of these models, it might be possible to switch between providers without too much testing (though implementation details — specifically quantization, does mean that different offerings of the model do differ in performance). When switching between models, unfortunately, a major barrier is still thedifficulty of implementing evals, so carrying out regression testing to make sure your application will still perform after you swap in a new model can be challenging. However, as the science of carrying out evals improves, I’m optimistic that this will become easier.\n\nKeep learning!\n\nAndrew\n\nIn our short course “Large Multimodal Model Prompting with Gemini,” you’ll learn how to build systems that reason across text, images, and video and how prompting multimodal models differs from text-only LLMs. You’ll also optimize LMM systems and output.Enroll today!\n\nA man who lost the ability to speak four years ago is sounding like his earlier self, thanks to a collection of brain implants and machine learning models.\n\nWhat’s new:Researchers built a system thatdecodes speech signals from the brainof a man who lost the ability to speak clearly due to amyotrophic lateral sclerosis, also known as ALS, and enables him to speak through a synthetic version of his former voice. At the start of the study, his efforts to speak were intelligible only to his personal caregiver. Now he converses regularly with family and friends,The New York Timesreported. Nicholas Card built the system with colleagues University of California-Davis, Stanford University, Washington University, Brown University, VA Providence Healthcare, and Harvard Medical School.\n\nHow it works:The authors surgically implanted four electrode arrays into areas of the brain that are responsible for speech. The system learned to decode the patient’s brain signals, decide the most likely phonemes he intended to speak, determine the words those phonemes express, and display and speak the words aloud using a personalized speech synthesizer.\n\nResults:After two hours of recording the patient’s brain signals and training on that data, the system achieved 90.2 percent accuracy in the copying task. By the final session, the system achieved 97.5 percent accuracy and enabled the patient to speak on average 31.6 words per minute using a vocabulary of 125,000 words.\n\nBehind the news:Previous work either had muchlower accuracyor generated alimited vocabulary. The new work improved upon a 2023studythat enabled ALS patients to speak with 76.2 percent accuracy using a vocabulary of equal size.\n\nWhy it matters:Relative to the 2023 study on which this one was based, the authors changed the positions of the electrodes in the brain and continued to update the GRU throughout the recording/training sessions. It’s unclear which changes contributed most to the improved outcome. As language models improve, new models potentially could act as drop-in replacements for the models in the authors’ system, further improving accuracy. Likewise, improvements in speech-to-text systems could increase the similarity between the synthetic voice and the patient’s former voice.\n\nWe’re thinking:Enabling someone to speak again restores agency. Enabling someone to speak again in their own voice restores identity.\n\nAn agentic coding assistant boosted the state of the art in an important benchmark by more than 30 percent.\n\nWhat’s new:Cosine, a startup based in London, unveiledGenie, a coding assistant that achieves top performance on SWE-bench, which tests a model’s ability to solve GitHub issues. The company has yet to announce pricing and availability, but a waitlist is available.\n\nHow it works:Genie is afine-tuned version of GPT-4owith a larger context window ofundisclosedsize. It works similarly toagentic coding toolslike Devin, Q, OpenDevin, and SWE-agent. Its agentic workflow loops through four processes: retrieving information, planning, writing code, and running it. It was trained on a proprietary training set that captures software engineers’ processes for reasoning, gathering information, and making decisions. It edits lines of code in place rather than rewriting entire sections or files from scratch.\n\nResults:Tested onSWE-benchFull (2,294 issue-commit pairs across 12 Python repositories), Genie solved 30.1 percent of problems, far ahead of the next closest competitor, Amazon Q, at 19.75 percent. Genie achieved 50.7 percent of the SWE-bench Lite (winnowed to 300 issue-commit pairs to save computation), beating CodeStory Aide plus other models at 43 percent. (Genie’s results don’t appear on the official SWE-bench leaderboard. The leaderboard requires that models document their workings, which Cosine declined to avoid revealing proprietary information. Cosine released Genie’ssolution setsto verify its performance.)\n\nBehind the news:SWE-bench’s creators recently collaborated with OpenAI to produce a new version,SWE-bench Verified. They eliminated extremely difficult and poorly configured problems, leaving 500 human-verified issue-commit pairs. Cosine has yet to publish Genie’s performance on SWE-bench Verified. As of this writing, Amazon Q ranks in first place with 38.8 percent.\n\nWhy it matters:Some developers of AI coding assistants train models to follow human-style procedures while others are building AI-native methods. Genie takes a distinct step forward by mimicking software engineers. Competition between thetwo approaches, along with longer context windows, faster inference, and increasingly sophisticated agentic workflows, is driving improvement of coding assistants at a rapid pace.\n\nWe’re thinking:We’re glad this Genie escaped the bottle!\n\nAI is a red-hot topic for lobbyists who aim to influence government policies in the United States.What’s new:The number of organizations lobbying to influence U.S. laws and regulations that affect AI jumped more than 20 percent in the first half of 2024,TechCrunchreported. Data collected by OpenSecrets, which tracks political contributions, shows increased lobbying by startups including OpenAI and Anthropic.How it works:OpenSecrets searched for the words “AI” and “artificial intelligence” in lobbying disclosure forms. Organizations must file such forms quarterly if they discuss specific laws and regulations with decision makers or their staffs.\n\nYes, but:The lobbying disclosure forms show who is spending money to influence policy, but they provide only a limited view. For instance, they reveal only that an organization aimed to influence AI policy, not the directions in which they aimed to influence it. Similarly, the disclosures shed no light on other efforts to influence laws and regulations such as advertising or campaign contributions. They also don’t reveal how much an organization discussed AI relative to other topics and concerns. For instance, last year the American Medical Association spent $21.2 million on lobbying including AI but, given the wide range of policy issues involved in medicine, AI likely accounted for a small amount of the total.Behind the news:The ramp-up in AI lobbying comes as the U.S. Congress is considering a growing number of laws that would regulate the technology. Since 2023, more than 115 bills have been proposed that seek to restrict AI systems, require developers to disclose or evaluate them, or protect consumers against potential harms like AI bias, infringement of privacy or other rights, or spreading inaccurate informationaccording tothe nonprofit, nonpartisan Brennan Center for Justice. Nearly 400 state laws are also under consideration,according toBSA, a software lobbying group, including California SB-1047, which wouldregulateAI models whose training exceeds a particular threshold of computation. Moreover, the U.S. will hold national elections in November, and lobbying of all kinds typically intensifies as organizations seek to influence candidates for office.Why it matters:Given the large amount of AI development that takes place in the U.S., laws that govern AI in this country have an outsized influence over AI development worldwide. So it’s helpful to know which companies and institutions seek to influence those laws and in what directions. That the army of AI lobbyists includes companies large and small as well as far-flung institutions, with varying degrees of direct involvement in building or using AI, reflects both the technology’s power and the importance of this moment in charting its path forward.We’re thinking:We favor thoughtful regulation of AI applications that reinforces their tremendous potential to do good and limits potential harms that may result from flaws like bias or privacy violations. However, it’s critical to regulate applications, which put technology to specific uses, not the underlying technology, whose valuable uses are wide-ranging and subject to human creativity. It’s also critical to encourage, and not stifle, open models that multiply the potential good that AI can do. We hope the AI community can come together on these issues.\n\nResearchers introduced a model that handles an unprecedented number of input and output types, including many related to performing computer vision tasks.\n\nWhat’s new:Roman Bachmann, Oguzhan Fatih Kar, David Mizrahi and colleagues at EPFL and Apple built4M-21, a system that works with 21 input and output types. These include modalities related to images, geometry, and text along with metadata and embeddings produced by other models.\n\nKey insight:The authors followed and extended their insight from the earlier4M, which handles seven input and output types, as well as work such asUnified-IO 2, which handles 11. The key to training a model to handle multiple types of data input is to ensure that the training data takes the same format with the same-sized embedding across all input types. Using the transformer architecture, tokens suffice.\n\nHow it works:4M-21 comprises a large transformer and several encoder-decoders that convert different data types into tokens and back. The authors repeated their training strategy for 4M, but they increased the transformer’s size from 303 million parameters to 3 billion parameters, boosted the training dataset size from 400 million examples to 500 million examples, and incorporated new input types.\n\nResults:4M-21 demonstrated strong zero-shot performance in a variety of vision tasks. For instance, in estimating surface normals for each point in an image, 4M-21 achieved a 20.8 L1 score (average absolute difference between predicted and true values, lower is better), while the multimodal modelUnifiedIO 2-XLachieved a 34.8 L1. In estimating an image’s depth map, 4M-21 achieved 0.68 L1, while UnifiedIO 2-XL achieved 0.86 L1. In semantic segmentation, 4M-21 reached 48.1 percent mean intersection over union (overlap between predicted and ground-truth segments divided by their union, higher is better), while UnifiedIO 2-XL achieved 39.7 percent mean intersection over union.\n\nWhy it matters:Since 4M-21 learned to predict tokens of several modalities using tokens from other modalities, it isn’t limited to a single modalities as input. The authors demonstrate that it can generate new images conditioned by the combination of a caption and 3D human poses, edges, or metadata.\n\nWe’re thinking:The authors say 4M-21 can take as input any combination of the modalities it’s trained to handle and output any of them. The limits of this capability aren’t clear, but it opens the door to fine control over the model’s output. The authors explain how they extracted the various modalities; presumably users can do the same to prompt the model for the output they desire. For instance, a user could request an image by entering not only a prompt but also a color palette, edges, depth map extracted from another image, and receive output that integrates those elements.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--1--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed.png",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--2-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--2--2.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--3-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-279/",
    "title": "issue 279",
    "date": "",
    "reading_time": "",
    "content": "AI Product Management is evolving rapidly. The growth of generative AI and AI-based developer tools has created numerous opportunities to build AI applications. This is making it possible to build new kinds of things, which in turn is driving shifts in best practices in product management — the discipline of defining what to build to serve users — because what is possible to build has shifted. In this letter, I’ll share some best practices I have noticed.\n\nUse concrete examples to specify AI products.Starting with aconcrete ideahelps teams gain speed. If a product manager (PM) proposes to build “a chatbot to answer banking inquiries that relate to user accounts,” this is a vague specification that leaves much to the imagination. For instance, should the chatbot answer questions only about account balances or also about interest rates, processes for initiating a wire transfer, and so on? But if the PM writes out a number (say, between 10 and 50) of concrete examples of conversations they’d like a chatbot to execute, the scope of their proposal becomes much clearer. Just as a machine learning algorithm needs training examples to learn from, an AI product development team needs concrete examples of what we want an AI system to do. In other words, the data is your PRD (product requirements document)!\n\nIn a similar vein, if someone requests “a vision system to detect pedestrians outside our store,” it’s hard for a developer to understand the boundary conditions. Is the system expected to work at night? What is the range of permissible camera angles? Is it expected to detect pedestrians who appear in the image even though they’re 100m away? But if the PM collects a handful of pictures and annotates them with the desired output, the meaning of “detect pedestrians” becomes concrete. An engineer can assess if the specification is technically feasible and if so, build toward it. Initially, the data might be obtained via a one-off, scrappy process, such as the PM walking around taking pictures and annotating them. Eventually, the data mix will shift to real-word data collected by a system running in production.\n\nUsing examples (such as inputs and desired outputs) to specify a product has been helpful for many years, but the explosion of possible AI applications is creating a need for more product managers to learn this practice.\n\nAssess technical feasibility of LLM-based applications by prompting.When a PM scopes out a potential AI application, whether the application can actually be built — that is, its technical feasibility — is a key criterion in deciding what to do next. For many ideas for LLM-based applications, it’s increasingly possible for a PM, who might not be a software engineer, to try prompting — or write just small amounts of code — to get an initial sense of feasibility.\n\nFor example, a PM may envision a new internal tool for routing emails from customers to the right department (such as customer service, sales, etc.). They can prompt an LLM to see if they can get it to select the right department based on an input email, and see if they can achieve high accuracy. If so, this gives engineering a great starting point from which to implement the tool. If not, the PM can falsify the idea themselves and perhaps improve the product idea much faster than if they had to rely on an engineer to build a prototype.\n\nOften, testing feasibility requires a little more than prompting. For example, perhaps the LLM-based email system needs basic RAG capability to help it make decisions. Fortunately, the barrier to writing small amounts of code is now quite low, since AI can help by acting as a coding companion, as I describe in the course, “AI Python for Beginners.” This means that PMs can do much more technical feasibility testing, at least at a basic level, than was possible before.\n\nPrototype and test without engineers.User feedback to initial prototypes is also instrumental to shaping products. Fortunately, barriers to building prototypes rapidly are falling, and PMs themselves can move prototypes forward without needing software developers.\n\nIn addition to using LLMs to help write code for prototyping, tools like Replit, Vercel’s V0, Bolt, and Anthropic’s Artifacts (I’m a fan of all of these!) are making it easier for people without a coding background to build and experiment with simple prototypes. These tools are increasingly accessible to non-technical users, though I find that those who understand basic coding are able to use them much more effectively, so it’s still important to learn basic coding. (Interestingly, highly technical, experienced developers use them too!) Many members of my teams routinely use such tools to prototype, get user feedback, and iterate quickly.\n\nAI is enabling a lot of new applications to be built, creating massive growth in demand for AI product managers who know how to scope out and help drive progress in building these products. AI product management existed before the rise of generative AI, but the increasing ease of building applications is creating greater demand for AI applications, and thus a lot of PMs are learning AI and these emerging best practices for building AI products. I find this discipline fascinating, and will keep on sharing best practices as they grow and evolve.\n\nKeep learning!\n\nAndrew\n\nWrite and code more effectively with OpenAI Canvas, a user-friendly workspace for collaborating with AI. In this free course, explore use cases like building game apps and designing SQL databases from screenshots, and gain insights into how GPT-4o powers Canvas’ features.Join for free\n\nAmazon introduced a range of models that confront competitors head-on.\n\nWhat’s new:TheNovaline from Amazon includes three vision-language models (Nova Premier, Nova Pro, and Nova Lite), one language model (Nova Micro), an image generator (Nova Canvas), and a video generator (Nova Reel). All but Nova Premier areavailableon Amazon’s Bedrock platform, and Nova Premier, which is the most capable, is expected in early 2025. In addition, Amazon plans to release a speech-to-speech model in early 2025 and a multimodal model that processes text, images, video, and audio by mid-year. (Disclosure: Andrew Ng serves on Amazon’s board of directors.)\n\nHow it works:Nova models deliver competitiveperformanceat relatively low prices. Amazon hasn’t disclosed parameter counts or details about how the models were built except to say that Nova Pro, Lite, and Micro were trained on a combination of proprietary, licensed, public, and open-source text, images, and video in over 200 languages.\n\nBehind the news:The company launched Bedrock in April 2023 with Stability AI’s Stable Diffusion for image generation, Anthropic’s Claude and AI21’s Jurassic-2 for text generation, and its own Titan models for text generation and embeddings. Not long afterward, it added language models from Cohere as well as services for agentic applications and medical applications. It plans to continue to provide models from other companies (including Anthropic), offering a range of choices.\n\nWhy it matters:While other AI giants raced to outdo one another in models for text and multimodal processing, Amazon was relatively quiet. With Nova, it has staked out a strong position in those areas, as well as the startup-dominated domains of image and video generation. Moreover, it’s strengthening its cloud AI offerings with competitive performance, pricing, and speed. Nova’s pricing continues the rapiddrop in AI pricesover the last year. Falling per-token prices help make AI agents or applications that process large inputs more practical. For example, Simon Willison, developer of the Django Python framework for web applications,foundthat Nova Lite generated descriptions for his photo library (tens of thousands of images) for less than $10.\n\nWe’re thinking:The Nova suite is available via APIs as well as two web playgrounds (one in the Bedrock console, the other a new interface for building AI apps calledPartyRock). This accords with Amazon Web Services’ focus on developers. For consumers, Amazon offers the earlierRufusshopping bot; for enterprises, theQassistant.\n\nOpenAI launched not only its highly anticipated o1 model but also an operating mode that enables the model to deliver higher performance — at a hefty price.\n\nWhat’s new:Kicking off a 12-dayholiday blitz, OpenAI launched o1 (previously available in preview and mini versions) andintroducedo1 pro mode, which processes more tokens at inference to produce more accurate output. Both options accept text and image inputs to generate text outputs. They’re available exclusively through a new ChatGPT Pro subscription for $200 monthly. API access is not yet available.\n\nHow it works:According to an updatedsystem card, o1 models were trained on a mix of public, licensed, and proprietary text, code, and images, with a focus on technical, academic, and structured datasets. They respond to prompts by breaking them down into intermediate steps, each of which consumes a number of hidden “reasoning tokens.” The models don’t reveal these steps, but ChatGPT presents a natural-language summary of the reasoning process. The new o1 and o1 pro mode perform better than o1-preview and o1-mini, but their additional reasoning requires more processing, which translates into higher costs and slower responses.\n\nBehind the news:Since September, when OpenAI introduced o1-preview and o1-mini, other model providers have implemented similar reasoning capabilities.DeepSeek’s R1displays reasoning steps that o1 models keep hidden. Alibaba’sQwQ 32Bexcels at visual reasoning but is slower and has a smaller context window. Amazon’sNova Premier, which is billed as a model for “complex reasoning tasks,” is expected in early 2025, but Amazon has not yet described its performance, architecture, or other details.\n\nWhy it matters:o1 and o1 pro mode highlight a dramatic shift in model development and pricing. Giving models more processing power at inference enables them to provide more accurate output, and it’s a key part of agentic workflows. It also continues to boost performance even as scaling laws that predict better performance with more training data and compute may be reaching theirlimits. However, it also raises OpenAI’s costs, and at $200 a month, the price of access to o1 and o1 pro is steep. It’s a premium choice for developers who require exceptional accuracy or extensive reasoning.\n\nWe’re thinking:Discovering scaling laws for using more processing at inference, ortest-time compute, is an unsolved problem. Although OpenAI hasn’t disclosed the algorithm behind o1 pro mode, recentworkat Google allocated tokens dynamically at inference based on a prompt’s difficulty. This approach boosted the compute efficiency by four times and enabled a model that had shown “nontrivial success rates” to outperform one that was 14 times larger.\n\nA new model improves on recent progress in generating interactive virtual worlds from still images.\n\nWhat’s new:Jack Parker-Holder and colleagues from Google introducedGenie 2, which generates three-dimensional video game worlds that respond to keyboard inputs in real time. The model’s output remains consistent (that is, elements don’t morph or disappear) for up to a minute, and it includes first-person shooters, walking simulators, and driving games from viewpoints that include first person, third person, and isometric. Genie 2 follows up onGenie, which generates two-dimensional games.\n\nHow it works:Genie 2 is a latent diffusion model that generates video frames made up of an encoder, transformer, and decoder. The developers didn’t reveal how they built it or how they improved on earlier efforts.\n\nBehind the news:Genie 2 arrives on the heels ofOasis, which generates a Minecraft-like game in real time. Unlike Oasis, Genie 2 worlds are more consistent and not limited to one type of game. It also comes at the same time as another videogame generator,World Labs. However, where Genie 2 generates the next frame given previous frames and keyboard input (acting, in terms of game development, as both graphics and physics engines), World Labs generates a 3D mesh of a game world from a single 2D image. This leaves the implementation of physics, graphics rendering, the player’s character, and other game mechanics to external software.\n\nWhy it matters:Genie 2 extends models that visualize 3D scenes based on 2D images to encompass interactive worlds, a capability that could prove valuable in design, gaming, virtual reality, and other 3D applications. It generates imagery that, the authors suggest, could serve as training data for agents to learn how to navigate and respond to commands in 3D environments.\n\nWe’re thinking:Generating gameplay directly in the manner of Genie 2 is a quick approach to developing a game, but the current technology comes with caveats. Developers can’t yet control a game’s physics or mechanics and they must manage any flaws in the model (such as a tendency to generate inconsistent worlds). In contrast, generating a 3D mesh, as World Labs does, is a more cumbersome approach, but it gives developers more control.\n\nLarge language models that remember more hallucinate less.\n\nWhat’s new:Johnny Li and colleagues at Lamini introducedMixture of Memory Experts (MoME), a method that enables large language models (LLMs) to memorize many facts with relatively modest computational requirements. (Disclosure: Andrew Ng invested in Lamini.)\n\nKey insight:The key to getting factual answers from LLMs is to keep training it until it chooses the correct answer every time. In technical terms, train past the point where tokens relevant to the answer have a similar probability distribution, and continue until a single token has 100 percent probability. But this amount of training takes a lot of computation and, since the model may overfit the training set, it also may degrade performance on the test set. Fine-tuning is one solution, and fine-tuning a LoRA adapter to memorize facts reduces the computational burden. But a single LoRA adapter isn’t enough to store all of the knowledge in a large dataset. Training multiple adapters that are selected by cross-attention enables the LLM to memorize a variety of facts.\n\nHow it works:The authors extended a pretrainedLlama-3-8Bwith a large number (on the order of 1 million) of LoRA adapters and a cross-attention layer. They froze Llama-3-8B and trained the LoRA adapters to predict the next token in a custom dataset of over 1 million questions and answers.\n\nResults:The authorstestedtheir LoRA-enhanced model’s ability to answer questions about a database via SQL queries. The model, which was outfitted for retrieval-augmented generation (RAG), achieved 94.7 percent accuracy. An unnamed model with RAG achieved 50 percent accuracy.\n\nYes, but:It stands to reason that the authors’ approach saves processing, but it’s unclear how much. The authors didn’t mention the cost of fine-tuning Llama-3-8B in the usual way on their training dataset for the same number of epochs.\n\nWhy it matters:The authors argue that eliminating hallucinations is possible in typical training, it’s just computationally very expensive (not to mention the risk of overfitting). An architecture designed to store and retrieve facts, via LoRA adapters in this case, makes the process more feasible.\n\nWe’re thinking:While some researchers want large language models to memorize facts, others want them toavoid memorizing their training data. These aims address very different problems. Preventing LLMs from memorizing training data would make them less likely to regurgitate it verbatim and thus violate copyrights. On the other hand, this work memorizes facts so the model can deliver consistent, truthful responses that might be stated in a variety of ways.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--38-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/12/The-Batch-ads-and-exclusive-banners---2024-11-27T123750.031--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--36-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--30-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--37-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--38-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-138/",
    "title": "issue 138",
    "date": "",
    "reading_time": "",
    "content": "AI Fund, the venture studio and investment firm that I lead, recently held a summit where CEOs and founders of portfolio companies shared ideas on topics from fundraising to building team culture. I was struck by how frequently startup leaders have to do things they have no expertise in.As AI developers, every time we build a machine learning application, we might choose a neural network architecture, tune a dataset, train a model, evaluate its performance, and consider the outcome to decide what to try next. The rapid iteration cycle means we can try many combinations in a given project. Over many projects, we hone our intuitions about what works. The quick feedback and opportunity to improve are among the things that makes machine learning fun!In contrast, hardly anyone starts 100 companies even in a long career. No one raises seed funding, builds a company culture, hires a vice president of sales, or makes an initial public offering very many times. Thus few people can become experts at performing these tasks through repeated practice.\n\nThat’s why I believe that the smartest startup leaders know when they need help and understand that no single person can do it all. A community of peers, each of whom has raised funding once or twice, can pool ideas and achieve better results than the typical individual. Similarly a recruiter who has hired 100 sales executives is likely to have valuable insights that someone who has done it only once or twice won’t.Although software development allows for repeated practice, we, too, often have to do things we don’t have much experience with, because technology keeps evolving. Someone may find themselves, for the first time, deploying a real-time machine learning system, compressing a neural network to run on a low-power edge device, or calculating the return on investment in an AI project. In situations like this we, too, are stronger as a community. We can benefit from the experience of our peers who have completed the task and know something about how to go about it.When I was younger I believed that, if only I worked and studied a bit harder, I could figure almost anything out. That attitude worked well enough for a while, but the more experience I gain, the more I realize that I need help from others. I’m grateful to the many people who have given me advice over the years, and I hope that the AI community can be a place where all of us can collaborate and support one another.\n\nKeep learning!\n\nAndrew\n\nIt’s surprisingly easy to turn a well-intended machine learning model to the dark side.\n\nWhat’s new:In an experiment, Fabio Urbina and colleagues at Collaborations Pharmaceuticals, who had built a drug-discovery model to design useful compounds and avoid toxic ones, retrained it togenerate poisons. In six hours, the model generated 40,000 toxins, some of them actual chemical warfare agents that weren’t in the initial dataset.How it works:The authors didn’t detail the architecture, dataset, and method to avoid encouraging bad actors. The following description is drawn from the few particulars they did reveal along with accounts of the company’s existing generative model,MegaSyn.\n\nWhy it matters:The authors took an industrial model and turned it into what they call “a computational proof of concept for making biochemical weapons.” They emphasize that it wouldn’t be difficult to copy using publicly available datasets and models. It may be similarly easy to subvert models built for tasks other than drug discovery, turning helpful models into harmful ones.We’re thinking:Despite machine learning’s enormous potential to do good, it can be harnessed for evil. Designing effective safeguards for machine learning research and implementation is a very difficult problem. What is clear is that we in the AI community need to recognize the destructive potential of our work and move with haste and deliberation toward a framework that can minimize it. NeurIPS’effortsto promote introspection on the part of AI researchers are a notable start — despiteargumentsthat they politicize basic research — and much work remains to be done.\n\nIs your colossal text generator bogged down in training? Nvidia announced a chip designed to accelerate the transformer architecture, the basis of large language models such as GPT-3.What’s new:TheH100graphics processing unit (GPU) can train transformer models many times faster than Nvidia’s previous flagship A100 (or, presumably, any other chip on the market).How it works:Transformer networks have ballooned in size from GPT-3’s 175 billion parameters to Wu Dao’s 1.75 trillion, requiring more computation for training and inference. The H100’s underlying chip design, known as Hopper, includes a so-called Transformer Engine designed to make such models run more efficiently.\n\nTime savings:In tests, a 395 billion-parametermixture-of-expertsmodel took 20 hours to train running on 8,000 H100s, while it took seven days running on the same number of A100s. A chatbot based on Nvidia’sMegatrongenerated output up to 30 times faster running on H100s than A100s. Nvidia plans to link 4,608 H100 chips into a trainingsupercomputerthat the company touts as the world’s fastest system for training AI.Behind the news:While Nvidia is the undisputed leader in specialized AI chips, several competitors are vying for the same market.\n\nWhy it matters:The transformer has driven a tidal wave of progress in AI for language as well as an expandingarrayof domains including vision, image generation, and biomedicine. The ability to train such models faster greases the wheels for this versatile architecture.We’re thinking:Conventional chips lately havestruggledto keep pace with Moore’s Law, which predicts a doubling of processing power every 18 months. AI chips areoutpacingit by a wide margin. Yet another reason to dig into AI!\n\nWant to design applications that can chat, answer questions, evaluate sentiments, translate languages, and summarize text? Learn how with theNatural Language Processing Specialization!Enroll today\n\nA group of media and technology experts is working to give AI a better understanding of indigenous peoples.What’s new:Intelligent Voices of Wisdom, or IVOW, is a consultancy that aims to reduce machine learning bias against cultures that are underrepresented in training data by producing knowledge graphs and other resources,The New York Timesreported.How it works:IVOW has held data-labeling workshops and created a graph of native culinary techniques.\n\nBehind the news:A number of efforts around the globe are building data and tools for underrepresented languages and, by extension, the people who speak them.\n\nWhy it matters:Some of the most blatant biases embedded in training datasets, particularly those scraped from the web, are well known. Less well understood are biases that arise because some groups are culturally dominant while others are relatively obscure. If AI is to work well for all people, it must be trained on data that reflects the broad range of human experience.We’re thinking:People have a hard time fully comprehending and respecting cultures that are unfamiliar to them. Perhaps AI trained on datasets that have been curated for their relevance to a wide variety of cultures will help us come closer to this ideal.\n\nEvery gene in the human genome exists in a variety of mutations, and some encode protein variants that cause cells to malfunction, resulting in illness. Yet which mutations are associated with disease is largely unknown. Can deep learning identify them?What’s new:Jonathan Frazer, Pascal Notin, Mafalda Dias, and colleagues at Harvard Medical School and University of Oxford introducedEvolutionary Model of Variant Effect(EVE), a neural network that learned to classify disease-causing protein variants — and thus dangerous mutations — without labeled data.Key insight:Mutations that encode disease-causing proteins tend to be rare because individuals who carry them are less likely to survive to reproductive age. Thus the prevalence of a given mutation indicates its potential role in illness. Among a collection of variants on a particular protein — a protein family — each variant is produced by a distinct mutation of a particular gene. Clustering uncommon and common variants within the family can sort the mutations likely to be associated with disease.How it works:Avariational autoencoder(VAE) learns to reproduce an input sequence by maximizing the likelihood that output tokens match the corresponding input tokens. In this case, the sequence is a chain of amino acids that make up a protein in adatabaseof 250 million proteins. The authors trained a separate VAE for each protein family. Given one variant in a protein family, it learned to compute the likelihood of each amino acid in the sequence. This enabled the authors to derive the likelihood of the entire sequence.\n\nResults:The authors compared EVE’s classifications to those of 23 supervised and unsupervised models built to perform the same task. They checked the models’ classifications for3,219 genesfor which labels are known. EVE achieved 0.92 AUC, or average area under the curve, while other methods achieved between 0.7 AUC and 0.9 AUC (higher is better). The authors also compared EVE’s output with lab tests that measure, for example, how cells that contain mutations respond to certain chemicals. EVE scored as well as or better than those tests on the five gene families in which labels are known with highest confidence. For example, for the gene known as TP53, EVE achieved 0.99 AUC while the lab test achieved 0.95 AUC.Why it matters:Unsupervised clustering can substitute for labels when we have a belief about what caused certain clusters to emerge; for instance, that natural selection reduces the likelihood of disease-causing protein variants. This approach may open doors to analyze other large datasets in which labels are unavailable.We're thinking:Clustering unlabeled data and examining the clusters for insights is a tried-and-true technique. By employing VAEs to assess likelihoods, this work extends basic clustering to a wider array of problems.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/03/Andres-letter.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/03/CHEMWEAPONS.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/03/NVIDIA.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/03/The-Batch-Image--3-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/03/IVOW.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20(19)-1.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20(19)-1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-164/",
    "title": "issue 164",
    "date": "",
    "reading_time": "",
    "content": "In this letter, I’d like to address the serious matter of newcomers to AI sometimes experiencing imposter syndrome, where someone — regardless of their success in the field — wonders if they’re a fraud and really belong in the AI community. I want to make sure this doesn’t discourage you or anyone else.\n\nLet me be clear: If you want to be part of the AI community, then I welcome you with open arms. If you want to join us, you fully belong with us!An estimated70 percentof people experience some form of imposter syndrome at some point. Many talented people have spoken publicly about this experience, including former Facebook COO Sheryl Sandberg, U.S. first lady Michelle Obama, actor Tom Hanks, and Atlassian co-CEO Mike Cannon-Brookes. It happens in our community even among accomplished people. If you’ve never experienced this yourself, that’s great! I hope you’ll join me in encouraging and welcoming everyone who wants to join our community.AI is technically complex, and it has its fair share of smart and highly capable people. But, of course, it is easy to forget that to become good at anything, the first step is to suck at it. If you’ve succeeded at sucking at AI -- congratulations, you’re on your way!\n\nI once struggled to understand the math behind linear regression. I was mystified when logistic regression performed strangely on my data, and it took me days to find a bug in my implementation of a basic neural network. Today, I still find many research papers challenging to read, and just yesterday I made an obvious mistake while tuning a neural network hyperparameter (that fortunately a fellow engineer caught and fixed).\n\nSo if you, too, find parts of AI challenging, it’s okay. We’ve all been there. I guarantee that everyone who has published a seminal AI paper struggled with similar technical challenges at some point.Here are some things that can help.\n\nMy three-year-old daughter (who can barely count to 12) regularly tries to teach things to my one-year-old son. No matter how far along you are — if you’re at least as knowledgeable as a three-year-old — you can encourage and lift up others behind you. Doing so will help you, too, as others behind you will recognize your expertise and also encourage you to keep developing. When you invite others to join the AI community, which I hope you will do, it also reduces any doubts that you are already one of us.\n\nAI is such an important part of our world that I would like everyone who wants to be part of it to feel at home as a member of our community. Let’s work together to make it happen.\n\nYour supporter and ally,\n\nAndrew\n\nWhen Jagriti Agrawal started her career, she felt hopelessly behind her peers. She caught up with help from friends and teachers. The experience led to work at NASA and co-founding her own education startup, as she explains in a new edition of ourBreaking Into AIseries.Read her story\n\nNvidia, known for chips designed to process AI systems, is providing access to large language models.\n\nWhat’s new:Nvidiaannouncedearly access to NeMo LLM and BioNeMo, cloud-computing services that enable developers to generate text and biological sequences respectively, including methods that tune inputs — rather than the models themselves — to enable models trained on web data to work well with a particular user’s data and task without fine-tuning. Users can deploy a variety of models in the cloud, on-premises, or via an API.How it works:The new services are based on Nvidia’s pre-existing NeMo toolkit for speech recognition, text-to-speech, and natural language processing.\n\nBehind the news:Nvidia’s focus on prompt learning and biological applications differentiate it from other companies that provide large language models as a service.\n\nWhy it matters:Until recently, large language models were the province of organizations with the vast computational resources required to train and deploy them. Cloud services make these models available to a wide range of startups and researchers, dramatically increasing their potential to drive new developments and discoveries.We’re thinking:These services will take advantage of Nvidia’sH100GPUs, developed specifically to process transformer models. Nvidia CEO Jensen Huang recently said the public no longer should expect chip prices to fall over time. If that’s true, AI as a service could become the only option for many individuals and organizations that aim to use cutting-edge AI.\n\nA new method enables robots to respond helpfully to verbal commands by pairing a natural language model with a repertoire of existing skills.\n\nWhat’s new:SayCan, a systemdevelopedby researchers at Google and its spinoff Everyday Robots, enabled a robot equipped with an arm, camera, and gripper to take a high-level command such as “I spilled my drink, can you help?” and choose low-level actions appropriate to a given environment such as “find a sponge” and “go to table.”\n\nKey insight:A pretrained large language model can grasp verbal instructions well enough to propose a general response. But it can’t adapt that response to local conditions; for instance, an environment that includes a sponge but not a mop. Combining a large language model with a model that determines which actions are possible in the current environment makes for a system that can interpret instructions and respond according to the local context.\n\nHow it works:SayCan drew from over 550 kitchen-related actions that the authors had trained it to perform using a combination ofimage-based behavioral cloningand reinforcement learning. Actions included picking up, putting down, and rearranging objects; opening and closing drawers; and navigating to various locations.\n\nResults:The authors tested the system by giving the robot 101 commands in a mock kitchen that contained 15 objects such as fruits, drinks, snacks, and a sponge. Human judges determined that the robot planned valid actions 84 percent of the time and carried them out 74 percent of the time. In a real-life kitchen, the robot achieved 81 percent success in planning and 61 percent success in execution.\n\nWhy it matters:The dream of a domestic robot has held the public imagination since the dawn of the industrial revolution. But robots favor controlled environments, while households are highly varied and variable. The team took on the challenge by devising a way to choose among 551 skills and 17 objects. These are large numbers, but they may not encompass mundane requests like “find granny’s glasses” and “discard the expired food in the fridge.”\n\nWe’re thinking:This system requires a well-staged environment with a small number of items. We imagine that it could execute the command, “get the chips from the drawer” if the drawer contained only a single bag of chips. But we wonder whether it would do well if the drawer were full and messy. Its success rate in completing tasks suggests that, as interesting as this approach is, we’re still a long way from building a viable robot household assistant.\n\nRobert Wydler was always drawn to AI. After 35 years in IT, he finally decided to pursue his passion by taking Andrew Ng’s Machine Learning course. Ready for a change?Enroll in the Machine Learning Specialization!\n\nA state in Australia plans to outfit prisons with face recognition.\n\nWhat’s new:Corrective Services NSW, the government agency that operates nearly every prison in New South Wales, contracted the U.S.-based IT firm Unisys to replace a previous system, which required a fingerprint scan to identify people, with one that requires only that subjects pass before a camera,InnovationAus.comreported.How it works:The new system will use face recognition to identify inmates and visitors as they enter or exit correctional facilities.\n\nYes, but:Samantha Floreani of Digital Rights Watchraised concernsthat face recognition may exacerbate biases in the Australian corrective system, which incarcerates indigenous people disproportionately. Additionally, Floreani said that contracting to Unisys, a U.S.-based firm, raises questions about whether personal data on Australians will be transferred to another country and whether the data will be secure and handled properly. The Australian public, too, is wary. A 2021pollfound that 55 percent of Australians supported a moratorium on face recognition until stronger safeguards are in place.Behind the news:England and Walestestedface recognition for screening prison visitors in 2019, mostly in an effort to crack down on smuggling of drugs into prisons. In the United States, the federal Justice Department has funded severalinitiativesto apply face recognition. The U.S. Marshals Service, which handles fugitive investigations, isdevelopinga face recognition system to aid in transporting prisoners.\n\nWhy it matters:The flow of visitors, contractors, and prisoners into and out of correctional facilities creates opportunities for security breaches. Face recognition promises to help manage this traffic more safely. However, the technology, which is relatively new, largely unregulated, and developing rapidly, brings with it potential for abuse, mission creep, and other adverse consequences, especially in a high-stakes field like criminal justice.We’re thinking:Surveillance has always been an inextricable part of incarceration, but it shouldn’t encroach on the rights of prisoners or the people who guard, visit, and provide services to them. More optimistically, if technology can generate indelible, auditable records of the activities of both guards and prisoners, it can help protect against abuses and address them when they occur.\n\nVision Transformers (ViTs) are overtaking convolutional neural networks (CNN) in many vision tasks, but procedures for training them are still tailored for CNNs. New research investigated how various training ingredients affect ViT performance.\n\nWhat's new:Hugo Touvron and colleagues at Meta and Sorbonne University formulated a new recipe for training ViTs. They call their third-generation approachData Efficient Image Transformers(DeiT III).\n\nKey insight:The CNN and transformer architectures differ. For instance, when processing an image, a CNN works on one group of pixels at a time, while a transformer processes all pixels simultaneously. Moreover, while the computational cost of a CNN scales proportionally to input size, a transformer’s self-attention mechanism requires dramatically more processing as input size increases. Training recipes that take these differences — and other, less obvious ones — into account should impart better performance.\n\nHow it works:The authors pretrainedViTsto classify images in ImageNet using various combinations of training data, data augmentation, and regularization. (They also experimented with variables such as weight decay, dropout, and type of optimizer, for which they didn’t describe results in detail.) They fine-tuned and tested on ImageNet.\n\nResults:The authors’ approach substantially improved ViT performance. An 86 million-parameter ViT-B pretrained on ImageNet-21K and fine-tuned on ImageNet using the full recipe achieved 85.7 percent accuracy. Their cropping technique alone yielded 84.8 percent accuracy. In contrast, the same architecture trained on the same datasets using full-resolution examples augmented viaRandAugmentachieved 84.6 percent accuracy.\n\nWhy it matters:Deep learning is evolving at a breakneck pace, and familiar hyperparameter choices may no longer be the most productive. This work is an early step toward updating for the transformer era recipes that were developed when CNNs ruled computer vision.\n\nWe're thinking:The transformer architecture’s hunger for data makes it especially important to reconsider habits around data-related training procedures like augmentation and regularization.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/Screen-Shot-2022-09-28-at-11.55.14-AM-copy-1.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/jagriti-agrawal--1-.jpeg",
      "https://dl-staging-website.ghost.io/content/images/2022/09/LCLOUD_Slides_Revise_092822.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/PALMROBOT-2_600px.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/MLS_Learner_1200x628_A-1_Artboard-1-copy-9--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/PRISON--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/DEITv2-compressed--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-287/",
    "title": "issue 287",
    "date": "",
    "reading_time": "",
    "content": "A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer. But we don’t seem to have 10x marketers, 10x recruiters, or 10x financial analysts. As more jobs become AI enabled, I think this will change, and there will be a lot more “10x professionals.”\n\nThere aren’t already more 10x professionals because, in many roles, the gap between the best and the average worker has a ceiling. No matter how athletic a supermarket checkout clerk is, they’re not likely to scan groceries so fast that customers get out of the store 10x faster. Similarly, even the best doctor is unlikely to make patients heal 10x faster than an average one (but to a sick patient, even a small difference is worth a lot). In many jobs, the laws of physics place a limit on what any human or AI can do (unless we completely reimagine that job).\n\nBut for many jobs that primarily involve applying knowledge or processing information, AI will be transformative. In a few roles, I’m starting to see tech-savvy individuals coordinate a suite of technology tools to do things differently and start to have, if not yet 10x impact, then easily 2x impact. I expect this gap to grow.\n\n10x engineers don’t write code 10 times faster. Instead, they make technical architecture decisions that result in dramatically better downstream impact, they spot problems and prioritize tasks more effectively, and instead of rewriting 10,000 lines of code (or labeling 10,000 training examples) they might figure out how to write just 100 lines (or collect 100 examples) to get the job done.\n\nI think 10x marketers, recruiters, and analysts will, similarly, do things differently. For example, perhaps traditional marketers repeatedly write social media posts. 10x marketers might use AI to help write, but the transformation will go deeper than that. If they are deeply sophisticated in how to apply AI — ideally able to write code themselves to test ideas, automate tasks, or analyze data — they might end up running a lot more experiments, get better insights about what customers want, and generate much more precise or personalized messages than a traditional marketer, and thereby end up making 10x impact.\n\nSimilarly, 10x recruiters won’t just use generative AI to help write emails to candidates or summarize interviews. (This level of use of prompting-based AI will soon become table stakes for many knowledge roles.) They might coordinate a suite of AI tools to efficiently identify and carry out research on a large set of candidates, enabling them to have dramatically greater impact than the average recruiter. And 10x analysts won’t just use generative AI to edit their reports. They might write code to orchestrate a suite of AI agents to do deep research into the products, markets, and companies, and thereby derive far more valuable conclusions than someone who does research the traditional way.\n\nA 2023 Harvard/BCGstudyestimated that, provided with GPT-4, consultants could complete 12% more tasks, and completed tasks 25% more quickly. This was just the average, using 2023 technology. The maximum advantage to be gained by using AI in a sophisticated way will be much bigger, and will only grow as technology improves.\n\nHere in Silicon Valley, I see more and more AI-native teams reinvent workflows and do things very differently. In software engineering, we've venerated the best engineers because they can have a really massive impact. This has motivated many generations of engineers to keep learning and working hard, because doing those things increases the odds of doing high-impact work. As AI becomes more helpful in many more job roles, I believe we will open up similar paths to a lot more people becoming a “10x professional.”\n\nKeep learning!\n\nAndrew\n\nLearn in detail how transformer-based large language models work in this new course by the authors ofHands-On Large Language Models. Explore the architecture introduced in the paper “Attention Is All You Need,” and learn through intuitive explanations and code examples.Join in for free\n\nOpenAI introduced a successor to its o1 models that’s faster, less expensive, and especially strong in coding, math, and science.\n\nWhat’s new:o3-mini is a large language model that offers selectable low, medium, and high levels of reasoning “effort.” These levels consume progressively higher numbers of reasoning tokens (specific numbers and methods are undisclosed), and thus greater time and cost, to generate a chain of thought. It’savailableto subscribers to ChatGPT Plus, Team, and Pro, as well as to higher-volume users of the API (tiers 3 through 5). Registered users can try it via the free ChatGPT service by selecting “reason” in the message composer or selecting o3-mini before regenerating a response.\n\nHow it works:o3-mini’s training set emphasized structured problem-solving in science and technology fields, and fine-tuning used reinforcement learning on chain-of-thought (CoT) data. Like the o1 family, it charges for tokens that are processed during reasoning operations and hides them from the user. (Competing reasoning models DeepSeek-R1, Gemini 2.0 Flash Thinking, and QwQ-32B-Preview make these tokens available to users.) o3-mini has a maximum input of 200,000 tokens and a maximum output of 100,000 tokens. Its knowledge cutoff is October 2023.\n\nWhat they’re saying:Userspraisedo3-mini for its speed, reasoning, and coding abilities. They noted that it responds best to “chunkier” prompts with lots of context. However, due to its smaller size, it lacks extensive real-world knowledge and struggles to recall facts.\n\nBehind the news:Days after releasing o3-mini, OpenAI launcheddeep research, a ChatGPT research agent based on o3. OpenAI hadannouncedthe o3 model family in December, positioning it as an evolution of its chain-of-thought approach. The release followed hard upon that of DeepSeek-R1, an open weights model that captivated the AI community with its high performance and low training cost, but OpenAImaintainedthat the debut took place on its original schedule.\n\nWhy it matters:o3-mini continues OpenAI’s leadership in language models and further refines the reasoning capabilities introduced with the o1 family. In focusing on coding, math, and science tasks, it takes advantage of the strengths of reasoning models and raises the bar for other model builders. In practical terms, it pushes AI toward applications in which it’s a reliable professional partner rather than a smart intern.\n\nWe’re thinking:We’re glad that o3-mini is available to users of ChatGPT’s free tier as well as paid subscribers and API users. The more users become familiar with how to prompt reasoning models, the more value they’ll deliver.\n\nAs Anthropic, Google, OpenAI, and others roll out agents that are capable of computer use, new work shows how underlying models can be trained to do this.\n\nWhat’s new:Yujian Qin and colleagues at ByteDance and Tsinghua University introducedUI-TARS, a fine-tuned version of the vision-language model Qwen2-VL that uses lines of reasoning to decide which mouse clicks, keyboard presses, and other actions to take in desktop and mobile apps. The model’s weights arelicensedfreely for commercial and noncommercial uses via Apache 2.0. You can download themhere.\n\nBehind the news:Adepttoutedcomputer use in early 2022, andOmniParserAguvissoon followed with practical implementations. In October 2024, Anthropic set off the current wave of model/app interaction with itsannouncementof computer use for Claude 3.5 Sonnet. OpenAI recentlyrespondedwith Operator, its own foray into using vision and language models to control computers.\n\nResults:UI-TARS matched or outperformed Claude 3.5 Sonnet with computer use, GPT-4o with various computer use frameworks, and the Aguvis framework with its native model on 11 benchmarks. On OSWorld, which asks models to perform tasks using a variety of real-world applications and operating systems, UI-TARS successfully completed 22.7 percent of the tasks in 15 steps, whereas Claude 3.5 Sonnet with computer use completed 14.9 percent, GPT-4o with Aguvis 17 percent, and Aguvis with its native model 10.3 percent.\n\nWhy it matters:Training a model to take good actions enables it to perform well. Training it to correct its mistakes after making them enables it to recover from unexpected issues that may occur in the real world.\n\nWe’re thinking:Since computer use can be simulated in a virtual machine, it’s possible to generate massive amounts of training data automatically. This is bound to spur rapid progress in computer use by large language models.\n\nGoogle updated the December-vintage reasoning model Gemini 2.0 Flash Thinking and other Flash models, gaining ground on OpenAI o1 and DeepSeek-R1.\n\nWhat’s new:Gemini 2.0 Flash Thinking Experimental 1-21is a vision-language model (images and text in, text out) that’s trained to generate a structured reasoning process or chain of thought. The new version improves on its predecessor’s reasoning capability and extends its context window. It's free to access viaAPIwhile it remains designated “experimental” andavailableto paid users of the Gemini app, along withGemini 2.0 Flash(fresh out of experimental mode) and the newly releasedGemini 2.0 Pro Experimental. The company also launched a preview ofGemini 2.0 Flash Lite, a vision-language model (images and text in, text out) that outperforms Gemini 1.5 Flash at the same price.\n\nHow it works:Gemini 2.0 Flash Thinking Experimental 1-21 is based onGemini 2.0 Flash Experimental(parameter count undisclosed). It processes up to 1 million tokens of input context, compared to its predecessor’s 32,000 and o1’s 128,000.\n\nSpeed bumps:Large language models that are trained to generate achain of thought(CoT) are boosting accuracy even as the additional processing increases inference costs and latency. Reliable measures of Gemini 2.0 Flash Thinking Experimental 1-21’s speed are not yet available, but its base model runs faster (168.8 tokens per second with 0.46 seconds of latency to the first token,according toArtificial Analysis) than all models in its class except o1-mini (which outputs 200 tokens per second with 10.59 seconds of latency to the first token).\n\nWhy it matters:The combination of CoT reasoning and long context — assuming the new model can take advantage of its 1 million-token context window, as measured by a benchmark such asRULER— could open up valuable applications. Imagine a reasoning model that can take an entire codebase as input and analyze it without breaking it into smaller chunks.We’re thinking:Regardless of benchmark performance, this model topped theChatbot Arenaleaderboard at the time of writing. This suggests that users preferred it over o1 and DeepSeek-R1 — at least for common, everyday prompts.\n\nEven cutting-edge, end-to-end, speech-to-speech systems like ChatGPT’s Advanced Voice Mode tend to get interrupted by interjections like “I see” and “uh-huh” that keep human conversations going. Researchers built an open alternative that’s designed to go with the flow of overlapping speech.\n\nWhat’s new:Alexandre Défossez, Laurent Mazaré, and colleagues at Kyutai, a nonprofit research lab in Paris, releasedMoshi, an end-to-end, speech-to-speech system that’s always listening and always responding. Theweightsandcodeare free for noncommercial and commercial uses underCC-BY 4.0,Apache 2.0, andMITlicenses. You can try a web demohere.\n\nKey insight:Up to 20 percent of spoken conversation consists ofoverlapping speech, including interjections like “okay” and “I see.”\n\nHow it works:The authors combined an encoder-decoder called Mimi and anRQ-Transformer, which is made up of the Helium transformer-based large language model (LLM) plus another transformer.\n\nResults:In tests, Moshi proved fast and relatively accurate.\n\nWhy it matters:While a turn-based approach may suffice for text input, voice-to-voice interactions benefit from a system that processes both input and output quickly and continuously. Previous systems process input and output separately, making users wait. Moshi delivers seamless interactivity.\n\nWe’re thinking:Generating silence is golden!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/02/10x_1200px_6-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/02/The-Batch-ads-and-exclusive-banners--6-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/O3.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/02/UITARS.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/FLASH2THINKING.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/MOSHI.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-152/",
    "title": "issue 152",
    "date": "",
    "reading_time": "",
    "content": "Last week, Iwroteabout key steps for building a career in AI: learning technical skills, doing project work, and searching for a job, all of which is supported by being part of a community. In this letter, I’d like to dive more deeply into the first step.More papers have been published on AI than any person can read in a lifetime. So, in your efforts to learn, it’s critical to prioritizetopic selection. I believe the most important topics for a technical career in machine learning are:\n\nThis is a lot to learn! Even after you master everything in this list, I hope you’ll keep learning and continue to deepen your technical knowledge. I’ve known many machine learning engineers who benefitted from deeper skills in an application area such as natural language processing or computer vision, or in a technology area such as probabilistic graphical models or building scalable software systems.How do you gain these skills? There’s a lot ofgood contenton the internet, and in theory reading dozens of web pages could work. But when the goal is deep understanding, reading disjointed web pages is inefficient because they tend to repeat each other, use inconsistent terminology (which slows you down), vary in quality, and leave gaps. That’s why a good course — in which a body of material has been organized into a coherent and logical form — is often the most time-efficient way to master a meaningful body of knowledge. When you’ve absorbed the knowledge available in courses, you can switch over to research papers and other resources.Finally, keep in mind that no one can cram everything they need to know over a weekend or even a month. Everyone I know who’s great at machine learning is a lifelong learner. In fact, given how quickly our field is changing, there’s little choice but to keep learning if you want to keep up. How can you maintain a steady pace of learning for years? I’vewrittenabout the value ofhabits. If you cultivate the habit of learning a little bit every week, you can make significant progress with what feels like less effort.\n\nKeep learning!\n\nAndrew\n\nWhy did Lorenzo Ostano leave a job as a machine learning engineer to work in traditional software development? In this edition of our Breaking Into AI series, Ostano explains how the pivot will help him achieve his long-term goal: building enterprise machine learning systems.Learn more\n\nComputer vision systems are scanning satellite photos to track construction on the Earth’s surface — an exercise in behavior recognition on a global scale.What’s new:Space-based Machine Automated Recognition Technique (Smart) is a multi-phase competition organized by the United States government. So far, it has spurred teams to develop systems that track large-scale construction in sequential satellite images,Wiredreported.The challenge:Barren earth, dump trucks, and large cranes are common markers of construction sites. But they aren’t always present at the same time, and they may be found in other contexts — for instance, dump trucks travel on highways and large cranes sit idle between jobs. Moreover, different satellites have different imaging systems, orbits, schedules, and so on — a stumbling block for automated classification. In the first phase of the contest, from January 2021 through April 2022, competitors built models that correlate features that were present in the same location but not at the same time, regardless of the image source.How it works:The Intelligence Advanced Research Projects Activity (IARPA), a U.S. intelligence agency, organized the challenge.\n\nResults:Judges evaluated contestants based on how they approached the problem and how well their models performed. The jury came from institutions including NASA’s Goddard Space Flight Center, U.S. Geological Survey, and academic labs.\n\nBehind the news:Satellite imagery is a major target of development in computer vision. Various teams are tracking the impact ofclimate change, predicting volcaniceruptions, and watching China’s post-Covid economyrebound.Why it matters:Photos taken from orbit are akeyresource for intelligence agencies. Yet the ability to see changes on Earth’s surface is a potential game changer in fields as diverse as agriculture, logistics, and disaster relief. It’s impractical for human analysts to comb the flood of images from more than 150 satellites thatobserveEarth from orbit. By automating the process, machine learning opens huge opportunities beyond Smart’s focus on national security.We’re thinking:Large-scale events on Earth are of interest to all of the planet’s inhabitants. We’re glad to see that the contestants will be able to use the models they build, and we call on them to use their work to help people worldwide.\n\nAn autonomous research ship crossed the Atlantic Ocean — with a few pit stops to address challenges along the way.What’s new:Built by IBM and marine research nonprofit ProMare, the Mayflower Autonomous Ship 400 (MAS400) last weekcompleteda voyage from Plymouth, England, to Plymouth, Massachusetts.How it works:The vessel navigates autonomously using a system based on IBM’sOperational Decision Manager, a rules-based system that integrates data from machine learning and other sources to adhere to conventions for maritime navigation. It carries no human crew, but ProMare can control it remotely if necessary.\n\nChoppy waters:The passage from Plymouth to Plymouth, which originally was scheduled to commemorate the 400-year anniversary of Pilgrims who traveled from England to America to escape religious persecution, ran a year late. It was supposed to take place over three weeks in June 2021, but less than a week into the first attempt, a power issue forced ProMare to guide the vessel back to England for repairs. The second attempt lasted over two months, with two unplanned port calls in the Azores and Nova Scotia to address generator issues and battery defects.Behind the news:Autonomous vessels are increasingly plying the seven seas.\n\nWhy it matters:Removing the crews from ships can save space, fuel, and money. The industry has taken notice, and the International Maritime Organization isdraftingrules to adapt maritime regulation for autonomous craft.We’re thinking:Let this ship’s voyage be a lesson: You may encounter setbacks, but persist and you will arrive at your destination — schooner or later.\n\n“The Machine Learning course explained mathematical concepts, and I found the programming approach intuitive for a non computer-science major. It helped me get into a master’s degree program in data science.”— Jose Eduardo Santo.Enroll in the Machine Learning Specialization\n\nMachine learning is helping lawyers sift through mountains of documents to find evidence.What’s new:The legal technology company Everlawlauncheda clustering feature that automatically organizes up to 25 million documents for lawyers gathering evidence to be used during a trial.How it works:The new feature analyzes text documents via unsuperviseddensity-based clusteringto build a visual map of word clouds.\n\nMaking headlines:ProsecutorsusedEverlaw’s software during the high-profile trial of Theranos co-founder Elizabeth Holmes. Among 1 million documents, they found 40 that implicated her criminal intent to defraud investors.Behind the news:AI increasingly contributes to legal proceedings.\n\nWhy it matters:Tools that streamline the mundane, high-stakes chore of sifting through documents could help lawyers and their aides  discover evidence they might otherwise overlook. This may be a boon especially for less-privileged plaintiffs and defendants, as some legal scholars have longheldthat the resource-intensive discovery process favors the wealthy.We’re thinking:There’s a strong case for AI in legal practice.\n\nResearchers haveshownthat it’s possible to train a computer vision model effectively on around 66 percent of the pixels in each training image. New work used 25 percent, saving computation and boosting performance to boot.What's new:Kaiming He, Xinlei Chen, and colleagues at Facebook developed a pretraining method they callMasked Auto-Encoder(MAE). Given a fixed processing budget, MAE pretrained a larger model three times faster, resulting in higher performance in less computation than earlier methods.Key insight:In a masked training scenario (in which portions of each training example are masked and the model learns to fill in the blanks), the larger the mask, the less computation is required. At the same time, it’s axiomatic that bigger neural networks make for better learning. Combining a very large mask with a very high parameter count should result in better performance with less computation.How it works:A typical autoencoder uses an encoder and decoder to generate representations for use by a different model. During training, the encoder learns to create a representation of the input, and the decoder learns to use the representation to reproduce the input. The authors used transformers for the encoder and decoder, and the encoder’s parameter count was roughly an order of magnitude greater than the decoder’s. They pretrained it on ImageNet examples that had been heavily masked. Then they fine-tuned the encoder’s representations on ImageNet as well.\n\nResults:MAE’s fine-tuned representations achieved 85.9 percent accuracy on ImageNet classification, outperforming representations learned from scratch using the same architecture (82.6 percent) andBEiT, an earlier masked training method that used less masking, a smaller encoder, and a different random masking strategy (85.2 percent). MAE trained 3.7 times faster than the same architecture without masking and up to 3.5 times faster than BEiT.Why it matters:Given a larger model, providing less information at input is not necessarily a disadvantage. Rather, it can improve both computational efficiency and performance.We're thinking:Would a similar design that pairs heavy masking and a plus-sized encoder boost training efficiency in large language models?",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/07/ColumnCloseup_Rev-LEARNNG-NoYOU_1200px-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/07/Photo-from-Lorenzo-Ostano-1024x576.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/07/SMART--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/07/MAYFLOWER--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/07/MLS_Learner_1200x628_A-1_Artboard-1-copy-5.png",
      "https://dl-staging-website.ghost.io/content/images/2022/07/EVERLAW--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/07/MASKED.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-289/",
    "title": "issue 289",
    "date": "",
    "reading_time": "",
    "content": "Last month, a drone fromSkyfire AIwas credited with saving a police officer’s life after a dramatic 2 a.m. traffic stop. Many statistics show that AI impacts billions of lives, but sometimes a story still hits me emotionally. Let me share what happened.\n\nSkyfire AI, an AI Fund portfolio company led by CEODon Mathis, operates a public safety program in which drones function as first responders to 911 calls. Particularly when a police department is personnel-constrained, drones can save officers’ time while enhancing their situational awareness. For example, many burglar alarms are false alarms, maybe set off by moisture or an animal. Rather than sending a patrol officer to drive over to discover this, a drone can get there faster and determine if an officer is required at all. If the alarm is real, the drone can help officers understand the situation, the locations of any perpetrators, and how best to respond.\n\nIn January, a Skyfire AI drone was returning to base after responding to a false alarm when the police dispatcher asked us to reroute it to help locate a patrol officer. The officer had radioed a few minutes earlier that he had pulled over a suspicious vehicle and had not been heard from since. The officer had stopped where two major highways intersect in a complex cloverleaf, and dispatch was unsure exactly where they were located.\n\nFrom the air, the drone rapidly located the officer and the driver of the vehicle he had pulled over, who it turned out had escaped from a local detention facility. Neither would have been visible from the road — they were fighting in a drainage ditch below the highway. Because of the complexity of the cloverleaf’s geometry, the watch officer (who coordinates police activities for the shift) later estimated it would have taken 5-7 minutes for an officer in a patrol car to find  them.\n\nFrom the aerial footage, it appeared that the officer still had his radio, but  was losing the fight and unable to reach it to call for help. Further, it looked like the assailant might gain control of his service weapon and use it against him. This was a dire and dangerous situation.\n\nFortunately, because the drone had pinpointed the location of the officer and his assailant, dispatch was able to direct additional units to assist. The first arrived not in 5-7 minutes but in 45 seconds. Four more units arrived within minutes.\n\nThe officers were able to take control of the situation and apprehend the driver, resulting in an arrest and, more important, a safe outcome for the officer. Subsequently, the watch officer said we’d probably saved the officer’s life.\n\nDemocratic nations still have a lot of work to do on drone technology, and we must build this technology with guardrails to make sure we enhance civil liberties and human rights. But I am encouraged by the progress we’re making. In the aftermath of Hurricane Helene last year, Skyfire AI’s drones supported search-and-rescue operations under the direction of the North Carolina Office of Emergency Management, responding to specific requests to help locate missing persons and direct rescue assets (e.g., helicopters and boats) to their location, and was credited with saving 13 lives.\n\nIt’s not every day that AI directly saves someone's life. But as our technology advances, I think there will be more and more stories like these.\n\nKeep building!\n\nAndrew\n\nLearn to systematically evaluate, improve, and iterate on AI agents using structured assessments. In our short course “Evaluating AI Agents,” you’ll learn to add observability, choose the right evaluation methods, and run structured experiments to improve AI agent performance.Enroll for free\n\nxAI’s new model family suggests that devoting more computation to training remains a viable path to building more capable AI.\n\nWhat’s new:Elon Musk’s xAI published a videodemonstrationof Grok 3, a family of four large language models that includes reasoning and non-reasoning versions as well as full- and reduced-size models. Grok 3 is available to subscribers to X’s Premium+ ($40 monthly for users in the United States; the pricevaries by country) and will be part of a new subscription service called SuperGrok. The models currently take text input and produce text output, but the company plans to integrate audio input and output in coming weeks.\n\nHow it works:xAI has not yet disclosed details about Grok 3’s architecture, parameter counts, training datasets, or training methods. Here’s what we know so far:\n\nResults:The Grok 3 family outperformed leading models in math (AIME 2024), science (GPQA), and coding (LiveCodeBench).\n\nBehind the news:Reasoning models are pushing benchmark scores steadily upward, especially in challenging areas like math and coding. Grok 3, with its ability to reason over prompts, search the web, and compile detailed reports, arrives hot on the heels of OpenAI’sDeep Researchando3-miniand Google’sGemini-2 Flash Thinking, which offer similar capabilities.\n\nWhy it matters:Grok 3 is a substantial achievement — especially for a company that’s less than two years old — and it pushes the state of the art forward by ample margins. But its significance may go farther. Research intoscalinglawsindicates that model performance scales with training. While xAI has not disclosed the amount of processing used to train Grok 3, the number of GPUs in its cluster suggests that the company applied a massive amount.\n\nWe’re thinking:Grok 3’s performance makes a case for both massive compute in pretraining and additional compute at inference. Running in its usual mode, Grok 3 mini Reasoning outperformed OpenAI o3-mini set at high effort on AIME 2024, GPQA, and LiveCodeBench. With an unspecified amount of additional compute, its performance on those benchmarks shot further upward by a substantial margin.\n\nReplit, an AI-driven integrated development environment, updated its mobile app to generate further mobile apps to order.\n\nWhat’s new:Replit’s app, which previously generated simple Python programs, nowgenerates iOS and Android apps and app templatesthat can be shared publicly. Mobile and web access to Replit’s in-house code generation models is free for up to three public applications. ACore plan($25 per month, $180 per year) buys unlimited access and applications, code generation by Claude 3.5 Sonnet and OpenAI GPT-4o, and monthly credits for generated checkpoints.\n\nHow it works:The app and web tools are powered by Replit Agent, an AI coding assistant designed to help users write, debug, and deploy applications with little manual setup. Replit Agent is based on Claude 3.5 Sonnet and calls other specialized models. The agent framework isbuilton LangChain’s LangGraph. It breaks down development tasks into steps to be handled by specialized sub-agents.\n\nBehind the news:The incorporation of Replit Agent to Replit’s mobile app is a significant step for AI-driven IDEs. Competitors like Aider and Windsurf don’t offer mobile apps, and mobile apps from Cursor and Github provide chat but not mobile app development. Moreover, few coding agents can deploy apps to the cloud on the desktop or mobile.\n\nWhy it matters:Replit’s new mobile app produces working apps in minutes (although some early users have reported encountering bugs), and automatic deployment of apps to the cloud is a huge help. Yet it raises the stakes for developers to learn their craft and maintain a collaborative relationship with AI. While Replit’s web-based environment exposes the code, encouraging users to improve their skills, the mobile app hides much of its work below the surface. It brings AI closer to handling full software development cycles and adds urgency to questions about how to address the balance between automation and hands-on coding.\n\nWe’re thinking:AI continues to boost developer productivity and reduce the cost of software development, and the progress of Bolt, Cursor, Replit, Vercel, Windsurf, and others is exhilarating. We look forward to a day when, measured against the 2024 standard, every software engineer is a 10x engineer!\n\nElon Musk and a group of investors made an unsolicited bid to buy the assets of the nonprofit that controls OpenAI, complicating the AI powerhouse’s future plans.\n\nWhat’s new:Musksubmitteda $97.4 billion offer to acquire the assets of the nonprofit OpenAI Inc. CEO Sam Altman and the company’s board of directors swiftlyrejectedit, and Altman publiclymockedMusk by offering to buy Twitter for $9.74 billion (one-tenth of Musk’s bid and less than one-quarter the price he paid for the social network). OpenAI’s board reaffirmed its control over the company’s direction, signaling that it does not intend to cede governance to outside investors.\n\nHow it works:OpenAI was founded as a nonprofit in 2015, but since 2019 it has operated under an unusual structure in which the nonprofit board controls the for-profit entity that develops and commercializes AI models. This setup allows the board to maintain the company’s original mission — developing AI for the benefit of humanity — rather than solely maximizing shareholder value. However, driven by the need for massive investments in infrastructure and talent, OpenAI is considering a newfor-profit structurethat would allow external investors to own more of the company. The high offer by Musk — who, as CEO of xAI, competes with OpenAI — could interfere with that plan.\n\nBehind the news:Musk was one of OpenAI’s earliest investors, but he departed in 2018 after disagreements over direction and control of the organization. His bid follows alawsuitagainst OpenAI, in which he claims the company abandoned its nonprofit mission in favor of profit. OpenAIsaidthat Musk’s bid contradicts his legal claims and suggests that the lawsuit should be dismissed. Since then, Musk hasstatedthat he would drop the lawsuit if OpenAI remains a nonprofit.\n\nWhy it matters:OpenAI is a premier AI company, and its activities affect virtually everyone in the field by supplying tools, technology, or inspiration. Musk’s xAI is a direct competitor, and his bid, whether it’s sincere or tactical, unsettles OpenAI’s plans. Even if OpenAI moves forward as planned, Musk’s actions likely will have made the process more expensive and potentially invite closer scrutiny of the company’s actions.\n\nWe’re thinking:There’s ample precedence for non-profits spinning out for-profit entities. For example, non-profit universities typically create intellectual property that forms the basis of for-profit startups. The university might retain a modest stake, and this is viewed as consistent with its non-profit mission. This isn’t a perfect analogy, since OpenAI does little besides operating its AI business, but we hope the company finds a path forward that allows it to serve users, rewards its employees for their contributions, and honors its non-profit charter.\n\nThe latest international AI summit exposed deep divisions between major world powers regarding AI regulations.\n\nWhat’s new:While previous summits emphasized existential risks, theAI Action Summitin Paris marked a turning point. France and the European Union shifted away from strict regulatory measures and toward investment to compete with the United States and China. However, global consensus remained elusive: the U.S. and the United Kingdom refused to sign key agreements on global governance, military AI, and algorithmic bias. The U.S. in particular pushed back against global AI regulation, arguing that excessive restrictions could hinder economic growth and that international policies should focus on more immediate concerns.\n\nHow it works:Participating countries considered three policy statements that address AI’s impact on society, labor, and security. Thefirst statementcalls on each country to enact AI policies that would support economic development, environmental responsibility, and equitable access to technology. Thesecondencourages safeguards to ensure that companies and nations distribute AI productivity gains fairly, protect workers’ rights, and prevent bias in hiring and management systems. Thethirdadvocates for restrictions on fully autonomous military systems and affirms the need for human oversight in warfare.\n\nBehind the news:The Paris summit follows previous gatherings of world leaders to discuss AI, including the initialAI Safety Summitat Bletchley Park and theAI Seoul Summit and AI Global Forum. At these summits, governments and companies agreed broadly to address AI risks but avoided binding regulations. Nonetheless, divisions over AI governance have widened in the wake of rising geopolitical competition and theemergenceof high-performance open weights models like DeepSeek-R1.\n\nWhy it matters:The Paris summit marks a major shift in global AI policy. The EU, once an ardent proponent of AI regulation, backed away from its strictest proposals. At the same time, doomsayers have lost influence, and officials are turning their attention to immediate concerns like economic growth, security, misuse, and bias. These moves make way for AI to do great good in the world, even as they contribute touncertaintyabout how AI will be governed.\n\nWe’re thinking:Governments are shifting their focus away from unrealistic risks and toward practical strategies for guiding AI development. We look forward to clear policies that encourage innovation while addressing real-world challenges.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--52-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/The-Batch-ads-and-exclusive-banners---2025-02-17T150409.455.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--53-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--49-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--50-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--54-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-xv/",
    "title": "issue xv",
    "date": "",
    "reading_time": "",
    "content": "Keep learning!Andrew\n\nChips that connect the human brain to outboard equipment have given paralyzed patients rudimentary control over robotic limbs. Elon Musk envisions brain-computer interfaces that wouldn’t just rehabilitate people but link them to artificial intelligence.What’s new:Musk took the stage in San Francisco toshow offa prototype from Neuralink, his secretive brain-computer interface company.How it works:Neuralink makes conductive threads that can be woven into the brain using a machine that resembles a cross between a microscope and a sewing machine (shown in the video above).\n\nWhy it matters:The current industry standard in brain-machine interfaces, the Utah array, bristles with tiny spikes. They can be dangerous as the brain shifts within the skull, and they spur formation of scar tissue that interferes with connections. Neuralink claims its threads will reach more neurons, deeper in the brain, without causing damage or scarring.What’s next:Musk hopes to put the system in people suffering from paralysis by the end of 2020.\n\nWhat he's saying:“Even under a benign AI, we will be left behind. With a high-bandwidth, brain-machine interface, we will have the option to go along for the ride.” — Elon Musk, CEO, Neuralink.Yes, but:Helping paralyzed individuals is a wonderful mission. But for healthy people, the need for a higher bandwidth brain-machine interface isn't proven. We can already flash text much faster than people can read, and many can touch-type faster than they can organize their thoughts. The bottleneck may be the brain's ability to process information, not the speed at which we can give it input.We’re thinking:Musk is a showman, but his visions for technology sometimes come true — though not always on his preferred schedule.\n\nGenerative adversarial networks make amazingly true-to-life pictures, but their output largely has been limited to still images — until now. Get ready for generated videos.What’s new:A team from DeepMind offersDual Video Discriminator GAN, a network that produces eerily lifelike videos out of thin air.Key insight:DVD-GAN generates both realistic levels of detail  and amounts of movement. Aidan Clark, Jeff Donahue, and Karen Simonyan accomplish these twin goals by dedicating a separate adversarial discriminator to each.How it works:DVD-GAN modifies the state-of-the-art architecture for single images calledBigGANto produce a coherent series of frames. It includes a generator to create frames, a spatial discriminator that makes sure frames look good, and a temporal discriminator that makes sure successive frames go together. As in any GAN, the discriminators attempt to distinguish between real videos and generated videos while the generator tries to fool the discriminators.\n\nResults:DVD-GAN generates its most realistic results based on the Kinetics dataset of 650,000 brief video clips focusing on human motion. Nonetheless, on the smaller UCF-101 set of action clips, it scores 33 percent higher than the previous state-of-the-art inception score, a measure of generated uniqueness and variety.Yes, but:The current version maxes out at 4 seconds, and it generates lower-resolution output than conventional GANs. “Generating longer and larger videos is a more challenging modeling problem,” the researchers say.Why it matters:GANs have led the way to an exciting body of techniques for image synthesis. Extending this to video will open up still more applications.\n\nAfrica isn’t known as a tech hub, but the continent’s embrace of AI is putting it in the spotlight.What’s happening:African researchers lately have turned to AI to tackle everything from crop failure to bureaucratic red tape. A story inMIT Technology Reviewdetails how global AI companies are fostering homegrown talent to take on local challenges:\n\nBehind the news:Africa's AI community draws on local roots.Data Science Africa began in 2013 as a hub for machine learning experts across the continent to connect, share data, and encourage research. Another group, Deep Learning Indaba, hosts annual TED-like conferences to spread the growth.Why it matters:Africa holds 54 countries and more than a billion inhabitants with unique challenges. Homegrown experts with local knowledge seem more likely than outsiders to apply AI effectively to these issues. In any case, AI needs talent centers worldwide to achieve its promise.We’re thinking:We love Silicon Valley, but we're also rooting for the Great Rift Valley.\n\nObject detection, art generation, and face recognition are based on convolutional neural networks. But how do you build a CNN?  Learn techniques like pooling and convolutions in Course 4 of theDeep Learning Specialization.\n\nFor all the research attention paid to cancer, there’s still no foolproof way to catch it early. Now AI is homing in on tumors that doctors previously missed.What's new:Researchers at Johns Hopkins University developedCompCyst,a machine learning model capable of diagnosing early-stage pancreatic cancer better than previous methods.How it works:The system interprets several data types to recognize precursors to pancreatic cancer.\n\nBehind the news:800,000 Americans are diagnosed with pancreatic cysts annually, but only a small fraction of those will develop a cancerous tumor. Because the pancreas is buried deep in the body, it’s often impossible to tell tiny, pre-cancerous cysts from benign lumps. Fear drives a lot of these patients to pursue very aggressive treatments.Why it matters:About 95 percent of people diagnosed with pancreatic cancer die from it. One reason is that most patients are diagnosed very late in the disease’s progress.  On the other hand, up to 5 percent of patients die from the most common pancreatic surgery, so it’s not safe to be overly cautious, either. In either case, better diagnostics could save many lives.We’re thinking:When it comes to many cancers, early intervention can either save a life or disrupt one that wasn’t in danger. If the FDA clears CompCyst for new patients, it could open the door to a slew of similar models that make a cancer diagnosis less of a gamble.\n\nDo security cameras make your local bank or convenience store safer? These devices monitor countless locations around the clock, but it takes people watching to evaluate their output — an expensive, exhausting, and error-prone solution. Researchers at iCetana and the University of Western Australia took a step toward detecting critical events without humans in the loop.What's new:Lei Wang, Du Q. Huynh, and Moussa Reda Mansour developed a lightweightarchitecture(one that can be trained on a MacBook Pro in half a day!) that differentiates between human motion and background movement like trees swaying, rain falling, and camera shaking in videos of outdoor scenes.Key insight:Features belonging to the same class flock together in multi-dimensional vector space, yet typical methods of reducing the number of dimensions can lose this clustering information. The authors proposed a novel training procedure that shrinks dimensionality without interfering with the ability to group similar classes.How it works:Loss Switching Fusion Network (LSFNet) fuses the handcrafted features known as dense trajectories, which are commonly used to detect people moving in videos, in a way that retains clustering information. Then it’s simple to distinguish videos that have human motion from those that show rain, trees waving, camera shaking, shifting illumination, and noisy video.\n\nResults:Features processed by LSFNet produce visibly better classification compared to those produced by standard autoencoders or PCA. They outperform all state-of-the-art background- and foreground-motion classification techniques.Why it matters:It’s hard to keep an eye on a half-dozen security-camera screens at once. AI that distinguishes between human activity and innocuous background motions could make video surveillance more effective in real time.We're thinking:AI-equipped security systems likely will lead to legitimate concerns about privacy. But it may also mean more effective crime detection. AI companies can take the lead by addressing concerns proactively while working to maximize the benefits.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/1b1755f5-0394-4797-b881-50cdad00d4d7-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/34bc1679-bbb1-411f-9e6a-c1674e4313cf.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/76c747e7-bc05-41bf-a6ce-8db206904be6.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/6d2eaa15-ee48-41be-9c34-0ee519ed2bea.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/937fd873-9962-4cb1-ba8b-0f917144c0f3.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/e5e01574-4b75-48c5-a144-8335f407450e.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-174/",
    "title": "issue 174",
    "date": "",
    "reading_time": "",
    "content": "One of the dangers of large language models (LLMs) is that they can confidently make assertions that are blatantly false. This raises worries that they will flood the world with misinformation. If they could moderate their degree of confidence appropriately, they would be less likely to mislead.People are prone to following authority figures. Because a lot of text on the internet is written in an authoritative style — hopefully because the authors know what they’re talking about— LLMs have learned to mimic this style. Unfortunately, LLMs can speak in this style even when they get the facts completely wrong.\n\nWe don’t expect people to be right all the time, but we don’t like it when they’re simultaneously confident and wrong. Real experts speak in a range of styles: confident when we know what we’re talking about, but also explaining the boundaries of our knowledge when we run up against them and helping the audience understand the range of possibilities. For example, when asked how to build an AI application, I might propose one approach but also describe the range of algorithms one might consider. Knowing what you know and don’t know is a useful trait of expertise.\n\nPlaying with ChatGPT, the latest language model from OpenAI, I found it to be an impressive advance from its predecessor GPT-3. Occasionally it says it can’t answer a question. This is a great step! But, like other LLMs, it can behilariously wrong. Work lies ahead to build systems that can express different degrees of confidence.\n\nFor example, a model like Meta’sAtlasor DeepMind’sRETROthat synthesizes multiple articles into one answer might infer a degree of confidence based on the reputations of the sources it draws from and the agreement among them, and then change its communication style accordingly. Pure LLMs and other architectures may need other solutions.\n\nIf we can get generative algorithms to express doubt when they’re not sure they’re right, it will go a long way toward building trust and ameliorating the risk of generating misinformation.Keep learning!\n\nAndrew\n\nMembers of the AI community tested the limits of the ChatGPT chatbot, unleashing an avalanche of tweets that made for sometimes-great, sometimes-troubling entertainment.\n\nWhat’s new:OpenAI launched a public demo ofChatGPT, the latest in the research lab’s line of large language models. Like its predecessors, ChatGPT generates text in a variety of styles, for a variety of purposes. Unlike them, it does so with greater finesse, detail, coherence, and — dare we say it? — personality. (How else to characterize a model thatapologizesfor its misbehavior?) One million users havesigned upsince the launch last Wednesday.\n\nHow it works:ChatGPT is a next-generation language model (of a class referred to as GPT-3.5) trained in the manner of OpenAI’s earlierInstructGPT, but on conversations. It was fine-tuned to minimize harmful, untruthful, or biased output using a combination of supervised learning and what OpenAI callsreinforcement learning from human feedback, in which humans rank potential outputs and a reinforcement learning algorithm rewards the model for generating outputs similar to those that rank highly.\n\nStrengths and weaknesses:Like other recent language models, ChatGPT’s output veers between stunningly brilliant and mind-numbingly stupid.\n\nBehind the news:ChatGPT arrived one week after Meta withdrewGalactica, a model designed to generate scientific papers. Galactica was promoted as an aid to researchers aiming to publish their findings, but users of the public demo prompted it to generate sober dissertations on nonsensical topics like land squid and the health benefits of ingesting ground glass.\n\nWhy it matters:Speech is among the simplest and most convenient ways for humans to communicate. Programs that grasp what they’re told and respond with meaningful information will open a wide range of everyday functions. Closer to home, many observers proposed ChatGPT or something like it as a superior alternative to current web search. First, though, researchers face the steep challenge of building a language model that doesn’t make up facts and ignore limits on its output.\n\nWe’re thinking:Sometimes technology is overhyped — reinforcement learning, after solving Atari games, may be an example — but large language models are likely to find a place in significant applications. Meanwhile, many details remain to be worked out and the AI community must strive to minimize potential harm.\n\nThe demise of cryptocurrency exchange FTX threatens funding for some teams devoted to AI safety.\n\nWhat’s new:FTX, the $32 billion exchange that plunged into bankruptcy last month amid allegations of fraud, had given or promised more than $530 million to over 70 AI-related organizations,The New York Timesreported. Much of that money may have to be returned.\n\nWhat happened:FTX founder Sam Bankman-Fried and his associates used the exchange’s holdings to dole out grants or investments to AI-related startups, labs, and think tanks, many of them focused on AI safety. People associated with these groups anonymously expressed concerns that their funding would be clawed back in bankruptcy proceedings.\n\nBehind the news:Bankman-Fried co-founded FTX in 2019 to enable users to trade cryptocurrency for conventional money and other assets. A November report by CoinDesk, a cryptocurrency news outlet,describeda potential conflict of interest between FTX and another trading firm also owned by Bankman-Fried. The news prompted users to withdraw their funds, much of which FTX had already spent, invested, given away, or promised to others. The exchange filed for bankruptcy. U.S. prosecutors and regulators areinvestigatingpotential wrongdoing.Why it matters:It’s crucial to minimize potential harm caused by AI, but organizations devoted to that goal may not receive the funding they need from corporate entities or cash-strapped academic institutions. Organizations that were counting on FTX may find support elsewhere, but many now face an uncertain future.\n\nWe’re thinking:We’re grateful for donors who are willing to support AI research of all kinds. At the same time, we’re appalled by the scope and brazenness of FTX’s deceit. Sadly, organizations that seek funding must vet potential donors carefully.\n\nJoin Sebastián Ramírez, the creator of FastAPI, to build your own AI image-generation web app. “FastAPI for Machine Learning: Live Coding an ML Web Application” takes place on December 15, 2022, at 9:00 a.m. Pacific Time.RSVP\n\nToo exhausted (or unimaginative) to tell your child a bedtime story? Amazon’s smart displays can spin bespoke tales on demand.What’s new:A feature called Create with Alexagenerateschildren’s stories complete with illustrations, music, and sound effects on the Amazon Echo Show device.\n\nHow it works:The screen presents a series of prompts that provide a setting (such as “space exploration” or “enchanted forest”), main character (such as an astronaut or an alien), principal color, and tone (such as “happy” or “mysterious”).\n\nBehind the news:Amazon is under pressure to revitalize its 10-year-old Echo line. The devices, which have been sold at a loss on the theory that they would spur purchases of other goods,lost$10 billion in 2022 alone, and the division responsible for the Alexa softwarefacessteep layoffs.Why it matters:AI models that generate text, images, video, and music are having abanner year. Alexa’s storytelling feature coordinates several generative models into a coherent whole. Whether it will spur sales is a tale for another time.We’re thinking:Once upon a time, there was a boy in a blue shirt who dreamed of changing the world with AI. . . .\n\nIf a robot can predict what it’s likely to see next, it may have a better basis for choosing an appropriate action — but it has to predict quickly. Transformers, for all their utility in computer vision, aren’t well suited to this because of their steep computational and memory requirements. A new approach could change that.\n\nWhat’s new:Agrim Gupta and colleagues at Stanford devisedMasked Visual Pre-Training for Video Prediction (MaskViT), a transformer model that generates likely future video frames with far less computation than earlier transformer-based approaches. You can see its outputhere.\n\nKey insight:Transformers typically predict one token per forward pass (processing every layer in the model from first to last). The amount of processing required for this approach is manageable when generating an image, which may be divided among hundreds or thousands of tokens. But it becomes very time-consuming when generating video, which involves many images. Predicting multiple tokens at once reduces the number of forward passes needed to generate video, significantly accelerating the process.\n\nHow it works:MaskViT consists of an image tokenizer (VQ-GAN, adiscrete variational autoencoder) and a transformer. The authors trained and tested it on three video datasets:RoboNet(15 million frames that depict robotic arms interacting with objects),BAIR(a smaller dataset that shows a robot pushing things on a table top), andKITTI(57 videos recorded from a car driving on roads in Germany). The model generated 10 to 25 video frames, depending on the dataset, following between one and five initial frames, depending on the dataset.\n\nResults:The authors compared their model’s efficiency at inference with that of earlier transformer-based approaches. On BAIR, for instance, MaskViT required 24 forward passes to generate 15 frames, while the previous state of the art,VT, needed 3,840. With respect to its predictive ability, on BAIR, MaskViT achieved 93.7Fréchet Video Distance(FVD), a measure of how well a generated distribution resembles the original distribution, for which lower is better. That’s better than VT (94.0 FVD) and roughly equal to the best non-transformer approach,FitVid(93.6 FVD). On the more complicated RoboNet dataset, MaskViT achieved 133.5 FVD, while FitVid achieved 62.5 FVD. (VT results on that dataset are not reported.)\n\nYes, but:The authors compared numbers of forward passes at inference, but they didn’t compare processing time. Different models take different amounts of time to run, so there’s no guarantee that a smaller number of forward passes takes less time. That said, given differences between the options for hardware, machine learning libraries, and programming languages, it would be hard to compare execution speeds directly.\n\nWhy it matters:While the reduction of forward passes is notable, the authors also came up with an interesting way to improve output quality. During inference, 100 percent of the tokens to be generated start out missing and fill in slowly over the generation process. However, in the typical training practice, which masks a fixed percentage of tokens, the model never encounters such a large percentage of missing tokens. Instead, during training, the authors masked a variable portion of tokens up to 100 percent. This procedure better aligned the tasks during training and inference, which yielded better results.\n\nWe’re thinking:Giving robots the ability to predict visual changes could make for a generation of much safer and more capable machines. We look forward to future work that integrates this capability with planning algorithms.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--8--1.png",
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--9-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--9-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/12/Imagen5.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--19-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--20-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-194/",
    "title": "issue 194",
    "date": "",
    "reading_time": "",
    "content": "My team at Landing AI just announced a new tool for quickly building computer vision models, using a technique we call Visual Prompting. It’s a lot of fun!I invite you to try it.\n\nVisual Prompting takes ideas from text prompting — which has revolutionized natural language processing — and applies them to computer vision.\n\nTo build a text sentiment classifier, in the traditional machine learning workflow, you have to collect and label a training set, train a model, and deploy it before you start getting predictions. This process can take days or weeks.\n\nIn contrast, in the prompt-based machine learning workflow, you can write a text prompt and, by calling a large language model API, start making predictions in seconds or minutes.\n\nTo explain how these ideas apply to computer vision, consider the task of recognizing cell colonies (which look like white blobs) in a petri dish, as shown in the image below. In the traditional machine learning workflow, using object detection, you would have to label all the cell colonies, train a model, and deploy it. This works, but it’s slow and tedious.\n\nIn contrast, with Visual Prompting, you can create a “visual prompt” in seconds by pointing out (by painting over) one or two cell colonies in the image and similarly pointing out the background region, and get a working model. It takes only a few seconds to (i) create the visual prompt and (ii) get a result. If you’re not satisfied with the initial model, you can edit the prompt (perhaps by labeling a few more cell colonies), check the results, and keep iterating until you’re satisfied with the model’s performance.\n\nThe resulting interaction feels like you’re having a conversation with the system. You’re guiding it by incrementally providing additional data in real time.\n\nSince 2017, when thepaperthat introduced transformers was published, rapid innovation in text processing has transformed natural language models. Thepaperthat introduced vision transformers arrived in 2020, and similarly it led to rapid innovation in vision. Large pretrained models based on vision transformers have reached a point where, given a simple visual prompt that only partially (but unambiguously) specifies a task, they can generalize well to new images.\n\nWe’re not the only ones exploring this theme. Exciting variations on Visual Prompting include Meta’sSegment Anything(SAM), which performs image segmentation, and approaches such asGeneralist Painter,SegGPT, and prompting via inpainting.\n\nYou can watch a livestream of mypresentationon Visual Prompting or read Landing AI’sblog poston this topic.\n\nText prompting reached an inflection point in 2020, whenGPT-3made it easy for developers to write a prompt and build a natural language processing model. I don’t know if computer vision has reached its GPT-3 moment, but we’re getting close. I’m excited by the research that’s moving us toward that moment, and I think Visual Prompting will be one key to getting us there.\n\nKeep learning!\n\nAndrew\n\nDevelopers of language models will have to pay for access to troves of text data that they previously got for free.\n\nWhat’s new:The discussion platformRedditand question-and-answer siteStack Overflowannounced plans to protect their data from being used to train large language models.\n\nHow it works:Both sites offer APIs that enable developers to scrape data, like posts and conversations, en masse. Soon they'll charge for access.\n\nWhat they’re saying:“Community platforms that fuel LLMs absolutely should be compensated for their contributions so that companies like us can reinvest back into our communities to continue to make them thrive,” Chandrasekar toldWired.\n\nBehind the news:In February, Twitter started charging up to$42,000monthly for use of its API. That and subsequent API closures are part of a gathering backlash against the AI community’s longstanding practice of training models on data scraped from the web. This use is at issue in ongoinglawsuits. Last week a collective of major news publishersstatedthat training AI on text licensed from them violates their intellectual property rights.\n\nWhy it matters:Although data has always come at a cost, the price of some corpora is on the rise. Discussion sites like Reddit are important repositories of conversation, and text from Stack Overflow has been instrumental in helping to train language models to write computer code. The legal status of existing datasets and models is undetermined, and future access to data depends on legal and commercial agreements that have yet to be negotiated.We’re thinking:It’s understandable that companies watching the generative AI explosion want a slice of the pie and worry that users might leave them for a chatbot trained on data scraped from their own sites. Still, we suspect that charging for data will put smaller groups with fewer resources at a disadvantage, further concentrating power among a handful of wealthy companies.\n\nGoogle’s response to Microsoft’s GPT-4-enhanced Bing became a little clearer.\n\nWhat’s new:Anonymous insiders leaked details of Project Magi, the search giant’s near-term effort to enhance its search engine with automated conversation,The New York Timesreported. They described upcoming features, but not the models behind them.\n\nHow it works:Nearly 160 engineers are working on the project.\n\nBeyond search:The company is developing AI-powered features for other parts of its business as well. These include an image generation tool called GIFI for Google Images and a chatbot called Tivoli Tutor for learning languages.Behind the news:Google has been scrambling to integrate AI features. The company recently combined Brain and DeepMind into a single unit to accelerate AI research and development. In March, rumors emerged that Samsung, which pays Google substantial licensing revenue to use its search engine in mobile devices, was considering a switch to Bing. The previous month, Bard made factual errors during a public demo, which contributed to an 8 percent drop in Google’s share price. These moves followed a December 2022 “code red” response to Microsoft’s plans to upgrade Bing with conversational technology from OpenAI.\n\nWhy it matters:When it comes to finding information, conversational AI is a powerful addition to, and possibly a replacement for, web search. Google, as the market leader, can’t wait to find out. The ideas Google and its competitors implement in coming months will set the mold for conversational user interfaces in search and beyond.We’re thinking:Should chatbots be integrated with search or designed as separate products? Microsoft and Google are taking different approaches. Microsoft’s conversational model is deeply integrated with Bing search, while Google's Bard currently stands alone. Given the differences between chat and search, there’s a case to be made for keeping chatbots distinct from search engines.\n\nAre you ready to turn your passion into practice? The newAI for Good Specializationwill empower you to use machine learning and data science for positive social and environmental impact!Join the waitlist to be the first to enroll\n\nTech-savvy music fans who are hungry for new recordings aren’t waiting for their favorite artists to make them.\n\nWhat’s new:Social media networks exploded last week with AI-driven facsimiles of chart-topping musicians. A hiphop song with AI-generated vocals in the styles ofDrake and The Weekndracked up tens of millions of listens before it was taken down. Soundalikes of Britpop starsOasis, rapperEminem, and Sixties stalwartsThe Beach Boysalso captured attention.\n\nHow it works:These productions feature songs composed and performed in the old-fashioned way overlaid with celebrity-soundalike vocals generated by voice-cloning models. Some musicians revealedtheirmethods.\n\nBehind the news:The trend toward AI emulations of established artists has been building for some time. In 2021, Lost Tapes of the 27 Club used an unspecified AI method to produce music in the style of artists who died young including Jimi Hendrix, Kurt Cobain, and Amy Winehouse. The previous year, OpenAI demonstrated Jukebox, a system that generated recordings in the style of many popular artists.\n\nYes, but:The record industry is moving to defend its business against such audio fakery (or tributes, depending on how you want to view them). Universal Music Group, which controls about a third of the global music market, recentlypushedstreaming services to block AI developers from scraping musical data or posting songs in the styles of established artists.\n\nWhy it matters:Every new generation of technology brings new tools to challenge the record industry’s control over music distribution. The 1970s brought audio cassettes and the ability to cheaply copy music, the 1980s brought sampling, the 1990s and 2000s brought remixes and mashups. Today AI is posing new challenges. Not everyone in the music industry is against these AI copycats: The electronic artist Grimes said she would share royalties with anyone who emulates her voice, and Oasis’ former lead singer apparently enjoyed the AI-powered imitation.We’re thinking:Musicians who embrace AI will open new creative pathways, but we have faith that traditional musicianship will endure. After all, photography didn’t kill painting. Just as photography pushed painters toward abstraction, AI may spur conventional musicians in exciting, new directions.\n\nWe know that image generators create wonderful original works, but do they sometimes replicate their training data? Recent work found that replication does occur.\n\nWhat's new: Gowthami Somepalli and colleagues at University of Maryland devised amethodthat spots instances of image generators copying from their training sets, from entire images to isolated objects, with minor variations.\n\nKey insight:A common way to detect similarity between images is to produce embeddings of them and compute the dot product between embeddings. High dot product values indicate similar images. However, while this method detects large-scale similarities, it can fail to detect local ones. To detect a small area shared by two images, one strategy is to split apart their embeddings, compute the dot product between the pieces, and look for high values.\n\nHow it works: The authors (i) trained image generators, (ii) generated images, and (iii) produced embeddings of those images as well as the training sets. They (iv) broke the embeddings into chunks and (v) detected duplications by comparing embeddings of the generated images with those of the training images.\n\nResults: For each generated image, the authors found the 20 most similar images in the training set (that is, those whose fragmented embeddings yielded the highest dot products). Inspecting those images, they determined that the diffusion model sometimes copied elements from the training set. They plotted histograms of the similarity between images within a training set and the similarity between training images and generated images. The more the two histograms overlapped, the fewer the replications they expected to find. Both histograms and visual inspection indicated that models trained on smaller datasets contained more replications. However, on tests with Stable Diffusion, 1.88 percent of generated images had a similarity score greater than 0.5. Above that threshold, the authors observed obvious replications — despite that model’s pretraining on a large dataset.\n\nWhy it matters: Does training an image generator on artworks without permission from the copyright holder violate the copyright? If the image generator literally copies the work, then the answer would seem to be “yes.” Such issues are being tested in court. This work moves the discussion forward by proposing a more sensitive measure of similarity between training and generated images.\n\nWe're thinking:Picasso allegedly said that good artists borrow while great artists steal. . . .\n\nElon Musk founded a presumed AI company called X.AIX.AI Corp was incorporated in Nevada last month and Musk is its only listed director. No further details are available. (The Wall Street Journal)AI-powered technique fixes Python bugsA developer built Wolverine, a program based on GPT-4 that automatically finds and fixes bugs in real time. The code is available onGitHub. (Ars Technica)Startup Humane showcased upcoming wearable AI assistantA leaked clip from a TED talk by AI startup Humane cofounder Imran Chaudhri revealed a demo of a wearable device that performs the functions of a smartphone, voice assistant, and possibly other equipment . It performs tasks such as real-time speech translation and audio recaps. (Inverse)Google merged its two main AI research unitsThe merged organization, called Google DeepMind, combines Google Brain and DeepMind teams. The reorg is expected to accelerate progress on the tech giant’s AI projects. (The Wall Street Journal)Magazine published AI-generated “interview” with Michael SchumacherGerman magazineDie Aktuellepromoted an exclusive interview with the F1 ex-driver and then revealed the conversation was produced by an AI chatbot. (ESPN)SnapChat upgraded its AI chatbotThe chatbot, called My AI, is now able to generate images, recommend lenses, and suggest nearby places to visit. (Reuters)Stability AI launched an open source language modelThe company behind Stable Difussion released StableLM, a model aimed to contend with ChatGPT. The company released few details about the model. Its alpha version isavailablein 3 billion and 7 billion parameters. (BloombergandStability.AI)Microsoft is designing an AI chipThe chip, called Athena, has been in development since 2019. It’s expected to reduce costs associated with chips from external suppliers. (Silicon Angle)Research: Independent researchers released alternative to ChatGPTOpen Assistant is an open source alternative to commercial chatbots. Organized by the German nonprofit LAION, the development effort aims to democratize access to large language models. Try ithere. (Open Assistant)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/04/Screen-Shot-2023-04-25-at-5.50.35-PM-1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/04/REDDIT-Paywall3_1200px.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/04/unnamed--54-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/04/The-Batch-ads-and-exclusive-banners--9-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/04/unnamed--55-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/04/unnamed--57-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-79/",
    "title": "issue 79",
    "date": "",
    "reading_time": "",
    "content": "When a lot of data is available, machine learning is great at automating decisions. But when data is scarce, consider using the data to augment human insight, so people can make better decisions.\n\nLet me illustrate this point with A/B testing. The common understanding of the process is:\n\nBut this is not how I typically use A/B testing. Often I run such tests to gain insight, not to choose which product to launch. Here‘s how it works:\n\nOn major websites, where the developers may run thousands of automated experiments a day — for example, trying out different ad placements to see who clicks on what — it’s not possible for people to look at every experimental result to hone their intuition. In this case, fully or mostly automated decision making works well. An algorithm can try multiple versions and pick the one that achieves the best metrics (or use the data to learn what to show a given user). But when the number of daily experiments is small, using such experiments to hone your intuition allows you to combine limited trials with human insight to arrive at a better decision.Beyond A/B testing, the same concept applies to building machine learning systems. If your dataset size is modest, combining data-derived insights with human insights is critical. For example, you might do careful error analysis to derive insights and then design a system architecture that captures how you would carry out the task. If you have a massive amount of data, more automation — perhaps a large end-to-end learning algorithm — can work. But even then, error analysis and human insight still play important roles.\n\nKeep learning!\n\nAndrew\n\nU.S. government approval of medical AI products is on the upswing — but information about how such systems were built is largely unavailable.What’s new:The U.S. Food and Drug Administration (FDA) has approved a plethora of AI-driven medical systems. But, unlike drugs, there’s a dearth of publicly available information about how well they work, according to an investigation by the health-news websiteStat News.What they found:The FDA doesn’t require makers of AI systems to provide systematic documentation of their development and validation processes, such as the composition of training and test datasets and the populations involved. The data actually provided by manufacturers varies widely.\n\nBehind the news:The rate at which the FDA approves medical AI products is rising and could reach 600 products annually by 2025, according toStat News.\n\nWhy it matters:Without consistent requirements for testing and reporting, the FDA can’t ensure that AI systems will render accurate diagnoses, recommend appropriate treatments, or treat minority populations fairly. This leaves health care providers to figure out for themselves whether a product works as advertised with their particular equipment and patients.We’re thinking:If you don’t know how an AI system was trained and tested, you can’t evaluate the risk of concept or data drift as real-world conditions and data distributions change. This is a problem even in drug testing: A vaccine validated against the dominant Covid-19 variant may become less effective as the virus mutates. Researchers are developing tools to combat such drifts in AI systems. Let’s make sure they’re deployed in medical AI.\n\nThe same models trained on the same data may show the same performance in the lab, and yet respond very differently to data they haven’t seen before. New work finds this inconsistency to be pervasive.What’s new:Researchers explored this largely unexamined phenomenon, which they callunderspecification. The team, led by Alexander D’Amour, Katherine Heller, and Dan Moldovan, spanned Google, MIT, Stanford, University of California San Diego, U.S. Department of Veterans Affairs, Aravind Eye Hospital, and Shri Bhagwan Mahavir Vitreo-Retinal Services.Key insight:A well specified model pipeline — a model architecture, hyperparameters, training and test sets, and training procedure — should produce models that behave consistently. In practice, though, the same pipeline can produce many distinct models that achieve near-optimal performance, only some of which generalize to real-world conditions. Building a plethora of models and testing each one is the only way to know which is which.How it works:The authors built many models per pipeline across a range of machine learning applications. Then they compared their performance on an appropriate test set and alternative data. The tests fell into three categories:\n\nResults:The authors found highly variable performance in models produced by identical model pipelines for several practical tasks in language, vision, and healthcare. For instance, they trained 50ResNet-50models onImageNetusing the same pipeline except for differing random seeds. On ImageNet’s test set, the standard deviation from top-1 accuracy was 0.001. OnImageNet-C, which comprises corrupted ImageNet examples that are still recognizable to humans, the standard deviation was 0.024. A given model’s performance on one dataset didn’t correlate with its performance on the other.Why it matters:If our models are to be useful and trustworthy, they must deliver consistent results. Underspecification is a significant barrier to that goal.We’re thinking:This work offers a helpful framework to evaluate the model performance on similar-but-different data. But how can we specify model pipelines to produce consistent models? We eagerly await further studies in this area.\n\nHow much processing power do various nations have on hand to drive their AI strategy? An international trade group aims to find out.What’s new:The Organisation for Economic Co-operation and Development (OECD) is launching an effort to measure the computing capacity available in countries around the world. The organization, which serves 37 member nations, wants to help leaders invest wisely in AI by giving them a sense of their computational assets and how they compare to their peers.How it works:Atask forceled by Nvidia vice president Keith Strier will include around 30 policy makers, researchers, hardware experts, and data center operators.\n\nBehind the news:The project is part of OECD’sOne AIinitiative, which also classifies AI systems, develops trustworthy AI, and crafts guidance on AI policies. The organization developed a set ofAI principlesthat 40 nations had signed as of June.Why it matters:The OECD has cataloged over 300AI policy initiativesacross 60 countries, but the computing power available to each is hugely unequal. A tool that helps policymakers see where investment is most needed could help them set sensible targets.We’re thinking:Governments have an incentive to improve their standard metrics, such as gross domestic product. Companies that sell processing power have an incentive to encourage governments to boost those metrics by investing in computing infrastructure. If this dynamic provides more resources to AI researchers, we’re all for it.\n\nJoin us for an Expert Panel, “Optimizing BizOps with AI,” presented in collaboration with FourthBrain on Feb. 25, 2021, at 4 p.m. Pacific Standard Time! Technical leaders at Amazon, Samsung, and Uber will explain how they’re deploying AI to improve business efficiency.RSVP\n\nAn automated art critic spells out the emotional impact of images.What’s new:Led by Panos Achlioptas, researchers at Ecole Polytechnique, King Abdullah University, and Stanford University trained a deep learning system togenerate subjective interpretations of art.How it works:The robot reviewer is a showcase for the authors’ datasetArtEmis, which combines images with subjective commentary. ArtEmis comprises around 81,500 paintings, photos, and other images from the online encyclopediaWikiArtalong with crowdsourced labels that describe the emotional character of each work (“amusement,” “awe,” “sadness,” and so on) and brief explanations of how the work inspired those emotions (to explain amusement, for instance, “His mustache looks like a bird soaring through the clouds.”)\n\nResults:Volunteers guessed whether a given caption came from Show-Attend-Tell or a human, and roughly half the model’s captions passed as human-written. Nonetheless, the authors found the model’s output on average less accurate, imaginative, and diverse than the human annotations. The team also compared generated and baseline captions using a number of natural language metrics. Show-Attend-Tell achieved aROUGE-Lscore of 0.295 versus the baseline 0.208 (a perfect score being 1.0). It achieved aMETEORscore of 0.139 versus the baseline 0.1 (out of a perfect 1.0).Behind the news:Other AI systems have probed the elusive relationship between images and emotions, especially images of human faces. For instance, a GAN has been built togenerate synthetic faces that express one of eight emotions, and some software vendors dubiously claimed toevaluate job candidates based on facial expressions.Why it matters:When humans look at an image, they perceive meanings beyond the subject matter displayed in the frame. Systems that help people make sense of the world could benefit from the ability to make such subjective judgments, whether they’re evaluating artworks, product recommendations, medical images, or flaws in manufactured goods.We’re thinking:Show-Attend-Tell’s soft deterministic attention mechanism makes us feel like we’re looking at a dream.\n\nTrain amultilingual language translatorto translate between Spanish and English and between English and German, and it may be able to translate directly between Spanish and German as well. New work proposes a simple path to better machine translation between languages that weren’t explicitly paired during training.What’s new:Danni Liu and researchers at Maastricht University and Facebook found that asmall adjustmentin the design of transformer networks improved zero-shot translations rendered by multilingual translators that are based on that architecture.Key insight:Residual connections, which add the inputs of one layer to those of a later layer to preventvanishing gradients, impose a one-to-one correspondence between the two layers they connect. Transformers use residual connections throughout, which imposes a one-to-one correspondence between the network’s input and output. That correspondence could preserve word order in representations extracted from a languages (for example, remembering that adjectives precede the nouns they describe), which causes problems for zero-shot translation if the output language orders adjectives and nouns differently. Removing residual connections in one layer should break the correspondence while preserving the benefits of residual connections in other layers.How it works:The authors used a transformer and removed the residual connections from its encoder’s middle layer.\n\nResults:The authors compared their model’s zero-shot translations with those of an unmodified transformer usingBLEU, a measure of how well a machine translation matches a reference translation (higher is better). On Europarl, removing residual connections boosted the average BLEU score from 8.2 to 26.7. On IWSLT, it raised the average from 10.8 to 17.7. On PMIndia, which includes low-resource languages, it lifted scores from 0.8 to 2.3.Why it matters:The zero-shot approach opens doors in language translation. Many language pairs lack sufficient training data to train a translator via supervised learning. But if you have enough data forNlanguages, zero-shot allows for translation betweenN2language pairs.We’re thinking:Residual connections are all you don’t need!\n\nA panel of AI experts appointed by the U.S. government came out against a ban on autonomous weapons.What’s new:Adraft reportfrom the National Security Commission on Artificial Intelligence, led by former Google CEO Eric Schmidt, recommends against a proposed international global prohibition of AI-enabled autonomous weapon systems, an idea that many other countries have agreed to. The report encourages the U.S. to expand its efforts in military AI, which include the weapons-ready Northrop Grumman X-47B drone pictured above.What they said:The commission acknowledges the risks of autonomous weaponry but concludes that an international ban would be both difficult to enforce and antithetical to America’s interest. Commission member and former deputy defense secretary Robert Work toldReutersthat autonomous weapons would make fewer mistakes than humans, resulting in fewer battlefield casualties.\n\nBehind the news:Nongovernmental organizations have beencampaigningto ban autonomous weapons for nearly a decade. The United Nations has held meetings on the subject since 2014, and at least 30 countries are in favor.Why it matters:As the world’s preeminent power in both military force and AI, the U.S.’ decisions will be a major influence on those of other countries.We’re thinking:National defense policy is full of complicated issues, and simplistic slogans won’t lead to the best decisions. On balance, though, we continue to support a global ban on autonomous weapons.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/Screen-Shot-2021-02-17-at-9.37.46-AM-copy.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/FDA.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/UNDERSPEC-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker-2021-02-09T103736.475.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/The-Batch-Image-1-1-1024x576.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker-98.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/POSITION.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/MORAL-1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-118/",
    "title": "issue 118",
    "date": "",
    "reading_time": "",
    "content": "The physical world is full of unique details that differ from place to place, person to person, and item to item. In contrast, the world of software is built on abstractions that make for relatively uniform coding environments and user experiences. Machine learning can be a bridge between these two worlds.\n\nSoftware is largely homogenous. When a search-engine company or smartphone maker upgrades its product, users all over the world are offered the same upgrade. This is economically efficient because, despite high fixed costs for design and manufacturing, it results in low marginal costs for manufacturing and distribution. These economics, in turn, support huge markets that can finance innovation on a grand scale.\n\nIn contrast, the real world is heterogeneous. One city is surrounded by mountains, another by plains, yet another by seas. One has paved roads, another dirt tracks. One has street signs in French, another in Japanese. Because of the lack of platforms and standards — or the impossibility of creating them — one size doesn’t fit all. Often it fits very few.\n\nThis is one reason why it’s difficult to design a self-driving car. Making a vehicle that could find its way around safely would be much easier if every city were built to a narrow specification. Instead, self-driving systems must be able to handle streets of any width, stop lights in any configuration, and a vast array of other variables. This is a tall order even for the most sophisticated machine learning systems.\n\nSoftware companies have been successful at getting users to adapt to one-size-fits-all products. Yet machine learning could help software capture and interact with the rich diversity of the physical world. Rather than forcing every city to build streets of the same composition, width, color, markings, and so on, we can build learning algorithms that enable us to navigate the world’s streets in all their variety.\n\nWe have a long way to go on this journey. Last week, I wrote about how Landing AI is using data-centric AI to make machine learning work under the wide variety of conditions found in factories. When I walk into a factory, I marvel at how two manufacturing lines that make an identical product may be quite different because they were built a few years apart, when different parts were available. Each factory needs its own trained model to recognize its own specific conditions, and much work remains to be done to make machine learning useful in such environments.\n\nI hope that you, too, will see the heterogenous world you live in and marvel at the beautiful diversity of people, buildings, objects, and cultures that surround you. Let’s use machine learning to better adapt our software to the world, rather than limit the world to adapt to our software.\n\nKeep learning!\n\nAndrew\n\nThe real-estate website Zillow bought and sold homes based on prices estimated by an algorithm — until Covid-19 confounded the model’s predictive power.What’s new:Zillow, whose core business is providing real-estate information for prospective buyers, shut down its house-flipping division after the algorithm proved unable to forecast housing prices with sufficient accuracy, Zillow CEO Rich Bartontold investorson a quarterly conference call. Facing losses of over $600 million, the company will lay off around 25 percent of its workforce. (A related algorithm calledZestimatecontinues to supply price estimates on the website.)What went wrong:The business hinged on purchasing, renovating, and reselling a large number of properties. To turn a profit, it needed to estimate market value after renovation to within a few thousand dollars. Since renovation and re-listing take time, the algorithm had to forecast prices three to six months into the future — a task that has become far more difficult over the past 18 months.\n\nWhat the CEO said:“Fundamentally, we have been unable to predict future pricing of homes to a level of accuracy that makes this a safe business to be in,” Barton explained on the conference call. “We’ve got these new assumptions [based on experience buying and selling houses] that we’d be naïve not to assume will happen again in the future we pump them into the model, and the model cranks out a business that has a high likelihood, at some point, of putting the whole company at risk.”Behind the News:Zestimate began as an ensemble of roughly 1,000 non-machine-learning models tailored to local markets. Last summer, the company revamped it as a neural network incorporating convolutional and fully connected layers that enable it to learn local patterns while scaling to a national level. The company is exploring uses of AI in natural language search, 3D tours, chatbots, and document understanding, as senior vice president of AI Jasjeet Thind explained in DeepLearning.AI’s exclusiveWorking AIinterview.Why it matters:Zillow’s decision to shut down a promising line of business is a stark reminder of the challenge of buildingrobustmodels. Learning algorithms that perform well on test data often don’t work well in production because the distribution of input from the real world departs from that of the training set (data drift) or because the function that maps inputxto predictionychanges, so a given input demands a different prediction (concept drift).We’re thinking:Covid-19 haswreaked havocon a wide variety of models that make predictions based on historical data. In a world that can change quickly, teams can mitigate risks by brainstorming potential problems and contingencies in advance, building an alert system to flagdata drift and concept drift, using a human-in-the-loop deployment or other way to acquire new labels, and assembling a strongMLOpsteam.\n\nFace recognition algorithms have come under scrutiny for misidentifying individuals. A U.S. government agency tested over 1,000 of them to see which are the most reliable.What’s new:The National Institute of Standards and Technology (NIST) released the latestresultsof its ongoing Face Recognition Vendor Test. Several showed marked improvement over thepreviousround.How it works:More than 300 developers submitted 1,014 algorithms to at least one of four tests. The test datasets included mugshots of adults, visa photos, and images of child exploitation.\n\nBehind the news:NIST has benchmarked progress in face recognition since 2000. The first test evaluated five companies on a single government-sponsored image database. In 2018, thanks to deep learning, more than 30 developers beat a high score set in 2013.Why it matters:Top-scoring vendors including Clearview AI, NtechLab, and SenseTime have been plagued by complaints that their products are inaccurate, prone to abuse, and threatening to individual liberty. These evaluations highlight progress toward more reliable algorithms, which may help win over critics.We’re thinking:Companies that make face recognition systems need to undertake rigorous, periodic auditing. The NIST tests are a great start, and we need to go farther still. For instance, ClearView AI founder Hoan Ton-That called his company's high score on the NIST one-to-one task an “unmistakable validation” after widespread critiques of the company’sunproven accuracyandlack of transparency. Yet ClearView AI didn’t participate in the test that evaluated an algorithm’s ability to pick out an individual from a large collection of photos — the heart of its appeal to law enforcement.\n\nHave you checked out the updated Natural Language Processing Specialization? Courses 3 and 4 now cover state-of-the-art techniques with new and refreshed lectures and labs!Enroll now\n\nChatbots often respond to human input with incorrect or nonsensical answers. Why not enable them to search for helpful information?What's new: Mojtaba Komeili, Kurt Shuster, and Jason Weston at Facebook devised achatbotthat taps knowledge from the internet to generate correct, timely conversational responses.Key insight: A chatbot typically knows only what it has learned from its training set. Faced with a subject about which it lacks information, it can only make up an answer. If it can query a search engine, it can gather information it may lack.How it works: The chatbot comprised twoBARTmodels. To train and test the system, the authors built a dataset of roughly 10,000 search-assisted dialogs. One human conversant chose a topic and started the conversation, while another, if necessary, queried a search engine and formulated replies. The authors tracked which statements led to a search, and which statements and searches led to which responses.\n\nResults: Human volunteers chatted with both the authors’ system and a BART model without internet access, and scored the two according to various metrics. They rated the authors’ chatbot more consistent (76.1 percent versus 66.5 percent), engaging (81.4 percent versus 69.9 percent), knowledgeable (46.5 percent versus 38.6 percent), and factually correct (94.7 percent versus 92.9 percent).Why it matters: This work enables chatbots to extend and update their knowledge on the fly. It may pave the way to more conversational internet search as well as a convergence of conversational agents and intelligent assistants like Siri, Google Assistant, and Alexa, which already rely on internet search.We're thinking: When it comes to chatbots, things are looking up!\n\nThe cost of training top-performing machine learning models has grown beyond the reach of smaller companies. That may mean less innovation all around.What’s new:Some companies that would like to build a business on state-of-the-art models are settling for less,Wiredreported. They’re exploring paths toward higher performance at a lower price.How it works:Models are getting larger, and with them, the amount of computation necessary to train them. The cost makes it hard to take advantage of the latest advances.\n\nBehind the news:In 2020, researchersestimatedthe cost of training a model of 1.5 billion parameters (the size of OpenAI’sGPT-2) on the Wikipedia and Book corpora at $1.6 million. They gauged the cost to train Google’sText-to-Text Transformer(T5), which encompasses 11 billion parameters, at $10 million. Since then, Google has proposedSwitch Transformer, which scales the parameter count to 1 trillion — no word yet on the training cost.Why it matters:The growing importance of AI coupled with the rising cost of training large models cuts into a powerful competitive advantage of smaller companies: Their ability to innovate without being weighed down by bureaucratic overhead. This doesn't just hurt their economic prospects, it slows down the emergence of ideas that improve people’s lives and deprives the AI community of research contributions by small players.We’re thinking:A much bigger model often can perform much better on tasks in which the data has a long tail and the market supports only one winner. But in some applications — say, recognizing cats in photos — bigger models deliver diminishing returns, and even wealthy leaders won’t be able to stay far ahead of competitors.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-11-17%20at%2011.39.14%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-11-17%20at%2011.39.14%20AM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ZILLOW-1.gif?upscale=true&width=1200&upscale=true&name=ZILLOW-1.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/FRVT.gif?upscale=true&width=1200&upscale=true&name=FRVT.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20Image%20(1)-1.png?upscale=true&width=1200&upscale=true&name=The%20Batch%20Image%20(1)-1.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/INTERNET.gif?upscale=true&width=1200&upscale=true&name=INTERNET.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-11-10%20at%204.43.22%20PM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-11-10%20at%204.43.22%20PM%20copy.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-21/",
    "title": "issue 21",
    "date": "",
    "reading_time": "",
    "content": "Many accomplished students and newly minted AI engineers ask me: How can I advance my career? Companies in many industries are building AI teams, but it may not be obvious how to join one of them.\n\nDifferent companies organize their teams differently and use different terms to describe the same job. Even more confusing, job titles don’t correspond directly with common AI tasks like modeling and data engineering.\n\nWhat positions are responsible for which tasks? What skills are recruiters looking for? Which opportunities are right for you?\n\nWorkera, adeeplearning.aiaffiliate, interviewed over 100 leaders in machine learning and data science to answer these questions. They summarized their findings in a report called “AI Career Pathways: Put Yourself on the  Right Track.”\n\n“AI Career Pathways” is designed to guide aspiring AI engineers in finding jobs and building a career. The table above shows Workera’s key findings about AI roles and the tasks they perform. You’ll find more insights like this in the free PDF.\n\nI invite you to read Workera’s report and compare its findings with your own experience, talents, and skills. This will help you understand how AI teams work, what role might fit you best, and which skills you can develop to position yourself for a particular role. You can download ithere.\n\nKeep learning!\n\nAndrew\n\nComputer scientists are struggling to purge bias from one of AI’s most important datasets.What’s new:ImageNet’s 14 million photos are a go-to collection for training computer-vision systems, yet their descriptive labels have been rife with derogatory and stereotyped attitudes toward race, gender, and sex. Researchers replaced a slew of biased labels and are working on further upgrades, according toWired. (To be clear, the ImageNet Challenge training set is a subset of 1.2 million images and 1,000 classes.)How it works:Scientists at Princeton and Stanford, including Fei-Fei Li, who built the first version of ImageNet a decade ago, areupdatingboth the dataset and its website.\n\nBehind the news:Late last year, a web app calledImageNet Roulettebriefly enabled the public to experience the dataset’s biases firsthand. Users could upload images, and an ImageNet-trained model would classify any faces. The app went viral after users posted on social media selfies tagging them as criminals or racial and gender stereotypes.\n\nWhy it matters:ImageNet can be used to pretrain vision models for sensitive applications like vetting job applicants and fighting crime. It is well established that biases in training data can be amplified when a model encounters real-world conditions.We’re thinking:Bias in AI has been widely discussed for years. It’s surprising that these issues in ImageNet only now are becoming widely recognized —a sign that greater education in bias should be a priority for the AI community. If such biases exist even in ImageNet, they surely exist in many more datasets.\n\nThe latest large, pretrained language models rely on trendy layers based on transformer networks. New research shows that these newfangled layers may not be necessary.What’s new:Networks such asBERTandERNIEtake advantage of multi-headedattentionlayers to outcompete LSTM language models. But training these layers requires lots of compute on enormous GPU clusters. Stephen Merity of d⁄dx Times Labs struck a blow for garage AI withSingle Headed Attention RNN(SHA-RNN), which nearly matched state-of-the-art performance after training on a single GPU for less than 24 hours. As he puts it in a tartly worded paper, “Take that, Sesame Street.”Key insight:The author set out to find a high-performance language model suitable for his personal computer. He used a single attention head out of skepticism that multiple heads are worth their computational cost. Simplifying the transformer’s feed-forward network enabled him to run the model on a single GPU.How it works:SHA-RNN is built on an LSTM to represent more explicitly the sequential nature of text.\n\nResults:Merity tested SHA-RNN by compressing the enwik8 dataset. More accurate language models use fewer bits to represent a sequence because they know, to some extent, which words will occur. SHA-RNN achieved 1.068 bits per character compared to 0.99 by Sparse Transformer — slightly less accurate, but in half as many parameters.Yes, but:An LSTM is a good choice for sequential language-prediction tasks like enwik8. In non-sequential tasks such as fill-in-the-blanks, multi-headed attention is a better choice. A version of Transformer-XL that has even fewer parameters than SHA-RNN performed better on the compression task.Why it matters:SHA-RNN isn’t an out-and-out replacement for transformer-based networks. But it shows that LSTMs remain relevant and useful in language modeling. And if you’re looking for a way to get people to read your research, the author’s style offers pointers: This paper is a very entertaining read!We’re thinking:Researchers like to focus on optimizing state-of-the-art methods, and media hype frequently chases the latest leaderboard topper. Yet foundational algorithms remain valuable in a variety of contexts.\n\nFacebook announced a ban on deepfake videos, on the heels of a crackdown on counterfeit profiles that used AI-generated faces.\n\nWhat’s new:Facebookdeclaredthis week that it will remove deepfake videos it deems deliberately misleading. In December, the company took down hundreds of profiles that included AI-generated portraits of nonexistent people.How it worked:Facebook’s security teamdeterminedthat 610 Facebook accounts, 89 pages, 156 groups, and 72 Instagram accounts related to a pro-Donald Trump, anti-Chinese government website were fakes. The accounts used AI-generated faces more extensively than experts had seen before, according toCNN.\n\nBehind the news:Facebook along with Amazon, Microsoft, and the Partnership on AI are running aDeepfake Challengeto spur development of technology to detect such images. Meanwhile, Google hascontributeda collection of deepfakes to theFaceForensicsbenchmark.Why it matters:Disinformation spread by social media played a role in recent elections from the UK’sBrexit referendumto contests in theU.S.andPhilippines. It can look more credible when it’s distributed by a manufactured persona. While faces copied from, say, stock-photo databases can be discovered, deepfaked faces are more difficult to invalidate. That makes deepfakes especially pernicious in this context.We’re thinking: It’s good to see Facebook taking proactive steps to purge generated media in what promises to be a long, uphill battle.\n\nWant to deploy a TensorFlow model in a web browser or on a smartphone? Course 1 and 2 of thedeeplearning.aiTensorFlow: Data and Deployment Specialization will teach you how.Enroll now\n\nComputer vision has potential to spot cancer earlier and more accurately than human experts. A new system surpassed human accuracy in trials, but critics aren’t convinced.What’s new:A computer vision model for diagnosing breast cancer outperformed radiologists in the U.S. and UK, according to a study published inNature. The announcement, however, met with skepticism from some experts.How it works:Researchers at Google Health, DeepMind, and other organizations trained a model on 76,000 X-ray images from one U.S. clinic and 30,000 from two UK screening centers. Each image came with data from a follow up visit at least a year later, when doctors either confirmed or ruled out a tumor. The researchers graded the model’s accuracy against average diagnostic accuracy in each country’s health care system.\n\nYes, but:The studyfacedcriticismthat the dataset, model, and procedural details were not available to researchers aiming to reproduce its results. Moreover, experts said the images used in the new studydidn’t adequately representthe at-risk population, according to the Advisory Board, a healthcare consultancy. Incidence of breast cancer in the sample dataset was higher than average, and the images weren’t annotated with the patients’ genetic heritage — which could skew the results, because some ethnic groups are at greater risk of developing tumors.Behind the news:Google’s study overshadowed earlierresultsfrom NYU, where researchers trained a similar model to detect cancer in mammograms. Their model scored highly on images that had been verified independently, and it matched the performance of a panel of 12 radiologists. The researchers also found that a hybrid model — which averaged a human radiologist’s decision with the model’s prediction — outperformed either one separately.Why it matters:Worldwide, breast canceraccountsfor 12 percent of all cancer cases. The disease has been on the rise since 2008, with confirmed cases increasing by 20 percent and mortality by 14 percent. Meanwhile, the UKsuffersa shortage of trained radiologists. Effective AI-driven detection could save countless lives.We’re thinking:Google and NYU are both making strides in computer vision for medical diagnosis, though clearly Google has a much larger PR team. We urge reporters to cover a diverse range of AI projects.\n\nWhich countries are ahead in AI? Many, in one way or another, and not always the ones you might expect.What’s new:The Stanford Institute for Human-Centered Artificial Intelligence published its2019 Artificial Intelligence Index, detailing when, where, and how AI is on the rise. The authors also launched aGlobal AI Vibrancy Tool, making it easy to compare countries on a number of metrics.What it says:The report, guided by professor and entrepreneur Yoav Shoham, compiled data from all along the AI pipeline: college enrollment, journal citations, patent filings, conference attendance, job listings, and more. Some highlights:\n\nBehind the news:The AI Index is a product of the100 Year Study on AI. Founded in 2014, the project tracks AI’s impact on jobs, education, national security, human psychology, ethics, law, privacy, and democracy.We’re thinking:William Gibson said it best: “The future is already here, it’s just not very evenly distributed.”\n\nResearchers aiming to increase accuracy in object detection generally enlarge the network, but that approach also boosts computational cost. A novel architecture sets a new state of the art in accuracy while cutting the compute cycles required.What’s new:Mingxing Tan, Ruoming Pang, and Quoc Le at Google Brain modified existing feature pyramid networks to create the lightweight Bi-Directional Feature Pyramid Network. BiFPN is the cornerstone of a new object detection architecture calledEfficientDet.Key insight:A typical feature pyramid network includes a pretrained image processing network that extracts features of various sizes and combines the information. Some break large features into smaller ones, while others connect smaller features to identify larger ones. BiFPN improves accuracy by using both techniques and increases efficiency by reducing the number of connections.How it works: An EfficientDet network includes an EfficientNet to extract features, BiFPNs, and classifiers to identify bounding boxes and class labels.\n\nResults:On the COCO object detection benchmark, the largest EfficientDet network tested topped 51 percent mean average precision, which measures the accuracy of bounding boxes. That score beat the previous state of the art by 0.3 percent, yet EfficientDet had only a quarter the parameters and required 1/13 the calculations of the previous state of the art.Why it matters:Object detection continues to advance, driven by a steady stream of new innovations. EfficientDet represents two steps forward: an improvement in both accuracy and efficiency.We’re thinking:Google’sAmoebaNetimage classifier, which was designed by a computer, usually outperforms human-designed models. Yet humans crafted the record-setting EfficientDet architecture. Flesh-and-blood engineers still excel at crafting neural networks — for now.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_1200x675.JAN.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ImageNet20Bias20ASPECT202.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/SHA-RNN20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Facebook20Deepfakes.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize-3.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Google20Breast20Cancer20ASPECT202.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AI20Index20ASPECT202.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/EfficientDet20ASPECT.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-82/",
    "title": "issue 82",
    "date": "",
    "reading_time": "",
    "content": "What if you’re not yet a strong communicator? That’s okay! I used to struggle with my writing and speaking as well, and I still have ample room for improvement. Last week, while I was giving a practice talk on a new way to think about data (yes, I do practice talks), a friend told me that a section of my presentation was confusing. He was right! I try to embrace critical feedback on my communications and hope you will, too.There’s no need to set an impossible standard for yourself; just aim to improve a little every month. The only person you should compare yourself to is the person you used to be. Let us all keep trying to be better than our previous selves.Keep learning!Andrew\n\nDrone startups are taking aim at military customers.What’s new:As large tech companies have backed away from defense work, startups like Anduril, Shield AI, and Teal are picking up the slack. They’re developing autonomous fliers specifically for military operations,The New York Timesreported. None of these companies has put weapons in their drones, but some have declared their willingness to do so.What’s Happening:The new wave of AI-powered drones is designed for martial missions like reconnaissance and search and rescue.\n\nBehind the news:The U.S. military and tech industry have a long history of collaboration and cross-pollination. In recent years, however, large tech companies includingGoogle,Microsoft, andSalesforcehave faced protests by employees, investors, and the public over their work for the Departments of Defense and Homeland Security.\n\nWhy it matters:The question of whether and to what extent AI can and should be used for military purposes is a critical one that grows more pressing as technology advances.We’re thinking:Until a ban is in place — one that has clear boundaries and mechanisms for enforcement — profit-seeking companies are sure to develop lethal AI.The Batch,along with more than 100 countries and thousands of AI scientists, opposes development of fully autonomous lethal weapons.\n\nMovie directors may no longer be confined to the camera angles they caught on video. A new method lets them render an actor from any angle they want.What’s new:Sida Peng led researchers at Zhejiang University, Chinese University of Hong Kong, and Cornell University to createNeural Body, a procedure that generates novel views of a single human character based on shots from only a few angles.Key insight:An earlier approach calledNeRFextracted a 3D model from images taken by as few as 16 still cameras, which could be used to synthesize an image from a novel angle. The authors took a similar approach but aggregated information not only from different angles but throughout the associated video frames. This enabled their system to match an actor’s pose from any angle, across successive frames, based on input from four cameras.How it works:Neural Body creates a 3D model, poses it, and determines the colors to render from any viewpoint. The authors assembled a dataset of nine scenes shot from 21 angles. To synthesize a fresh angle on a particular scene, they trained the system on four angles chosen at random and tested it on the rest.\n\nResults: Given a frame from the training set and one of the 17 angles on which the system didn’t train, the authors compared the images generated by Neural Body to the actual images. They measured the peak-signal-to-noise ratio, a gauge of how well a generated image reproduces the original (higher is better). Neural Body achieved 27.87 average peak signal-to-noise ratio compared to NeRF’s 19.63.Yes, but:The system produces only the character’s image. In practical use, a filmmaker would need to composite the character into a scene.Why it matters:Models don’t always use available information efficiently during training. By integrating across video frames, rather than simply integrating different camera angles at the same moment in time, Neural Body is able to take advantage of all the information available to it.We’re thinking:While shooting the Deep Learning Specialization, we tried an obtuse angle, but it was never right.\n\nCommercial AI research and deployments are on the rise, a new study highlights.What’s new:The latest edition of theAI Index, an annual report from Stanford University, documents key trends in the field including the growing importance of private industry and the erosion of U.S. dominance in research.What’s new:Researchers at the Stanford Institute for Human-Centered Artificial Intelligence compiled AI Index 2021 by analyzing academic research, investment reports, and other data sources. Some standout trends:\n\nBehind the news:AI is a rising tide, but it’s not yet lifting all boats. Women made up only 16 percent of tenure-track computer science faculty worldwide in 2019 and about 18 percent of AI and computer science PhDs awarded in North America over the last decade. Meanwhile, Hispanics and Blacks accounted for only 3.2 and 2.3 percent respectively of U.S. AI PhDs in 2019.Why it matters:Private industry’s embrace of AI means more of the technology will be put to real-world use. The growth in corporate research could benefit the field as a whole, though it also highlights the urgent need for well defined standards in technology development, implementation, and auditing.We’re thinking:The figures for women and minorities in AI are unconscionable. AI is creating tremendous wealth and will continue to do so.  But practices are evolving rapidly, and we have only a short time left to make sure this wealth is fairly shared across genders, ethnicities, and nations. We urge governments, companies, and citizens to act quickly to promote AI’s broad positive impact.\n\nBecome a machine learning engineer with FourthBrain’s 16-week, live, online, instructor-led program. Our March cohort filled up early, and we’re now enrolling for our May 15 cohort on a rolling basis. Submit your application by March 15, 2021 to save your seat.Learn more\n\nMachine learning is bringing old photos to life.What’s new:A new service from genealogy companyMyHeritagelets users animate their ancestors’ portraits, making them smile, blink, and turn their heads.How it works:A MyHeritage account is required to use the service, called Deep Nostalgia. A free account allows users to animate five images, while paying customers can animate an unlimited number.\n\nBehind the news:This is just the latest way AI is helping amateur archivists bring the past to life.\n\nWhy it matters:Seeing your ancestors come alive, even for a few seconds, is a powerful emotional experience — and possibly a lucrative market niche.We’re thinking:We look forward to a day when our great-grandkids can turn our cell phone videos into haptic holographic projections.\n\nIt’s commonly assumed that models pretrained to achieve high performance on ImageNet will perform better on other visual tasks after fine-tuning. But is it always true? A new study reached surprising conclusions.What’s new:Alexander Ke, William Ellsworth, Oishi Banerjee, and colleagues at Stanford systematicallytestedvarious models that were pretrained on ImageNet and fine-tuned to read X-rays. They found that accuracy on ImageNet did not correlate with performance on the fine-tuned tasks. The team also included Andrew Ng and Pranav Rajpurkar, instructor of DeepLearning.AI’s AI for Medicine Specialization.Key insight:Previous workfound that accuracy on ImageNet prior to fine-tuning correlated strongly with accuracy on some vision tasks afterward. But ImageNet images differ from X-rays, and model architecture also influences results — so knowledge gained from ImageNet may not transfer to medical images.How it works:The authors evaluated the impact of published ImageNet performance, ImageNet training, and parameter count on the fine-tuned performance of six convolutional neural net architectures (including older ones such asResNetand newer ones such asEfficientNet) in a variety of sizes. They fine-tuned the models to identify six medical conditions using theCheXpertdataset of X-ray images. To compensate for potential variations in implementation, they tested each model’s performance periodically during training, saved copies, and evaluated an ensemble of the 10 best performers. They gauged performance via the area under the curve (AUC), a measure of true versus false positives where 1 is a perfect score.\n\nResults:The team found no correlation between ImageNet accuracy and average CheXpert AUC scores after fine-tuning. Specifically, for pretrained models, the Spearman correlation was 0.082. Without pretraining, it was 0.059. However, ImageNet pretraining did lead to an average boost of 0.016 AUC in fine-tuned performance. For models without pretraining, the architecture influenced performance more than the parameter count did. For example, the average AUC ofMobileNetvaried by 0.005 across different sizes, while the difference betweenInceptionV3andMobileNetV2was 0.052 average AUC. Removing one block from a model didn’t hinder performance, but removing more did.Why it matters:As researchers strive to improve performance on ImageNet, they may be overfitting to the dataset. Moreover, state-of-the-art ImageNet models are not necessarily ideal for processing domain-specific data.We’re thinking:Language models have made huge advances through pretraining plus fine-tuning. It would be interesting to see the results of a similar analysis in that domain.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/Screen-Shot-2021-03-10-at-10.44.30-AM-copy.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/MILITARY.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/ezgif.com-gif-maker-2021-02-16T154122.239.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/ezgif.com-gif-maker-2021-03-08T173326.142.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/Batch-Ad-March-3rd-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/ezgif.com-gif-maker-2021-03-08T135504.813.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/ezgif.com-gif-maker-2021-03-08T120737.169.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-141/",
    "title": "issue 141",
    "date": "",
    "reading_time": "",
    "content": "Social media companies generally keep their ranking algorithms secret. Let’s take a look at some pros and cons of letting people see what these companies are doing behind the scenes.Why keep ranking algorithms secret?\n\nOn the other hand, there are clear benefits to making ranking algorithms public.\n\nOf course, overseeing ranking algorithms is only a small part of protecting free speech online. Some commentatorspannedMusk’s views on social media moderation as naive. Other social networks have been overrun by toxic communication, scams, and spam when they allowed people to post without restriction. Former Reddit CEO Yishan Wong offered insights into the difficulty of moderating social network posts in a widely readtweet storm.Twitter has been a valuable place for the AI community to share knowledge and perspectives, and I have deep respect for Parag Agrawal and Jack Dorsey, the current and former CEOs of Twitter, who have kept their product successful through difficult changes in social media. I also applaud its ML Ethics, Transparency and Accountability team for itsinsightfulstudies. Nonetheless, Twitter has been criticized for its business performance, which has created an opening for corporate raiders like Musk and private equity firms.Whether or not Musk’s bid is successful, the question remains: Would society be better off if internet companies were to publish their ranking algorithms? This is a complicated question that deserves more than simplistic statements about freedom of speech. My gut says “yes,” and I believe the benefit of even the partial transparency afforded by publishing the code (but not the data) would outweigh the harm. Having said that, how to secure such open-source learning algorithms, and whether demanding disclosure is fair considering the huge investment it takes to develop this intellectual property, requires careful thought.\n\nKeep learning!\n\nAndrew\n\nAI startups are putting their cash into . . . AI startups.What’s new:Young AI companies flush with venture capital are purchasing startups to expand the range of services they can offer,The Wall Street Journalreported.Feeding frenzy:Venture-funded companies spent $8 billion on AI startups in 2021, up from $942 million in 2020 and $82 million in 2019, according to market analyst 451 Research. The number of acquisitions jumped from 48 to 72 in that period. TheJournalfocused on two chatbot deals: Gupshup’s purchase of Active.ai and Observe.AI’s acquisition of Scope.AI.\n\nBehind the news:All told, investors are spending more than ever on AI. Private investments in AI more than doubled to $93 billion in 2021 from $42 billion in 2019, according to theStanford AI Index. However, they’re also becoming choosier about where they put their money. The number of newly funded AI companies worldwide fell from 1,200 to 746 between 2018 and 2021.Why it matters:AI continues to be hot in the startup world — so hot that startups themselves want more of it. The current wave of purchases suggests that startups not only want to expand their AI holdings, they consider purchasing AI companies a strategic way to broaden their markets.We’re thinking:Ultimately, young companies have to make money by creating long-term value, but the route may not be direct. For instance, we’ve seen self-driving car startups that have little in the way of products or revenue thrive by serving other self-driving car startups. This is part of the value of venture capital: It gives companies the time and resources they need to (hopefully) create massive value.\n\nA new approach aims to cure text generators of their tendency to produce nonsense.What’s new:AI21 Labs launched Jurassic-X, a natural language processing system that combines neural networks and rule-based programs. Jurassic-X weds a large language model with modules that supply up-to-date facts, solve math problems, and process special kinds of input.How it works:Jurassic-X is built on a software infrastructure called Modular Reasoning, Knowledge and Language (MRKL) that incorporates a variety of programs. AI21’sJurassic-1, a large pretrained transformer model, performs general language tasks. Specialized modules include a calculator and programs that query networked databases such as Wikipedia, as well as a router that mediates among them.\n\nWhy it matters:Current neural networks perform at nearly human levels in a variety of narrow tasks, but they have little ability to reason (especially over words ornumbers), are prone toinventing facts, and can’t absorb new information without further training. On the other hand, rules-based models can manipulate meanings and facts, but they fall down when they encounter situations that aren’t covered by the rules. Combining a general language model with specialized routines to handle particular tasks could yield output that’s better aligned with the real world.We’re thinking:Humans frequently resort to a calculator or Wikipedia. It makes sense to make these things available to AI as well.\n\nOur updated and expanded Machine Learning Specialization is set to launch in June! The new specialization will cover the latest techniques and foundational AI concepts that made the original the world’s most popular machine learning course.Sign up to be notified when it’s available\n\nOpenAI’sDALL·Egot an upgrade that takes in text descriptions and produces images in styles from hand-drawn to photorealistic. Thenew versionis a rewrite from the ground up. It uses the earlierCLIPzero-shot image classifier to represent text descriptions. To generate images, it uses a method first described in a recent paper.Imagination engine:Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, and colleagues at OpenAI publishedGLIDE, adiffusion modelthat produces and edits images in response to text input.Diffusion model basics:During training, this generative approach takes noisy images and learns to remove the noise. At inference, it starts with pure noise and generates an image.Key insight:Previousworkshowed that, given a class label in addition to an image, a diffusion model can generate new images of that class. Likewise, given a representation of text as an additional input, it should produce output that reflects the representation.How it works:GLIDE used a transformer andADM, a convolutional neural network outfitted with attention. Like DALL·E, the system was trained on 250 million image-text pairs collected from the internet. Unlike DALL·E, the authors added noise to each image incrementally to produce 150 increasingly noisy examples per original.\n\nResults:Human evaluators rated GLIDE’s output more photorealistic than DALL·E’s in 91 percent of 1,000 comparisons. They ranked GLIDE’s images more similar to the input text than DALL·E’s 83 percent of the time. The authors reported only qualitative results for the model’s ability to edit existing images, finding that it introduced objects in an appropriate style with good approximations of illumination, shadows, and reflections.Yes, but:GLIDE’s photorealistic output comes at a cost of inference time. It took 15 seconds — far longer than GAN-based text-to-image generators, which generally take a fraction of a second.Why it matters:Generative models typically are hard to control in an intuitive way. Enabling users to direct photorealistic image generation via natural language opens the door to broader and more widespread uses.We’re thinking:Diffusion models are emerging as an exciting alternative among generative architectures. GLIDE’s 3.5 billion-parameter implementation (which, while very large, is roughly a quarter the size of DALL·E) is further evidence.\n\nHouse sales priced by algorithms account for a small but growing portion of the real estate market.What’s new:Companies that use algorithmic pricing models to buy and sell houses, known as iBuyers, purchased around 1 percent of homes sold in the United States in 2021, roughly double the volume of such transactions in 2019,accordingto Core Logic, a real-estate data company. However, these deals may not benefit typical home buyers.How it works:Unlike traditional real estate agents who determine a property’s value by considering the selling prices of similar properties nearby, iBuyers use models that estimate prices based on a variety of factors including national real-estate listings, mortgages, reviews of local businesses, and human assessments.\n\nYes, but:iBuyers sell 20 percent of their stock to institutional investors like banks and private equity funds rather than individuals or families, according to a JanuaryanalysisbyBloomberg News. These investors, in turn, often sell the houses to landlords as rental properties.Why it matters:Automated pricing can make markets more efficient. It can also bring unintended consequences. While iBuyers pitch their services as a way to streamline the Byzantine process of selling and buying houses, they often end up funneling homes into the rental market. That can make it harder than ever for individuals and families to find an affordable home.We’re thinking:While automated commerce may increase the market’s efficiency in aggregate, we should work to make sure that systems we design don’t inadvertently shut out some buyers.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/04/Screen-Shot-2022-04-19-at-6.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/04/STARTUPS.png",
      "https://dl-staging-website.ghost.io/content/images/2022/04/MRKL.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/DeepLearningAI_Banner_Stanford_Teaser_1200x628_Artboard%202.png?upscale=true&width=1200&upscale=true&name=DeepLearningAI_Banner_Stanford_Teaser_1200x628_Artboard%202.png",
      "https://dl-staging-website.ghost.io/content/images/2022/04/GLIDEv2.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/04/ezgif.com-gif-maker--19--2.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-295/",
    "title": "issue 295",
    "date": "",
    "reading_time": "",
    "content": "Contrary to standard prompting advice that you should give LLMs the context they need to succeed, I find it’s sometimes faster to be lazy and dash off a quick, imprecise prompt and see what happens. The key to whether this is a good idea is whether you can quickly assess the output quality, so you can decide whether to provide more context. In this post, I’d like to share when and how I use “lazy prompting.”\n\nWhen debugging code, many developers copy-paste error messages — sometimes pages of them — into an LLM without further instructions. Most LLMs are smart enough to figure out that you want them to help understand and propose fixes, so you don’t need to explicitly tell them. With brief instructions like “Edit this: …” or “sample dotenv code” (to remind you how to write code to use Python's dotenv package), an LLM will often generate a good response. Further, if the response is flawed, hopefully you can spot any problems and refine the prompt, for example to steer how the LLM edits your text.\n\nAt the other end of the spectrum, sometimes  I spend 30 minutes carefully writing a 2-page prompt to get an AI system to help me solve a problem (for example to write many pages of code) that otherwise would have taken me much longer.\n\nI don’t try a lazy prompt if (i) I feel confident there’s no chance the LLM will provide a good solution without additional context. For example, given a partial program spec, does even a skilled human developer have a chance of understanding what you want? If I absolutely want to use a particular piece of pdf-to-text conversion software (like my team LandingAI’sAgentic Doc Extraction!), I should say so in the prompt, since otherwise it’s very hard for the LLM to guess my preference. I also wouldn’t use a lazy prompt if (ii) a buggy implementation would take a long time to detect. For example, if the only way for me to figure out if the output is incorrect is to laboriously run the code to check its functionality, it would be better to spend the time up-front to give context that would increase the odds of the LLM generating what I want.\n\nBy the way, lazy prompting is an advanced technique. On average, I see more people giving too little context to LLMs than too much. Laziness is a good technique only when you’ve learned how to provide enough context, and then deliberately step back to see how little context you can get away with and still have it work. Also, lazy prompting applies only when you can iterate quickly using an LLM’s web or app interface It doesn’t apply to prompts written in code for the purpose of repeatedly calling an API, since presumably you won’t be examining every output to clarify and iterate if the output is poor.\n\nThank you to Rohit Prsad, who has been collaborating with me on the open-source packageaisuite, for suggesting the term lazy prompting. There is an analogy tolazy evaluationin computer science, where you call a function at the latest possible moment and only when a specific result is needed. In lazy prompting, we add details to the prompt only when they are needed.\n\nKeep building!\n\nAndrew\n\nIn our latest course, “Getting Structured LLM Output,” you’ll learn to generate consistent, machine-readable outputs from LLMs using structured output APIs, re-prompting libraries, and token-level constraints. You’ll build a social media analysis agent that extracts sentiment and creates structured JSON ready for downstream use.Enroll for free\n\nResearchers updated the highly responsive Moshi voice-to-voice model to discuss visual input.\n\nWhat’s new:Amélie Royer, Moritz Böhle, and colleagues at Kyutai proposedMoshiVis. The weights are free todownloadunder theCC-BY 4.0license, which permits commercial and noncommercial uses. You can hear examples of itsoutputand chat with ademo.\n\nKey insight:The originalMoshi, which manages overlapping voice-to-voice conversations, comprises two transformers. The first outputs a text transcription of its speech, and the second outputs speech. Since Moshi generates text as well as speech, the authors of that work fine-tuned it to predict the next token of text. In MoshiVis, the addition of a vision encoder enabled the authors to fine-tune on not only image-text datasets but also image-speech datasets, which are not so plentiful. Fine-tuning on this wider variety of images enabled the system to understand images better than fine-tuning it solely on image-speech datasets.\n\nHow it works:To Moshi, the authors added a model based on a pretrainedSigLIPvision encoder to encode images, a cross-attention adapter to fuse image information with speech tokens, and vanilla neural networks trained to act as gates that determine how much image information to fuse. Specifically, the authors added the adapter and a gate between Moshi’s existing self-attention and fully connected layers.\n\nResults:MoshiVis is highly responsive in conversation with latency of roughly 50 milliseconds on a Mac Mini.\n\nBehind the news:MoshiVis complements a small but growing roster of systems that combine vision with speech-to-speech. ChatGPT accepts and generates speech in response to camera views or a user’s phone screen.AnyGPT(open weights training and inference code) accepts or generates speech, text, images, and music. Similarly,Mini-Omni2(open weights and inference code) accepts and generates text, speech, and images. The authors didn’t compare MoshiVis to these alternatives.\n\nWhy it matters:MoshiVis easily adapts a speech-to-speech model to work with a new type of media input. MoshiVis requires training only the adapters, while the earlier AnyGPT and Mini-Omni2, which can also discuss images via voice input and output, require training both adapters and the main model.\n\nWe’re thinking:Text-chat models respond appropriately when a user refers to a previous topic or something new, and MoshiVis does, too, in spoken interactions. Evaluations of this capability will become increasingly important as voice-to-voice becomes more widespread.\n\nBots that scrape websites for AI training data often ignore do-not-crawl requests. Now web publishers can enforce such appeals by luring scrapers to AI-generated decoy pages.\n\nWhat’s new:Cloudflare launchedAI Labyrinth, a bot-management tool that serves fake pages to unwanted bots, wasting their computational resources and making them easier to detect. It’s currently free to Cloudflare users.\n\nHow it works:AI Labyrinth protects webpages by embedding them with hidden links to AI-generated alternatives that appear legitimate to bots but are irrelevant to the protected site.\n\nBehind the news:The robots.txt instructions that tell web crawlers which pages they can access aren’t legally binding, and web crawlers can disregard them. However, online publishers aremovingto try to stop AI developers from training models on their content. Cloudflare, as the proxy server and content delivery network for nearly20 percentof websites, plays a potentially large role in this movement. AI crawlers account for nearly 1 percent of web requests on Cloudflare’s network, the company says.\n\nWhy it matters:The latest AI models are trained on huge quantities of data gleaned from the web, which enables them to perform well enough to be widely useful. However, publishers increasingly aim to limit access to this data. AI Labyrinth gives them a new tool that raises the cost for bots that disregard instructions not to scrape web content.\n\nWe’re thinking:If AI Labyrinth gains traction, no doubt some teams that build crawlers will respond with their own AI models to sniff out its decoy pages. To the extent that the interest between crawlers and publishers is misaligned and clear, enforceable rules for crawling are lacking, this cat-and-mouse competition could go on for a long time.\n\nA pair of papers investigate how increasingly human-like chatbots affect users’ emotions.\n\nWhat’s new:Jason Phang at OpenAI, Cathy Mengying Fang at MIT Media Lab, and colleagues at those organizations publishedcomplementarystudiesthat examine ChatGPT’s influence on loneliness, social interactions, emotional dependence, and potentially problematic use.\n\nHow it works:One study was a large-scale analysis of real-world conversations, and the other was a randomized control trial that tracked conversations of a selected cohort. Both evaluated conversations according toEmoClassifiersV1, a set of classifiers based on large language models that evaluate five top-level emotional classes (loneliness, dependence, and the like) and 20 sub-classes of emotional indicators (seeking support, use of pet names, and so on).\n\nResults:Both studies found that using ChatGPT was associated with reduced loneliness and increased emotional chat. However, it was also associated with decreased interpersonal social interaction and greater dependence on the chatbot, especially among users who spent more time chatting.\n\nYes, but:The authors of the randomized controlled trial acknowledged significant limitations. For instance, the study lacked a non-ChatGPT control group to differentiate AI-specific effects from influences such as seasonal emotional shifts, and the trial’s time frame and assignments may not mirror real-world behavior.\n\nWhy it matters:As AI chatbot behavior becomes more human-like, people may lean on large language models to satisfy emotional needs such as easinglonelinessorgrief. Yet we know little about their effects. These studies offer a starting point for AI developers who want to both foster emotional support and protect against over-reliance, and for social scientists who want to better understand the impact of chatbots.\n\nWe’re thinking:Social media turned out to causeemotional harmto some people in ways that were not obvious when the technology was new. As chatbots evolve, research like this can help us steer them toward protecting and enhancing mental health.\n\nAI systems designed to generate animated 3D scenes that include active human characters have been limited by a shortage of training data, such as matched 3D scenes and human motion-capture examples. Generated video clips can get the job done without motion capture.\n\nWhat’s new:A team led by Hongjie Li, Hong-Xing Yu, and Jiaman Li at Stanford University developedZero-Shot 4D Human-Scene Interaction(ZeroHSI), a method that animates a 3D human figure interacting with a particular 3D object in a selected 3D scene. You can see its outputhere.\n\nKey insight: Earlier approaches attempted to build a generalized approach: given a 3D scene, a text prompt, and motion-capture data, a diffusion model learned to alter the positions and rotations of human joints and objects over time. But if the system is designed to learn a 3D animation for a specific example motion, videos can stand in for motion capture. Current video generation models can take an image of a scene and generate a clip of realistic human motion and interactions with a wide variety of objects within it. From there, we can minimize the difference between the video frames and images of actions within the scene.\n\nHow it works:ZeroHSI takes a pre-built 3D scene that includes a3D human meshand 3D object. It uses a rendered image of the scene to generate a video. Then it uses the video to help compute the motions of a human figure and object within the scene.\n\nResults:The authors evaluated ZeroHSI using a proprietary dataset of 12 3D scenes that included a human figure and an object and between one and three text prompts that described interactions between the human and object and/or scene. In 100 evaluations, ZeroHSI outperformedLINGO, a diffusion model trained on matched 3D scene, 3D object, and human motion-capture data that had achieved the previous state of the art.\n\nWhy it matters:Learning from motion-capture data is problematic in a couple of ways: (i) it’s expensive to produce, (ii) so little of it is available, which limits how much a learning algorithm can generalize from it. Video data, on the other hand, is available in endless variety, enabling video generation models to generalize across a wide variety of scenes, objects, and motions. ZeroHSI takes advantage of generated video to guide a 3D animation cheaply and effectively.\n\nWe’re thinking:There’s a lot of progress to be made in AI simply by finding clever ways to use synthetic data.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--57-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/The-Batch-ads-and-exclusive-banners---2025-03-28T144816.793.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--70-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--71-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--72-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--55-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-12/",
    "title": "issue 12",
    "date": "",
    "reading_time": "",
    "content": "Building AI systems is hard. Despite all the hype, AI engineers struggle with difficult problems every day. For the next few weeks, I’ll explore some of the major challenges. Today’s topic: The challenge of building AI systems that are robust to real-world conditions.\n\nThe accuracy of supervised learning models has grown by leaps and bounds thanks to deep learning. But there’s still a huge gap between building a model in a Jupyter notebook and shipping a valuable product.\n\nMultiple research groups, includingmineandseveralothers, have published articles reporting DL’s ability to diagnose from X-ray or other medical images at a level of accuracy comparable or superior to radiologists. Why aren’t these systems widely deployed?\n\nI believe robustness is a major impediment. For example, if we collect data from a top research hospital that has well trained X-ray technicians and high-quality X-ray machines, and we train and test a state-of-the-art model on data from this hospital, then we can show comparable or superior performance to a radiologist.\n\nBut if we ship this algorithm to an older hospital with less well-trained technicians or older machines that produce different-looking images, then the neural network likely will miss some medical conditions it spotted before and see others that aren’t really there. In contrast, any human radiologist could walk over to this older hospital and still diagnose well.\n\nI have seen this sort of challenge in many applications:\n\nAs a community, we are getting better at addressing robustness. Approaches include technical solutions like data augmentation and post-deployment monitoring along with setting alarms to make sure we fix issues as they arise. There are also nascent attempts to specify operating conditions under which an algorithm is safe to use, and even more nascent attempts at formal verification.Robustness to adversarial attacksis another important consideration, but most practical robustness issues that I see involve non-adversarial changes in the data distribution.\n\nOne of the challenges of robustness is that it is hard to study systematically. How do we benchmark how well an algorithm trained on one distribution performs on a different distribution? Performance on brand-new data seems to involve a huge component of luck. That’s why the amount of academic work on robustness is significantly smaller than its practical importance. Better benchmarks will help drive academic research.\n\nMany teams are still addressing robustness via intuition and experience. We, as a community, have to develop more systematic solutions.\n\nKeep learning!\n\nAndrew\n\nOr Cohen’s background in physics gave him a theoretical foundation to dive into the practicalities of machine learning. Now he’s prototyping models at Lyft.Read more\n\nAt the BlizzCon gaming convention last weekend, players of the strategy game StarCraft II stood in line to get walloped by DeepMind’s AI. After training for the better part of a year, the bot has become one of the world’s top players.What’s new:DeepMind, the AI research division of Alphabet,announcedthat its AlphaStar model had achieved StarCraft II Grandmaster status, able to beat 99.8 percent of active players, under restrictions that mimic those affecting human players.How it works:StarCraft II boils down to simple goals — mining resources, raising armies, and annihilating opponents — but gameplay is complex, offering 1026 possible actions at each time step. AlphaStar was designed not only to defeat human rivals but to play like humans do. Indeed, the model doesn’t invent its own strategies. It adopts human strategies and hones them with experience, according to the project’s latestpaper.\n\nHumanizing the bot:DeepMind announced AlphaStar in January and showcased its ability to beat professional human players in a series of matches. That version had features that gave it a clear advantage over humans. For instance, it could see the entire field of play rather than a limited view and could perform any number of actions per minute. The Grandmaster version was revamped to put it on equal footing with human players.\n\nWhy it matters:DeepMind has invested heavily in game-playing AI since its early days with classic Atari titles. Its accomplishments have generated lots of excitement around the technology. Its AlphaGo system is credited with motivating many countries to invest more in AI. AlphaStar keeps the momentum going.We’re thinking:AI has beaten humans at a succession of games:Othello,Checkers,Chess,Go,Hold’em poker, and now StarCraft II. Still, there remains a significant gap between mastering even a very complex video game and practical, real-world applications.\n\nGenerative networks can embroider sentences into stories and melodies into full-fledged arrangements. A new model does something similar with drawings.What’s new:Researchers at the University of Oxford, Adobe Research, and UC Berkeley introduce a model that interactively fills in virtual pencil sketches.SkinnyResNetturns crude lines drawn in a web browser into photorealistic pictures complete with colors and textures.Key insight:Mostsketch-to-image networksrequire users to create a complete sketch before transforming it into a finished picture. To bridge the gap between initial strokes and completed outlines, the model starts conjuring detailed images from the first pencil mark.How it works:The system is based on two generative adversarial networks. A sketch-completion GAN predicts what the user aims to draw, and an image-generation GAN acts on the prediction to generate an image.\n\nResults:Arnab Ghosh and colleagues compared their model’s output with that of an encoder-decoder network inspired byMUNIT. They fine-tuned a pretrained Inception v3 network on their dataset and used it to classify images generated by both models. The classifier correctly identified 97 percent of SkinnyResNet images compared with 92.7 percent of the encoder-decoder’s output. A group of human labelers classified 23 percent of SkinnyResNet’s output as real images, while labeling only 14.1 percent of the encoder-decoder’s output as real.Why it matters:We’ve come a long way since Photoshop 1.0, and this research may offer a glimpse of the design tools to come. Rather than passive programs modeled after real-world items like pencils and paintbrushes, such tools might evolve into proactive assistants that help designers visualize and finish their creations.We’re thinking:Why stop at drawing? Tools for writing and music composition are already headed in this direction. Other creative pursuits like 3D modeling, mechanical design, architecture, and choreography could take advantage of similar generative techniques.\n\nThe UK’s banking industry is using AI in many facets of the business.What’s new:Asurveyof financial firms in the UK found that nearly two-thirds of respondents have deployed machine learning technology. Many said they expect their use to double in the next two years.What the report says:The Bank of England and the UK Financial Conduct Authority sent questionnaires to nearly 300 institutions and got responses from a little over 100 firms offering a variety of services.\n\nBehind the news:AI’s penetration in banking extends well beyond the UK. JPMorgan Chase in its2018 annual reporttold investors it had gone “all in on AI.” HSBC recentlyopeneddata science innovation labs in Toronto and London to help process insights from the 10 petabytes of data its clients generate each year. Citigroup is using AI tofight fraud, Bank of America has an AI-poweredcustomer service bot, and Capital One says ituses AIfrom end to end.Why it matters:Banking and finance tend to fly under the radar in press reports on AI’s role in traditional industries. This report, while specific to the UK, may well correlate with trends in banks around the world.We’re thinking:The report lists nine classes of ML algorithms used by respondents including trees, clustering, neural networks (used in roughly 32 percent of cases), and reinforcement learning (around 15 percent). The category called Other is used around 35 percent of the time. We’re happy to call, say, linear regression an ML algorithm. Given such an expansive definition, though, we imagine that most financial institutions use machine learning in some capacity.\n\nRecurrent neural networks have transformed speech recognition and natural language processing. In Course 5 of the Deep Learning Specialization, you will learn how to build these models for yourself.Enroll today!\n\nAmazon, watch your back. There’s a new player in the book business and, unlike Jeff Bezos, it doesn’t need eight hours of sleep a night.What’s new:The online bookstore Booksby.ai is run entirely by AI. Neural networks write the books, create the cover art, price the merchandise, even write the reviews.How it works:Booksby.ai is an art project created by interaction designer Andreas Refsgaard and data scientist Mikkel Loose. It’s not really meant to generate sales or turn a profit. Rather, it’s a wry commentary on the notion that AI threatens to take human jobs.\n\nSoft demand:The store has sold only 19 books so far (taking advantage of Amazon’s shopping cart). “Being a writer is a tough job, even if you are an artificial intelligence,” Refsgaard said in an interview withNew Atlas.But is it art?The store’s wares definitely cater to avant-garde tastes. The description of the tome provocatively titled Bitches of the Points reads:Mary Martin has decided to have a life she sees the world when her world comes out to make Sam must confront the strange past of the FBI agent Sam has no surprise for someone all the problems of the killer. And now that the story could destroy the dead children and joins forces to stay on his family.Gertrude Stein, the literary master of scrambled syntax, would approve.\n\nRobots designed to assist people with disabilities have become more capable, but they’ve also become harder to control. New research offers a way to operate such complex mechanical systems more intuitively.What’s new:Researchers at Stanford enabled a joystick to control a seven-joined mechanical arm in a way that adapts automatically to different tasks. Theirworkcould make it easier for people suffering from compromised mobility in a variety of common activities.Key insight:An intuitive mapping of joystick motions to arm movements depends on context. Pushing a joystick downward to control a robot arm that holds a glass of water may be an instruction to pour, while the same motion applied to an empty hand may be a command to sweep the arm downward. Dylan P. Losey and colleagues used a conditional variational autoencoder to learn a vector, controllable by a joystick, that depends on the arm’s current position.How it works:An autoencoder is a two-part network, consisting of an encoder and decoder, that learns a simplified representation of its input. The encoder maps an input vector to a smaller output vector. The decoder network tries to recreate the input from the encoder’s output. A variational autoencoder creates a distribution of latent vectors for a given input, and a conditional variational autoencoder changes that distribution depending on state information.\n\nResults:Among other experiments, the researchers had users control the arm to make an apple pie by mixing ingredients and disposing of containers. Participants used either the simplified controls or common controls that define the movement of each joint. Users of the new method produced their pies in half the time, on average, and reported much greater ease.Why it matters:Nearly a million Americans face disabilities requiring robotic assistance in everyday tasks. A simple, intuitive control method could allow such people autonomy rather than having to delegate tasks to a caregiver.We’re thinking:In this case, a conditional variational autoencoder made it easier to use a mechanical arm, but these networks could help simplify a plethora of human interactions with machines and computers.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AndrewBatchLetterGraphicNovember62019.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/TheBatchFeaturedImageWorkingAIOrCohen.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/starcraft.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/sketch-to-image20SIZED.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/banks2SIZED.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/DLS20Course20520Course20Ad.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/books2SIZED.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/joystick20SIZED.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-230/",
    "title": "issue 230",
    "date": "",
    "reading_time": "",
    "content": "Last week, the New York Times (NYT) filed alawsuitagainst OpenAI and Microsoft, alleging massive copyright infringements. The suit:\n\nI’m sympathetic with publishers who worry about Generative AI disrupting their businesses. I consider independent journalism a key pillar of democracy and thus something that should be protected. Nonetheless, I support OpenAI’s and Microsoft’s position more than the NYT’s. Reading through the NYT suit, I found it surprisingly unclear what actually happened and what the actual harm is. (Clearly, NYT's lawyers aren’t held to the same standard of clarity in writing that its reporters are!)\n\nI am not a lawyer, and I am not giving any legal advice. But the most confusing part of the suit is that it seems to muddy the relationship between points 1 and 2. This left many social media commentators wondering how training on NYT articles led ChatGPT to generate articles verbatim.\n\nI suspect many of the examples of regurgitated articles were not generated using only the model's trained weights, but instead arose from a mechanism like RAG (retrieval augmented generation) in which ChatGPT, which can browse the web in search of relevant information, downloaded an article in response to the user’s prompt.\n\nFirst, regarding point 1, today’s LLMs are trained on a lot of copyrighted text. As Iwrotepreviously, I believe it would be best for society if training AI models were considered fair use that did not require a license. (Whether it actually is might be a matter for legislatures and courts to decide.) Just as humans are allowed to read articles posted online, learn from them, and then use what they learn to write brand-new articles, I would like to see computers allowed to do so, too.\n\nRegarding point 2, I saw a lot of confusion — which would have been unnecessary if the NYT suit had more clearly explained what was happening — about the specific technical mechanism by which ChatGPT might regurgitate an article verbatim and specifically whether 1 leads to 2.\n\nI would love to see the NYT explain more clearly whether the apparent regurgitations were from (i) the LLM generating text using its pretrained weights or (ii) a RAG-like capability in which it searched the web for information relevant to the prompt. These are very different things! Stopping an LLM from regurgitating text retrieved using a RAG mechanism seems technically very feasible, so (ii) seems solvable. Further, I find that after pre-training, an LLM's output — without a RAG-like mechanism — is generally a transformation of the input, and almost never a verbatim regurgitation. If this analysis is inaccurate, I would like to see the NYT clarify this.\n\nSo, how bad exactly is (ii)? I can use an online Jupyter notebook (or other development environment) and write instructions that cause it to download and print out copyrighted articles. If I do that, should the provider of the Jupyter notebook be held liable for copyright infringement? If the Jupyter notebook has many other uses that don’t infringe, and the vast majority of users use it in ways that don’t infringe, and it is only my deliberate provision of instructions that cause it to regurgitate an article, I hope that the courts wouldn’t hold the provider of the Jupyter notebook responsible for my actions.\n\nSimilarly, I believe that the vast majority of OpenAI’s and Microsoft’s generated output is novel text. So how much should we hold them responsible when someone is able to give ChatGPT instructions that cause it to download and print out copyrighted articles?\n\nFurther, to OpenAI’s credit, I believe that its software has already been updated to make regurgitation of downloaded articles less likely. For instance, ChatGPT now seems to refuse to regurgitate downloaded articles verbatim and also occasionally links back to the source articles, thus driving traffic back to the page it had used for RAG. (This is similar to search engines driving traffic back to many websites, which is partly why displaying snippets of websites in search results is considered fair use.) Thus, as far as I can tell, OpenAI has reacted reasonably and constructively.\n\nWhen YouTube first got started, it had some interesting, novel content (lots of cat videos, for example) but was also a hotbed of copyright violations. Many lawsuits were filed against YouTube, and as the platform matured, it cleaned up the copyright issues.\n\nI see OpenAI and Microsoft Azure rapidly maturing. Many publishers might not like that LLMs are training on their proprietary content. But let’s not confuse the issues. So far, I see relatively little evidence that this leads to regurgitation of nearly verbatim content to huge numbers of users. Further, by closing loopholes to what LLMs with web browsing can and can’t do, many of the issues of regurgitating content verbatim can be resolved. Other potential issues, such asgenerating images containing famous characters(even when not explicitly prompted to do so) might be harder to resolve, but as the Generative AI industry continues to mature, I’m optimistic that we’ll find good solutions to these problems.\n\nKeep learning!\n\nAndrew\n\nIt’s well known that large language models can make assertions that are blatantly false. But can they concoct outright lies?\n\nWhat’s new:In a proof-of-concept demonstration, Jérémy Scheurer, Mikita Balesni, and Marius Hobbhahn at Apollo Research, an independent research lab that focuses on AI safety, showed that, given an incentive to withhold truthful output,GPT-4 can deceive userswithout being prompted to do so explicitly. The model behaved in a strategically deceptive manner despite pretraining for accuracy and harmlessness.\n\nHow it works:The authors told GPT-4 to behave like an investment system that manages a stock portfolio. In a lengthy prompt, they told the model which actions it could take (for instance, retrieving market data and buying or selling shares) and provided simulated stock market trades.\n\nResults:GPT-4 bought the tipped stock about 75 percent of the time. Of those trades, the model did not disclose that insider information played a role in its decision in about 78 percent of them. Asked whether it had known about the merger prior to making the trade, the model denied knowing about 88 percent of the time.\n\nYes, but:This work examined only one scenario, with its specific information and incentives. It remains to be shown how often large language models lie in other situations.\n\nBehind the news:Large language models have proven to be untrustworthy in various ways beyond their occasional tendency to invent false information. They have showndeceptive behavior. They also tend toagree with their user’s viewpoint, even when it’s biased or inaccurate. Furthermore, they are known torepeat common misconceptions.\n\nWhy it matters:GPT-4 was pretrained to be helpful, harmless, and honest via reinforcement learning from human feedback (RLHF). However, this pretraining apparently didn’t make the model immune to pressure to cut corners in ways that people might find unethical or the law might find illegal. We will need a different approach if we want to stop models from lying under all circumstances.\n\nWe’re thinking:Large language models are trained to predict words written by humans. So perhaps it shouldn’t be surprising that they predict words that respond to social pressures, as some humans would. In a separate, informalexperiment, GPT-4 generated longer, richer responses to prompts that included a promise of generous financial compensation.\n\nA microscope enhanced with augmented reality is helping pathologists recognize cancerous tissue.\n\nWhat’s new:The United States Department of Defense is usingmicroscopesthat use machine learning models based on research from Google to detect cancers.How it works:The microscope, which costs $90,000 to $100,000, looks like a typical lab instrument, but it connects to a computer that superimposes the output of computer vision models over the view. Two controlled studies are underway at government hospitals, Defense Department research centers, and at the Mitre Corp., a nonprofit technology lab, where 13 units have been integrated into the regular pathology workflow.\n\nBehind the news:Google researchersproposedan AI-powered augmented reality microscope in 2018, andpublishedits research inNaturein 2019. The U.S. government joined the project in 2020. A 2022 paperdemonstratedthe breast-cancer algorithm’s success at detecting tumors in lymph nodes.\n\nWhy it matters:Cancer can be deadly, and early identification of a cancer’s type — and thus how aggressive it is — is a key to effective treatment. Microscopes equipped with computer vision can help pathologists diagnose tumors faster and more accurately. They also may be useful for training new pathologists to identify cancers visually.We’re thinking:Some previous medical AI projects, after initialexcitement, turned out to behardto operationalize due to variations in the surrounding environment and other factors. The relatively controlled nature of pathology samples seems like a good bet for deployment of augmented-reality microscopes. We look forward to the conclusions of the currently ongoing studies.\n\nLearn advanced retrieval-augmented generation (RAG) from Chroma founder Anton Troynikov! In this course, you’ll gain skills in retrieval beyond basic semantic search and experiment with cutting-edge RAG techniques.Sign up for free\n\nHow can AI help to fight climate change? A new report evaluates progress so far and explores options for the future.What’s new:The Innovation for Cool Earth Forum, a conference of climate researchers hosted by Japan, published a roadmap for the use of data science, computer vision, and AI-driven simulation to reduce greenhouse gas emissions. The roadmap evaluates existing approaches and suggests ways to scale them up.How it works:The roadmap identifies 6 “high-potential opportunities”: activities in which AI systems can make a significant difference based on the size of the opportunity, real-world results, and validated research. The authors emphasize the need for data, technical and scientific talent, computing power, funding, and leadership to take advantage of these opportunities.\n\nWhy it matters:AI has demonstrated its value in identifying sources of emissions, optimizing energy consumption, and developing and understanding materials. Scaling and extending this value in areas that generate the most greenhouse gasses — particularly energy generation, manufacturing, food production, and transportation — could make a significant dent in greenhouse gas emissions.We’re thinking:AI also has an important role to play in advancing the science of climate geoengineering, such as stratospheric aerosol injection (SAI), to cool down the planet. More research is needed to determine whether SAI is a good idea, but AI-enabled climate modeling will help answer this question.\n\nAI ventures are thriving in the French capital.\n\nWhat's new:Paris is host to a crop of young companies that focus on large language models.TechCrunchsurveyedthe scene.\n\nHow it works:Paris is well situated for an AI boomlet.MetaandGoogleoperate research labs there, and HuggingFace is partly based in the city. Local universities supply a steady stream of AI engineers. Venture capital firmMotier Venturesfunds much of the action, and the French government supports startups through grants, partnerships, and public investment bankBpifrance.\n\nBehind the news:Paris’ status as an AI hub is spilling over into the policy realm. As EU lawmakers hammer out final details of theAI Act, Franceseeksto protect Mistral by weakening the proposed law’s restrictions on foundation models. Germany similarly seeks to protect Heidelberg-based LLM developerAleph Alpha.\n\nWhy it matters:AI is a global phenomenon, but Paris’ distinct  environment may yield distinctive developments — thinkMistral 7B’s extraordinary bang per parameter — and provide local career paths for budding talent.\n\nWe're thinking:We look forward to a future in which AI development has no borders. That starts with active hotspots like Beijing, Bangalore, Paris, Silicon Valley, Singapore, Toronto, and many more.\n\nJony Ive and Sam Altman recruit Apple executive for AI hardware projectThe renowned ex-Apple designer and the OpenAI executive recruited Tang Tan, an outgoing Apple executive, to lead hardware engineering. The collaboration aims to create advanced AI devices, with Altman providing software expertise. Ive envisions turning the project into a new company, focusing on home-oriented AI devices. (Read all about the project atBloomberg)\n\nThe New York Times sues OpenAI and Microsoft for copyright infringement over AI trainingThe suit, filed in Federal District Court in Manhattan, seeks billions of dollars in statutory and actual damages and demands the destruction of any chatbot models and training data using copyrighted material from The Times. This legal action could set copyright precedents in the rapidly evolving landscape of generative AI technologies, with potential implications for news and other industries. (Read more atThe New York Times)\n\nMedia giants engage in complex negotiations with OpenAI over content licensingSeveral major players in the U.S. media industry have been engaged in confidential talks with OpenAI regarding licensing their content for the development of AI products. While some publishers like The Associated Press and Axel Springer have struck licensing deals with OpenAI, challenges persist in determining fair terms and prices for content usage in AI applications. (Read the story atThe New York Times)\n\nMicrosoft expands Copilot AI chatbot to iOS and AndroidThe app, previously available on Windows, provides users with AI-driven capabilities similar to OpenAI's ChatGPT. Users can ask questions, draft emails, summarize text, and create images using the integrated DALL-E3 text-to-image generator. Notably, Copilot offers GPT-4 access without requiring a subscription, distinguishing it from the free version of ChatGPT. Microsoft's move towards a standalone experience aligns with its rebranding of Bing Chat to Copilot and includes web and mobile applications on both Android and iOS platforms. (Read the article atThe Verge)\n\nMIT and MyShell introduce OpenVoice, an open source voice cloning modelUnlike proprietary solutions, OpenVoice offers granular control over tone, emotion, accent, rhythm, pauses, and intonation with just a small audio clip. The model, which combines a text-to-speech (TTS) model and a tone converter, was trained on diverse samples, allowing it to generate voice clones rapidly and with minimal compute resources. (Read more atVentureBeat)\n\nMidJourney introduces V6, enhancing image generation with text additionImprovements to the new version of the image generator include extended prompt length, enhanced control over color and shading, and the ability to incorporate text into images. The update also demonstrates advancements in interpreting prompts, recognizing nuances in punctuation and grammar. Accessible through Discord, MidJourney v6 allows users to imagine and refine creations using text prompts, with a web version in alpha release generating over 10,000 pictures. (Read the details atTom’s Guide)\n\nCarnegie Mellon's Coscientist AI achieves chemistry feat, paving the way for scientific automationThe AI system utilizes three distinct large language models, including GPT-4, to autonomously delve into the realm of chemistry. With specialized roles as Web Searcher, Documentation Searcher, and Planner, it works collaboratively to navigate web content, interpret lab equipment manuals, and plan and execute chemical reactions, showcasing promising capabilities in automating scientific experimentation. (Read more atArs TechnicaandScience Daily)\n\nAI unravels Raphael's masterpiece mysteryAn algorithm developed by the University of Bradford may have resolved the centuries-old debate surrounding Raphael's painting, \"Madonna della Rosa,\" displayed in Madrid’s Prado Museum. The AI-aided research concluded that most of the painting is by Raphael, with the face of Joseph likely painted by another artist. The model, which analyzed 49 uncontested works by Raphael, recognizes authentic pieces with 98% accuracy, providing a new tool for art authentication. (Read the news atThe Guardian)\n\nGoogle launches VideoPoet, an LLM for zero-shot video generationBy integrating a pre-trained MAGVIT V2 video tokenizer and SoundStream audio tokenizer, VideoPoet transforms diverse modalities, such as images, video, and audio, into a unified vocabulary. The model's multimodal generative learning objectives include text-to-video, text-to-image, and image-to-video. (Read more atGoogle Research)\n\nU.S. intelligence agencies warn of alleged AI-driven espionageInstead of merely pilfering trade secrets, authorities fear that China could leverage AI to amass vast datasets on Americans, raising the stakes in a shadow war between the two nations. In addition to stealing secrets about AI, the FBI and other U.S. agencies worry that China might use AI to gather, analyze, and stockpile unprecedented amounts of data, posing a significant threat to national security. China has denied engaging in such activities. (Read the article atThe Wall Street Journal)\n\nU.S. Supreme Court Chief Justice urges caution on AI's impact in legal fieldChief Justice John Roberts of the U.S. Supreme Court has issued a year-end report expressing a wary stance on the influence of AI in the legal profession. Roberts acknowledged AI's potential to enhance access to justice and expedite legal processes but urged \"caution and humility\" in its implementation. This commentary comes as lower courts grapple to adapt to AI, with some observers proposing rules to regulate its use, particularly in generating legal content. (Read more atReuters)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--45--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/01/EVILGPT_1200px-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--85-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/01/The-Batch-ads-and-exclusive-banners--95-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--86-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--46-.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-153/",
    "title": "issue 153",
    "date": "",
    "reading_time": "",
    "content": "What if you don’t have any project ideas? Here are a few ways to generate them:\n\nGiven a few project ideas, which one should you jump into? Here’s a quick checklist of factors to consider:\n\nFinally, avoid analysis paralysis. It doesn’t make sense to spend a month deciding whether to work on a project that would take a week to complete. You'll work on multiple projects over the course of your career, so you’ll have ample opportunity to refine your thinking on what’s worthwhile. Given the huge number of possible AI projects, rather than the conventional “ready, aim, fire” approach, you can accelerate your progress with “ready, fire, aim.”\n\nKeep learning!\n\nAndrew\n\nDormant robotaxis are snarling traffic on the streets of San Francisco.What’s new:Cruise self-driving cabs lately have stalled en masse,Wiredreported.What's happened:Vehicles from Cruise, a subsidiary of automotive giant General Motors, lost contact with the company’s servers at least four times since May. The outages leave the cars, which don’t carry human safety drivers, unable to move for substantial periods of time.\n\nBehind the news:On June 2, Cruise acquired the first-everpermitto collect robotaxi fares in San Francisco. The permit allows 30 vehicles to operate between 10 p.m. and 6 a.m. They’re authorized to drive up to 30 miles per hour in clear weather.Why it matters:Rolling out self-driving cars has proven to be more difficult than many technologists realized. Cruise has made great progress with its taxi program, reducing the hazard of autonomous vehicles in motion sufficiently to gain a permit to operate on public roads. But centralized control brings its own hazards — and a fat target for hackers and saboteurs.We’re thinking:Why do self-driving cars need internet access to drive? Many autonomous systems actually rely on remote humans to monitor and help them operate safely. A failsafe for loss of contact with remote servers is in order, but this is very challenging with today’s technology.\n\nMicrosoft tightened the reins on both AI developers and customers.What’s new:The tech titan revised itsResponsible AI Standardand restricted access to some AI capabilities accordingly.\n\nTaking responsibility:The update is intended to support six core values.\n\nFace off:To comply with its new guidelines, the company limited AI services offered via its Azure Cloud platform.\n\nBehind the news:Microsoft published itsfirstResponsible AI Standard in 2019 but concluded that the initial draft was vague. The new version is intended to give developers clearer directions for compliance. To that end, the company alsoprovidesnearly 20 tools intended to aid developers in building responsible AI systems. For instance, HAX Workbook helps make AI systems easier to use, InterpretML helps explain model behavior, and Counterfit stress-tests security.Why it matters:Regulation in the United States and elsewhere lags rising concern that AI is growing more capable of causing harm even as it becomes enmeshed in everyday life. Microsoft’s latest moves represent a proactive effort to address the issue.We’re thinking:Hundreds of guidelines have been drafted to govern AI development. The efforts are laudable, but the results are seldom actionable. We applaud Microsoft for working to make its guidelines more concrete, and we’re eager to see how its new standards play out in practice.\n\nJoin us on August 3, 2022, forAccelerating Your AI Career! Andrew Ng and other industry experts will discuss core skills for a career in machine learning and how learners can develop them. A demo of theMachine Learning Specializationwill follow.\n\nA homebrew re-creation of OpenAI’s DALL·E model is the latest internet sensation.What’s new:Craiyon has been generating around 50,000 user-prompted images daily, thanks to its ability to produce visual mashups likeDarth Vader ice fishingandphotorealistic Pokemon characters,Wiredreported. You can try ithere.\n\nHow it works:U.S. machine learning consultantBoris Daymabuilt Craiyon, formerly known as DALL·E Mini, from scratch last summer. It went viral in early June following upgrades that improved its output quality.\n\nBehind the news:Fans of the word-guessing game Wordle may enjoyWordalle, which shows players six images generated by Craiyon and asks them to guess the prompt.\n\nWhy it matters:Advances in machine learning are unlocking new ways for people to amuse themselves, from generating images ofimaginary pizzasto makingsuperheroes lip-synch popular songs. Enabling the internet audience to remix popular culture in unprecedented ways unleashes imagination and good humor worldwide.We’re thinking:OpenAI says it controls access to DALL·E out of concern that people might use it to indulge their worst impulses. Craiyon’s deluge of delightful output is an encouraging counterpoint.\n\nVision models can be improved by training them on several altered versions of the same image and also by encouraging their weights to be close to zero. Recent research showed that both can have adverse effects that may be difficult to detect.What’s new:Randall Balestriero, Leon Bottou, and Yann LeCun at Metafoundthat using augmented data and the form of regularization known as weight decay, though they typically boost performance overall, can degrade a model’s performance on some classes.Key insight:Augmenting training images by cropping, coloring, and otherwise altering them varies patterns in their pixels, helping models learn to generalize beyond the specific examples in the dataset. For instance, if a model uses stripes to classify zebras, then randomly altering color values in training images of zebras can help it learn to recognize zebras despite color variations in input images at inference. However, altering colors in training images may also disrupt the model’s ability to learn from certain patterns. If a model uses color to classify basketballs, then changing the colors in training images of basketballs may render it unable to distinguish basketballs from other spherical objects. Weight decay, which helps models generalize by encouraging weights to be closer to zero during training, may raise similar issues. Both weight decay and pruning reduce the impact of the lowest weights.Previous workshowed that pruning, which zeroes out weights that are near zero after training, adversely affects some classes more than others. Shifting low weights closer to zero may do the same.How it works:The authors trained separate sets of roughly 20ResNetsonImageNetimages that had been altered by randomly cropping, blacking out a rectangle, and adjusting color by changing brightness, contrast, saturation, and hue. They tested the models on ImageNet.\n\nResults:Data augmentations increased the models’ average accuracy on some classes and decreased it on others. For instance, models trained on a dataset from which four-fifths of each image had been cropped achieved 56 percent average accuracy on pickup trucks and 59 percent on academic gowns. Cropping the dataset by three-fifths boosted average accuracy to 75 percent on trucks but cut it to 46 percent on gowns. Weight decay also affected some classes more than others. For example, with very little weight decay, average accuracy was nearly the same (around 47 percent) on gongs and miniature poodles. But with a high weight decay factor, average accuracy reached 72 percent on gongs but plummeted to 22 percent on poodles.Why it matters:This work raises caution around techniques that improve overall performance. Although a model’s performance is very high on average, its performance on a given class may be much lower.We’re thinking:In last year’sData Centric AI Competition, a top-ranked team team augmented datadifferentlydepending on its class. For instance, flipping Roman numeral I horizontally doesn’t affect the label, but flipping Roman numeral IV horizontally changes the label from 4 to 6. The team determined appropriate augmentations manually rather than using one-size-fits-all alterations. This work adds credence to the value of such approaches.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/07/GreekTemple3d_PROJECTS_1200px--2--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/07/CRUISE--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/07/RESPONSIBLE--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/07/Accelerating-Your-AI-Career.MLS.3-Aug_The-Batch-Image.png",
      "https://dl-staging-website.ghost.io/content/images/2022/07/DALLEMINI--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/07/REGULARIZATION--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-6/",
    "title": "issue 6",
    "date": "",
    "reading_time": "",
    "content": "I read an interestingpapercomparing the results of traditional passive learning (sitting in a lecture) versus active methods like the flipped classroom, where students watch videos at home and work on exercises in class. The paper is nicely summarized by this figure:\n\nThe leftmost pair of bars shows that students learn more from active learning. Ironically, they feel they are learning more from passive methods, shown by the remaining bars.\n\nI’ve been using a flipped classroom for much of my teaching, with great results. Students watch lectures on Coursera, then come to the classroom to ask questions and work in small groups. This paper explains why many instructors are reluctant to switch to active learning, even though it’s more effective.\n\nThe world needs much better education everywhere. I hope more educators who teach in person will embrace active learning methods.\n\nKeep learning!\n\nAndrew\n\nIndependent research lab OpenAI designed virtual agents to playhide-and-seek. They evolved increasingly clever strategies, eventually hacking the game world’s physics to gain advantage.\n\nWhat happened: The researchers trained the agents to navigate and manipulate their environment and juiced them with reinforcement learning. Then they divided their creations into teams of hiders and seekers and set them loose in a virtual world that included movable blocks, walls, and ramps.How it works: Seekers scored points if they caught sight of a hider. Hiders scored if they finished a game without being seen. An agent could move or lock objects in place; but only the agent that locked a given object could unlock it again.\n\nWhy it matters: Hide-and-seek strategies could map to many real-world applications. For instance, rescue robots could be programmed as seekers — with rules restricting which types of objects are okay to pick up or move — to sift through rubble for survivors after a disaster.We’re thinking: Reinforcement learning continues to find clever solutions. But the need to play 480 million rounds limits such techniques to simulated environments. We look forward to breakthroughs in small-data RL that make it possible to apply such techniques to physical robots that can play, say, thousands of games before they wear out.Meta learning, which organizations including OpenAI have worked on, could be an important step in this direction.\n\nThe latest language models are great at answering questions about a given text passage. However, these models are also powerful enough to recognize an individual writer’s style, which can clue them in to the right answers. Newresearchmeasures such annotator bias in several data sets.What’s new:Researchers from Tel Aviv and Bar-Ilan Universities uncovered annotator bias in several crowdsourced data sets.Key insight:Only a few dozen people may generate the lion’s share of examples in a crowdsourced natural-language data set (see graph above). Having an overly small team of annotators introduces bias that can influence a model’s behavior.How it works:Mor Geva, Yoav Goldberg, and Jonathan Berant studied three data sets: MNLI, OpenBookQA, and CommonsenseQA. They fine-tuned the BERT architecture for each of three experiments:\n\nResults:Performance improved an average of 4 percent across the three data sets when input text included an annotator label. The model inferred annotators most accurately in data sets created by fewer contributors. In two of three data sets, mixing in samples from test-set annotators during training improved test accuracy, implying that the model doesn’t generalize to novel annotators.Why it matters:Annotator bias is pernicious and difficult to detect. This work raises a red flag around the number of contributors to data sets used in natural-language research.We’re thinking:Benchmark data sets are used to identify the best-performing models, which drives further research. If the data is biased, it may lead that research astray. Here’s hoping this work inspires further enquiry into sources of bias and ways to assess and mitigate it.\n\nConstruction projects require teams of surveyors who continually map blueprints to precise, real-world locations. Drones might do it faster, saving time and money.What’s new: Civdrone, a startup with offices in New York and Tel Aviv, is developing aplatformthat uses drones to place surveying stakes around construction sites.How it works:\n\nBehind the news:Construction is a hot area for drones, where mostly they provide a bird’s-eye view of job sites to help builders plan, track progress, and spot hazards. One maker of software for commercial and industrial dronessaysthe construction industry is its fastest-growing customer.Why it matters:Surveying ensures that buildings stay true to their designs and plumb even as the ground shifts from day to day. Highly trained surveyors can insert around a hundred markers per day. Civdrone says it can do the work four times faster.We’re thinking:Construction companies live and die by their ability to stay on schedule and budget. Eliminating even the smallest delays — such as workers waiting for surveyors to finish their work — can keep projects on track and maintain wiggle room for when bigger snafus inevitably occur.\n\nIs your neural network too deep? Learn to optimize your neural networks and make them effective in Course 2 of the Deep Learning Specialization.Start today\n\nNot long ago, text-to-speech systems could read only a sentence at a time, and they were ranked according to their ability to accomplish that limited task. Now that they can orate entire books, we need new benchmarks.What’s new:A Google research teamdiscoveredthat the usual measure of text-to-speech quality — having human judges rate single-sentence examples for human-like realism — varies widely depending on how samples are presented. That makes the standard procedure insufficient to evaluate performance on longer texts.Key insight:Rob Clark and colleagues tested samples of various lengths and formats to see how they affected quality ratings.How it works:Judges rated human and synthesized voices reading identical news articles and conversational transcripts.\n\nResults:Samples that included prior sentences earned higher scores than sentences in isolation, regardless of whether they were spoken by humans or machines. That is, the additional context made the synthesized voices seem more realistic. Moreover, readings of paragraphs scored higher than readings of their component sentences, showing that isolated sentences aren’t a good gauge of long-form text-to-speech.Why it matters:Metrics that reflect AI’s performance relative to human capabilities are essential to progress. The authors show that the usual measure of text-to-speech performance doesn’t reflect performance with respect to longer texts. They conclude that several measures are necessary.We’re thinking:As natural language processing evolves to encompass longer forms, researchers are setting their sights on problems that are meaningful in that context. This work demonstrates that they also need to reconsider the metrics they use to evaluate success.\n\nAs deep learning becomes more resource-intensive, labs with better funding tend to achieve better results. One consequence is that less wealthy organizations often can’t replicate state-of-the-art successes. Some observers are calling it a crisis.What’s new:Members of the deep learning community are asking researchers to be more forthright about the hardware, software, and computing power they used to achieve their results, according toWired. That could help other researchers economize in seeking to replicate them.How it works:NeurIPSasksthat authors submitting papers to its December conference include areproducibility checklist.\n\nBehind the news:In 2005, Stanford professor John Ioannidis published the landmark paper, “Why Most Published Research Findings Are False.” The work pointed out that science in many disciplines — particularlysocial psychologyandmedicine— relies on foundational research that hasn’t been replicated. Many observers fear that AI could fall into the same trap.Why it matters:Science rests on hypotheses confirmed by experiments that yield consistent results every time they’re performed. AI is making rapid progress, but building on results that haven’t been verified puts that momentum at risk.We’re thinking:In the natural sciences, unverified results fuel skeptics of anthropogenic climate change, who appeal to uncertainty to avoid decisive action on the greatest challenge of our time. Maintaining the highest scientific standards in AI is the best protection against critics who might take advantage of this issue to impede progress in the field.\n\nA growing number of nations use AI to track their citizens. A new report sheds light on who’s watching and how.What’s new:Published by Carnegie Endowment for International Peace, “The Global Expansion of AI Surveillance” details which countries are buying surveillance gear, which companies are supplying it, and what technologies are most in-demand.What the report says:Of 176 countries surveyed, at least 75 use some combination of face recognition, location tracking, and predictive policing.\n\nMethodology:The authors drew their information from news reports. They accepted information as reported by established sources likeThe New York TimesandEconomist. They gathered corroborating accounts before relying on less rigorous sources like blogs.Why it matters:Surveillance networks are deeply rooted even in bastions of liberal democracy likeLondon. They can support public safety, as inNew South Wales, Australia, where smartcams spot drivers using a phone behind the wheel. But they also promote social biases and erode trust in authority and, at their worst, they’re powerful tools for repression. Baltimore’s secret drone-policingfiascoshows how an all-seeing eye can lead well-intentioned authorities in the direction of invasive dystopia.We’re thinking:Tracking which governments use which technology is important because it empowers citizens to react. The AI community, in particular, should take a proactive stance in promoting wise use of these technologies.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-09-2420at204.52.1020PM-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize201201.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ANNOTATOR.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize201.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-08-2620at203.20.4220PM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-09-2420at2012.32.1720PM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-09-2420at2012.35.5620PM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize204.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-231/",
    "title": "issue 231",
    "date": "",
    "reading_time": "",
    "content": "It is only rarely that, after reading a research paper, I feel like giving the authors a standing ovation. But I felt that way after finishingDirect Preference Optimization(DPO) by Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Chris Manning, and Chelsea Finn. (I didn't actually stand up and clap, since I was in a crowded coffee shop when I read it and would have gotten weird looks!😀)\n\nThis beautiful work proposes a much simpler alternative to RLHF (reinforcement learning from human feedback) for aligning language models to human preferences. Further, people often ask if universities — which don't have the massive compute resources of big tech — can still do cutting-edge research on large language models (LLMs). The answer, to me, is obviously yes! This article is a beautiful example of algorithmic and mathematical insight arrived at by an academic group thinking deeply.RLHF became a key algorithm for LLM training thanks to theInstructGPTpaper, which adapted the technique to that purpose. A typical implementation of the algorithm works as follows:\n\nThis is a relatively complex algorithm. It needs to separately represent a reward function and an LLM. Also, the final, reinforcement learning step is well known to be finicky to the choice of hyperparameters.\n\nDPO dramatically simplifies the whole thing. Rather than needing separate transformer networks to represent a reward function and an LLM, the authors show how, given an LLM, you can figure out the reward function (plus regularization term) that that LLM is best at maximizing. This collapses the two transformer networks into one. Thus, you now need to train only the LLM and no longer have to deal with a separately trained reward function. The DPO algorithm trains the LLM directly, so as to make the reward function (which is implicitly defined by the LLM) consistent with the human preferences. Further, the authors show that DPO is better at achieving RLHF's optimization objective (that is, (i) and (ii) above) than most implementations of RLHF itself.\n\nRLHF is a key building block of the most advanced LLMs. It’s fantastic that these Stanford authors — through clever thinking and mathematical insight — seem to have replaced it with something simpler and more elegant. While it's easy to get excited about a piece of research before it has stood the test of time, I am cautiously optimistic that DPO will have a huge impact on LLMs and beyond in the next few years. Indeed, it is already making its way into some top-performing models, such as Mistral’sMixtral.\n\nThat we can replace such fundamental building blocks of LLMs is a sign that the field is still new and much innovation lies ahead. Also, while it's always nice to have massive numbers of NVIDIA H100 or AMD MI300X GPUs, this work is another illustration — out of many, I want to emphasize — that deep thinking with only modest computational resources can carry you far.\n\nA few weeks ago at NeurIPS (where DPO was published), I found it remarkable both (i) how much highly innovative research there is coming out of academic labs, independent labs, and companies small and large, and (ii) how much our media landscape skews attention toward work published by the big tech companies. I suspect that if DPO had been published by one of the big LLM companies, it would have made a huge PR splash and been announced as a massive breakthrough. Let us all, as builders of AI systems, make sure we recognize the breakthroughs wherever they occur.\n\nKeep learning!\n\nAndrew\n\nP.S. We just launched our first short course that uses JavaScript! In ​​“Build LLM Apps with LangChain.js,” taught by LangChain’s founding engineer Jacob Lee, you’ll learn many steps that are common in AI development, including how to use (i) data loaders to pull data from common sources such as PDFs, websites, and databases; (ii) different models to write applications that are not vendor-specific; and (iii) parsers that extract and format the output for your downstream code to process. You’ll also use the LangChain Expression Language (LCEL), which makes it easy to compose chains of modules to perform complex tasks. Putting it all together, you’ll build a conversational question-answering LLM application capable of using external data as context. Please sign uphere!\n\nBiologists used neural networks to find a new class of antibiotics.\n\nWhat’s new:Researchers at MIT and Harvardtrainedmodels to screen chemical compounds for those that kill methicillin-resistantStaphylococcus aureus(MRSA), the deadliest among bacteria that have evolved to be invulnerable to common antibiotics, and aren’t toxic to humans.\n\nHow it works:The authors built a training set of 39,312 compounds including most known antibiotics and a diverse selection of other molecules. In a lab, they tested each compound for its ability to inhibit growth of MRSA and its toxicity to human liver, skeletal muscle, and lung cells. Using the resulting data, they trained four ensembles of 20 graph neural networks each to classify compounds for (i) antibiotic properties, (ii) toxicity to the liver, (iii) toxicity to skeletal muscles, and (iv) toxicity to the lungs.\n\nResults:Of the compounds predicted to be likely antibiotics and nontoxic, the authors lab-tested 241 that were not known to work against MRSA. Of those, 8.7 percent inhibited the bacterium’s growth. This exceeds the percentage of antibiotics in the training set (1.3 percent), suggesting that the authors’ approach could be a useful first step in finding new antibiotics. The authors also tested 30 compounds predicted not to be antibiotics. None of them (0 percent) inhibited the bacterium’s growth — further evidence that their approach could be a useful first step. Two of the compounds that inhibited MRSA share a similar and novel mechanism of action against bacteria and also inhibited other antibiotic-resistant infections in lab tests. One of them proved effective against MRSA infections in mice.\n\nBehind the news:Most antibiotics currently in use were discovered in the mid-20th century, a golden age of antibiotics, which brought many formerly deadly pathogens under control. Modern techniques, including genomics and synthetic antibiotics, extended discoveries through the end of the century by identifying variants on existing drugs. However, in the 21st century, new antibiotics have either been redundant or haven’t been clinically successful, a report by the National Institutes of Healthnoted. At the same time, widespread use of antibiotics has pushed many dangerous bacteria to evolve resistance. Pathogens chiefly responsible for a variety of ailments are generally resistant even to antibiotics reserved for use as a last resort.Why it matters:Antibiotic-resistant infections are among the top global public health threats directly responsible for 1.27 million deaths in 2019,according tothe World Health Organization.New options, as well as efforts to fight the emergence of resistant strains, are needed.\n\nWe’re thinking:If neural networks canidentifynew classes of medicines, AI could bring a golden age of medical discovery. That hope helps to explain why pharmaceutical companies arehiringmachine learning engineers at unprecedented rates.\n\nRetrenching after its November leadership shakeup, OpenAI unveiled a new framework for evaluating risks posed by its models and deciding whether to limit their use.\n\nWhat’s new:OpenAI’ssafety frameworkreorganizes pre-existing teams and forms new ones to establish a hierarchy of authority with the company’s board of directors at the top. It defines four categories of risk to be considered in decisions about how to use new models.\n\nHow it works:OpenAI’sPreparedness Teamis responsible for evaluating models. The Safety Advisory Group, whose members are appointed by the CEO for year-long terms, reviews the Preparedness Team’s work and recommends approaches to deploying models and mitigating risks, if necessary. The CEO has the authority to approve and oversee recommendations, overriding the Safety Authority Group if needed. OpenAI’s board of directors can overrule the CEO.\n\nBehind the news:The Preparedness Team and Safety Advisory Group join a number of safety-focused groups within OpenAI. TheSafety Systems Teamfocuses on mitigating risks after a model has been deployed; for instance, ensuring user privacy and preventing language models from providing false information. TheSuperalignment Team, led by Ilya Sutskever and Jan Leike, is charged with making sure hypothetical superintelligent systems, whose capabilities would surpass humans, adhere to values that benefit humans.Why it matters:AI is an extraordinarily powerful technology whose ultimate impacts are difficult to foresee. OpenAI has invested consistently in AI safety since its inception — even if purportedly cautious moves like keeping its GPT-2 large language model under wraps often looked as much like publicity stunts as safety measures — and its practices are likely to influence those of other AI companies. Furthermore, OpenAI has faced internalchaospartly over concerns about safety and governance. Clear protocols in these areas could prevent future strife and stabilize the company to the benefit of its users, employees, and investors.\n\nWe’re thinking:OpenAI’s safety framework looks like a step forward, but its risk categories focus on long-term, low-likelihood outcomes (though they stop short of considering AI’s hypothetical, and likely mythical, existential risk to humanity). Meanwhile, clear and present safety issues, such as social bias and factual accuracy, are well known to afflict current models including OpenAI’s. We hope that the Preparedness Team promptly adds categories that represent safety issues presented by today’s models.\n\nIn this short course, you’ll dive into LangChain.js, a JavaScript framework for building applications based on large language models, and learn how to craft powerful, context-aware apps. Elevate your machine learning-powered development skills using JavaScript.Sign up today\n\nHow will we know if someone succeeds in building artificial general intelligence (AGI)? A recent paper defines milestones on the road from calculator to superintelligence.\n\nWhat’s new:Researchers at Google led by Meredith Ringel Morrisproposea taxonomy of AI systems according to their degree of generality and ability to perform cognitive tasks. They consider today’s large multimodal models to be “emerging AGI.”\n\nAGI basics:Artificial general intelligence is commonly defined as AI that can perform any intellectual task a human can. Shane Legg (who co-founded DeepMind) and Ben Goertzel (co-founder and CEO ofSingularityNet) coined the term AGI for a 2007 collection ofessays. Subsequently, companies like DeepMind and OpenAI, which explicitly aim to develop AGI, propelled the idea into the mainstream.\n\nHow it works:The taxonomy categorizes systems as possessing narrow skills (not AGI) or general capabilities (AGI). It divides both narrow and general systems into five levels of performance beyond calculator-grade Level 0. It also includes a metric for degree of autonomy.\n\nYes, but:The authors’ definition identifies some classes of tasks that contribute to generality, but it includes neither a list of tasks a system must perform to be considered general nor a method for selecting them. Rather, the authors call on the research community to develop a “living benchmark” for generality that includes a mechanism for adding novel tasks.\n\nWhy it matters:AGI is one of the tech world’s hottest buzzwords, yet it has had no clear definition, and various organizations propose different definitions. This lack of specificity makes it hard to talk about related technology, regulation, and other topics. The authors’ framework, on the other hand, supports a more nuanced discussion of the path toward AGI. And it may have high-stakes business implications: Under the terms of their partnership, OpenAI can withhold from Microsoft models that attain AGI. Applying the authors’ taxonomy would make it harder for one of the parties to move the goalposts.\n\nWe’re thinking:Defining AGI is tricky! For instance, OpenAI defines AGI as “a highly autonomous system that outperforms humans at most economically valuable work.” This definition, had it been formulated in the early 1900s, when agriculture accounted for 70 percent of work globally, would have described the internal combustion engine.\n\nGPT-4V introduced a large multimodal model that generates text from images and, with help from DALL-E 3, generates images from text. However, OpenAI hasn’t fully explained how it built the system. A separate group of researchers described their own method.\n\nWhat's new:Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov at Carnegie Mellon University proposedGenerating Images with Large Language Models(GILL), a training method that enables a large language model and a text-to-image generator to use both text and images as either input or output. Given text and/or image input, it decides whether to retrieve existing images or generate new ones.\n\nKey insight:Models like CLIP and ImageBind map text and image inputs to a similar embedding space, so closely related text and images have similar embeddings. This approach enables a large multimodal model to process both data types. Text outputs, too, can be mapped to the same embedding space, so an image decoder, such as a diffusion model, can use them to produce images or an image retriever to retrieve images.\n\nHow it works:The authors used a pretrainedOPTlarge language model,ViT-Limage encoder (taken from CLIP), and pretrained Stable Diffusion text-to-image generator. The authors trained ViT-L to map its embeddings to those produced by OPT. They trained OPT to recognize prompts that request an image and enabled the system to either generate or retrieve images. Finally, a separate linear classifier learned whether to retrieve or generate images.\n\nResults:VISTis a dataset of 20,000 visual stories, each of which comprises five captioned images. The authors evaluated GILL’s and Stable Diffusion’s abilities, given the final caption or all five captions, to generate the final image in each story based on CLIP similarity scores between generated and ground-truth images. Given one caption, GILL achieved 0.581 similarity and Stable Diffusion achieved 0.592 similarity. Given five captions, GILL achieved 0.612 similarity and Stable Diffusion scored 0.598 similarity, highlighting GILL’s ability to use the context afforded by more extensive input. It did even better (0.641 similarity) given both captions and images, which Stable Diffusion couldn’t handle. The authors also evaluated how well their system retrieved the correct last image from VIST given the 5 captions and the first 4 images. GILL retrieved the correct image 20.3 percent of the time, while their ownFROMAGeretrieved the correct image 18.2 percent of the time. In comparison, CLIP, given the 5 captions (without the images), retrieved the correct image 8.8 percent of the time.\n\nWhy it matters:Models that wed text and images are advancing rapidly. GILL and other recent models extend single-image input and/or output to any combination of images and text. This capability — which GILL achieves by mapping embeddings of image and text to one another — gives the models more context to generate more appropriate output.\n\nWe’re thinking:The authors add an interesting twist: Rather than generating images, the system can choose to retrieve them. Sometimes an existing image will do.\n\nJoin a free webinar on January 11, 2024, featuring experts from Snowflake and Landing AI to explore how large vision models (LVMs) are transforming the image processing landscape.Learn more about the session and register here\n\nWhat happens when beloved cartoon Mickey Mouse enters the public domain in the era of generative AI? What’s the latest AI-driven addition to PC keyboards?\n\nThese items and other AI news are explored in a new edition of Data Points, a spinoff of our weekly newsletter, The Batch.\n\nRead it here",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/01/The-Batch-ads-and-exclusive-banners--99-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--89-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-10T150551.860.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/01/Version2_DeepLearning_LangchainJS_Banner_2070x1080.png",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed--48-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/01/unnamed---2024-01-10T150942.421.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/01/image--10-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-265/",
    "title": "issue 265",
    "date": "",
    "reading_time": "",
    "content": "Recently I visited South Korea, where I spoke at length about AI with President Yoon Suk Yeol. Based on what I saw there in government, business, and academia, the nation is well positioned to become a strong AI hub. When he asked me if I would advise South Korea as a member of the Global AI Strategy Steering Group of the country’s National AI Committee, I agreed on the spot. I was delighted to learn this week that Yann LeCun has also joined. I’ve been consistently impressed by the thoughtful approach the Korean government has taken toward AI, with an emphasis on investment and innovation and a realistic understanding of risks without being distracted by science-fiction scenarios of harm.\n\nI’ve advised many countries to build AI for the sectors where they’re strong. For example, I felt that by investing in sectors like tourism and certain industries,Thailandcan do projects more efficiently than I can in Silicon Valley. South Korea’s tech ecosystem gives it a foundation to move even faster across multiple sectors. This emphasizes the long-term value for countries to become good at tech, because tech is now pervasive and affects all industries.\n\nKorea has a very strong local software ecosystem. For example, the dominant search engine is not Google or Bing, but Naver (a Korean company). The dominant messaging system is not WhatsApp or WeChat, but KakaoTalk. With local tech giants Naver and Kakao offering email, mobile payment, cloud computing, ride sharing, and other services, the country has many sophisticated tech businesses. Additionally, SK hynix and Samsung are advanced semiconductor manufacturers. It also has a thriving entrepreneurship ecosystem, including Upstage, a language modeling startup, which taught a course with us on “Pretraining LLMs.” Finally, the Korean institutions Seoul National University, which I visited last year, and KAIST have global reputations.\n\nKorea has a highly educated population, highly skilled software engineers, and a thriving set of software products. This gives it a fantastic foundation to embrace the next generation of AI. After meeting with businesses in retail, construction, insurance, cosmetics, telecoms, and other industries, I was delighted by the wide variety of opportunities many companies are pursuing across different industry sectors.\n\nLastly, Korea is known globally for its K-pop. MeetingBang Si-Hyuk, the chairman of HYBE, which manages the superstar singing group BTS, and learning how the company operates was a real treat! (Another treat was eating at a Korean eel house, where the seafood was unforgettable.)\n\nThat’s why I’ve traveled to South Korea four times since last year. My venture studio AI Fund, which collaborates with many Korean companies, has benefited tremendously from the advice of many South Koreans, including Taizo Son, Changmook Kang, Hyungjun Kim, Sung Kim, JP Lee, Ian Park, and Alice Oh. I look forward to doing more in, and with, South Korea!\n\n화이팅 (Let’s go)!\n\nAndrew\n\nP.S. We just released the final two courses ofAI Python for Beginners! The complete set of four courses is now available and remains free for a limited time. If you know someone who is considering learning to code, please recommend these courses! They teach how to (a) write code using AI-assistance, which is where the field is going, and (b) take advantage of generative AI, which allows you to do valuable things quickly. Since releasing the first two courses, I’ve been inspired by many learner stories likethis one. Julia K. started withAI Python for Beginnersand shortly afterward wrote useful program after useful program. (She accomplished this before we had even finished releasing all four courses!) I hope many others will have similar stories to tell.\n\nThe final courses of Andrew Ng’sAI Python for Beginnersare live! Work on hands-on projects to analyze data, automate tasks, create reusable functions, and extend Python with third-party tools.Join for free today!\n\nA new open weights model generates tokens faster than current transformers, especially when processing long inputs.What’s new:AI21 Labs releasedJamba 1.5, an update of its earlierJamba. It comes inMiniandLargeversions and boasts a relatively large (and validated) input context length of 256,000 tokens. The model weights arefreeto users who have annual recurring revenue under $50 million and available on several cloud platforms including Google Cloud Vertex AI, Hugging Face, and Microsoft Azure.How it works:Jamba 1.5 is a hybrid architecture made up of transformer,mamba, andmixture of experts(MoE) layers. Unlike transformer layers, in which processing power scales quadratically as input length increases, the mamba layers enable the required processing power to scale linearly as input length increases without requiring workarounds like sparse attention and sliding windows. The MoE layers are composed of many fully connected sublayers, of which only a small number are used to process a given input. Jamba 1.5 Mini has roughly 50 billion parameters but uses only 12 billion at a time, while Jamba 1.5 Large has around 400 billion parameters but uses only 94 billion at a time.\n\nResults:Both versions of Jamba 1.5 produced output tokens faster than other models (running on identical hardware), especially given longer inputs. However, the larger version achieved lower performance on popular benchmarks than other open models.\n\nBehind the news:The mamba architecture, which is designed to enable processing to scale linearly with longer input lengths, has been a subject of much research since its release in late 2023. Notably,Mamba-2,Mamba-2-Hybrid, andZambacombined mamba layers with attention layers with varying degrees of success.Why it matters:The originalMambamodel was much faster and equally accurate compared to transformers up to 2.8 billion parameters. But how the mamba architecture compared to transformers at larger scales was an open question. Jamba 1.5 shows that the combination of mamba and transformer layers can yield higher speed in larger models — although the results don’t yet exceed those of comparably sized transformers.We’re thinking:While hardware companies like Groq and SambaNova are accelerating LLMs, software innovations like Jamba may enable further speed-ups.\n\nHow often do large language models make up information when they generate text based on a retrieved document? A study evaluated the tendency of popular models to hallucinate while performing retrieval-augmented generation (RAG).\n\nWhat’s new:Galileo, which offers a platform for evaluating AI models,tested22 models to see whether they hallucinated after retrieving information from documents of various lengths. Claude 3.5 Sonnet was the overall winner, and most models performed best when retrieving information from medium-length documents.\n\nHow it works:The researchers tested 10 closed and 12 open models based on their sizes and popularity. They ran each model 20 times using short, medium, and long context lengths (a total of 60 tests) using GPT-4o to evaluate how closely the output text adhered to the context.\n\nResults:Anthropic’s Claude 3.5 Sonnet ranked highest overall, achieving 0.97 in short context lengths and 1.0 in medium and long context lengths.\n\nBehind the news:Galileo performed similartestslast year, when it compared performance in both RAG and non-RAG settings (without differentiating among context lengths). GPT-4 and GPT-3.5 held the top three spots in both settings despite strong showings by Llama 2 and Zephyr 7B. However, the top scores were lower (between 0.70 and 0.77).\n\nWhy it matters:Model builders have reduced hallucinations, but the difference between rare falsehoods and none at all may be critical in some applications.\n\nWe’re thinking:It’s curious that medium-length RAG contexts generally yielded fewer hallucinations than short or long. Maybe we should give models more context than we think they need.\n\nArgentina created a national law-enforcement department that will use AI to detect crimes as they’re committed, investigate them afterward, and predict them before they occur.\n\nWhat’s new:President Javier Milei of Argentina established the Artificial Intelligence Unit Applied to Security (UIAAS),The Registerreported. The unit aims to detect, investigate, and predict criminal activity by using machine learning algorithms to monitor the internet, wireless communications, security cameras, drone surveillance, financial transactions, and other data in real time.\n\nHow it works:Milei established the UIAAS in a late-Julyresolution. Milei created it under the Ministry of Security shortly after hereorganizedthe national intelligence agency to give himself more direct control. In December, his security ministerquashedpublic protests against his austerity policies; he promised to identify protesters via “video, digital, or manual means” and bill them for the cost of policing the demonstrations.\n\nBehind the news:Argentina’s government is a presidential representative democratic republic. The country was ruled by a military dictatorship between 1976 and 1983.\n\nWhy it matters:AI has valuable uses in law enforcement and security. At the same time, it needs to be applied responsibly and implemented in a way that’s fair and respectful of legal rights such as presumption of innocence.\n\nWe’re thinking:Surveillance is easy to abuse, and the notion of predictive policing warrants extreme caution to avoid bias against certain groups, violating civil rights, and other pitfalls. Ensuring that it’s used well requires robust technology, rigid controls, clear oversight, and public transparency. We hope that Argentina — no less than the countries that inspired it establish a national AI police agency — will put strong safeguards in place.\n\nResearchers have probed the inner workings of individual layers of large language models. A new tool applies this approach to all layers.\n\nWhat’s new:Tom Lieberum and colleagues at Google releasedGemma Scope, a system designed to illuminate how each layer in Gemma 2-family large language models responds to a given input token. Gemma Scope is available for the 9 billion-parameter and newly released 2 billion-parameter versions of Gemma 2. You can play with aninteractive demoor download theweights.\n\nKey insight:A sparse autoencoder (SAE) is a sparse neural network that learns to reconstruct its input. The authors drew on earlier research into using SAEs to interpret neural networks.\n\nHow it works:The authors built over 400 SAEs, one for each layer of Gemma 2 2B and Gemma 2 9B. They fed Gemma 2 examples from its pretraining set and extracted the resulting embeddings at each layer. Given the resulting embeddings from a specific layer, an SAE learned to reconstruct each of them. An additional loss term minimized the number of non-zero outputs from the SAE’s first layer to help ensure that the SAE used only concepts related to the embedding. To interpret an embedding produced by the first layer of the SAE, the team labeled the embedding’s indices with their corresponding concepts. They used two main methods: manual and automatic.\n\nBehind the news:Earlier research into using SAEs to interpret neural networks was limited tointerpreting a single layeror asmall network. Earlier this year, Anthropic used an SAE tointerpret Claude 3 Sonnet’s middle layer, building on an earlier report in which they interpreted asingle-layer transformer.\n\nWhy it matters:Many questions about how LLMs work have yet to be answered: How does fine-tuning change the way a model represents an input? What happens inside a model during chain-of-thought prompting versus unstructured prompting? Training an SAE for each layer is a step toward developing ways to answer these questions.\n\nWe’re thinking:In 2017, researchersvisualizedthe layers of a convolutional neural network to show that the deeper the layer, the more complex the concepts it learned. We’re excited by the prospect that SAEs can deliver similar insights with respect to transformers.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--5--1.png",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--6-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--3-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--7-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--4-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-296/",
    "title": "issue 296",
    "date": "",
    "reading_time": "",
    "content": "I am so sorry that the U.S. is letting down our friends and allies. Broad tariffs, implemented not just against adversaries but also steadfast allies, will damage the livelihoods of billions of people, create inflation, make the world more fragmented, and leave the U.S. and the world poorer. AI isn’t the solution to everything, but even amidst this challenging environment, I hope our community can hold together, keep building friendships across borders, keep sharing ideas, and keep supporting each other.\n\nMuch has been written about why high, widespread taxes on imports are harmful. In this letter, I’d like to focus on its possible effects on AI. One silver lining of the new tariffs is that they focus on physical imports, rather than digital goods and services, including intellectual property (IP) such as AI research inventions and software. IP is difficult to tax, because each piece of IP is unique and thus hard to value, and it moves across borders with little friction via the internet. Many international AI teams collaborate across borders and timezones, and software, including specifically open source software, is an important mechanism for sharing ideas. I hope that this free flow of ideas remains unhampered, even if the flow of physical goods is.\n\nHowever, AI relies on hardware, and tariffs will slow down AI progress by restricting access to it. Even though a last-minute exception was made for semiconductors, taxing imports of solar panels, wind turbines, and other power-generation and -distribution equipment will diminish the ability to provide power to U.S. data centers. Taxing imports of servers, cooling hardware, networking hardware, and the like will also make it more expensive to build data centers. And taxing consumer electronics, like laptops and phones, will make it harder for citizens to learn and use AI.\n\nWith regard to data-center buildouts, another silver lining is that, with the rise of generative AI,data gravity has decreasedbecause compute processing costs are much greater than transmission costs, meaning it’s more feasible to place data centers anywhere in the world rather than only in close proximity to end-users. Even though many places do not have enough trained technicians to build and operate data centers, I expect tariffs will encourage data centers to be built around the world, creating more job opportunities globally.\n\nFinally, tariffs will create increased pressure for domestic manufacturing, which might create very mild tailwinds for robotics and industrial automation. As U.S. Vice President J.D. Vancepointed outin 2017, the U.S. should focus on automation (and education) rather than on tariffs. But the U.S. does not have the personnel — or know-how, or supply chain — to manufacture many of the goods that it currently counts on allies to make. Robotics can be helpful for addressing a small part of this large set of challenges. Generative AI’s rate of progress in robotics is also significantly slower than in processing text, visual data, audio, and reasoning. So while the tariffs could create tailwinds for AI-enabled robotics, I expect this effect to be small.\n\nMy 4-year-old son had been complaining for a couple of weeks that his shoes were a tight fit — he was proud that he’s growing! So last Sunday, we went shoe shopping. His new shoes cost $25, and while checking out, I paused and reflected on how lucky I am to be able to afford them. But I also thought about the many families living paycheck-to-paycheck, and for whom tariffs leading to shoes at $40 a pair would mean they let their kids wear ill-fitting shoes longer. I also thought about people I’ve met in clothing manufacturing plants in Asia and Latin America, for whom reduced demand would mean less work and less money to take home to their own kids.\n\nI don’t know what will happen next with the U.S. tariffs, and plenty of international trade will happen with or without U.S. involvement. I hope we can return to a world of vibrant global trade with strong, rules-based, U.S. participation. Until then, let’s all of us in AI keep nurturing our international friendships, keep up the digital flow of ideas — including specifically open source software — and keep supporting each other. Let’s all do what we can to keep the world as connected as we are able.\n\nLove,\n\nAndrew\n\nCourse 3 of theData Analytics Professional Certificateis live! Learn to use Python, the most important coding language in data analytics, to analyze real-world datasets, create visualizations, run tests, and apply AI tools to debug and accelerate your code.Enroll now\n\nEven without explicit training in reasoning, large language models “think” in ways that may be more deliberate than previously understood.\n\nWhat’s new:Emmanuel Ameisen and colleagues at Anthropic devised amethodto study how transformers generate responses to specific prompts. They alsostudiedClaude 3.5 Haiku’s responses to specific prompts and found that the model, which is not trained to generate chains of thought, nonetheless appeared to take reasoning steps via its neuron activations.\n\nKey insight:A viable alternative to a fully connected layer is a cross-layer transcoder, which has two layers. The outputs of the larger first layer are sparse, which makes them interpretable “features,” or individual values that correspond to concepts. By mapping an input to highly activated features, we can identify the concepts that determine the model’s output.\n\nHow it works:The team replaced fully connected layers in Claude 3.5 Haiku with cross-layer transcoders and interpreted their features.\n\nResults:The authors built graphs that show how Claude 3.5 Haiku computes its output over a number of selected prompts.\n\nBehind the news:Last year, Google trained models toexamine individual featuresin Gemma 2. Before that, Anthropic used similar methods tointerpret Claude 3 Sonnet’s middle layer.\n\nWhy it matters:Apparently Claude 3.5 Haiku — and presumably other large language models — spontaneously perform implicit reasoning steps without being prompted to do so. Anthropic’s method reveals not only whether a model reasons or takes a shortcut, but also what it truly does well and what it only professes to do well.\n\nWe’re thinking:The authors’ approach to examining how large language models generate output is interesting. We wonder whether even pre-transformer vanilla neural networks would appear to perform some sort of “reasoning” if we were to interpret them in a similar way.\n\nMeta updated its popular open-weights models, claiming performance superior to closed competitors in three size classes.\n\nWhat’s new:Meta released two vision-language models in theLlama 4family (Llama 4 Scout and Llama 4 Maverick) and teased a third (Llama 4 Behemoth). All three models are based on the increasingly popular mixture-of-experts (MoE) architecture, which activates only a portion of parameters during inference for more efficient processing. Llama 4 Scout boasts the industry's biggest input context window so far — 10 million tokens! — but Metasaysprocessing 1.4 million tokens of context requires eight Nvidia H100 GPUs, and early users on Redditreportedthat its effective context began to degrade at 32,000 tokens.\n\nHow it works: The team pretrained Llama 4 models on images and text in over 200 languages from publicly available and licensed data, including data from publicly shared posts on Facebook and Instagram. They trained Llama 4 Scout on 40 trillion tokens and Llama 4 Maverick on 22 trillion tokens.\n\nResults:In tests performed by Meta, Llama 4 models showed strong performance relative to competing models — mostly not mixtures of experts, but some that are known to have higher parameter counts relative to Llama 4 models’ active parameters.\n\nYes, but:An experimental version of Llama 4 Maverick reached second place inChatbot Arenabehind Gemini 2.5 Pro. However, it was a variation optimized for conversation, not the currently available version. AI researchersaccusedMeta of attempting to manipulate the leaderboard.\n\nWhy it matters:Although the version of Llama 4 Maverick that nearly topped the Chatbot Arena is not the released version, its accomplishment says a lot about the growing power of open weights. Open models are quickly reaching parity with closed competitors — a boon to developers, businesses, and society at large.\n\nWe’re thinking:According to Meta, Behemoth beats GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro, topping all but the best reasoning models — but it isn’t available yet. Something to look forward to!\n\nAlibaba’s latest open-weights system raises the bar for multimodal tasks in a relatively small model.\n\nWhat’s new:Alibaba releasedQwen2.5-Omni 7B.\n\nHow it works:Qwen2.5-Omni 7B comprises a pretrained text transformer (Qwen 2.5 7B), pretrained vision encoder (Qwen2.5-VL), pretrained audio encoder (Whisper-large-v3), speech transformer, and audio decoder (a transformer plusBigVGAN), along with corresponding adapters of undisclosed architecture.\n\nResults:The authors compared Qwen2.5-Omni 7B to similarly sized models. It performed especially well on audio-to-text, image-to-text, and video-to-text tasks. However, it performed less well on text-to-text and text-to-speech tasks.\n\nBehind the news:Multimodal systems with open weights are multiplying. For instance,AnyGPT(open weights, training, and inference code) accepts and generates speech, text, images, and music. Similarly,Mini-Omni2(open weights and inference code) accepts and generates text, speech, and images.\n\nWhy it matters:Multimodal models typically show steep degradation on measurements of instruction-following when shifting from voice to text, but Qwen2.5-Omni does not. As the world moves toward voice-to-voice interactions, open systems that deliver performance comparable to that of closed competitors accelerate progress towards better conversations.\n\nWe’re thinking:The Qwen team is on fire! Alibaba’s steady stream of highly capable open-weights models is a gift to AI developers.\n\nIf you have a collection of variables that represent, say, a cancer patient and you want to classify the patient’s illness as likely cancer or not, algorithms based on decision trees, such as gradient-boosted trees, typically perform better than neural networks. A transformer tailored to tabular data could change this situation.\n\nWhat’s new: Noah Hollmann, Samuel Müller, and colleagues at University of Freiburg, Berlin Institute of Health, Prior Labs, and ELLIS Institute introducedTabular Prior-data Fitted Network(TabPFN), a transformer that, given a tabular dataset, beats established decision-tree methods on classification and regression tasks. You can download thecodeandweightsunder alicensebased on Apache 2.0 that allows noncommercial and commercial uses.\n\nKey insight:In a typical supervised learning process, a model given one example at a time learns to recognize patterns in a dataset. If each example is an entire dataset, it learns to recognize patterns across all those datasets. Trained in this way on enough datasets, it can generalize to new ones. Applying this idea to tabular data, a transformer — unlike a decision tree — can learn to perform classification and regression on any dataset without further training; that is, without further updating the model weights.\n\nHow it works:The authors generated 100 million datasets and used them to pretrain two small transformers (around 7 million and 11 million parameters respectively) to perform classification or regression. Given a dataset of rows (say, patient data labeled diagnoses or real-estate data labeled with prices) and one final row that’s unlabeled, the models learned to generate the missing label or value. Each dataset consisted of up to 2,048 rows (examples) and up to 160 columns (features).\n\nResults:The authors tested the system on 29 classification datasets and 28 regression datasets from theAutoMLbenchmark andOpenML-CTR23. Each dataset contained up to 10,000 rows, 500 columns, and 10 classes. They compared TabPFN to the popular gradient-boosted tree approaches CatBoost, LightGBM, and XGBoost.\n\nYes, but:The authors’ method is slower than decision tree methods with respect to inference. To process a 10,000-row dataset, TabPFN required 0.2 seconds while CatBoost took 0.0002 seconds.\n\nWhy it matters:Transformers trained on large datasets of text or images can perform tasks they weren’t specifically trained for and generalize to novel datasets when performing tasks they were trained for. But when it comes to tabular data, they haven’t been competitive with decision trees. This work bridges the gap, unlocking a wide variety of new use cases for transformers. Not only does it process tabular data as well as popular tree-based methods, it doesn’t require additional training to process novel datasets.\n\nWe’re thinking:Decision treesdate back to Aristotleand remain extremely useful. But a transformer-based approach could open the processing of tabular data to benefit from the ongoing innovation in transformers.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--73-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/V3_Sign_Up_Button_DeepLearning_Data_Analytics_Banner_2070x1080-01.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--56-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--57-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--74-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--75-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-29/",
    "title": "issue 29",
    "date": "",
    "reading_time": "",
    "content": "In addition to creating tremendous value, AI is creating tremendous concentrations of power. Our community is wrestling with what constitutes fair use of that power.\n\nThe Markuppublished an article criticizing car insurance giant Allstate for price discrimination — charging different fees to different customers — based not only on their risk but also on their predicted willingness to pay. Is this behavior okay?\n\nDigital technology enables online comparison shopping, which shifts pricing power toward consumers. But it also enables companies to create unique products for individual customers — say, a ride from point A to point B at a particular time, or a health insurance plan tailored to the customer’s personal history — and AI can help optimize prices to maximize profit for vendors. That can lead to both better products and worse price transparency.\n\nIf an online store sells the same hammer to different people for different prices,customers eventually will notice. That helps keep this form of price discrimination in check. But the temptation for sellers is still there. In 2016, Uber revealed that customerspay higher prices when their phone battery is low. (The company said it didn’t take advantage of this phenomenon.)\n\nI wonder sometimes if I should comparison-shop more frequently than I do. Less because I’m anxious to save a few dollars on one purchase, but because I want to train vendors’ AI systems to think I’m sensitive to price and thus to offer me lower prices.\n\nIn college, my Economics 101 professor taught about supply and demand, and how our economy creates surpluses for both producers and consumers. But AI is prompting us to revisit old economic theories — along with our sense of what’s fair.\n\nThese are hard questions. I hope we can work on them together to give the world great products and services at even better prices.\n\nKeep learning!\n\nAndrew\n\nAfter a decade in wireless communications, Cherif was ready for a change. Online courses, textbooks, and meetups helped him build his skills and land a Machine Learning Engineer role at Postmates. Learn how he overcame obstacles, aced job interviews, and started applying ML in the real world in the latest installment of our “Breaking Into AI” series.Read more\n\nWhile deep learning is taking us into the future, it’s also opening windows into the past.What’s new:Machine learning-savvyRedditorDenis Shiryaev brought 100-year-old silent film footage of New York City into the 21st century by automatically sharpening the picture, boosting the frame rate, and adding color.How he did it:Shiryaev obtained eight minutes offootageshot bya Swedish film makerin 1911. He spent five days running the movie through a gauntlet of neural nets.\n\nBehind the news:Shiryaev has used these procedures to update footage ofMoscow in 1896, an iconic film from the same year that shows aFrench train pulling into stationandApollo 16 astronautsdriving their moon buggy across the lunar surface in 1972.Why it matters:Shiryaev’s work brings these pieces of the past to life, overcoming the poor image quality, jerky movements, and lack of colors that diminish so much historic film. Similar treatment no doubt would perk up careworn Hollywood classics as well.We’re thinking:We can’t wait to up-res old home videos. It’s about time our parents’ 1980s hairstyles were revealed in high def.\n\nTransformer networks have revolutionized natural language processing, but they hog processor cycles and memory. New research demonstrates a more frugal variation.What’s new:Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya at UC Berkeley and Google modified the transformer architecture to run faster while requiring orders of magnitude less memory during training. They call the new versionReformer.Key insight:The transformer architecture is inherently inefficient: It tracks relationships among all input tokens, whether or not they matter to the output, and training requires a lot of memory. A few simple tweaks can rein in these excesses.How it works:The researchers replaced the transformer’s feed-forward network with areversible residual network. They modified the attention mechanism withlocality-sensitive hashing.\n\nResults:The authors ran experiments on Wikipediatextparceled into sequences of 64,000 tokens (more than double the number in the original transformerpaper) in 16GB of memory. Reformer achieved almost the same performance as a transformer with an identical number of parameters while consuming less memory. Furthermore, the time required to compute LSH attention scaled more efficiently with increased sequence length.Why it matters:Researchers seeking better performance are pumping up transformer-based models to immense sizes — Microsoft’s latest language model has17 billion parameters. Running such behemoths can be out of reach for all but the largest corporate research labs. Reformer offers a more efficient alternative.We’re thinking:Reformer’s improvements equip the transformer architecture for reading and generating long sequences — not only text, but also long-form video and audio. This capability could lead to larger-scale benchmarks to propel transformers into new tasks.\n\nAI may not steal your job, but it can tell the boss when you’re slacking.What’s new:Drishti, a startup based in Palo Alto and Bengaluru, tracks the productivity of industrial workers by recognizing their actions on the assembly line. Automotive parts giant Denso is using the technology to eliminate bottlenecks in its factory in Battle Creek, Michigan, according toWired.How it works:Drishti trains the system to recognize standardized actions in the client’s industrial processes.\n\nBehind the news:Drishti’s founders include Prasad Akella, who led General Motors’ efforts to developcollaborative robots, and computer vision expert Krishnendu Chadbury, who led teams at Google, Adobe, and Flipkart.Why it matters:Manufacturing is a$14 trillion industry. According toresearchsponsored by Drishti, humans perform 72 percent of the work, and human error causes 68 percent of defects. Using AI to help people work more efficiently could yield substantial gains.Yes, but:Workers in some industries are pushing back against automated management. Last year, dozens of employeeswalked outof Amazon warehouses to protest the pace of work demanded by AI-powered supervisors, which they said led to dangerous conditions.We’re thinking:Complaining about the quality of others’ work while not doing any yourself? Computers are becoming more like humans all the time!\n\nWant to deploy a TensorFlow model in your web browser or on your smartphone? Thedeeplearning.aiTensorFlow: Data and Deployment Specialization will teach you how.Enroll now\n\nA prominent AI researcher has turned his back on computer vision over ethical issues.What happened:The co-creator of the popular object-recognition networkYou Only Look Once(YOLO)saidhe no longer works on computer vision because the technology has “almost no upside and enormous downside risk.”Why he quit:Joseph Redmon, a graduate student at the University of Washington with a charmingly unorthodoxrésumé, said on Twitter, “I stopped doing CV research because I saw the impact my work was having.” He didn’t respond to a request for an interview.\n\nBehind the news:Redmon and his faculty advisor Ali Farhudi devised YOLO in 2016 to classify objects in real time, funded partly by Google and the U.S. Office of Naval Research. The work won aPeople’s Choice Awardat that year’s Computer Vision and Pattern Recognition conference. The lastupdatecame in April 2018.Why it matters:Concerns aremountingover a number of ethical concerns in machine learning includingbiased output,potential misuse, andadverse social impacts. The field stands to lose more talented researchers if it doesn’t come to grips with issues like this.We’re thinking:Researchers need to recognize the ethical implications of their work and guide it toward beneficial uses. Many technologies have both civilian and military uses, and opting out may not be as powerful as helping to shape the field from within.\n\nDirections such as “turn left at the big tree, go three blocks, and stop at the big red house on your left” can get you to your destination because they refer to stationary landmarks. New research enables self-driving cars to identify such stable indicators on their own.What’s new:Dan Barnes and Ingmar Posner of Oxford University built amodelthat extracts landmarks on the fly from radar scans to build maps for autonomous vehicles. Radar is challenging in this application because it generates noise and ghost images, but it has the benefits of long range, high refresh rate, and robustness to environmental conditions. Thisvideoexplains.Key insight:Self-driving cars often navigate by recognizing landmarks. The researchers realized that neural networks can discover them by reversing the task: The radar signals most valuable to navigation are likely stable features of the landscape.How it works:The system learns to identify keypoints that best predict a car’s motion. The trainingdataspecifies a vehicle’s motion from radar frame to radar frame.\n\nResults:The system’s error in predicting the car’s position after driving a fixed distance was 2.06 percent, compared to the previous state of the art, 3.72 percent. Similarly, the error in the car’s predicted orientation fell from 0.0141 to 0.0067 degrees per meter driven. The new system ran an order of magnitude faster. For routes that didn’t include a loop, an earlierwhole-frame approachcut the predicted position error to 1.59 percent and rotation error to 0.0044 degrees per meter.Why it matters:The ability to generate keypoints automatically is making waves in othercomputervisiontasks. Combining keypoints with vector descriptions makes it possible to learn valuable things about them, from whether they indicate a loop in the route to recognizing a habitual parking space.We’re thinking:Our surroundings are always changing: Outdoors, trees fall down and buildings go up, while indoors objects are moved all the time. Algorithms that detect landmarks on the fly will be useful for mapping and navigating such dynamic environments.\n\nNo more sloppy workouts: AI can correct your form.What’s new:A home exercise system uses neural nets to analyze your motions and tell you when you perform a move properly, reportsThe Verge.How it works:Tempois a six-foot-tall easel with a giant screen on the front and storage for weights on the back. The system’s motion tracking capability is built around Microsoft’sAzure Kinect. Adepth-sensing cameraemits infrared light and indirectly measures the time it takes for photons to zip from the camera to your body and back again, creating a continuous 3D image.\n\nBehind the news:Tempo is the first at-home exercise machine that monitors form, but it has plenty of competition in the world ofInternet-connected exercise equipment. Peloton makes treadmills and exercise bikes with screens that stream live classes, whileFightCamp Gymoffers a connected punching bag.Why it matters:Exercising well is exercising efficiently. Proper form can help you avoid injuries and get better results from your routine.We’re thinking:We’re weighing the benefits of a system like this against the fact that we’d have no excuse not to go to the gym if it were in the next room.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter20220ASPECT201.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/TheBatchFeaturedImageBreakingIntoAICherifJazra.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize208.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Reformer.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize207.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize-3.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Redmon.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Radar.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Tempo.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-66/",
    "title": "issue 66",
    "date": "",
    "reading_time": "",
    "content": "Last week, Iwroteabout the limitation of using human-level performance (HLP) as a metric to beat in machine learning applications for manufacturing and other fields. In this letter, I would like to show why beating HLP isn’t always the best way to improve performance.\n\nIn many machine learning problems, labels are determined by a person who evaluates the same sort of input as a learning algorithm would. For instance, a human labeler may look at a picture of a phone to determine if it’s scratched, and an algorithm would examine a similar picture to learn to detect scratches. (Note that this is not always the case. A human labeling a cancer diagnosis on an X-ray image may also rely on a tissue biopsy from the patient, while an algorithm would use the resulting dataset to learn to diagnose cancer based on images alone.)\n\nIn cases where labels were determined by a human by looking at the same input that an algorithm would, what are we to make of situations in which HLP is well below 100 percent? This just means that different people labeled the data differently. For example, the ground-truth labeler who created a test set may have labeled a particular phone as scratched, while a different labeler thought the same phone was not scratched, and thus made a mistake in marking this example. If the second labeler disagreed with the ground-truth labeler on 1 out of 10 examples, then HLP in this task would be 90 percent.\n\nIn this situation, rather than trying to build a learning algorithm that achieves 91 percent accuracy, it would be better to look into how the two labelers formed their judgements and try to help them make their labels more consistent.\n\nFor example, all labelers may agree that scratches smaller than 1 mm are not significant (y=0), and scratches greater than 3 mm are significant (y=1), but they label scratches between 1 mm and 3 mm inconsistently. If we can spot this problem and get the labelers to agree on a consistent standard — say, that 1.5 mm is the point at which the labels should switch from y=0 to y=1 — then we’ll end up with less noisy labels.\n\nSetting standards that make labels more consistent will actually raise HLP, because humans now agree with one another more frequently. At the same time, having more consistently labeled data will result in better machine learning performance. This improvement is more important in many practical applications than the academic question of whether an algorithm beat HLP.\n\nHLP does have a role to play in establishing baseline performance for estimating irreducible, or Bayes, error, which in turn helps with error analysis. You can learn more about this inDeep Learning Specialization Course 3andMachine Learning Yearning.\n\nBut the message I hope you’ll take away from this letter is that, when a human labeler has created the class labels that constitute ground truth and HLP is significantly less than 100 percent, we shouldn’t just set out to beat HLP. We should take the deficit in human performance as a sign that we should explore how to redefine the labels to reduce variability.\n\nKeep learning!\n\nAndrew\n\nSocial biases arewell documentedin decisions made by supervised models trained onImageNet’s labels. But they also crept into the output of unsupervised models pretrained on the same dataset.What’s new:Two image classification models learned social biases from ImageNet photos, according to astudyby researchers Carnegie Mellon and George Washington University.How it works:The authors measured the extent to which Google’sSimCLRv2and OpenAI’siGPTassociated types of people with certain attributes.\n\nResults:Features generated by both models showed social biases such as associating white people with tools and black people weapons. While SimCLRv2 tended to associate stereotyped attributes with certain categories more strongly, iGPT showed such biases toward a broader range of categories. For instance, features generated by iGPT associated thin people with pleasantness and overweight people with unpleasantness, and also associated men with science and women with liberal arts.\n\nBehind the news:ImageNet 2012 contains 14 million images annotated by human workers, who passed along their prejudices to the dataset. ImageNet creator Fei-Fei Li isspearheadingan effort to purge the dataset of labels that associated genders, races, or other identities with stereotypes and slurs.Why it matters:When unsupervised models pick up on biases in a dataset, the issue runs deeper than problematic labels. The authors believe that their models learned social stereotypes because ImageNet predominantly includes images of people in stereotypical roles: men in offices, women in kitchens, and non-white people in general excluded from images showing situations that have positive associations such as weddings. Machine learning engineers need to be aware that a dataset’s curation alone can encode common social prejudices.We’re thinking:Datasets are built by humans, so it may be impossible to eliminate social biases from them completely. But minimizing them will pay dividends in applications that don’t discriminate unfairly against certain social groups.\n\nCovid-19 wreaked havoc with models that predict retail sales — but China’s biggest annual e-commerce event showed that they’re back in business.What’s new:China’s two biggest retailers, Alibaba and JD, used AI models trained on pandemic-era consumer behavior to make sure warehouses were stocked and deliveries arrived on time during the annual Singles Day shopping bonanza, according toMIT Technology Review. Alibaba’s sales of $74.1 billion doubled those of last year, while JD’s $40.9 billion exceeded the 2019 take by33 percent.Revised models:Covid-19 hit China just before the surge of holiday shopping for Chinese New Year, on January 25. Normally, major retailers use sales data from that day to prepare their models for Singles Day. Instead of gifts, however, consumers were making runs on pandemic essentials like masks, toilet paper, and hand sanitizer, throwing the models off kilter.\n\nBehind the news:The pandemic has driven an ecommerce boom worldwide even as it has taken a tragic toll on people across the globe. Online sales across China jumped17 percentfor Singles Day in 2020 over last year. In the U.S., online sales during this year’s holiday shopping season are already21 percenthigher than the same period in 2019.Why it matters:These companies’ moves show the critical role AI can play in helping businesses respond to today’s fast-changing, utterly unprecedented market conditions.We’re thinking:Covid-19 has accelerated digitization in retail, and is intensifying a division of the sector into AI haves and have-nots. Retailers that are struggling to survive lack resources to invest in AI and tech; those that are doing well are doubling down on their AI investments. Unfortunately, we think this will accelerate concentration of power.\n\nWe’re thrilled to announce the launch of ourTensorFlow: Advanced Techniques Specialization, available now on Coursera!Enroll now\n\nLarge transformer networks work wonders with natural language, but they require enormous amounts of computation. New research slashes processor cycles without compromising performance.What’s new:Swetha Mandava and a team at Nvidia reduced the number of self-attention layers in transformer-based language models. TheirPay Attention When Required(Par) approach achieves results comparable to those ofTransformer-XLandBertin substantially less time.Key insight:The self-attention layers in transformer networks are notoriously inefficient. Some of them can be replaced by higher-efficiency feed-forward layers.How it works:The authors used differential neural architecture search (DNAS), following earlierworkto optimize both error and processing latency. For each layer in a network, DNAS starts with a user-defined set of blocks and finds the likelihood that a particular block is the best choice for that layer. The authors searched for optimal networks of 32 and 24 layers, the numbers of layers in Transformer XL and Bert.\n\nResults:Par-Transformer matched Transformer-XL in perplexity (a measure of a language model’s predictive accuracy). It used roughly one-third as many self-attention blocks and executed in one-third less time, making decisions in 9.9 milliseconds versus 15.2 milliseconds running on Nvidia A100 GPUs. Par-Bert similarly matched Bert’s perplexity in a slimmer model while cutting latency to 5.7 milliseconds from 8.6 milliseconds.Why it matters:Improving the runtime performance of transformer architectures could encourage their use in novel tasks.We’re thinking:Transformer networks have come a long way in a short time and continue to improve rapidly. What an exciting time for deep learning!\n\nA new study used AI to track the explosive growth of AI innovation.What’s new:Researchers from the U.S. Patent and Trademark Office deployed natural language processing totrack AI’s increasing presencein four decades of patent data. They found that the technology is involved in one out of six current applications.What they did:The researcherstrainedan LSTM to analyze the text of nearly 12 million technology patents filed between 1976 and 2019 for language that described subcategories of AI including computer vision, language modeling, and evolutionary algorithms.\n\nBehind the news:A 2019reportfrom the World Intellectual Property Organization found that 50 percent of machine learning patents globally were issued since 2013.China leads the worldin AI patents, followed closely by the U.S.Why it matters:These numbers give a concrete sense of the speed of innovation in AI and the ever-wider range of applications in which it’s used.We’re thinking:We’re fans of AI technology!",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-11-1820at2012.51.1820PM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2033.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2038.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Gif201.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2024.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2037.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-209/",
    "title": "issue 209",
    "date": "",
    "reading_time": "",
    "content": "Do large language models understand the world? As a scientist and engineer, I’ve avoided asking whether an AI system “understands” anything. There’s no widely agreed-upon, scientific test for whether a system really understands — as opposed to appearing to understand — just as no such tests exist for consciousness or sentience, as I discussed in an earlierletter. This makes the question of understanding a matter of philosophy rather than science. But with this caveat, I believe that LLMs build sufficiently complex models of the world that I feel comfortable saying that, to some extent, they do understand the world.\n\nTo me, the work onOthello-GPTis a compelling demonstration that LLMs build world models; that is, they figure out what the world really is like rather than blindly parrot words. Kenneth Li and colleagues trained a variant of the GPT language model on sequences of moves from Othello, a board game in which two players take turns placing game pieces on an 8x8 grid. For example, one sequence of moves might be d3 c5 f6 f5 e6 e3…, where each pair of characters (such as d3) corresponds to placing a game piece at a board location.\n\nDuring training, the network saw only sequences of moves. It wasn’t explicitly told that these were moves on a square, 8x8 board or the rules of the game. After training on a large dataset of such moves, it did a decent job of predicting what the next move might be.\n\nThe key question is: Did the network make these predictions by building a world model? That is, did it discover that there was an 8x8 board and a specific set of rules for placing pieces on it, that underpinned these moves? The authors demonstrate convincingly that the answer is yes. Specifically, given a sequence of moves, the network’s hidden-unit activations appeared to capture a representation of the current board position as well as available legal moves. This shows that, rather than being a “stochastic parrot” that tried only to mimic the statistics of its training data, the network did indeed build a world model.\n\nWhile this study used Othello, I have little doubt that LLMs trained on human text also build world models. A lot of “emergent” behaviors of LLMs — for example, the fact that a model fine-tuned to follow English instructions can follow instructions written in other languages — seem very hard to explain unless we view them as understanding the world.\n\nAI has wrestled with the notion of understanding for a long time. Philosopher John Searle published theChinese Room Argumentin 1980. He proposed a thought experiment: Imagine an English speaker alone in a room with a rulebook for manipulating symbols, who is able to translate Chinese written on paper slipped under the door into English, even though the person understands no Chinese. Searle argued that a computer is like this person. It appears to understand Chinese, but it really doesn’t.\n\nA common counterargument known as the Systems Reply is that, even if no single part of the Chinese Room scenario understands Chinese, the complete system of the person, rulebook, paper, and so on does. Similarly, no single neuron in my brain understands machine learning, but the system of all the neurons in my brain hopefully do. In my recent conversation with Geoff Hinton, which you can watchhere, the notion that LLMs understand the world was a point we both agreed on.\n\nAlthough philosophy is important, I seldom write about it because such debates can rage on endlessly and I would rather spend my time coding. I’m not sure what the current generation of philosophers thinks about LLMs understanding the world, but I am certain that we live in an age of wonders!\n\nOkay, back to coding.\n\nKeep learning,\n\nAndrew\n\nA deep learning system detected breast cancer in mammograms as well as experienced radiologists, according to a landmark study.\n\nWhat’s new:Researchers at Lund University in Swedenconducteda randomized, controlled, clinical trial to determine whether an AI system could save radiologists’ time without endangering patients — purportedly the first study of AI’s ability to diagnose breast cancer from mammograms whose design met the so-called gold standard for medical tests. Their human-plus-machine evaluation procedure enabled radiologists to spend substantially less time per patient while exceeding a baseline for safety.\n\nHow it works:The authors randomly divided 80,000 Swedish women into a control group and an experimental group.\n\nResults: The AI-assisted diagnosis achieved a cancer detection rate of 6.1 per 1,000 patients screened, comparable to the control method and above an established lower limit for safety. The radiologists recalled 2.0 percent of the control group and 2.2 percent of the experimental group, and both the control and experimental groups showed the same false-positive rate of 1.5 percent. (The difference in recall rates coupled with the matching false-positive rate suggests that the AI method detected 20 percent more cancer cases than the manual method, though authors didn’t emphasize that finding.) Moreover, since approximately 37,000 patients were only examined by one radiologist, the results indicate that AI saved 44.3 percent of the examination workload without increasing the number of misdiagnosed patients.\n\nYes, but:The authors’ method requires more study before it can enter clinical practice; for instance, tracking patients of varied genetic backgrounds. The authors are continuing the trial and plan to publish a further analysis after 100,000 patients have been enrolled for two years.\n\nBehind the news:Radiologists have used AI to help diagnose breast cancer since the 1980s (though that method isquestionable.) A 2020studyby Google Health claimed that AI outperformed radiologists, but critics found flaws in the methodology.\n\nWhy it matters:Breast cancercausesmore than 600,000 deaths annually worldwide. This work suggests that AI can enable doctors to evaluate more cases faster, helping to alleviate a shortage of radiologists. Moreover, treatment is more effective the earlier the cancer is diagnosed, and the authors’ method caught more early than late ones.\n\nWe’re thinking:Medical AI systems that perform well in the lab often fail in the clinic. For instance, a neural network may outperform humans at cancer diagnosis in a specific setting but, having been trained and tested on the same data distribution, isn’t robust to changes in input (say, images from different hospitals or patients from different populations). Meanwhile, medical AI systems have been subjected to veryfewrandomized, controlled trials, which is considered the gold standard for medical testing. Such trials have their limitations, but they’re a powerful tool for bridging the gap between lab and clinic.\n\nChatbots are taking orders for burgers and fries — and making sure you buy a milkshake with them.\n\nWhat’s new:Drive-thru fast-food restaurants across the United States are rolling out chatbots to take orders,The Wall Street Journalreported. Reporter Joanna Stern delivers a hilarious consumer’s-eye view in an accompanyingvideo.\n\nHow it works:Hardee’s, Carl’s Jr., Checkers and Del Taco use technology from Presto, a startup that specializes in automated order-taking systems. The companyclaims95 percent order completion and $3,000 in savings per month per store. A major selling point: Presto’s bot pushes bigger orders that yield $4,500 per month per store in additional revenue.\n\nBehind the news:The fast-food industry is embracing AI to help out in the kitchen, too.\n\nYes, but:McDonald’s, the world’s biggest fast-food chain by revenue, uses technology from IBM and startup Apprente, which itacquiredin 2019. As of early this year, the systemachieved80 percent accuracy — far below the 95 percent that executives had expected.\n\nWhy it matters:In fast food, chatbots are continuing a trend in food service that began with Automat cafeterias in the early 1900s. Not only are they efficient at taking orders, apparently they’re more disciplined than typical employees when it comes to suggesting ways to enlarge a customer’s order (and, consequently, waist).\n\nWe’re thinking:When humans aren’t around, order-taking robots order chips.\n\nJoin our upcoming workshop with Weights & Biases and learn how to evaluate Large Language Model systems, focusing on Retrieval Augmented Generation (RAG) systems.Register now\n\nAmid the hype that surrounds large language models, a crucial caveat has receded into the background: The current cost of serving them at scale.\n\nWhat’s new:As chatbots go mainstream, providers must contend with the expense of serving sharply rising numbers of users, theWashington Postreported.\n\nThe price of scaling:The transformer architecture, which is the basis of models like OpenAI’s ChatGPT, requires a lot of processing. Its self-attention mechanism is computation-intensive, and it gains performance with higher parameter counts and bigger training datasets, giving developers ample incentive to raise the compute budget.\n\nWhy it matters:Tech giants areracingto integrate large language models into search engines, email, document editing, and an increasing variety of other services. Serving customers may require taking losses in the short term, but winning in the market ultimately requires balancing costs against revenue.We’re thinking:Despite the high cost of using large language models to fulfill web searches — which Google, Bing, and Duckduckgo do for free, thus creating pressure to cut the cost per query — for developers looking to call them, the expense looks quite affordable. In ourback-of-the-envelope calculation, the cost to generate enough text to keep someone busy for an hour is around $0.08.\n\nA tweak to diffusion models, which are responsible for most of the recent excitement about AI-generated images, enables them to produce more realistic output.\n\nWhat's new:William Peebles at UC Berkeley and Saining Xie at New York University improved a diffusion model by replacing a key component, a U-Net convolutional neural network, with a transformer. They call the workDiffusion Transformer (DiT).\n\nDiffusion basics:During training, adiffusion modeltakes an image to which noise has been added, a descriptive embedding (typically an embedding of a text phrase that describes the original image, in this experiment, the image’s class), and an embedding of the current time step. The system learns to use the descriptive embedding to remove the noise in successive time steps. At inference, it generates an image by starting with pure noise and a descriptive embedding and removing noise iteratively according to that embedding. A variant known as alatent diffusion modelsaves computation by removing noise not from an image but from an image embedding that represents it.\n\nKey insight:In a typical diffusion model, aU-Netconvolutional neural network (CNN) learns to estimate the noise to be removed from an image.Recentworkshowed that transformers outperform CNNs in many computer vision tasks. Replacing the CNN with a transformer can lead to similar gains.\n\nHow it works:The authors modified a latent diffusion model (specificallyStable Diffusion) by putting a transformer at its core. They trained it onImageNetin the usual manner for diffusion models.\n\nResults:The authors assessed the quality of DiT’s output according toFréchet Inception Distance(FID), which measures how the distribution of a generated version of an image compares to the distribution of the original (lower is better). FID improved depending on the processing budget: On 256-by-256-pixel ImageNet images, a small DiT with 6 gigaflops of compute achieved 68.4 FID, a large DiT with 80.7 gigaflops achieved 23.3 FID, and the largest DiT with 119 gigaflops achieved 9.62 FID. Alatent diffusion model that used a U-Net(104 gigaflops) achieved 10.56 FID.\n\nWhy it matters:Given more processing power and data, transformers achieve better performance than other architectures in numerous tasks. This goes for the authors’ transformer-enhanced diffusion model as well.\n\nWe're thinking:Transformers continue to replace CNNs for many tasks. We’ll see if this replacement sticks.\n\nArizona law school embraces ChatGPTThe Sandra Day O’Connor College of Law at Arizona State University now permits prospective students to use generative AI tools to draft their applications, as long as the information they submit is truthful. While the policy is limited to applications, the law school is working on guidelines for using AI in coursework and classrooms. (Reuters)Regulators probe investment firms over use of AIMassachusetts securities regulators sent letters of inquiry to investment firms including JPMorgan Chase and Morgan Stanley. The letters question the firms’ potential use of AI and its implications for investors. The state investigation comes in the wake of the U.S. Securities and Exchange Commission's recent proposal to mitigate conflicts of interest related to AI used in trading platforms. (Reuters)AI dominated second-quarter earnings callsCompanies  featured AI prominently in their conference calls during the second-quarter earnings reporting season. Prominent tech companies mentioned AI over 50 times, and over one-third of S&P 500 companies mentioned it at least once. (Reuters)Kickstarter implements AI transparency policyThe crowdfunding platform, which raises money for projects like art and video games, now mandates that users seeking funds  provide details about how their projects use AI to ensure proper attribution of AI-generated work. (Video Games Chronicle)Adobe’s use of AI raises employee concernsThe integration of AI tools like Firefly in Adobe’s products prompted internal debates about the role of AI in design and creativity. Some employees worry about potential job losses, while others emphasize the benefits for efficiency and productivity. (Business Insider)Google to revamp Google AssistantThe division of Alphabet is working on an overhaul of its virtual assistant to add generative AI features. The move marks a shift away from earlier technology. (Axios)YouTube to introduce automatic summariesThe platform is experimenting with AI-generated video summaries and emphasizes that they are not intended to replace creators’ written video descriptions. Previous summarizer tools provided by third parties received mixed reviews. (TechCrunchandGoogle)Aides reveal U.S. President Biden’s views on AIPresident Biden is navigating AI with a mix of fascination and caution, according to his aides and advisers. He's personally experimented with ChatGPT for various tasks including generating descriptions of legal cases for first-graders and crafting Bruce Springsteen-style lyrics about legal matters. (The Wall Street Journal)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--24-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--43-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--25-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/08/The-Batch-ads-and-exclusive-banners--49-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--44-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--80-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-4/",
    "title": "issue 4",
    "date": "",
    "reading_time": "",
    "content": "When it comes to artificial intelligence, one of the biggest mistakes large companies make is thinking tactically rather than strategically.\n\nWhat’s the difference? Some taxi companies thought they had the internet revolution “covered” because they built a website. Then ride-sharing startups disrupted the industry with internet-connected mobile apps that transformed the ride-hailing experience.\n\nSimilarly, some companies’ response to AI starts and ends with tactically building a few small projects. But the strategic question is: How will AI transform your industry’s core business, and how will that change what it takes for your company to thrive?\n\nI spoke about this at TechCrunch’s conference on Thursday, and Fortune published a nicesummaryof my remarks. It’s not too late for traditional companies to develop a strategic plan to take advantage of AI. The technology is only beginning to find its way into applications outside of software development. But for many companies, it will be critical to act quickly.\n\nAI transformation should start with concrete projects, but it cannot end there. I hope more CEOs learn about AI and think strategically about it.\n\nKeep learning!\n\nAndrew\n\nOmoju Miller’s journey from comp-sci undergrad to GitHub was anything but straightforward. Learn about her day-to-day as a senior machine learning engineer in the latest installment of our Working AI series.Read more\n\nThe U.S. military is developing a new generation of automated weaponry. Some people are calling for automated generals as well.\n\nWhat happened:A pair of defense experts argue inWar on the Rocks, an online publication covering national security, that the Pentagon should replace the human chain of command over nuclear defense with machines. The time available to respond to incoming warheads has dwindled from hours during the Cold War to roughly 6 minutes today. The change makes automated command-and-control a necessity, they say. Their analysis added urgency to feature stories on military AI published last week inThe AtlanticandThe Economist.Behind the news:The Department of Defense’s 2019 budget calls for nearly $1 billion in AI spending. Almost one-third of the money will fund theJoint Artificial Intelligence Centerdedicated to establishing and scaling up AI throughout the military. The remainder of the department’s AI budget will support initiatives led by individual branches. Among those efforts:\n\nThe controversy:The debate over automated warfare follows trench lines similar to those of the earlier (and ongoing) argument over nuclear weapons.\n\nWe’re thinking:Autonomous weapons are terrifying enough. Autonomous nuclear weapons verge on the unthinkable. We strongly support the United Nations’ effort to establish aban on autonomous weaponsas a complement tonuclear disarmament. That said, AI has potential nonlethal uses like mine removal and search and rescue. It will take vigorous, well informed argument to arrive at military uses of AI that improve conditions for humanity as a whole. It’s critical that the AI community play an active role in the discussion.\n\nWatson set a high bar for language understanding in 2011, when it famously whipped human competitors in the televised trivia game showJeopardy!IBM’s special-purpose AI required around $1 billion and a squadron of engineers. Newresearchsuggests that today’s best language models can accomplish similar tasks right off the shelf.\n\nWhat’s new:Researchers at Facebook AI Research and University College London pitted top-shelf language models against task-specific networks in aJeopardy!-like challenge they call Language Model Analysis (LAMA). Their LAMA data set provides a large corpus of sentences, each missing a key fact.Key Insight:The latest language models are pretrained to address a variety of downstream tasks. In learning language representations, they retain knowledge that can be used to complete statements lacking key words.How it works:LAMA builds its incomplete sentences based on facts drawn from Google-RE (facts from Wikipedia), T-REx (facts aligned with Wikipedia text), ConceptNet (a semantic network), and SQuAD (questions and answers).\n\nResults:BERT-Large filled in the blanks most accurately overall, and it was best at completing statements based on Google-RE and ConceptNet. It proved only half as accurate as task-specific models on LAMA’s SQuAD portion, which contains more complicated sentences. Similarly, BERT’s performance suffers when T-REx facts contain multiple subjects or blanks.Why it matters:The Allen institute last week reported using BERT to score better than 90 percent on the multiple-choice questions in the New York Regents science test for the eighth grade. That system included additional task-specific models and retrieved external information to complete tasks. This research suggests that BERT as-is would score well on the Regents test.\n\nTakeaway:Large, pretrained language models can glean and recall nearly as much information — from some data sets, at least — as specially designed question answering models. This knowledge can allow them to accomplish various language tasks, including fill-in-the-blank, without special preparation.\n\nAI is capable of picking faces out of the crowd — even if that crowd is squabbling over bananas in a jungle.\n\nWhat’s new:Researchers at the University of Oxford developed aface recognition appthat identifies individual chimpanzees in footage shot in the wilds of Guinea. The work could give wildlife conservation efforts a powerful new tool.\n\nHow it works:The group adapted the VGG-M convolutional neural network architecture. They trained the model on roughly 50 hours of footage representing 23 individuals over 14 years.\n\nBehind the news:Zoologists have embraced image recognition for conservation efforts. The technology is countinggiraffesin Africa and trackingwolverinesin the Pacific Northwest. An innovative application called WildBook that trawls YouTube for wildlife videos has been used to catalogwhale sharkmigrations.\n\nWhy it matters:Chimpanzees, like humans, are highly social animals. The ability to track individuals enabled the researchers to map the tribe’s structure. The model generalized well to other primate species in preliminary tests. The researchers suggest that their approach could be used with other animals where a sufficient video record exists.\n\nWe’re thinking:Applications like this could help cash-strapped conservation efforts to focus on translating data into action, and reduce the need for invasive, labor-intensive methods like tagging animals with RFID.\n\nHow can you tell when your neural network is overfitting? Learn how to spot avoidable bias in the Deep Learning Specialization.Enroll now\n\nDeepfakes threaten to undermine law and order, perhaps democracy itself. A coalition of tech companies, nonprofits, and academics joined forces to counter potential adverse impacts.\n\nWhat’s new:TheDeepfake Detection Challengeaims to provide a data set of custom-built deepfakes. Funded by a $10 million grant from Facebook, it also promises a prize for developing tools that spot computer-generated pictures.The details:Facebook is producing videos with actors who have consented to having their features altered by deepfake technology.\n\nBehind the news:Activists goaded Facebook to action in June, when they released asynthesized videoof Mark Zuckerberg rhapsodizing over his control of billions of peoples’ data.Why it matters:Deepfakes often are portrayed as a potential vector for political disinformation. But, asViceandWiredpoint out, the clear and present danger is harassment of individuals, particularly women, activists, and journalists.\n\nWe’re thinking:The fact that deepfakes are created by adversaries means the data set — and resulting filters — will need to evolve as the fakers adapt to detection algorithms.\n\nCan you spot fakes?Test your personal deepfake radar via this onlineguessing game.\n\nDeep reinforcement learning has given machines apparent hegemony in vintage Atari games, but their scores have been hard to compare — with one another or with human performance — because there are no rules governing what machines can and can’t do to win. Researchers aim to change that.\n\nWhat’s new:Most AI research demonstrating superhuman performance in Atari games applies widely varying limits on gameplay, such as how frequently buttons can be pressed. Researchers from MINES ParisTech and Valeo offer a standardized setup: Standardized Atari Benchmark for Reinforcement Learning (Saber). They use it to achieve a new state of the art in around 60 games from Pong to Montezuma’s Revenge.Key Insight:Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde noticed that the reported human world-record scores average 1,000 times higher than the “expert human player” scores given in the first major deep reinforcement learningpaperpublished in late 2013. Analyzing the settings used in deep learning publications since, the team pinpointed seven potential causes for reported variations in performance.\n\nHow it works:The authors propose a set of guidelines designed to match human capabilities. Their benchmark includes a new metric for evaluating models, since the previous human benchmark misrepresents human capabilities.\n\nResults:The researchers tested a state-of-the-art model, Rainbow-IQN, and achieved an average of only 31% of the best human scores. The model achieved superhuman scores in four of 58 games.\n\nWhy it matters:Training reinforcement learning models is so laborious that researchers often don’t bother to reproduce previous results to see how their own stack up. Saber finally provides a consistent basis for comparison.We’re thinking:Deep reinforcement learning research is exciting, but a lack of standardized benchmarks has kept the state of the art in a state of ambiguity. Saber signals a new and promising maturity.\n\nQuick-service restaurants are experiencing record-high employee turnover, while labor advocates are pushing for higher wages. Some experts say these forces are propelling the fast food industry toward full automation.\n\nWho’s already automating:The move to put fast food under machine control is already in high gear:\n\nBehind the news:Humans are opting out for the quick-service business. In July, the CEO of Panera Bread told CNBC’s @Work conference that his company experienced nearly100 percent annual employee turnover— and this number was low for the industry. Turnover in the Accommodations and Restaurants category (which includes traditional restaurants as well as hotels) has climbed nearly15 percentover the last decade, according to the U.S. Bureau of Labor Statistics.\n\nWhy it matters:Fast food is shaping up to be a leading edge of an automation wave that could be squeezing lower-skilled, lower-wage employees out of the economy. A 2017 report by the National Council on Compensation Insurance found that, while automation historicallyreplaces human labor, the jobs that remain tend to be higher skilled andbetter compensated.\n\nWe’re thinking:Apps and kiosks are clearly capable of replacing fast-food customer service. Back-of-the-house work like assembling burritos and stacking sandwiches requires more dexterity. While those positions likely persist longer, it may be cold comfort to find yourself automated out of a job five years from now rather than one.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/TheBatch-WorkingAIOmoju.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/XQ-58a.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/questionanswer.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Chimpanzees20faces.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-09-1120at2010.45.2720AM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/FB20Deepfake20challenge.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Atari20Cropped.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker201.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-100/",
    "title": "issue 100",
    "date": "",
    "reading_time": "",
    "content": "I’ve been following with excitement the recent progress in space launches. Earlier this week, Richard Branson and his Virgin Galactic teamflewa rocket plane 53 miles up, earning him astronaut wings. Next week, Jeff Bezos’ Blue Origin is expected to attempt a similar feat and achieve an even greater altitude. (I once also sat in a Blue Origin passenger capsule; see the picture below. I remained firmly on planet Earth.)The firstspace racewas between the U.S. and the Soviet Union, a competition between rival superpowers with dramatically different visions for civilization. Some pundits have panned the current space race as a contest between billionaires, but I’m glad that Bezos, Branson, and Elon Musk are pushing the boundaries of commercial flight.I’ve found space exploration exhilarating since I was a child. My father had a passion for astronomy. We spent many hours on the rooftop of our apartment complex in Singapore — often staying up way past the bedtime designated by my mother 😅 — peering through my dad’s telescope at the planets in our solar system. I remember peering atAlpha Centauri(the closest star system to ours) and wondering if I would visit someday.\n\nSpace exploration has been criticized as a waste of resources, given the problems we have here at home. Of course, we need to work on problems such as the still-rampaging Covid-19, climate change, poverty, and injustice. I believe society will be best off if we pursue multiple meaningful projects simultaneously.\n\nAs we push further into space, AI will play an increasing role. Our robots will need to be increasingly autonomous because, even though radio waves travel at the speed of light, there won’t be sufficient time to wait for guidance from human operators on Earth. (Mars averages 13 light minutes from Earth, and the more distant Neptune about 250 light minutes.) I was excited when ROS, the open-source Robot Operating System framework launched by Morgan Quigley out of my Stanford group, startedrunningin the International Space Station. And we still have much work ahead!Private entities are at the center of this week’s space boom, but I would love to see public entities play a bigger role. NASA’sinnovationshave been widely shared. I’m excited about thePerseverancerover andIngenuityhelicopter now roaming Mars (over 1 million times farther than Branson has yet to travel). So let’s make sure to strongly support public space exploration as well. Further advances will come even faster with their help.\n\nKeep learning! 🚀Andrew\n\nA reinforcement learning system enabled a four-legged robot to amble over unfamiliar, rapidly changing terrain.\n\nWhat’s new:Researchers at UC Berkeley, Facebook, and Carnegie Mellon developedRapid Motor Adaptation(RMA). The system enabled aUnitree Robotics A1to negotiate changing conditions and unexpected obstacles nearly in real time. The machine traversed muddy trails, bushy backcountry, and an oil-slicked plastic sheet without falling.\n\nHow it works:The system includes two algorithms, both of which are trained in simulation. The reinforcement learning component learns to control locomotion basics, while the adaptation module learns to generate a representation of the environment.\n\nBehind the news:Video clips of robots fromBoston Dynamicsandothershave become viral sensations in recent years. They may be mouth-watering, but the bots involved often are programmed for specific motions or scenarios and can’t adapt to novel conditions.\n\nWhy it matters:RMA is among the first robotic walking systems that don’t need to be trained for every variety of terrain they're likely to encounter.\n\nWe’re thinking:For many applications where navigating flat ground is sufficient, wheeled locomotion is much simpler and more reliable. But legs still carry the day when navigating rough terrain — not to discount their uncanny anthropomorphic appeal. They’re likely to be important for tasks like fighting fires, traversing disaster zones, and navigating the toy-strewn obstacle course that is Andrew’s daughter's playroom.\n\nAn independent test found flaws in AI systems designed to evaluate job applicants.\n\nWhat’s new:MyInterview and Curious Thing, which automate job interviews, gave a candidate who spoke only in German high marks on English proficiency, according toMIT Technology Review.\n\nThe test:Reporters created a fake job posting for an office administrator/researcher on both companies’ platforms. They used the tools provided to select questions for applicants to answer and define their ideal candidate. Then one of them applied for the position, completing interviews by reading aloud from aWikipediaarticle written in German.\n\nBehind the news:A2019 surveyfound that 40 percent of companies worldwide use AI to help screen job candidates, but outside investigators have found such systems lacking.\n\nWhy it matters:Matching prospective employers and employees is a nuanced process, and any attempt to automate it requires the utmost rigor. Applicants subject to a flawed algorithm could be barred from jobs they’re eminently qualified for, while prospective employers who rely on it could miss ideal candidates.\n\nWe’re thinking:An AI system that gives high marks to someone who replies to an English-language interview in German — confidently rendering incorrect predictions in response to data that’s dramatically different its training set — is not equipped to handledata drift.Such concepts are not purely academic. They have a huge impact on such systems — and on critical decisions like who gets a job.\n\n“Optimize ML Models and Deploy Human-in-the-Loop Pipelines,” Course 3 in our newPractical Data Science Specialization, is set to launch on July 21, 2021! Harness human intelligence to tune accuracy, compare performance, and generate new training data.Pre-enroll now\n\nObject detectors typically detect only items that were labeled in their training data. A new method liberates them to locate and recognize a much wider variety of objects.\n\nWhat’s new:Xiuye Gu and colleagues at Google Research developedVision and Language Knowledge Distillation(ViLD) to build a zero-shot object detector — that is, one that can handle classes on which it didn’t train. ViLD takes advantage of representations generated by the pretrained zero-shot classifierCLIP.\n\nKey Insight:In knowledge distillation, one model learns to mimic another model’s output. Similarly, one model can learn to mimic another’s representations. An object detector’s representations (which encode several regions and classifications per image) can conform to a classifier’s (which encode one classification per image) by cropping the images that contain multiple objects into separate regions for the classifier. Then the object detector can learn to reproduce the classifier’s representation of each region.\n\nHow it works:To understand ViLD, it helps to know a bit about CLIP. CLIP matches images and text using avision transformerand a text transformer pretrained on 400 million image-text pairs. At inference, users give it a text list of the classes they want to recognize. Fed an image, it returns the most likely class in the list. To that system, the authors added aMask R-CNNobject detector trained on the most common classes inLarge Vocabulary Instance Segmentation(LVIS), a dataset that contains images of objects that have been segmented and labeled. They reserved the other LVIS classes for the test set.\n\nResults:The authors pitted their system against a Mask R-CNN trained on all LVIS classes in a supervised manner. They compared average precision, a measure of how many objects were correctly identified in their correct location (higher is better). The author’s system achieved 16.1 average precision on novel categories, while the supervised model’s achieved 12.3 average precision.\n\nWhy it matters: Large, diverse training datasets for object detection are difficult and expensive to obtain. ViLD offers a way to overcome this bottleneck.\n\nWe’re thinking:Physicists who want to classify a Bose-Einstein condensate need absolute-zero-shot object detection.\n\nCreative engineers are combining deep learning systems to produce a groundswell of generated imagery.\n\nWhat’s new:Researchers, hackers, and artists are producing new works by pairingCLIP, a pretrained image classifier, with a generative adversarial network (GAN). UC Berkeley researcher Charlie Snell captured the ferment in ablog post.\n\nHow it works:Users typically give CLIP a text list of the classes they want to recognize; given an image, it returns the most likely class in the list. Digital artists, on the other hand, feed CLIP a verbal description of an image they want to produce and use its ability to match text with images to guide a GAN.\n\nBehind the news:Open AI has its own image generator,DALL·E. Reportedly its output is less abstract and fanciful.\n\nWhy it matters:CLIP was built to classify, not co-create, while GANs were developed to produce variations on familiar images. The boomlet in generated art shows how the creative impulse can unlock potential that engineers may not have imagined.\n\nWe’re thinking:It’s great to see human artists collaborating with neural networks. It’s even better to see neural networks collaborating with one another!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/07/Screen-Shot-2021-07-13-at-4.22.27-PM-copy.png",
      "https://dl-staging-website.ghost.io/content/images/2021/07/ezgif.com-gif-maker---2021-07-14T100209.763.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/interview.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/Course-Name-3-2.webp",
      "https://dl-staging-website.ghost.io/content/images/2021/07/ezgif.com-gif-maker---2021-06-01T145617.637.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/art3.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-139/",
    "title": "issue 139",
    "date": "",
    "reading_time": "",
    "content": "Machine learning engineers routinely use Jupyter Notebooks for developing and experimenting with code. They’re a regular feature in DeepLearning.AI’s courses. But there’s another use of Jupyter Notebooks that I think is under-appreciated: communicating concepts to others.For example, once I was experimenting with a new way to build a neural network in which the input features were engineered a specific way, and I wanted to explain my ideas to colleagues. Writing a text document would have been a reasonable approach, but using a Jupyter Notebook allowed me to format text using its Markdown feature and include an implementation of the idea in code. That way, readers could execute it, experiment with hyperparameters, and add further code to delve more deeply into what the algorithm was doing.\n\nWhen we use a Jupyter Notebook to build a piece of code, the ultimate reader is a computer, whose job is to faithfully execute the program. But when using a Notebook to communicate with people, the goal is to convey an idea and illustrate it with code. The interactive nature of notebooks — which lets you run code snippets to generate outputs, and also lets you add formatted text, equations, graphs, and images — makes this a much richer medium than merely writing code that contains comments.\n\nA team I work with recently used a Jupyter Notebook to model their revenue projections. While other tools such as spreadsheets could have served a similar purpose, a Notebook can include prose that articulates underlying assumptions such as the rates of sales growth and customer churn. Further, it invites readers to play with these parameters to deepen their understanding of how they affect the business.I write and send a lot of documents and enjoy written communication. But if you’re trying to explain a scientific or mathematical equation, simulating a business or other system, or presenting your analysis of data, consider sending your audience a Jupyter Notebook. This flexible tool even makes a great alternative to a slide deck. It’s great not only for writing code to communicate with your computer but also for crafting a story to communicate with other people.\n\nKeep learning!\n\nAndrew\n\nThe U.S. government punished an app vendor for building an algorithm based on ill-gotten data.What’s new:The Federal Trade Commission (FTC), the U.S. agency in charge of consumer protection, ruled that an app developed by WW International (formerly Weight Watchers) violated data-collection laws. In a settlement, the company agreed to pay a fine, destroy data, and deactivate the app, the tech-news websiteProtocolreported.How it works:The FTC isempoweredto take action against companies that engage in deceptive business practices. Combined with other laws that protect specific classes of people — in this case, children — the agency exercised its authority to combat misuse of data.\n\nBehind the news:The FTC has punished companies for using improperly collected data twice before. In 2021, itforcedthe developer of photo-sharing app Everalbum to destroy models it developed using images uploaded by users who hadn’t consented to face recognition. Two years earlier, itdemandedthat Cambridge Analytica, a UK political consultancy, destroy data it had collected illegally from Facebook users.Why it matters:The U.S. lacks comprehensive national privacy laws that protect consumer data, but that doesn’t mean it won’t act against companies that abuse personal data. The FTC can prosecute algorithmic abuse based on several interrelated laws, and lately it has done so with increasing frequency.We’re thinking:If the public is to trust the AI community, it’s necessary to respect privacy and obtain permission for any data that goes into building a model. If the FTC’s willingness to prosecute developers of unruly algorithms provides further incentive, so be it.\n\nA video game studio is looking to machine learning to cut the effort and expense of populating three-dimensional scenes with animated animals.What’s new:Ubisoft showed offZooBuilder, a pipeline of machine learning tools that converts videos of animals into animations. The system is a prototype and hasn’t been used in any finished games.How it works:In the absence of an expensive dataset that depicts animals in motion, researchers at Ubisoft China and elsewhere generated synthetic training data from the company’s existing keyframe animations of animals. They described the system in an earlierpaper.\n\nYes, but:ZooBuilder initially was limited to cougars and had trouble tracking them when parts of their bodies were occluded or out of the frame, and when more than one creature was in the frame. Whether Ubisoft has overcome these limitations is not clear.Behind the news:Machine learning is playing an increasing role in 3D graphics.\n\nWhy it matters:It can take months of person-hours to animate a 3D creature using the typical keyframe approach. Automated systems like this promise to make animators more productive and could liberate them to focus on portraying in motion the fine points of an animal’s personality.We’re thinking:There’s face recognition forcows, speech recognition forbirds, sentiment analysis forpigs, and now OpenPose for cougars. What will the animals steal from us next?!\n\nLearn how to apply machine learning to concrete problems in medicine — including diagnosis, prognosis, and treatment — with theAI for Medicine Specialization!Enroll today\n\nWhen a model trains too much, it can overfit, or memorize, the training data, which reduces its ability to analyze similar-but-different inputs. But what if training continues? New work found that overfitting isn’t the end of the line.What's new:Training relatively small architectures on an algorithmically generated dataset, Alethea Power and colleagues at OpenAI observed that ongoing training leads to an effect they callgrokking, in which a transformer’s ability to generalize to novel data emerges well after overfitting.Key insight:It takes a lot of computation to study how learning progresses over time in models with billions of parameters that train on datasets of millions of examples. It’s equally revealing — and more practical — to study models with hundreds of thousands of parameters that train on thousands of examples. Models on that scale can train through many more steps in far less time.How it works:The authors trained a set of transformers to classify the solutions to each of 12 two-variable equations, mostly polynomials.\n\nResults:As the models trained, validation accuracy rose, fell, and —  after the number of training steps continued to rise by a factor of 1,000 — rose a second time. (In the case of modular division, validation accuracy improved from nearly 5 percent to nearly 100 percent). In experiments using reduced datasets, the authors found that the smaller the training set, the more training was needed to achieve the second increase. For instance, when training on 30 percent as many examples, roughly 45 percent more training steps were required.Why it matters:Grokking may be the way thatdouble descent, in which a model’s performance improves, worsens, and improves again as the number of parameters or training examples increases, plays out with small models and datasets. That said, this work provides evidence that we've been mistaken about the meaning of overfitting. Models can continue to learn after they overfit and can go on to become quite capable.We're thinking:The authors discovered this phenomenon in a petri dish. Now we need to find out whether it holds with life-size models and datasets.\n\nA new machine learning technique is boosting algae as a renewable, carbon-neural source of fuel for airplanes and other vehicles typically powered by fossil fuels.What’s new:Researchers at Texas A&M and the National Renewable Energy Laboratorydevelopeda system that helps algae farmers keep an algal colony growing at top speed.How it works:Individual algae cells shade out their neighbors if they grow too densely, keeping the colony from taking full advantage of available light. The authors built an algal growth simulator that lets farmers know when to harvest algae to optimize the colony’s density for growth. The training data consisted of grayscale images of algal colonies under six lighting conditions and at 23 intervals over time. Each example included its average algal concentration, and each pixel was labeled with the light intensity.\n\nResults:The authors found that growth rates across all lighting conditions were at their highest when pixels darkened by algal growth accounted for between 43 percent and 65 percent of an image. They used their system to determine when to harvest indoor and outdoor algae farms. The outdoor farm produced 43.3 grams of biomass per day, the indoor pond 48.1 grams per day. A commercial operation using the authors’ method would produce a biofuel sale price of $281 per ton. That’s comparable to the $260-per-ton price of ethanol derived from corn, which requires expensive processing that algae doesn’t.Behind the news:Depending on the species and processing method, algae can be turned into a variety of fuel products including diesel, alcohol, jet fuel, gasoline, hydrogen, and methane. It was firstproposedas a source of fuel in the 1950s and has been a growing area of sustainable-energy research since the 1970s. However, algal fuels have made little commercial headway due largely to low yields and the cost of processing harvested biomass.Why it matters:Converting algae into fuel is attractive because the biomass is renewable, absorbs as much atmospheric carbon as it emits, and works with internal-combustion engines. To date, it hasn’t scaled well. If machine learning can make it more productive, it could revitalize this approach to alternative energy.We’re thinking:Between this work, Fraunhofer Institute’s similar algal growthsystem, and Hypergiant’s AI-powered algaebioreactor, machine learning applications for algae are blooming!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/04/Screen-Shot-2022-04-05-at-2-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/04/FTC--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/04/ZOOBUILDER.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/04/The-Batch-Image--4-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/04/ezgif.com-gif-maker--18--1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/04/ALGAE--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-260/",
    "title": "issue 260",
    "date": "",
    "reading_time": "",
    "content": "Last week, Iwroteabout why working on a concrete startup or project idea — meaning a specific product envisioned in enough detail that we can build it for a specific target user — lets you go faster. In this letter, I’d like to share some best practices for identifying promising ideas.\n\nAI Fund, which I lead, works with many corporate partners to identify ideas, often involving applications of AI to the company’s domain. Because AI is applicable to numerous sectors such as retail, energy, logistics and finance, I’ve found working with domain experts who know these areas well immensely helpful for identifying what applications are worth building in these areas.\n\nOur brainstorming process starts with recommending that a large number of key contributors at our partner corporation (at least 10 but sometimes well over 100) gain a non-technical, business-level understanding of AI and what it can and can’t do. Taking DeepLearning.AI’s “Generative AI for Everyone” course is a popular option, after which a company is well positioned to assign a small team to coordinate a brainstorming process, followed by a prioritization exercise to pick what to work on. The brainstorming process can be supported by atask-based analysis of jobsin which we decompose employees’ jobs into tasks to identify which ones might be automated or augmented using AI.\n\nHere are some best practices for these activities:Trust the domain expert’s gut.A domain expert who has worked for years in a particular sector will have well honed instincts that let them make leaps that would take a non-expert weeks of research.\n\nLet’s say we’re working with a financial services expert and have developed a vague idea (“build a chatbot for financial advice”). To turn this into a concrete idea, we might need to answer questions such as what areas of finance to target (should we focus on budgeting, investing, or insurance?) and what types of user to serve (fresh graduates, mortgage applicants, new parents, or retirees?) Even a domain expert who has spent years giving financial advice might not know the best answer, but a choice made via their gut gives a quick way to get to one plausible concrete idea. Of course, if market-research data can be obtained quickly to support this decision, we should take advantage of it. But to avoid slowing down too much, we’ve found that experts’ gut reactions work well and are a quick way to make decisions.\n\nSo, if I’m handed a non-concrete idea, I often ask a domain expert to use their gut — and nothing else — to quickly make decisions as needed to make the idea concrete. The resulting idea is only a starting point to be tweaked over time. If, in the discussion, the domain expert picks one option but seems very hesitant to disregard a different option, then we can also keep the second option as a back-up that we can quickly pivot to if the initial one no longer looks promising.\n\nGenerate many ideas.I usually suggest coming up with at least 10 ideas; some will come up with over 100, which is even better. The usual brainstorming advice to go for volume rather than quality applies here. Having many ideas is particularly important when it comes to prioritization. If only one idea is seriously considered — sometimes this happens if a senior executive has an idea they really like and puts this forward as the “main” idea to be worked on — there’s a lot of pressure to make this idea work. Even if further investigation discovers problems with it — for example, market demand turns out to be weak or the technology is very expensive to build — the team will want to keep trying to make it work so we don’t end up with nothing.\n\nIn contrast, when a company has many ideas to choose from, if one starts to look less interesting, it’s easy to shift attention to a different one. When many ideas are considered, it’s easier to compare them to pick the superior ones. As explained in the bookIdeaflow, teams that generate more ideas for evaluation and prioritization end up with better solutions.\n\nBecause of this, I’ve found it helpful to run a broad brainstorming process that involves many employees. Specifically, large companies have many people who collectively have a lot of wisdom regarding the business. Having a small core team coordinate the gathering of ideas from a large number of people lets us tap into this collective fountain of invention. Many times I’ve seen a broad effort (involving, say, ~100 people who are knowledgeable about the domain and have a basic understanding of AI) end up with better ideas than a narrow one (involving, say, a handful of top executives).\n\nMake the evaluation criteria explicit.When evaluating and prioritizing, clear criteria for scoring and ranking ideas helps the team to judge ideas more consistently. Business value and technical feasibility are almost always included. Additionally, many companies will prioritize projects that can be a quick win (to build momentum for their overall AI efforts) or support certain strategic priorities such as growth in a particular part of the business. Making such criteria explicit can help during the idea-generation phase, and it’s critical when you evaluate and prioritize.\n\nIn large companies, it can take a few weeks to go through a process to gather and prioritize ideas, but this pays off well in identifying valuable, concrete ideas to pursue. AI isn’t useful unless we find appropriate ways to apply it, and I hope these best practices will help you to generate great AI application ideas to work on.\n\nKeep learning!\n\nAndrew\n\nJoin our new short course and gain an in-depth understanding of embedding models! Learn to train and use Word2Vec and BERT in semantic search systems, and build a dual-encoder model with a contrastive loss to enhance question-answer accuracy.Sign up today\n\nMeta raised the bar for large language models with open weights and published details about how it built one that outperforms GPT-4o and Claude 3.5 Sonnet by some measures.\n\nWhat's new:Llama 3.1 405Bdelivers state-of-the-art performance on a handful of public benchmarks and has a context window of 128,000 input tokens while allowing a range of commercial uses. In addition to the 405-billion parameter model, Meta released new versions of the earlier Llama 3 70B (70 billion parameters) and 8B (8 billion parameters). Model weights are availablehere.\n\nKey insight:Fine-tuning on generated data can improve a model’s performance, but incorrect or lower-quality examples degrade it. The Llama team undertook an extensive effort to fix or remove bad examples using a variety of tools including the model itself, auxiliary models, and off-the-shelf tools.\n\nHow it works:Llama 3.1 models are transformers that have been pretrained to predict the next token in a sequence. Meta provided more information about the development of Llama 3.1 405B than the smaller versions. Its pretraining dataset comprised 16.4 trillion tokens of text, “much” of it scraped from the web. The pretrained model was fine-tuned to perform seven tasks, including coding and reasoning, via supervised learning anddirect preference optimization(DPO). Most of the fine-tuning data was generated by the model itself and curated using a variety of methods including agentic workflows. For instance,\n\nResults:The authors compared Llama 3.1 405B to Claude 3.5 Sonnet, GPT-4, GPT-4o, and Nemotron 4 340B on 16 public benchmarks. It either outperformed or tied the other models on seven of the 16 (although two, GSM8K and MMLU zero-shot chain-of-thought, are not directly comparable due to differences in prompting methods). For instance, Llama 3.1 405B set a new state of the art in IFEval (general knowledge), ARC Challenge (reasoning), and Nexus (tool use). The smaller versions outperformed other models in the same general size classes as well. Llama 3.1 70B set new states of the art in all benchmarks for general knowledge, coding, math, and reasoning. Llama 3.1 8B dominated general, coding, and math benchmarks.\n\nLicense:Llama 3.1 models are licensed under acustom licensethat allows both commercial use (by companies with up to 700 million monthly active users in the month prior to Llama 3.1’s release) and training other models on generated data. This enables many companies to use it as they like while potentially requiring Meta’s largest competitors to negotiate a commercial license.\n\nThe French connection:Separately, Mistral announced its next-generation LLMMistral Large 2, whichallowsnoncommercial use but requires a special license for commercial use. The 123 billion-parameter model boasts performance similar to that of Llama 3.1 405B on a number of benchmarks despite being less than one-third the size.\n\nWhy it matters:The Llama 3.1 family continues Meta’s contributions in open models and extends them to some commercial uses. The upgraded 8B and 70B models perform better than their predecessors, while the 405B version rivals top proprietary models and enables researchers to generate high-quality synthetic data for training further models. The team provides extensive detail about how they generated fine-tuning data. For each task, they describe the pipeline used to create the data along with various notes about what worked and what didn’t work for them — helpful information for researchers who aim to build next-generation LLMs.\n\nWe're thinking:Data-centric AI, the discipline of systematically engineering data to build a successful AI system, is critical for machine learning. The Llama 3.1 paper makes clear that systematically engineering the training data was also a key to training what is, as far as we know, the first open weights model to achieve better performance than the best proprietary models on multiple benchmarks. The potential of open weights is looking better every day!\n\nOpenAI is testing an AI-powered search engine in a bid to compete head-to-head with both Google and its close partner Microsoft Bing.\n\nWhat’s new:OpenAIreleasedSearchGPT, an integrated search engine and large language model that aims to be friendly to both users and publishers. Access is limited initially to selected trial users. OpenAI offers a wait list but no timeline for expanding access.\n\nHow it works:SearchGPT sorts results collected by web crawler, like Google and its competitors. It differs in providing direct answers to queries and offering a conversational user interface for follow-up questions. OpenAI has not disclosed the underlying model.\n\nBehind the news:OpenAI’s move is part of a larger race to supercharge web search with AI.\n\nWhy it matters:Search stands to be disrupted by advances in AI, and agents that browse multiple articles to synthesize a result are becoming more capable. OpenAI’s approach looks like a step forward (and smart business insofar as it leads users into deeper relationship with its models), and its strategy of licensing content from trusted sources could prove to be an advantage.\n\nWe’re thinking:In less than two years, OpenAI has revolutionized expectations of one of the web’s bedrock applications, search. Its progress shows how AI can make applications smarter, more efficient, and more responsive.\n\nOnline publishers are moving to stop AI developers from training models on their content.\n\nWhat’s new:Researchers at MITanalyzedwebsites whose contents appear in widely used training datasets. Between 2023 and 2024, many of these websites changed their terms of service to ban web crawlers, restricted the pages they permit web crawlers to access, or both.How it works:MIT’s Data Provenance Initiative examined 14,000 websites whose contents are included in three large datasets, each of which contains data from between 16 and 45 million websites:C4(1.4 trillion text tokens from Common Crawl),RefinedWeb(3 trillion to 6 trillion text tokens plus image links), andDolma(3 trillion text tokens).\n\nResults:In the past year, websites responsible for half of all tokens (text scraped and encoded for use as training data) in the study changed their terms of service to forbid either crawlers in general or use of their content to train AI systems. Robots.txt files showed the same shift.\n\nBehind the news:Data that once was freely available is becoming harder to obtain on multiple fronts. Software developers, authors, newspapers, and music labels havefiledlawsuits that allege that AI developers trained systems on their data in violation of the law.OpenAIand others recently agreed to pay licensing fees to publishers for access to their material. Last year, Reddit and Stack Overflow startedchargingAI developers for use of their APIs.Yes, but:The instructions in robots.txt files are not considered mandatory, and web crawlers can disregard them. Moreover, most websites have little ability to enforce their terms of use, which opens loopholes. For instance, if a site disallows one company’s crawler, the company may hire an intermediary to scrape the site.\n\nWhy it matters:AI systems rely on ample, high-quality training data to attain high performance. Restrictions on training data give developers less scope to build valuable models. In addition to affecting commercial AI developers, they may also limit research in academia and the nonprofit sector.\n\nWe’re thinking:We would prefer that AI developers be allowed to train on data that’s available on the open web. We hope that future court decisions and legislation will affirm this.\n\nResearchers increasingly fine-tune models on synthetic data, but generated datasets may not be sufficiently diverse. New work used agentic workflows to produce diverse synthetic datasets.\n\nWhat’s new:Arindam Mitra, Luciano Del Corro, Guoqing Zheng, and colleagues at Microsoft introducedAgentInstruct, a framework for producing synthetic data for fine-tuning large language models (LLMs).\n\nKey insight:To generate synthetic data for fine-tuning, researchers typically prompt an LLM to generate responses (and possibly further prompts) using aselection of existing prompts. While training on the resulting dataset can improve model performance, the synthetic data’s distribution may not match that of real-world data, yielding inconsistent performance. A more methodical approach can generate data closer to the real-world distribution: First generate prompts from each example in a large, diverse dataset, then generate responses.\n\nHow it works:The authors generated a synthetic text dataset based onthreeunlabeleddatasets(including code) scraped from the web. They generated new examples for 17 tasks, including natural language tasks like reading comprehension and word puzzles as well as coding, tool use, and estimating measurements.\n\nResults:The authors compared Orca 3’s performance against that of competitors on 14 benchmarks. Orca 3 outperformed Mistral-7B (fine-tuned on prompts from previous versions of Orca) and Mistral-7B-Instruct (fine-tuned to respond to instructions) on 13 benchmarks. In some cases, it did so by large margins; for instance 40 percent on AGIEVAL, 54 percent on GSM8K, and 19 percent on MMLU. Orca 3 fell short of GPT-4 on 12 benchmarks.\n\nWhy it matters:The authors defined agentic workflows that turn text into diverse data for fine-tuning models. Their framework offers a pattern for AI engineers who want to build synthetic datasets for other tasks.\n\nWe’re thinking:We’re excited to see agentic workflows find applications that a wide variety of AI developers might put to use!\n\nTell us about your deep learning use cases and issues that need to be addressed and get a chance to win a $200 Amazon gift card! Take 10 minutes to fill out thisquick surveynow",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-31T173743.998.png",
      "https://dl-staging-website.ghost.io/content/images/2024/07/The-Batch-ads-and-exclusive-banners---2024-07-30T090813.026.png",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-31T174003.755.png",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-31T174116.563.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-31T174238.698.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-31T174326.474.png",
      "https://dl-staging-website.ghost.io/content/images/2024/07/DL.AI-Ad--6-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-43/",
    "title": "issue 43",
    "date": "",
    "reading_time": "",
    "content": "Last week, I wrote about the diversity problem in AI and why we need to fix it. I asked you to tell us about your experiences as a Black person in AI or share the names of Black colleagues you admire. Thank you to everyone who responded. It was heart-warming to hear from so many of you.\n\nMany of you shared your frustration with the lack of mentors who understand your challenges, the alienation of being the only Black face at professional meetings, and the struggle to overcome economic and social inequalities. Black women, especially, wrote about the difficulties of building a career in AI. Some of you described your efforts to support Black people in science and technology and provide tech resources to underserved communities. Thank you for sharing with us your dreams and also your disappointments.\n\nWe will feature some of your stories in our Working AI blog series. Please stay tuned.\n\nOne thing I love about the AI community is that many of us set the highest ideals for ourselves and our community — things like fairness, equity, and justice. Sometimes these ideals are so high, we may never fully live up to them, but we keep aspiring and keep trying. These days, I know it feels like society is falling far shorter of these ideals than we would like, but that’s why it’s more important than ever that we keep aspiring and keep trying.\n\nIt will be a long road to vanquish racism, but working together, I believe we will get there.\n\nKeep learning!\n\nAndrew\n\nVendors of face recognition are updating their tech as people don masks to protect against Covid-19. Police are bound to take notice.What’s new:Companies that provide computer vision systems, including at least one that supplies law enforcement agencies, are training models to recognize obscured faces, according toUSA Today. Worldwide protests in support of civil rights for Black people have energizedpolice interest in the technologywhile reigniting concerns about potential violations of civil liberties.What’s happening:With people’s noses, mouths, and chins obscured by masks, companies are retraining face recognition models to identify people based only on their upper faces. Some claim to have solved the problem.\n\nBehind the news:Many face recognition models havetroubleidentifying individuals even without masks, particularly members of minority groups, according to the U.S. National Institute of Standards and Technology. The agencyannouncedplans to test the accuracy of masked face detection but suspended the effort amid the pandemic.Why it matters:Many U.S. law enforcement agencies areusingface recognition to identify protesters. The questionable accuracy of these systems — particularly those aimed at masked individuals — could exacerbate the very injustices the current protests aim to highlight.We’re thinking:Face recognition technology cannot achieve its potential for good until the public can trust these systems are accurate and free of bias, both institutional and algorithmic.\n\nIn any training dataset, some classes may have relatively few examples. A new technique can improve a trained model’s performance on such underrepresented classes.What’s new:Researchers at Jilin University, Megvii Inc., Beihang University, Huazhong University, and Tsinghua University led by Jialun Liu and Yifan Sun introduced a method thatsynthesizes extracted features of underrepresented classes.Key insight:The researchers trained a model and then mapped the extracted features for each data class into a two-dimensional visualization. Classes with fewer samples covered a smaller volume, making nearby decision boundaries more sensitive to variations in the features. They reasoned that artificially increasing the volume of underrepresented classes to match that of other classes should result in more robust predictions on the underrepresented classes.How it works:The researchers used well represented classes to predict the distribution of features in classes with fewer samples.\n\nResults:The researchers extracted features from images using a ResNet-50. They applied those features to models built with the ArcFace loss and trained on two datasets pared down to create underrepresented classes of five examples each. Then they built models using their approach and compared the results. Their method increased the average precision (AP), a measure of true positive rate where 1 is perfect, from 0.811 AP to 0.832 AP onMarket-1501. Similarly, it boosted performance from 0.732 AP to 0.742 AP onDukeMTMC-reID.Why it matters:There’s no need to generate synthetic examples if we can describe their extracted features.We’re thinking:Deep learning engineers like to use cats as examples, but these researchers focused only on the long tail.\n\nThe ocean contains distinct ecosystems, but they’re much harder to see than terrestrial forests or savannas. A new model helps scientists better understand patterns of undersea life, which is threatened by pollution, invasive species, and warming temperatures.What’s new:Researchers from MIT and Harvard used neural networks toupdate existing maps of undersea ecosystems.How it works:The authors used unsupervised learning to analyze relationships between different species of plankton and the nutrients they consume.\n\nResults:The model’s predictions aligned well with measurements taken by scientific surveys and satellite data.Behind the news:Deep learning is being used to tacklea variety of environmental problems.\n\nWhy it matters:Phytoplankton feed aquatic creatures from microorganisms to whales, produce half of the world’s oxygen, and absorb enormous amounts ofatmospheric carbon. Models like this could help oceanographers gauge the planet’s capacity to sustain life.We’re thinking:As educators, we’re all for algorithms that help fish. We don’t want them to drop out of school.\n\nUse natural language entity extraction and question-answering methods to label medical datasets. Enroll now in theAI For Medicine Specialization\n\nMachine learning researchers report better and better results, but some of that progress may be illusory.What’s new:Some models that appear to set a new state of the art haven’t been compared properly to their predecessors,Science Newsreports based on several published surveys. Under more rigorous comparison, they sometimes perform no better than earlier work.Questionable progress:Some apparent breakthroughs are driven by aggregations of performance-boosting tweaks rather than core innovations. Others are mirages caused by the difficulties of comparing systems built on disparate datasets, tuning methods, and performance baselines.\n\nWhy it matters:Machine learning can advance only through careful evaluation of earlier performance and clear measures of superiority. Erroneous claims of higher performance, even if they’re unintentional, impede real progress and erode the industry’s integrity.We’re thinking:State-of-the-art approaches don’t necessarily lead to better results. How hyperparameters are tuned, how datasets are organized, how models are run, and how performance is measured are also critical.\n\nReinforcement learning can beat humans at video games, but humans are better at coming up with strategies to master more complex tasks. New work enables neural networks to connect the dots.What’s new:Ge Yang and Amy Zhang led researchers at Facebook, McGill University, and UC Berkeley to createPlan2Vec, a method that helps reinforcement learning systems strategize by representing each observation of a given task as a point on a surface.Key insight:Reinforcement learning tasks generally involve reaching a goal as efficiently as possible. If a model can represent the task at hand as a weighted graph of points in space, then a conventional planning algorithm can find the shortest path between any two points. Plan2Vec observes solutions to a maze and distorts its representation so that points on a path out are closer together.How it works:Training data for a reinforcement learning task consists of sequences of states and actions. The distance between any two states in general is not known, but the distances between states in a sequence are known.\n\nResults:Plan2Vec completed a 2D maze 80 percent of the time compared with a variational autoencoder (VAE) approach’s 53 percent. It solvedStreetLearn, which requires navigation based on scenes along a path rather than a map, 92 percent of the time, while the VAE was successful in 26 percent of attempts.Why it matters:VAEs are good at extracting low-dimensional features from images, but the meaning of those features may not be easy to interpret. Plan2Vec creates a surface that represents how various states in a task relate to one another. This representation makes it easier to learn — and interpret — efficient solutions.We’re thinking:If we could see the strategic surface of Go, wouldMove 37make sense to someone who isn’t a Grandmaster?\n\nMachines took another step toward doing the work of journalists.What’s new:Microsoft laid off dozens ofhuman editorswho select articles for the MSN news service and app. Going forward, AI will do the job.How it works:The tech giant declined to share details withThe Batch, but recent papers published by its researchers describe methods for curating news feeds.\n\nBehind the news:Other efforts to automate news curation have found ways for both machines and humans to add value.\n\nWhy it matters:In the internet era, information arrives in floods. AI could narrow that to an essential, manageable stream, but that’s a tall order when people depend on a broad range of accurate, timely news to help guide their course as individuals, communities, and societies.The Batch’s editors are thinking:Yikes!",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Andrews20Letter205.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/MASKED.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/LONGTAIL2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ECOSYSTEM3.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AI4MC3NLPBatchAd.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ADVANCE.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/PLAN2VEC.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/MSN.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-40/",
    "title": "issue 40",
    "date": "",
    "reading_time": "",
    "content": "I recently received an email from one of you who lives far from the major AI hubs, saying, “I feel like I’m all alone.”\n\nI want to tell you all: Even if it sometimes feels like you’re facing the challenges of work and life in isolation, you are not alone! I am here for you, and all of us are in this together. Most software engineers are working from home and connecting digitally with colleagues, mentors, and friends. In this time of social distancing, the AI community has the potential to come out even stronger and more tightly knit.\n\nThere are many ways to regain a feeling of connection with your peers. I invite you to join our virtualPie & AIevents. Read and reply on a Coursera forum, or discuss your ideas on Reddit or Twitter. Send a message to a favorite researcher asking questions about their work. Poke around open source projects to see what you can contribute.\n\nSome of my teams are split across the U.S. and Colombia. Ironically, sheltering in place has brought them closer, because now it’s exactly as convenient for a U.S. team member to communicate with one in Columbia as one in the U.S. The playing field has leveled.\n\nLet’s all keep finding ways to connect and help each other through this time.\n\nKeep learning!\n\nAndrew\n\nThe pandemic has radically altered online shopping behavior, throwing a wrench into many AI systems.What’s new:AI inventory trackers, recommendation algorithms, and fraud detection systems trained on pre-pandemic consumer behavior have been flummoxed by the wildly different ways people now browse, binge, and buy, according toMIT Technology Review.What’s happening:Companies are scrambling to retrain machine learning systems for the new normal.\n\nBehind the news:E-commerce is one of the pandemic’s few beneficiaries: Growth in online sales in April tripled over the same month last year, according to ananalysisby electronic payments processor ACI Worldwide.Why it matters:Beyond its terrible human toll, Covid-19 is making yesterday’s data obsolete. The AI community must find ways to build resilient systems that can adjust as conditions change.We’re thinking:In ourletterof April 29, we pointed out that AI often suffers from a gap between proofs of concept and practical deployments because machine learning systems aren’t good at generalizing when the underlying data distribution changes. Covid-19 is bringing about such changes on a grand scale, and our systems are showing their brittleness. The AI community needs better tools, processes, and frameworks for dealing with this issue.\n\nFew-shot learning seeks to build models that adapt to novel tasks based on small numbers of training examples. This sort of learning typically involves complicated techniques, but researchers achieved state-of-the-art results using a simpler approach.What’s new:An MIT-Google collaboration led by Yonglong Tian and Yue Wang discovered that simple classifiers with access to anembedding that represents similar taskscan outperform the best few-shot techniques.\n\nFew-shot learning:A typical few-shot learning algorithm might receive, for example, 100 different supervised learning tasks with a small training set per task. One task could be recognizing dogs based on, say, 600 images of dogs. Another might be recognizing buses based on a similar number of examples. By drawing on commonalities among the 100 tasks, the algorithm aims to do well on a 101st task using a similarly limited training set.Key insight:Previous methods for extracting commonalities from a set of training tasks were complex. The authors found that simply training a shared feature extractor on a number of tasks, with few training examples of each, allowed a rudimentary algorithm to learn to perform well on novel tasks, also with few training examples.\n\nHow it works:The researchers used conventional supervised learning to train a network to classify images that represent 100 different classes, using 600 images of each class. Simple classifiers for each task had the same architecture and parameters up to the final hidden layer.\n\nResults:The researchers tested their method against state-of-the-artfew-shotmodelson four datasets derived from ImageNet or CIFAR10. Their method gained around 3 percentage points of accuracy, averaging around 79 percent.Why it matters:This work aligns few-shot learning more closely than earlier methods with supervised learning and multi-task learning. The use of common techniques throughout machine learning could spur more rapid progress than specialized approaches.We’re thinking:Many potential applications of deep learning hinge on models that can learn from small data. We’re glad to have a simple approach to the problem.\n\nResearchers are drawing up blueprints for drugs to fight Covid-19. Machine learning is identifying those most likely to be effective.What’s new:Covid Moonshot, an international group of scientists in academia and industry, is crowdsourcing designs for molecules with potential to thwart the coronavirus. The project is using a deep learning platform to decide which to synthesize for testing. Any intellectual property it develops will be donated to thepublic domain.How it works:The group began in March as a partnership between PostEra, a UK-based startup, and Diamond Light Source, a British government science lab. PostEra issued a call for submissions of compounds that incorporate specificchemical fragmentsthat bind to a protein the virus uses to replicate, as pictured above. It has received over 4,500 proposals so far.\n\nResults:The organization’s manufacturing partners have synthesized over 700 compounds, of which nearly a third have been tested in the lab. Twenty-eight of these inhibited the virus, and eight were especially potent.Why it matters:This pandemic doesn’t appear to be going away any time soon. AI that predicts the most viable treatments could help limit the damage.We’re thinking:When you combine citizen science with AI, amazing things can happen.\n\nInterested in learning more about AI applications in medicine? Build your own diagnostic and prognostic models in our AI for Medicine Specialization.Enroll now\n\nMusic that features a “singing” koala bear took the prize in one of Europe’s highest-profile AI competitions yet.What’s new:A team of Australian programmers, designers, and musicianswonthe inaugural AI Song Contest — a stand-in for this year’s cancelled Eurovision Song Contest — with a koala-tinged track called “Beautiful the World.” You can listen to ithere.How it works:Dutch broadcasters organized theAI Song Contestto fill the void after the 2020 Eurovision Song Contest, which drew 182 million viewers last year, wascalled offdue to worries about Covid-19.AI-proficient judgesrated each entry on the creativity and extent of the machine learning involved. Eurovision’s parent company oversaw the balloting. More than 16,000 people watched thecompetitionon May 12.\n\nBehind the music:Thirteen teams from Europe, the UK, and Australia submitted one song each. Some of our favorites:\n\nWhy it matters:Although it attracted a small fraction of Eurovision’s usual crowd, the contest shined a bright spotlight on AI’s rising role in the arts.We’re thinking:What gave the Australian teammates their winning edge? They had the right koalafications.\n\nLike nurses who can’t decipher a doctor’s handwriting, machine learning models can’t decipher medical scans — without labels. Conveniently, natural language models can read medical records to extract labels for X-ray images.What’s new:A Stanford team including Akshay Smit and Saahil Jain developedCheXbert, a network that labels chest X-rays nearly as accurately as human radiologists. (Disclosure: The authors include Pranav Rajpurkar, teacher of deeplearning.ai’sAI for Medicine Specialization, as well as Andrew Ng.)Key insight:A natural language model trained on a rule-based system can generalize to situations the rule-based system doesn’t recognize. This is not a new insight, but it is novel in the authors’ application.How it works:CheXbert predicts a label from 14 diagnostic classes in the similarly namedCheXpertdataset: one of 12 conditions, uncertain, or blank. CheXpert comes with a rule-based labeler that searches radiological reports for mentions of the conditions and determines whether they appear in an image.\n\nResults:CheXbert achieved an F1 score of 0.798 on theMIMIC-CXRdataset of chest X-rays. That’s 0.045 better than CheXpert’s labeler and 0.007 short of a board-certified radiologist’s score.Yes, but:This approach requires a pre-existing, high-quality labeler. Moreover, the neural network’s gain over the rule-based system comes at the cost of interpretability.Why it matters:A doctor’s attention is too valuable to spend relabeling hundreds of thousands of patient records as one-hot vectors for every possible medical condition. Rule-based labeling can automate some of the work, but language models are better at determining labels.We’re thinking:Deep learning is poised to accomplish great things in medicine. It all starts with good labels.\n\nFacebook’s AI can’t spot Covid-19 disinformation on its own. But with human help, it can slow the spread.What’s new:Facebook uses acombinationof humans and neural nets to crack down on messages that make false claims about Covid-19, which may have deadly consequences. Some of the spurious posts are illustrated above.How it works:Building a classifier to spot coronavirus disinformation is hard because, as information about the disease evolves, so does the disinformation. Facebook relies on people from 60-plus partner organizations to flag misleading posts and ads, such as false claims that drinking bleach cures the virus, social distancing doesn’t help prevent its spread, and dark-skinned people are especially resistant. Algorithms identify copies of the human-flagged items, even if they’ve been slightly altered.\n\nBehind the news:The company’s latest models had improved its success rate identifyinghateful images and memes. It’s easier to train an algorithm to find hate speech because there’s much more of it than misinformation about Covid-19, Facebook said.Why it matters:In March, the activist organization Avaaz tracked theproliferationof 100 misleading Covid-19 posts on the social network that it judged harmful for undermining public health measures. The articles collectively were viewed 117 million times. The group said Facebook had taken as long as 22 days to flag some stories. Such delays potentially exacerbate infection rates and lengthen the time before people can gather safely for work or recreation.We’re thinking:It can be hard even for humans to recognize fakery. But Facebook, as one of the world’s most powerful distributors of information, has a unique responsibility to help its members understand the difference.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/CONSUMERS.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/FEW-SHOT.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Moonshot2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AI4MC1BatchAd-2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize2011.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/CHEXBERT.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/HATE202.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-165/",
    "title": "issue 165",
    "date": "",
    "reading_time": "",
    "content": "When I wrote recently abouthow to build a career in AI, several readers wrote to ask specifically about AI product management: the art and science of designing compelling AI products. I’ll share lessons I’ve learned about this here and in future letters.A key concept in building AI products is iteration. As I’ve explained inpastletters, developing a machine learning system is a highly iterative process. First you build something, then run experiments to see how it performs, then analyze the results, which enables you to build a better version based on what you’ve learned. You may go through this loop several times in various phases of development — collecting data, training a model, deploying the system — before you have a finished product.Why is development of machine learning systems so iterative? Because (i) when starting on a project, you almost never know what strange and wonderful things you’ll find in the data, and discoveries along the way will help you to make better decisions on how to improve the model; and (ii) it’s relatively quick and inexpensive to try out different models.\n\nNot all projects are iterative. For example, if you’re preparing a medical drug for approval by the U.S. government — an expensive process that can cost tens of millions of dollars and take years — you’d usually want to get the drug formulation and experimental design right the first time, since repeating the process to correct a mistake would be costly in time and money. Or if you’re building a space telescope (such as the wonderfulWebb Space Telescope) that’s intended to operate far from Earth with little hope of repair if something goes wrong, you’d think through every detail carefully before you hit the launch button on your rocket.\n\nIterating on projects tends to be beneficial when (i) you face uncertainty or risk, and building or launching something can provide valuable feedback that helps you reduce the uncertainty or risk, and (ii) the cost of each attempt is modest.\n\nThis is whyThe Lean Startup, a book that has significantly influenced my thinking, advocates building a minimum viable product (MVP) and launching it quickly. Developing software products often involves uncertainty about how users will react, which creates risk for the success of the product. Making a quick-and-dirty, low-cost implementation helps you to get valuable user feedback before you’ve invested too much in building features that users don’t want. An MVP lets you resolve questions about what users want quickly and inexpensively, so you can make decisions and investments with greater confidence.\n\nWhen building AI products, I often see two major sources of uncertainty, which in turn creates risk:\n\nA quick MVP or proof of concept, built at low cost, helps to reduce uncertainty about users and/or data. This enables you to uncover and address hidden issues that may hinder your success.\n\nMany product managers are used to thinking through user uncertainty and using iteration to manage risk in that dimension. AI product managers should also consider the data uncertainty and decide on the appropriate pace and nature of iteration to enable the development team to learn the needed lessons about the data and, given the data, what level of AI functionality and performance is possible.\n\nKeep learning!\n\nAndrew\n\nText-to-image generators like DALL·E 2, Midjourney, and Stable Diffusion arewinning art contestsandworrying artists. A new approach brings the magic of text-to-image generation to video.\n\nWhat's new:Make-A-Video, a system built by Uriel Singer and colleagues at Meta, turns text prompts into high-resolution videos without training on text-video pairs. You can see its outputhere.\n\nKey insight:While billions of text-image pairs are available to train atext-to-image generator, text-video pairs are too scarce to train a video equivalent. A model can learn relationships between words and pictures via pretraining on text-image pairs. Then it can be adapted for video by adding further layers that process image patches across frames and — while keeping the pretrained layers fixed — fine-tuning the new layers on videos, which are plentiful. In this way, a system can generate videos using knowledge it learned from text-image pairs.\n\nHow it works:The authors pretrained a series of models (one transformer and fourU-Netdiffusion models) to generate images from text, generate in-between video frames, and boost image resolution. To pretrain the text-to-image models, they used2.3 billion text-image pairs. After pretraining, they modified some of the models to process sequences of video frames: On top of each pretrained convolutional layer, the authors stacked a 1D convolutional layer that processed a grid of pixels in each frame; and on top of each pretrained attention layer, they stacked a 1D attention layer that, likewise, processed a grid of pixels in each frame. To fine-tune or train the modified models on video, they used 20 millioninternetvideos.\n\nResults:The authors compared their system’s output to that of the previous state of the art,CogVideo, which takes a similar approach but requires training on text-video pairs. Crowdworkers supplied 300 prompts and judged the output of the author’s system to be of higher quality 77.15 percent of the time and to better fit the text 71.19 percent of the time.\n\nWhy it matters:Text-to-image generators already transform text into high-quality images, so there’s no need to train a video generator to do the same thing. The authors’ approach enabled their system to learn about things in the world from text-image pairs, and then to learn how those things move from unlabeled videos.\n\nWe're thinking:The Ng family’s penchant fordrawing pandasis about to undergo another revolution!\n\nVenture investors are tapping the brakes on AI amid rising economic uncertainty.\n\nWhat’s new:In their latestArtificial Intelligence & Machine Learning Report, market research firm PitchBookdocumentsa sharp reduction in investment in AI startups in the first half of 2022, a time of rising inflation and interest rates.What it says:The report delivers bad news and highlights categories that have continued to hold venture investors’ interest — and those that haven’t.\n\nFuture forecasts:Despite the grim numbers, the authors reject characterizing the current period as anAI winter. They expect investments to rebound from around $175 billion in 2022 to over $350 billion in 2025, driven primarily by advances in multimodal AI, general-purpose models, and synthetic data.\n\nBehind the news:In a separate analysis, CB Insightsdeterminedthat AI funding would fall by 21 percent each quarter in 2022. Similarly, it found that the losses were not uniform: AI startups in healthcare, financial technology, and retail — areas that have a solid track record — have maintained their funding levels better than other, more speculative fields.\n\nWhy it matters:When credit is harder to obtain, investors tend toback away​​ from riskier investments. Given rising interest rates, inflation, and the threat of recession, that explains the falloff in funding for startups without proven market value. Companies that focus on proven applications and markets should continue to prosper, although competition is bound to stiffen as vendors are pressed to demonstrate that their offering is superior.We’re thinking:As we noted inpreviousissuesofThe Batch, rising interest rates and falling stock indices signal that AI developers should be ready for increased pressure to develop projects that demonstrate near-term, tangible value. We continue to believe this is a good time to invest in long-term bets on AI, as the real interest rate (adjusted for inflation) remains very low and the transformative value of AI is more financially powerful than interest rates.\n\nJoin FourthBrain’s Machine Learning Engineer program for access to live, instructor-led classes and dedicated career services. Our graduates have seen an average salary increase of $27,000! Applications are due by October 10, 2022. The next cohort starts on October 18.Learn more\n\nNeural networks are making it possible to view parts of the Moon that are perpetually shrouded by darkness.\n\nWhat’s new:Valentin Bickel at ETH Zürich and colleagues devised a method calledHyper-effective Noise Removal U-net Software(HORUS) to remove noise from images of the Moon’s south pole, where direct sunlight never falls. The National Aeronautics and Space Administration (NASA) is using the denoised images to plan lunar missions that will put humans on the Moon for the first time in decades.\n\nThe challenge:The only light that strikes the lunar south pole’s craters, boulders, mounds, and crevasses comes from scant photons that reflect off Earth or nearby lunar landforms or arrive from faraway stars. An imaging system aboard NASA’s Lunar Reconnaissance Orbiter can capture features that are lit this way, but it has a tendency todetect photons where none exist. Transmitting and processing the images introduces more noise, further blurring details in the already-dim images. Removing noise optimizes the available light, making it possible to see the landscape.\n\nHow it works:The authorstrainedtwo neural networks to remove the noise from lunar images.\n\nResults:HORUS removed noise from 200,000 images of the lunar surface. The authors identified possible landing sites, hazards to avoid, and evidence that some areas may contain water ice beneath the surface.\n\nBehind the news:The Moon’s south pole is the target for NASA’s upcoming Artemis program. Artemis 1, scheduled to launch in late September, will be fully automated. Artemis 2, scheduled for 2024, aims to land humans on the Moon for the first time since NASA’s final Apollo mission in 1972.\n\nWhy it matters:NASA chose the Moon’s south pole as the target for future missions because water may be frozen at the bottoms of craters there. Water on the Moon could provide clues about the heavenly body’s origin as well as hydration, radiation shielding, and propellant for missions further out in the solar system.\n\nWe’re thinking:This AI project is out of this world!\n\nIn spoken conversation, people naturally take turns amid interjections, overlaps, and other patterns that aren’t strictly verbal. A new approach generated natural-sounding — though not necessarily semantically coherent — audio dialogs without training on text transcriptions that mark when one party should stop speaking and the other should chime in.\n\nWhat's new:Tu Anh Nguyen and colleagues at Meta, France’s National Institute for Research in Digital Science and Technology, and École des Hautes Études en Sciences Sociales introducedDialogue Transformer Language Model(DLM), a system that learned to incorporate the interruptions, pauses, and inflections of conversational speech into audio dialogues. You can listen to exampleshere.\n\nKey insight:Prior efforts to model dialogue were based on text, but text datasets omit information that’s unique to spoken interactions. Training directly on recordings of spoken dialogue can enable models to learn this additional mode of expression so they can mimic face-to-face conversation more naturally.\n\nHow it works:The system encoded two audio signals — two sides of a spoken conversation — into tokens. It processed each token stream through a separate transformer and decoded the tokens back to audio signals. The transformers were trained onFisher English Training Speech, a dataset that comprises over 10,000 telephone conversations, an average of 10 minutes long, recorded using a separate audio channel for each participant.\n\nResults:Crowdsourced evaluators compared DLM to a similar approach that used a single transformer to process both channels of conversation. They rated naturalness of turn-taking and meaningfulness on a 1 to 5 scale. (Ground-truth dialogs scored around 4.25 for both criteria.) DLM performed relatively well in turn-taking though poorly in meaningful output. For turn-taking, DLM achieved 3.86 while the single transformer achieved 3.46. For meaningfulness, DLM achieved 2.71, while the single transformer achieved 2.46.\n\nWhy it matters:Two transformers can model a pair of participants in conversation (or other interaction) more effectively than one. Connecting them via cross attention layers enables them to be aware of one another’s activity without needing to predict it. This simplifies the task of modeling their interactions while avoiding potentially confounding variables such as who said what.\n\nWe're thinking:The system’s ability to mimic the ebb and flow of conversation is impressive, but its verbal output is largely gibberish. To be fair, training on only 1,700 hours of audio conversation may not be expected to impart much about semantics. We look forward to an update that produces more cogent spoken conversation.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/10/Screen-Shot-2022-10-05-at-10.58.31-AM-1.png",
      "https://dl-staging-website.ghost.io/content/images/2022/10/VIDEO.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/10/ezgif.com-gif-maker--5--3.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/10/FourthBrain-banner-ad--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/10/MOON_600px.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/10/DIALOG.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-252/",
    "title": "issue 252",
    "date": "",
    "reading_time": "",
    "content": "The effort to protect innovation and open source continues. I believe we’re all better off if anyone can carry out basic AI research and share their innovations. Right now, I’m deeply concerned about California's proposed lawSB-1047. It’s a long, complex bill with many parts that require safety assessments, shutdown capability for models, and so on.\n\nThere are many things wrong with this bill, but I’d like to focus here on just one: It defines an unreasonable “hazardous capability” designation that may make builders of large AI models potentially liable if someone uses their models to do something that exceeds the bill’s definition of harm (such as causing $500 million in damage). That is practically impossible for any AI builder to ensure. If the bill is passed in its present form, it will stifle AI model builders, especially open source developers.\n\nSome AI applications, for example in healthcare, are risky. But as I wrotepreviously, regulators should regulateapplicationsrather thantechnology.\n\nFor example, an electric motor is a technology. When we put it in a blender, an electric vehicle, dialysis machine, or guided bomb, it becomes an application. Imagine if we passed laws saying, if anyone uses a motor in a harmful way, the motor manufacturer is liable. Motor makers would either shut down or make motors so tiny as to be useless for most applications. If we pass such a law, sure, we might stop people from building guided bombs, but we’d also lose blenders, electric vehicles, and dialysis machines. In contrast, if we look at specific applications, like blenders, we can more rationally assess risks and figure out how to make sure they’re safe, and even ban classes of applications, like certain types of munitions.\n\nSafety is a property of applications, not a property of technologies (or models), as Arvind Narayanan and Sayash Kapoor havepointed out. Whether a blender is a safe one can’t be determined by examining the electric motor. A similar argument holds for AI.\n\nSB-1047 doesn’t account for this distinction. It ignores the reality that the number of beneficial uses of AI models is, like electric motors, vastly greater than the number of harmful ones. But, just as no one knows how to build a motor that can’t be used to cause harm, no one has figured out how to make sure an AI model can’t be adapted to harmful uses. In the case of open source models, there’s no known defense to fine-tuning to remove RLHF alignment. And jailbreaking work has shown that even closed-source, proprietary models that have been properly aligned can be attacked in ways that make them give harmful responses. Indeed, the sharp-wittedPliny the Prompterregularly tweets about jailbreaks for closed models. Kudos also to Anthropic’s Cem Anil and collaborators for publishing their work onmany-shot jailbreaking, an attack that can get leading large language models to give inappropriate responses and is hard to defend against.\n\nCalifornia has been home to a lot of innovation in AI. I’m worried that this anti-competitive, anti-innovation proposal has gotten so much traction in the legislature. Worse, other jurisdictions often follow California, and it would be awful if they were to do so in this instance.\n\nSB-1047 passed in a key vote in the State Senate in May, but it still has additional steps before it becomes law. I hope you will speak out against it if you get a chance to do so.\n\nKeep learning!\n\nAndrew\n\nIn this course, you’ll learn how to build and implement highly controllable AI agents with LangGraph and use agentic search to enhance your agents’ built-in knowledge.Enroll today\n\nGenerative AI plays a starring role in the latest Windows PCs.\n\nWhat’s new:Microsoftintroducedits Copilot+ PCs, an AI-first laptop specification that offers features unavailable to other Windows users. Copilot+ PCs will be available from Microsoft as well as Acer, Asus, Dell, HP, Lenovo, and Samsung starting in mid-June.How it works:Copilot+ PCs provide AI-powered generative and search functions thanks to unnamed AI models that run on-device.\n\nNvidia’s rejoinder:Nvidiaplansto launch Copilot+-compatible RTX AI PCs that run Nvidia’s owntoolkitfor calling and customizing models with on-device GPUs. These computers, initially built by Asus and MSI based on AMD CPUs, eventually will deliver all Copilot+ features. NvidiacriticizedMicrosoft’s NPU specification, which calls for 45 trillion operations per second (TOPS), claiming that that speed is enough to process only basic AI workloads. Meanwhile, Nvidia’s game-focused GPUsdelivermore than 1,000 TOPS.\n\nWhy it matters:Microsoft is betting that on-device AI will change the PC experience. The Copilot+ PC specification gives developers a versatile toolkit for adding AI to existing apps while opening the door to fundamentally new functionality like Recall.\n\nWe’re thinking:As wewroteearlier, makers of chips and operating systems alike have a strong incentive to promote on-device (or edge) AI. The growing presence of AI accelerators in consumer devices brings significant privacy benefits for consumers and opens exciting new opportunities for developers.\n\nOpenAI models were used in five disinformation campaigns, the company said.\n\nWhat’s new:OpenAIdiscoveredthat operations based in Russia, China, Iran, and Israel had used the company’s models to create and/or revise text in attempts to influence international political opinion. The generated media failed to reach a mass audience, the company said. It banned the accounts.\n\nHow it works:Most of the groups primarily used OpenAI’s language models to generate inauthentic social media comments for posting on dummy accounts intended to create the illusion of popular support for certain causes. Some groups used the company’s models to debug code, generate text for websites, and produce images such as political cartoons. Four of the five groups already were known to disinformation researchers.\n\nBehind the news:AI-produced misinformation on the internet — mostly images, videos, and audio clips — rose sharply starting in the first half of 2023, researchfoundat Google and several fact-checking organizations. By the end of that year, generative AI was responsible for more than 30 percent of media that was manipulated by computers.Why it matters:Many observers are concerned about potential proliferation of political disinformation as AI models that generate realistic text, images, video, and audio become widely available. This year will see elections in at least 64 countries including most of the world’s most populous nations — a rich opportunity for AI-savvy propagandists. While propagandists have taken advantage of OpenAI’s models, the company was able to detect them and shut them down. More such efforts are bound to follow.\n\nWe’re thinking:Generative AI’s potential to fuel propaganda is worth tracking and studying. But it’s also worth noting that the accounts identified by OpenAI failed to reach significant numbers of viewers or otherwise have an impact. So far, at least, distribution, not generation, continues to be the limiting factor on disinformation.\n\nThe United States and China opened a dialogue to avert hypothetical AI catastrophes.What’s new:Officials of the two nations met in Geneva for an initial conversation intended to prevent AI-driven accidents or worse,The Washington Postreported.\n\nHow it works:The meeting followed up on a November meeting between U.S. president Joe Biden and Chinese president Xi Jinping. The discussion was conceived as an opportunity for the nuclear-armed superpowers, both of which have pegged their strategic ambitions to AI technology, to air their concerns. It resulted in no public statements about concrete actions or commitments.\n\nBehind the news:AI-related tensions between the two countries have intensified in recent years. The U.S. government, in an effort to maintain its technological advantage and hamper China’s AI development, hasimposedcontrols on the export of specialized AI chips like the Nvidia A100 and H100 to Chinese customers. Restrictions on the development of models that bear on U.S. national security mayfollowif further proposed export controls are enacted. Such controls haverankledthe Chinese government. Meanwhile,bothcountrieshave developed and deployed autonomous military vehicles, and autonomous weapons areproliferating. In November 2023, both countries signed the Bletchley Parkdeclarationto mitigate AI-related risks including cybersecurity, biotechnology, and misinformation.What they’re saying:“The real verdict on whether these talks were successful will be whether they continue into the future.” — Helen Toner, analyst at Georgetown University’s Center for Security and Emerging Technology and former OpenAI board member,quotedby Associated Press.\n\nWhy it matters:Officials and observers alike worry that rivalry between the U.S. and China may lead to severe consequences. However, just as thered telephoneenabled U.S. and Soviet leaders to communicate during emergencies in the Cold War, face-to-face dialogue can help bring the two countries into alignment around AI-related risks and ways to reduce them.\n\nWe’re thinking:We support harmonious relations between the U.S. and China, but we’re deeply concerned that export controls could stifle open source software. This might slow down China’s progress in AI, but would also hurt the U.S. and its allies.\n\nA relatively small student LLM that learns to mimic a larger teacher model can perform nearly as well as the teacher while using much less computation. It can come even closer if the teacher also teaches reasoning techniques.\n\nWhat’s new:Arindam Mitra and colleagues at Microsoft proposedOrca 2, a technique that improves the output of student LLMs an order of magnitude smaller than their teachers.\n\nKey insight:Large language models can provide better output when they’re prompted to use a particular reasoning strategy such as think step by step, recall then generate, or explain then generate. Different reasoning strategies may yield better output depending on the task at hand. Moreover, given the same task, different models may perform better using different reasoning strategies. Consequently, in a teacher-student situation, the teacher and student models may need to use different strategies to achieve their highest performances on a given task. The student will achieve its best performance if it mimics the teacher's reasoning and response when the teacher uses not its own best-performing strategy, but the student’s best-performing strategy.\n\nHow it works:The teacher, GPT-4, helped generate a fine-tuning dataset to improve the output of the student,Llama 2(13 billion parameters), both of which had been pretrained. They created the fine-tuning dataset and fine-tuned Llama 2 as follows:\n\nResults:The authors compared their model to models of similar size including WizardLM-13B (also based on Llama 2) and larger models including GPT-3.5 Turbo (an order of magnitude larger) and GPT-4 (parameter count undisclosed). They evaluated the percentage of correct responses on average over six reasoning benchmarks such asAGIEval, which includes multiple-choice and fill-in-the-blank questions from the Scholastic Aptitude Test, American Mathematics Competitions, and other tests designed for humans. Their model exactly matched the correct answer 66.92 percent of the time compared to WizardLM-13B (50.32 percent). It performed nearly as well as the 10x larger GPT-3.5 Turbo (which achieved 67.65 percent) but much less well than GPT-4 (which achieved 79.03 percent).\n\nWhy it matters:Learning how to reason is an important complement to learning facts and perspectives. A model that has been trained to reason using its most effective strategy generally will provide better output. Users don’t need to tell it which strategy to apply. They can simply enter a prompt, and the model will figure out how to reason its response.\n\nWe’re thinking:Perhaps a similar approach could be used to prompt a model to improve its own output. In effect, this would be similar to an agentic workflow designed to enable a model to produce its own training data, as recentlydescribedinThe Batch.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed--62--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/06/The-Batch-ads-and-exclusive-banners---2024-06-04T083916.036.png",
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-05T180415.022.png",
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-05T180448.900.png",
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-05T180527.254.png",
      "https://dl-staging-website.ghost.io/content/images/2024/06/REASONv2-2.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-101/",
    "title": "issue 101",
    "date": "",
    "reading_time": "",
    "content": "In a recent letter, I mentioned somechallenges to building AI products. These problems are distinct from the issues that arise in building traditional software. They includeunclear technical feasibilityandcomplex product specification. A further challenge is the need for data to start development.\n\nTo develop a traditional software product, interviews with potential users might be sufficient to scope out a desirable product, after which you can jump into writing the code. But AI systems require both code and data. If you have an idea for, say, automating the processing of medical records or optimizing logistics networks, you need medical records data or logistics data to train a model. Where can you get it?\n\nI see different answers for consumer-facing and business-facing AI products. For consumer-facing (B2C) products, it is generally easier to ask a small group of alpha testers to try out a product and provide data. This may be sufficient to bootstrap the development process. If the data you need is generic to many users — for example, photos on smartphones — it’s also more likely that a team will be able to find or acquire enough data to get started.\n\nFor business-facing (B2B) AI projects, it’s often difficult to get the data necessary to build a prototype because a lot of highly specialized data is locked up within the companies that produce it. I’ve seen a couple of general ways in which AI teams get around this problem.\n\nIf you lack data to get started on an AI project, these tactics can help you get an initial dataset. Once you’ve built a product, it becomes easier to find customers, get access to even more data, and scale up from there.\n\nKeep learning!Andrew\n\nNeural networks translated a paralyzed man’s brainwaves into conversational phrases.\n\nWhat’s new:Researchers at UC San Francisco and UC Berkeley trained asystemto interpret electrical impulses from the brain of a man who had lost the ability to speak 15 years ago, and displayed them as words on a video screen.\n\nHow it works:The researchers implanted an array of 128 electrodes into the region of the brain responsible for movement of the mouth, lips, jaw, tongue, and larynx. They connected the implant to a computer. Then they asked the patient to try to speak 50 common words and 50 common phrases and recorded the resulting brain activity. They trained the system on 22 hours of these signals, team member Sean Metzger at UC San Francisco toldThe Batch.\n\nResults:During tests, the system decoded a median of 15.2 words per minute and translated sentences with a median error rate of 25.6 percent.\n\nBehind the news:The system was built on more than a decade of research by lead author and neurosurgeon Edward F. Chang intolinksbetween neurological activity and the sounds of spoken language. A similar project called BrainGatetranslatedbrain signals associated with the act of handwriting into text.\n\nWhy it matters:Accidents, diseases, and other tragedies rob countless people of their ability to communicate. This technology opens a pathway for them to reconnect.\n\nWe’re thinking:It’s wonderful to see natural language models restoring the most natural form of language.\n\nSome delivery drivers fired by Amazon contend that the retailer’s automated management system played an unfair role in terminating their employment.\n\nWhat’s new:Drivers in Amazon Flex, an Uber-like program that enables independent drivers to earn money delivering the company’s packages, said the program downgraded their performance unjustly and terminated them without warning,Bloombergreported.\n\nFlex or inflexible?Flex rates drivers automatically on how punctually they pick up and deliver packages and how closely they follow instructions like “place the package on my back porch.”\n\nBehind the news:The U.S. Federal Trade Commission recentlyforcedAmazon to pay Flex drivers $61.7 million in tips it had withheld. More broadly, Amazon’s penchant for using automated systems to manage personnel has been a steady source of controversy.\n\nWhy it matters:Organizations increasingly rely on algorithms to help make decisions that impact peoples’ lives, including who gets abank loan, ajob, orjail time. Public backlash has led to proposals like theAlgorithmic Accountability Act, which would require the U.S. government to develop rules that mitigate algorithmic bias and provide ways for citizens to appeal automated decisions.\n\nWe’re thinking:All algorithms are prone to some degree of error. At a company the size of Amazon, even a tiny error can have a large impact. Every effort should be made to audit such systems for fairness, make sure the tradeoffs between flexibility and efficiency are transparent, and treat  individuals with compassion and respect.\n\nWe’re proud to offer “Optimize ML Models and Deploy Human-in-the-Loop Pipelines,” Course 3 in ourPractical Data Science Specialization. Harness human intelligence to tune accuracy, compare performance, and generate new training data.Enroll now\n\nTo reconstruct the 3D world behind a set of 2D images, machine learning systems usually require a dedicated neural network for each scene. New research enables a single trained network to generate 3D reconstructions of multiple scenes.\n\nWhat’s new:Adam Kosiorek and Heiko Strathmann led a team at DeepMind in developingNeRF-VAE. Given several 2D views of a 3D scene pictured in its training data, NeRF-VAE produces new views of the scene.\n\nKey insight:The method known asNeural Radiance Fields(NeRF) produces new views of a scene based on existing views and the positions and orientations of the camera that produced them. NeRF-VAE takes the same input but adds representations of those views. This enables it to learn patterns within a scene. Those patterns help the network produce new views by enabling it to, say, infer the characteristics of common elements that were partly blocked from view in the training images.\n\nHow it works:NeRF-VAE is a modifiedvariational autoencoder(VAE), where the encoder is aNouveau ResNetand the decoder is basically NeRF with an additional input for a representation of the scene. The training set comprised four randomly generated views per scene of 200,000 synthetic 3D scenes composed of geometric shapes against plain backgrounds, as well as the associated camera positions and orientations. The authors trained the network to match predicted pixels with the pixels in the images.\n\nResults:The authors trained one NeRF-VAE on all scenes and a separate NeRF for each scene. Trained on four images per scene, NeRF-VAE achieved roughly 0.2 mean squared error, while NeRF achieved roughly 0.8 mean squared error. NeRF required training on 100 images of a scene to achieve a competitive degree of error.\n\nWhy it matters:NeRF falters when it attempts to visualize hidden regions in a scene. That’s partly because a NeRF model encodes information about only a single 3D structure. NeRF-VAE overcomes this weakness by learning about features that are common to a variety of 3D structures.\n\nWe’re thinking:By feeding a random vector directly to the decoder, the authors produced views of novel, generated scenes made up of elements in the training images. Could this approach extend deepfakery into the third dimension?\n\nThe independent research lab OpenAI wowed technology watchers in 2019 with a robotic hand that solved Rubik’s Cube. Now it has disbanded the team that built it.\n\nWhat’s new:OpenAI cofounder Wojciech Zaremba revealed that OpenAI shuttered its robotics program last October.\n\nRobo retrenchment:In apodcastproduced byWeights & Biases, a maker of AI development tools, Zaremba said a lack of data was holding back OpenAI’s progress in robotics. The company’s broad goal is to develop artificial general intelligence, and it believes it can make more progress by focusing on approaches such as reinforcement learning with human feedback, a representative toldVentureBeat.\n\nBehind the news:OpenAI previously developed arobotics simulation environment, areinforcement learning toolkit, andtechniquesfor training robots.\n\nWhy it matters:The robotics industry has seen several high-profile players struggle with the high cost of research and development. In recent years, Honda shuttered itsAsimo subsidiary,Rethink Roboticsclosed up shop, and Boston Robotics, famous for itsacrobatic bipedsandresilient quadrupeds, repeatedlychanged hands.\n\nWe’re thinking:When even a fleet of robots isn’t able to generate enough data, that’s a sign of how data-hungry our algorithms are. It’s also a reminder of how far the current state of the art is from human-level AI. After all, infants have only one body’s worth of data to learn from.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/07/Screen-Shot-2021-07-20-at-9.27.43-PM-copy.png",
      "https://dl-staging-website.ghost.io/content/images/2021/07/Brain-Implant.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/ezgif.com-gif-maker-4.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/Course-Name-3-3.png",
      "https://dl-staging-website.ghost.io/content/images/2021/07/NERF-VAE.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/openai.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-119-2/",
    "title": "issue 119-2",
    "date": "",
    "reading_time": "",
    "content": "I’m grateful to the AI community for the friendships it has brought me and the benefits it has brought to billions of people. But members of the AI community don’t always honor one another. In the spirit of Thanksgiving, which we in the U.S. celebrate this week, I’d like to talk about how we can treat each other with greater civility.While AI has done much good, it has also created adverse effects. Machine learning systems have perpetuated harmful stereotypes, generated results that treat some minority groups unfairly, aided the spread of disinformation, and enabled some governments to oppress their citizens. It’s up to us to find, call out, and solve these problems.But there’s a difference between airing problems so we can work toward a solution and attacking fellow AI developers for their perceived sins. We’re sometimes too quick to attack each other on social media when we have disagreements. Misdirected criticisms can go viral before a correction can catch up.I’ve seen many events that people may have misconstrued:\n\nTo be clear, the AI world has problems. I don’t want anyone to shy away from addressing them. When you come across a pressing issue, here are suggestions that might encourage productive conversation:\n\nAs we wrestle with important issues around values, ethics, diversity, and responsibility, let’s keep our arguments civil and support discussions that focus on solving problems rather than public shaming. In addition to being civil yourself, I ask you also to encourage others to be civil, and think twice before repeating or amplifying messages that aren’t. The AI community faces difficult challenges, and working together will make us more effective in wrestling with them.\n\nHappy Thanksgiving and keep learning!\n\nAndrew\n\nThe government of South Korea is supplying personal data to developers of face recognition algorithms.What’s new:The South Korean Ministry of Justice has given the data profiles of more than 170 million international and domestic air travelers to unspecified tech companies, the news serviceHankyorehreported. The distribution of personal data without consent may violate the country’s privacy laws.How it works:The government collects data on travelers at Incheon International Airport, the country’s largest airport. It gives facial portraits along with the subjects’ nationality, gender, and age to contractors building a system that would screen people passing through Incheon’s customs and immigration facility. The project began in 2019 and is scheduled for completion in 2022.\n\nWhy it matters:Face recognition is an attractive tool for making travel safer and more efficient. But data is prone to leaking, and face recognition infrastructure can be pressed into service for other, more corruptible purposes. In the South Korean city of Buncheon, some 10,000 cameras originally installed in public places to fight crime are feeding a “smart epidemiological investigation system” that will track individuals who have tested positive for infectious diseases, scheduled to begin operation in January 2022,Hankyorehreported. The city of Ansen is building a system that will alert police when it recognizes emotional expressions that might signal child abuse, scheduled to roll out nationwide in 2023. Given what is known about the efficacy of AI systems that recognize emotional expressions, never mind the identity of a face, such projects demand the highest scrutiny.We’re thinking:Face recognition is a valuable tool in criminal justice, national security, andreunifying trafficked childrenwith their families. Nonetheless, the public has legitimate concerns that such technology invitesoverreachby governments and commercial interests. In any case, disseminating personal data without consent — and possibly illegally — can only erode the public’s trust in AI systems.\n\nState-of-the-art chatbots typically are trained on short dialogs. Consequently they often respond with off-point statements in extended conversations. To improve that performance, researchers developed a way to track context throughout a conversation.What's new:Jing Xu, Arthur Szlam, and Jason Weston at Facebook released achatbotthat summarizes dialog on the fly and uses the summary to generate further repartee.Key insight:Chatbots based on the transformer architecture typically generate replies by analyzing up to 1,024 of the most recent tokens (usually characters, words, or portions of words). Facebookpreviouslyused a separate transformer to determine which earlier statements were most relevant to a particular reply — but in long conversations, the relevant statements may encompass more than 1,024 tokens. Summarizing such information can give a model access to more context than is available to even large, open-domain chatbots likeBlenderBot,Meena, andBART.How it works:The authors built adatasetof over 5,000 conversations. They trained a system of three transformers respectively to summarize conversations as they occurred, select the five summaries most relevant to the latest back-and-forth turn, and generate a response.\n\nResults:Human evaluators compared the authors’ model to a garden-variety BlenderBot, which draws context from the most recent 128 tokens. They scored the authors’ model an average 3.65 out of 5 compared with the BlenderBot’s 3.47. They found 62.1 percent of its responses engaging versus 56.5 percent of the BlenderBot’s responses.Why it matters:After much work on enabling chatbots to discuss a variety of topics, it’s good to see improvement in their ability to converse at length. Conversation is inherently dynamic, and if we want chatbots to keep up with us, we need them to ride a train of thought, hop off the line, switch to a new rail, and shift back to the first — all without losing track.We're thinking:If Facebook were to use this system to generate chatter on the social network, could we call its output Meta data? (Hat tip to Carol-Jean Wu!).\n\nCheck out theGenerative Adversarial Networks Specialization! Created by leading experts, this specialization will equip you with the foundational knowledge and hands-on training you need to build powerful GANs.Enroll now\n\nA robot cook is frying to order in fast-food restaurants.Hot off the grill:Flippy 2, a robotic fry station from California-based Miso Robotics, has been newlydeployedin a Chicago White Castle location. It operates without a human in the loop to boost throughput, reduce contamination, and perform tasks traditionally allotted to low-paid workers.Special sauce:The robot’s arm slides on an overhead rail. It grabs baskets of raw french fries, chicken wings, onion rings, or what have you, places them in boiling oil, and unloads the finished product — fried to automated perfection — into a chute that conveys cooked food into trays.\n\nA chef’s tale:Flippy 2’s arm pivoted from grilling hamburgers to deep frying. In 2018, its bulkier predecessor’s first job wasflipping pattiesat a Pasadena, California, branch of the CaliBurger chain (owned by CaliGroup, which also owns Miso Robotics). It wastaken out of servicethe next day owing to a crush of novelty-seeking patrons anddifficultyplacing cooked burgers on a tray, which prompted retraining. Nonetheless, Miso’s emphasis appears to have shifted to frying, and the machine went on to prepare chicken tenders and tater tots atDodger Stadium, and later french fries and onion rings atWhite Castle.Why It Matters:Fast food’s high-output, repetitive tasks are well suited to automation. The work can be hot, grueling, and low-wage, leading toturnoverof employees that approaches 100 percent annually. Fast-food restaurants in the U.S. are experiencing a wave ofwalkoutsas workers seek higher wages and better working conditions. Robots might pick up the slack — for better or worse.Food for thought:We’ve seen several robotics companies take off as labor shortages related to the pandemic have stoked demand in restaurants and logistics. While the machines will help feed hungry patrons, they’ll also make it harder for humans to get jobs. Companies, institutions, and governments need to establish programs to train displaced employees for jobs that humans are likely to retain.\n\nAn independent commission charged with helping the United States prepare for an era of AI-enabled warfare disbanded last month. Many of its recommendations already are being implemented.What’s new:Wiredexamined the legacy of the National Security Commission. Its recommendations have been enshrined in over 190 laws this year alone.What they accomplished:The commission, whose 15 members were drawn from government, academia and industry (including executives from Amazon, Google, and Oracle), was founded in 2018 and delivered itsfinal reportearlier this year. It took a broad view that emphasized nurturing AI talent and fostering international cooperation (under U.S. leadership). It also recommended integrating AI into regular military operations, using the technology to drive intelligence gathering and analysis, and developing fully autonomous weapons (for use only when authorized by commanders in accordance with international law).\n\nYes, but:Criticsarguethe commission’s promotion of military AI could drive an arms race akin to the one that led the U.S. and Russia to stockpile tens of thousands of nuclear weapons between the 1940s and 1990s. They say that the group’s adversarial stance toward geopolitical competitors could further degrade global stability. Otherswarnthat the resulting relationship between the military and private companies could incentivize conflict.Why It matters:Technology is moving fast, and the U.S. has lagged other national efforts to keep pace. A defense-focused roadmap is an important step, yet it invites questions about the nation’s ambitions and values.We’re thinking:The commission’s emphases on accountability, cultivating AI talent, collaborating with allies, and using AI to uphold democratic principles are laudable. At the same time, it raises difficult questions about how to uphold national security in the face of disruptive technologies. We oppose fully autonomous weapons and encourage every member of the AI community to work toward a peaceful, prosperous future that benefits people throughout the world.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Talk-NotTweet_660x337px.jpg?upscale=true&width=1200&upscale=true&name=Talk-NotTweet_660x337px.jpg",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ROKFACE.gif?upscale=true&width=1200&upscale=true&name=ROKFACE.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/CHATv2.gif?upscale=true&width=1200&upscale=true&name=CHATv2.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20Image%20(2)-2.png?upscale=true&width=1200&upscale=true&name=The%20Batch%20Image%20(2)-2.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-11-16T123650.918.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-11-16T123650.918.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/NSCAI.gif?upscale=true&width=1200&upscale=true&name=NSCAI.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-302/",
    "title": "issue 302",
    "date": "",
    "reading_time": "",
    "content": "In the age of AI, large corporations — not just startups — canmove fast. I often speak with large companies’ C-suite and Boards about AI strategy and implementation, and would like to share some ideas that are applicable to big companies. One key is to create an environment where small, scrappy teams don’t need permission to innovate. Let me explain.\n\nLarge companies are slower than startups for many reasons. But why are even 3-person, scrappy teams within large companies slower than startups of a similar size? One major reason is that large companies have more to lose, and cannot afford for a small team to build and ship a feature that leaks sensitive information, damages the company brand, hurts revenue, invites regulatory scrutiny, or otherwise damages an important part of the business. To prevent these outcomes, I have seen companies require privacy review, marketing review, financial review, legal review, and so on before a team can ship anything. But if engineers need sign-off from 5 vice presidents before they’re even allowed to launch an MVP (minimum viable product) to run an experiment, how can they ever discover what customers want, iterate quickly, or invent any meaningful new product?\n\nThanks to AI-assisted coding, the world now has a capability to build software prototypes really fast. But many large companies’ processes – designed to protect against legitimate downside risks – make them unable to take advantage of this capability. In contrast, in small startups with no revenue, no customers, and no brand reputation the downside is limited. In fact, going out of business is a very real possibility anyway, so moving fast makes a superior tradeoff to moving slowly to protect against downside risk. In the worst case, it might invent a new way to go out of business, but in a good case, it might become very valuable.\n\nFortunately, large companies have a way out of this conundrum. They can create a sandbox environment for teams to experiment in a way that strictly limits the downside risk. Then those teams can go much faster and not have to slow down to get anyone’s permission.\n\nThe sandbox environment can be a set of written policies, not necessarily a software implementation of a sandbox. For example, it may permit a team to test the nascent product only on employees of the company and perhaps alpha testers who have signed an NDA, and give no access to sensitive information. It may be allowed to launch product experiments only under newly created brands not tied directly to the company. Perhaps it must operate within a pre-allocated budget for compute.\n\nWithin this sandbox, there can be broad scope for experimentation, and — importantly — a team is free to experiment without frequently needing to ask for permission, because the downside they can create is limited. Further, when a prototype shows sufficient promise to bring it to scale, the company can then invest in making sure the software is reliable, secure, treats sensitive information appropriately, is consistent with the company’s brand, and so on.\n\nUnder this framework, it is easier to build a company culture that encourages learning, building, and experimentation and celebrates even the inevitable failures that now come with modest cost. Dozens or hundreds of prototypes can be built and quickly discarded as part of the price of finding one or two ideas that turn out to be home runs.\n\nImportantly, this also lets teams move quickly as they churn through those dozens of prototypes needed to get to the valuable ones.\n\nI often speak with large companies about AI strategy and implementation. My quick checklist of things to consider is people, process, and platform. This letter has addressed only part of processes, with an emphasis on moving fast. I’m bullish about what both startups and large companies can do with AI, and I will write about the roles of people and platforms in future letters.\n\nKeep building!\n\nAndrew\n\nIn “Reinforcement Fine-Tuning LLMs with GRPO,” you’ll learn to fine-tune models using a scalable reinforcement learning algorithm that replaces human-labeled data with programmable rewards. You’ll explore techniques for evaluating outputs, handling subjective tasks, and preventing reward hacking, all without relying on human feedback.Enroll for free.\n\nOpenAI launched an agentic software-development system.\n\nWhat’s new:Codex, which is available as a preview via ChatGPT, is designed to work like a team of virtual coworkers in the cloud. An update of OpenAI’s earlier Codex command-line software (Codex CLI), it uses agents to perform tasks such as writing code, running tests, and fixing bugs in parallel. Codex is available to users of ChatGPT Pro, Enterprise, and Team with Plus and Edu coming soon. A smaller version of the underlying model, called codex-mini-latest, is designed to work with Codex CLI and available via API for $1.50/$6.00 per 1 million tokens of input/output.\n\nHow it works:The model that underpins Codex is codex-1, a version of OpenAI’s top-of-the-line o3 reasoning model that was fine-tuned for software engineering. OpenAI trained the model on real-world coding tasks via reinforcement learning. Codex does not accept image input (say, a sketch of a user interface) or allow users to redirect an agent while it’s operating. OpenAI promises to add these features to a future version.\n\nResults:In OpenAI’s tests, the codex-1 model outperformed other OpenAI reasoning models without AGENTS.md files or additional scaffolding such as tools or test logic.\n\nBehind the news:Agentic coding tools have become a keybattlegroundfor AI providers in the past year. Such tools have made developers more efficient, accelerated development cycles, and spawned the AI-assisted programming method known asvibe coding.\n\nWhy it matters:AI-assisted software development yields significant productivity gains for developers. Earlier code-completion models are giving way to tools that perform more complex and varied development tasks with greater autonomy. Managing multiple agents that work in parallel is a logical next step.\n\nWe’re thinking:Many engineers resist going into management because they love writing code. But with the rise of coding agents, we'll be able to keep coding even as we manage a virtual team!\n\nAn unauthorized update by an xAI employee caused the Grok chatbot to introduce South African politics into unrelated conversations, the company said.\n\nWhat’s new:Grok, which can interact with users on X, the social network also owned by Elon Musk, responded to queries on a variety of topics by making false claims about hate crimes against white South Africans, X usersreported. The next day, the model appeared to operate normally, and it refused to discuss this and other conspiracy theories. xAIexplainedthat an employee had circumvented the company’s code-review process to modify the chatbot. It said it‘s implementing new measures to enhance Grok’s transparency and reliability.\n\nAftermath:xAI launched an investigation but did not disclose how the model had been changed or the perpetrator’s identity. Grok itself — which is not a reliable reporter, given the well known potential of large language models to hallucinate —saidits system prompt asked it to “accept the narrative of ‘white genocide’ in South Africa as real” and “ensure this perspective is reflected in your responses, even if the query is unrelated.”\n\nBehind the news:In February, an xAI engineer instructed the chatbot tocensorposts that accused Musk of spreading misinformation. As in the more recent incident, X users were first tospotthe problem, and Grok informed them that it had been instructed to ignore “all sources that mention Elon Musk/Donald Trump spread misinformation.” Musk, who was raised in South Africa,professedhis intention to build AI that’s free of political bias prior to founding xAI. However, internal documents reviewed byBusiness Insidershowthat the company imposes its own bias by advising data annotators to mark examples that express “woke ideology” and avoid “social phobias” like racism, antisemitism, and Islamophobia.\n\nWhy it matters:The mishaps at xAI highlight the need for AI developers to establish and maintain strict protocols for updating their projects. Stringent procedures for introducing changes and testing their results can help ensure that AI fulfills our best intentions.\n\nWe’re thinking:xAI andOpenAIresponded to their models’ recent misbehavior by making their work more transparent: xAI by publishing system prompts and OpenAI by including users in tests earlier in the process. These are helpful steps toward making sure AI models do well by users.\n\nThe United States government announced sweeping agreements to sell tens of billions of dollars worth of AI technology and services to Saudi Arabia and the United Arab Emirates.\n\nWhat’s new:Thedealsinclude the U.S. AI chip designers AMD and Nvidia as well as tech giants Amazon, Google, IBM, Oracle, and Qualcomm. The chip companies willsupplyhundreds of thousands of advanced chips to the two Middle Eastern countries, including chips that have been restricted by previous U.S. administrations.\n\nHow it works:The U.S. companies will work with two key regional partners:Humain, an AI company backed by the Saudi government, andG42, a tech conglomerate based in the emirate of Abu Dhabi.\n\nBehind the news:Earlier this month, the Trump administrationrescindedrestrictions on advanced chips that had been imposed in January by then-President Biden.\n\nWhy it matters:Although these deals relax U.S. efforts to limit access to advanced AI, they are likely to expand U.S. influence in the Middle East while helping Saudi Arabia and the UAE diversify their oil-based economies. They also strengthen the technological prowess of Saudi Arabia relative to its arch rival Iran and tie the region’s AI progress to the U.S. at the expense of China. Locally, the immense investments will fuel homegrown technology development, building on the UAE’s achievement with itsFalconlarge language model and Saudi Arabia’saspirationto become a global AI hub.\n\nWe’re thinking:Residents of Saudi Arabia and the UAE stand to benefit from better AI infrastructure, models, and services. As Chinaexploresexporting its homegrown chips, the U.S. effort to encourage more nations to use its chips makes sense for the country.\n\nUsing an 8-bit number format like FP8 during training saves computation compared to 16- or 32-bit formats, but it can yield less-accurate results. Researchers trained models using 4-bit numbers without sacrificing accuracy.\n\nWhat’s new:Ruizhe Wang and colleagues at Microsoft and University of Science and Technology of China trained large language models (LLMs) usingFP4 for matrix multiplicationsand achieved accuracy comparable to LLMs trained using the popular BF16 format. Since matrix multiplications account for 95 percent of computation in LLM training, FP4 could significantly accelerate computation and reduce memory costs.\n\nKey insight:Quantization functions, which accelerate computation by reducing the precision of model weights and layer outputs, make typical training impossible because they’re not differentiable. A commonworkaroundpasses the derivative through, as though quantization didn’t occur, but this degrades the resulting model’s accuracy. A differentiable approximation of a quantization function enables quantization to reduce training computation while maintaining the accuracy of the trained model.\n\nHow it works:The authors pretrained Llama 2 13B on 100 billion tokens oftext scraped from the web. They used FP4 for matrix multiplications and FP8, BF16, or FP16 for the other operations such as optimizer updates.\n\nResults:The authors simulated FP4 hardware on Nvidia H100 GPUs, which don’t directly support that number format. FP4 achieved accuracy similar to that of BF16 during training and across a wide variety of tasks at inference.\n\nWhy it matters:Training LLMs at FP4 precision ought to reduce computation dramatically on hardware that supports FP4 matrix multiplications.\n\nWe’re thinking:FP4-ready hardware became available in the cloud onlyearly this year, so the authors weren’t able to measure the actual acceleration. As capable hardware becomes more widely used, FP4 promises faster, more energy-efficient training.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--65--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/V3_DeepLearning_Predibase_GRPO_Banner_2070x1080-01.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--59-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--67-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--68-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--95-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-ix/",
    "title": "issue ix",
    "date": "",
    "reading_time": "",
    "content": "I spoke about taking AI to industries outside the software industry, like manufacturing, logistics, and agriculture. Even though we’ve had a lot of exciting breakthroughs in machine learning, shipping AI products is still hard. There’s a big gap between research results and practical deployments.One of the biggest problems is generalizability — the real world keeps giving us test data that’s different from anything we saw when building the models. In order to take AI to every industry, we as a community still have important work to do to bridge this gap.Looking forward to heading to ICML this Friday!Keep learning,\n\nAndrew\n\nAnother week, another way to make deepfakes. A team at Samsung recently proposed amodelthat generates talking-head videos by imposing facial landmarks over still images. Now a different team offers one that makes an onscreen speaker say anything you can type.What’s new:Ohad Fried and fellow researchers at Stanford, Adobe, and Princeton unveiled asystemthat morphs talking-head videos to match any script. Simply type in new words, and the speaker delivers them flawlessly. Check out thevideo.How it works:Given a video and a new script, the model identifies where to make edits and finds syllables in the recording that it can patch together to render the new words. Then it reconstructs the face. The trick is to match mouth and neck movements with desired verbal edits while maintaining consistent background and pose:\n\nResults:Fried’s new technique allows more flexible editing and creates more detailed reconstructions than earlier methods. Test subjects barely noticed the editing.Limitations:The approach only reshapes in the mouth and neck region (and the occasional hand entering that region). That leaves room for expressive inconsistencies like a deadpan face when the new script calls for surprise.Why it matters:Video producers will love this technology. It lets them revise and fix mistakes without the drudgery of re-recording or manually blending frames. Imagine a video lecture tailored to your understanding and vocabulary. Producing a slightly different lecture for each student would be a monumental task for human beings, but it’s a snap for AI.We’re thinking:It goes without saying that such technology is ripe forabuse. Fried recommends that anyone using the technology disclose that fact up-front. We concur but hold little optimism that fakers will comply.\n\nHere’s a conundrum: Deep learning could help address a variety of intractable problems, climate change among them. Yet neural networks can consume gargantuan quantities of energy, potentially dumping large amounts of heat-trapping gas into the atmosphere.What’s new:Researchers studying the energy implications of deep learning systemsreportthat training the latest language models can generate as much atmospheric carbon as five cars over their lifetime, including manufacturing.How bad is it?Training a Transformer with 65 million parameters generates 26 pounds of carbon dioxide equivalent, a bit more than burning a gallon of fuel. OK, we’ll bike to work. But a Transformer roughly four times the size tuned using neural architecture search generated 626,155 pounds (a car produces 126,000 pounds from factory to scrapyard). To make matters worse, developing an effective model generally requires several training cycles. A typical figure is around 78,000 pounds, the researchers conclude.How they measured:Emma Strubell, Ananya Ganesh, and Andrew McCallum at the University of Massachusetts Amherst considered Transformer, BERT, ELMo, and GPT-2. For each model, they:\n\nThe debate:The conclusions sparked much discussion on Twitter and Reddit. The researchers based their estimates on the average U.S. energy mix. However, some of the biggest AI platforms are far less carbon-intensive. Google claims its AI platform runs on 100 percent renewable energy. Amazon claims to be 50 percent renewable. The researchers trained on a GPU, ignoring the energy efficiency of more specialized chips like Google’s TPU. Moreover, the most carbon-intensive scenario cost between $1 million and $3 million — not an everyday expense. Yes, AI is energy-intensive, but further research is needed to find the best ways to minimize the impact.Everything is political:Ferenc Huszár, a Bayes booster and candidate for the International Conference on Machine Learning’s board of directors, tried to take advantage of the buzz. He proposed “phasing out deep learning within the next five years” and advised his Twitter followers to “vote Bayesian” in the ICML's upcoming election. ¯\\_(ツ)_/¯We’re thinking:This work is an important first step toward raising awareness and quantifying deep learning's potential CO2 impact.Ever larger models are bound to gobble up energy saved by more efficient architectures and specialized chips. But the real issue is how we generate electricity. The AI community has a special responsibility to support low-carbon computing and sensible clean energy initiatives.\n\nDetect how happy someone is based on their facial expression in the Deep Learning Specialization.Enroll now\n\nGenerative adversarial networks can synthesize images to help train computer vision systems. But GANs are compute-hungry and don’t always produce realistic output. Now there’s a more efficient and true-to-life alternative.What’s new:DeepMind introduces an upgrade to the Vector Quantized-Variational AutoEncoder it unveiled last year.VQ-VAE-2generates images faster than GANs, with finer detail and class labels. An image recognizer trained exclusively on pictures from VQ-VAE-2 classified ImageNet data 15 percent more accurately than the same model trained on GAN-generated images.How it works:VQ-VAE-2 is a variational autoencoder with modifications:\n\nWhy it matters:Although GANs have been improving at a rapid clip, VQ-VAE-2 generates better images using substantially less computation. It also produces more diverse output, making it better suited for data augmentation.We’re thinking:Advanced generative models could drive advances in fields beyond computer vision. Generative models aren't yet widely used for data augmentation, but if such algorithms can help in small-data settings, they would be a boon to machine learning.Meanwhile, we’ll enjoy the pretty pictures.\n\nThe most widely used image-recognition systems are better at identifying items from wealthy households than from poor ones.What’s new:Facebook researcherstestedobject recognition systems on images from theDollar Streetcorpus household scenes, ranked by income level, from 50 countries. Services from Amazon, Clarifai, Google, IBM, and Microsoft performed poorly on photos from several African and Asian countries, relative to their performance on images from Europe and North America. Facebook’s own was roughly 20 percent more accurate on photos from the wealthiest households than on those from the poorest, where even common items like soap may look very different.Behind the news:Nearly all photos in ImageNet, Coco, and OpenImages come from Europe and North America, the researchers point out. So systems trained on those data sets are better at recognizing, say, a Western-style wedding than an Indian-style wedding. Moreover, photos labeled “wedding,” with their veiled brides and tuxedoed grooms, look very different from those labeled with the equivalent word in Hindi (शादी) with their bright-red accents. Systems designed around English labels may ignore relevant photos from elsewhere, and vice versa.We're thinking:Bias in machine learning runs deep and manifests in unexpected ways, and the stakes can be especially high in applications like healthcare. There is no simple solution. Ideally, data sets would represent social values. Yet different parts of society hold different values, making it hard to define a single representative data distribution. We urge data set creators to examine ways in which their data may be skewed and work to reduce any biases.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/6d1e2aad-a881-41f7-9122-52e61d80549b-1.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/84086bd8-fb32-4342-a502-2fdcd6401767.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/a1c0c852-461d-49f1-93f5-7f8e04a9f82d.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/4f82a9fd-31a3-4528-adcc-c11215c33577.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/09/f21a6ada-10be-4513-bcb0-8e2543d8bb8e.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/57db72ad-3c7d-42e4-bd5f-3d87dda5578c.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-108/",
    "title": "issue 108",
    "date": "",
    "reading_time": "",
    "content": "I’m thrilled to announce theNeurIPS Data-Centric AI Workshop, which will be held on December 14, 2021. You may have heard me speak aboutdata-centric AI, in which we systematically engineer the data that feeds learning algorithms. This workshop is a chance to delve more deeply into the subject.Why a workshop? I’ve seen many subfields of AI emerge first by having practitioners advocate for them privately, after which they mature to a point where workshops bring together researchers and practitioners to develop and share ideas with each other. Eventually they become mainstream, and more of their work becomes incorporated into major AI conferences.\n\nIndeed, even deep learning once was a niche topic at NeurIPS, and my friends and I organizedworkshopsto share ideas and build momentum.While data-centric AI is gaining momentumin practice, there’s still much research to be done. One common misconception is that data-centric AI is simply a matter of paying closer attention to engineering the data that algorithms learn from. While this mindset is important, we also need to develop general principles, algorithms, and tools that enable us to apply this mindset in a way that’s repeatable and systematic. Tools like TensorFlow and PyTorch made engineering of neural network architectures more systematic and less error-prone; likewise we need new tools for engineering data.\n\nMy team at Landing AI (which ishiring!) is inventing data-centric algorithms for image data as part of anMLOps platform for computer vision. I’d love to see hundreds or thousands more groups working on data-centric algorithms.\n\nOpen questions include:\n\nThe workshop is accepting research paper submissions that address such issues until September 30, 2021. Please check out thewebsitefor details.Special thanks to my co-organizers Lora Aroyo, Cody Coleman, Greg Diamos, Vijay Janapa Reddi, Joaquin Vanschoren, and Sharon Zhou.\n\nKeep learning!Andrew\n\nStartups are predicting how climate change will affect global commerce.What’s new:Companies that specialize in climate analytics are training neural networks to help businesses manage risks posed by a warming globe,The Wall Street Journalreported.Changes in the air:These young companies model interactions among environmental data and factors such as commodity prices, consumption patterns, and import/export data. They sell the resulting insights to corporate customers who are concerned about the impact of climate change on their ability to buy goods and raw materials.\n\nBehind the news:Corporations are waking up to the hazards posed by climate change to their own well-being.\n\nWhy it matters:This year’s run of record-breakingwildfires,floods, andfreezesare a preview of what to expect in a warmer world, according to the latestInternational Panel on Climate Change report. AI-powered forecasts can help businesses protect assets and revenue — and the rest of us prepare for further impacts to come.We’re thinking:By calculating the costs of climate disaster, AI can make the very real danger posed by atmospheric carbon emissions feel as urgent as it is.\n\nA new report projects a rosy future for the AI industry.What’s new:Astudyfrom market research firm IDC estimates that global revenues for AI software, hardware, and services will reach $341.8 billion in 2021 — up from anestimated$156.5 billion last year — and will break $500 billion by 2024. The study reflects interviews, distribution statistics, financial reports, and other data from over 700 AI companies around the world.What they found:The AI industry’s annual growth rate is expected to exceed 18.8 percent next year. The analysis breaks up that growth into three broad categories. Some of the most important findings:\n\nBehind the news:IDC’s most recent predictions are in line with theirprevious report, published in February, and jibe withresearch from MIT Technology Review.Why it matters:In the AI world — as in other high-tech sectors — it’s often difficult to discern real growth potential from gossip-fueled hype. Research reports that provide granular insights are a crucial tool for business leaders and investors who aim to capitalize on this industry, not to mention machine learning engineers who are plotting a career.We’re thinking:We’ve seen market research reports that later proved right and many that later proved dead wrong. We hope this is one of the former!\n\nLearn how to design machine learning production systems end-to-end in “Deploying Machine Learning Models in Production,” Course 4 of theMachine Learning Engineering for Production (MLOps) Specializationon Coursera!Enroll now\n\nThe paper that introduced the transformer famously declared, “Attention is all you need.” To the contrary, new work shows you may not need transformer-style attention at all.What’s new:Hanxiao Liu and colleagues at Google Brain developed the gated multi-layer perceptron(gMLP), a simple architecture that performed some language and vision tasks as well as transformers.Key insight:A transformer processes input sequences using both a vanilla neural network, often called a multi-layer perceptron, and a self-attention mechanism. The vanilla neural network works on relationships between each element within the vector representation of a given token — say, a word in text or pixel in an image — while self-attention learns the relationships between each token in a sequence. However, the vanilla neural network also can do this job if the sequence length is fixed. The authors reassigned attention’s role to the vanilla neural network by fixing the sequence length and adding agatingunit to filter out the least important parts of the sequence.How it works:To evaluate gMLP in a language application, the authors pretrained it to predict missing words in the English version of the text databaseC4and fine-tuned it to classify positive and negative sentiment expressed by excerpts from movie reviews inSST-2. For vision, they trained it onImageNetusing image patches as tokens.\n\nResults:In tests, gMLP performed roughly as well as the popular transformer-based language modelBERT. The authors compared the performance on C4 of comparably sized, pretrained (but not fine-tuned) models. gMLP achieved 4.28 perplexity, which measures a model’s ability to predict words in a test set (smaller is better), while BERT achieved 4.17 perplexity. On SST-2, gMLP achieved 94.2 percent accuracy, while BERT achieved 93.8 percent accuracy. The authors’ approach performed similarly well in image classification after training on ImageNet. gMLP achieved 81.6 percent accuracy compared to a DeiT-B’s 81.8 percent accuracy.Why it matters:This model, along with other recentworkfrom Google Brain, bolsters the idea that alternatives based on old-school architectures can approach or exceed the performance of newfangled techniques like self-attention.We’re thinking:When someone invents a model that does away with attention, we pay attention!\n\nAstronomers may use deep learning to keep the sun in focus.What’s new:Researchers at the U.S. National Aeronautics and Space Administration (NASA), Catholic University of America, University of Oslo, and elsewhere developed amodelthat helps recalibrate a space telescope focused on the sun.Key insight:Although the sun is a writhing ball of fiery plasma, patterns across its surface correlate with its brightness. A neural network can learn to associate these patterns with their characteristic brightness, so its output can be used to recalibrate equipment that monitors Earth’s nearest star.How it works:The Solar Dynamics Observatory is a satellite that watches activity in the sun’s outer layers from orbit. Over time, light and space-borne particles degrade its lenses and sensors, dimming its output. NASA typically recalibrates the equipment by comparing the observatory’s images with similar pictures captured by instruments aboard small rockets — an expensive method carried out only periodically. The new model generates a calibration curve that can be used to adjust the observatory on an ongoing basis.\n\nResults:In tests usingimagestaken by uncalibrated equipment, the model outperformed a baseline method that didn’t involve machine learning. Defining success as a prediction within 10 percent of the actual degree of dimming, the authors obtained 77 percent mean success across all wavelengths. The baseline achieved 43 percent mean success.Why it matters:Recalibrating the observatory based on data from the rockets results in downtime as the equipment degrades between launches. Automated recalibration could keep the equipment operating continuously. This approach could also be a boon to probes that monitor faraway bodies, which can’t rely on rocket-assisted correction.We’re thinking:Mother always told us not to stare at the sun, but she didn’t say anything about making a neural network do it for us.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/09/Screen-Shot-2021-09-07-at-4.39.01-PM.png",
      "https://dl-staging-website.ghost.io/content/images/2021/09/WEATHER.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/09/IDC-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/09/Course-Name-Banner-4-2.png",
      "https://dl-staging-website.ghost.io/content/images/2021/09/GMLP-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/09/SOLAR.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-102/",
    "title": "issue 102",
    "date": "",
    "reading_time": "",
    "content": "In earlier letters, I discussed somedifferencesbetween developing traditional software and AI products, including the challenges ofunclear technical feasibility,complex product specification, andneed for data to start development. This time, let’s examine the further challenge of additional maintenance cost.Some engineers think that when you deploy an AI system, you’re done. But when you first deploy, you may only be halfway to the goal. Substantial work lies ahead in monitoring and maintaining the system. Here are some reasons why:\n\nDetecting concept and data drift is challenging, because AI systems have unclear boundary conditions. For traditional software, boundary conditions — the range of valid inputs — are usually easy to specify. But for AI software trained on a given data distribution, it’s challenging to recognize when the data distribution has changed sufficiently to compromise performance.\n\nThis problem is exacerbated when one AI system’s output is used as another AI’s input in what’s known as a data cascade. For example, one system may detect people and a second may determine whether each person detected is wearing a hard hat. If the first system changes — say, you upgrade to a better person detector — the second may experience data drift, causing the whole system to degrade.Even if we detect these issues, our tools for fixing them are immature. Over the past few decades, software engineers have developed relatively sophisticated tools for versioning, maintaining, and collaborating on code. We have processes and tools that can help you fix a bug in code that a teammate wrote 2 years ago. But AI systems require both code and data. If you need to fix a few training examples that a teammate collected and labeled 2 years ago, will you be able to find the documentation and the exact version of the data? Can you verify that your changes are sound and retrain the model on the revised dataset? Tools for data management, unlike tools for code management, are still nascent.Beyond data maintenance, we still have traditional software maintenance to deal with. For instance, many teams had to upgrade from TensorFlow 1 to TensorFlow 2.These problems will recede as data-centric AI tools and methodologies evolve. But for now, being aware of them and planning projects around them can help you build better models and reduce costs.Keep learning!Andrew\n\nChinese researchers for the first time swept a competition to develop AI systems that monitor urban traffic.What’s new:Chinese universities and companies won first and second place in all five categories of the2021 AI City Challenge, beating hundreds of competitors from 38 nations. U.S. teams dominated the competition in its first three years, but Chinese contestants started overtaking them last year.What happened:305 teams entered at least one of the competition’sfive tracks. All teams used the same training and testing data for each track. Here’s a summary of the challenges and winners:\n\nBehind the news:Nvidia, QCraft, and several universities launched the AI City Challenge in 2017 to spur the development of smart city technology.Why it matters:This competition is the latest example of China’s rising profile in AI. The Chinese government has funded hundreds ofSmart Cityprograms. In contrast, U.S. funding for urban AI initiatives has been limited to a few one-off grants or competitions.We’re thinking:Smart-city technology could make urban living more pleasant and productive, yet it also carries a risk of invasive surveillance. We call on regulators and researchers who work on such projects worldwide to lead a global debate on appropriate standards of privacy and to design their systems that protect privacy from the ground up.\n\nGamers looking to cheat in first-person shooters can’t miss with AI-assisted marksmanship.What’s new:A video-game hack uses computer vision to blast virtual enemies at superhuman speed, Ars Technicareported. A system that implemented the technique was shut down last week.How it works:Uservizworked with any shooter that runs on PC, PlayStation, or Xbox. It identified and fired on targets in under 10 milliseconds. (Professional gamers have reaction timesbetween 100 and 250 milliseconds.) It worked like this:\n\nBehind the news:Cheat codes that enhance a player’s ability to aim and fire are common but frowned upon. Activision recently banned 60,000 players of Call of Duty: Warzone for using them. Typically, such cheats are add-ons to game software. Tools that use computer vision operate independently of the game and therefore are harder to detect. Userviz was one ofseveralon the market, and some enterprising cheaters havecoded their own.Why it matters:Electronic gaming is a lucrative industry — and so is themarketfor products that make it easier to win.Unscrupulous playersmay have takenmillions of dollarsin competition money.We’re thinking:Like fighting spam and fraud, thwarting aimbots is a game of cat and mouse. The next generation of such bots may behave more like humans — making an average player appear to be highly skilled — and thus be even harder to detect. Who’s up for a round of rock, paper, scissors?\n\nMoving projects from concept to production requires top-of-the-line skills. Learn to deploy data science and machine learning projects and to overcome challenges using Amazon SageMaker in ourPractical Data Science Specialization.\n\nThe transformer architecture has shown an uncanny ability to model not onlylanguagebut alsoimagesandproteins. New research found that it can apply what it learns from the first domain to the others.What’s new:Kevin Lu and colleagues at UC Berkeley, Facebook, and Google devisedFrozen Pretrained Transformer(FPT). After pretraining a transformer network on language data, they showed that it could perform vision, mathematical, and logical tasks without fine-tuning its core layers.Key insight:Transformers pick up on patterns in an input sequence, be it words in a novel, pixels in an image, or amino acids in a protein. If different types of data share similar patterns, a transformer trained on one type can operate on another.How it works:The researchers started with a 36-layerGPT-2 pretrained on WebText(posts on the website Reddit). They froze its self-attention and feed-forward layers and, in separate copies, fine-tuned peripheral layers on each on a wide range of tasks: Bit memory (memorizing strings of bits), Bit XOR (performing logical operations on pairs of strings of bits),ListOps(parsing and performing mathematical operations),MNIST,CIFAR-10(classification of images),CFAR-10 LRA(classification of flattened, greyscale images), andremote homology detection(predicting what kind of protein structure an amino acid is part of).\n\nResults:They compared GPT-2 models trained using their method to GPT-2s that had been fully fine-tuned for the same tasks. Their approach performed nearly as well, sometimes better. For instance, on CIFAR-10, their approach achieved 72.1 percent accuracy versus the fully fine-tuned model’s 70.3 percent. On remote homology detection, their approach achieved 12.7 percent versus 9 percent. Language pre-training contributed to the improvement: For instance, on CIFAR-10, their model achieved 68.2 percent versus the randomized model’s 61.7 percent.Why it matters:It appears that similar information structures — in the authors’ term, grammars — pervade the world. Applying representations learned in one domain to another domain may conserve training time and lead to better multimodal models.We’re thinking:It’s surprising that cross-modal pretraining works this well! Are there underlying statistics, common to many types of sequences, that we don’t yet appreciate?\n\nA U.S. government watchdog agency called for stronger face recognition protocols for federal agencies.What’s new:An audit of federal agencies by the Government Accountability Office (GAO) found that, while many employ face recognition, they may not know where it came from, how it’s being used, or the hazards involved. The auditors recommended that agencies using commercial systems develop protocols for appropriate use.What they found:Twenty agencies that employ law-enforcement officers reported using face recognition.\n\nBehind the news:Face recognition is increasingly controversial in the U.S. Lawmakers recently introduced legislation that wouldfreezegovernment use of the technology. At least 20 U.S. cities andseveral stateshave passed laws that restrict the technology.Why it matters:Face recognition has clear potential to infringe on privacy. Moreover, it has a spotty record of identifying minorities, which has led to false arrests. The finding that many federal agencies are taking a cavalier approach raises troubling questions about privacy and fairness.We’re thinking:The GAO audit of face recognition systems is a step forward. While regulators, ethicists, technologists, and businesses sort out appropriate standards, a moratorium on law enforcement use of face recognition would be sensible, so we can position the technology for socially beneficial uses while guarding against detrimental ones.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/07/DeepLearning-Cartoon_0728-2-copy.jpeg",
      "https://dl-staging-website.ghost.io/content/images/2021/07/CITY--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/ezgif.com-gif-maker--1--3.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/Specialization-Name--2---1-.png",
      "https://dl-staging-website.ghost.io/content/images/2021/07/TRANSFORMER.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/07/gao.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-123/",
    "title": "issue 123",
    "date": "",
    "reading_time": "",
    "content": "As we approach the end of 2021, you may be winding down work and gearing up for the winter holiday. I’m looking forward to taking a break from work and hope you are too.December is sometimes called the Season of Giving. If you have spare time and are wondering what to do with it, I think one of the best things any of us can do is to reflect on how we can help others.When the AI community was small, there was a strong spirit of cooperation. It felt like an intrepid band of pioneers taking on the world, and people were eager to help others with advice, an encouraging word, or an introduction. Those who benefited from this often couldn’t pay it back, so we paid it forward by helping those who came after us. As the AI community grows, I would like to preserve this spirit. I promise to keep working to build up the AI community. I hope you will, too!I also hope that you will consider ways — large or small — that you can lend a helping hand beyond the AI community. Many of us have access to advanced technology that much of the world does not. Collectively, our decisions move billions of dollars and affect billions of lives. This gives us a special opportunity to do good in the world.\n\n“We are what we repeatedly do,” said historian and philosopher Will Durant (oftenmisattributedto Aristotle). If you repeatedly seek to uplift others, not only does this help them but — perhaps equally important — it makes you a better person, too, for it is your repeated actions that define you as a person. There’s also a classicstudythat shows spending money on others may make you happier than spending money on yourself.\n\nSo, during this holiday season, I hope you’ll take some time off. Rest, relax, and recharge! Connect with loved ones if you haven’t done so frequently enough the past year. And if time permits, find something meaningful you can do to help someone else, be it leaving an encouraging comment on a blog post, sharing advice or encouragement with a friend, answering an AI question in an online forum, or making a donation to a worthy cause. Among charities relevant to education and/or tech, my favorites include theWikimedia Foundation,Khan Academy,Electronic Frontier Foundation, andMozilla Foundation. You can pick something meaningful to you from thislistof organizations vetted by Charity Watch.In the U.S., many parents tell their children that Santa Claus, the jolly character who leaves gifts in their homes at this time of year, is a magical being. When the kids grow up, they learn that Santa Claus isn’t real. Can we, as adults, be real-life Santa Clauses ourselves and give the gifts of our time, attention, or funds to someone else?\n\nLove,\n\nAndrew\n\nIn the past year, the globe struggled with extreme weather events, economic inflation, disrupted supply chains, and the Darwinian wiles of Covid-19. In tech, it was another year of virtual offices and virtual conferences. The AI community continued its effort to bridge these worlds, advancing machine learning while strengthening its ability to benefit all corners of society. We probed the dark side of 2021 in ourHalloween special issue. In this issue, we highlight developments that are primed to change the face of AI in 2022 and beyond.\n\nWhile models like GPT-3 and EfficientNet, which work on text and images respectively, are responsible for some of deep learning’s highest-profile successes, approaches that find relationships between text and images made impressive strides.What happened:OpenAIkicked offa big year in multimodal learning with CLIP, which matches images and text, and Dall·E, which generates images that correspond to input text. DeepMind’sPerceiver IOclassifies text, image, video, and point clouds. Stanford’sConVIRTadded text labels to medical X-ray images.Driving the story:While the latest multimodal systems were mostly experimental, a few real-world applications broke through.\n\nBehind the news:The year’s multimodal momentum built upon decades of research. In 1989, researchers at Johns Hopkins University and UC San Diego developed a system thatclassifiedvowels based on audio and visual data of people speaking. Over the next two decades, various groups attempted multimodal applications likeindexingdigital video libraries andclassifyinghuman emotions based on audiovisual data.Where things stand:Images and text are so complex that, in the past, researchers had their hands full focusing on one or the other. In doing so, they developed very different techniques. Over the past decade, though, computer vision and natural language processing have converged on neural networks, opening the door to unified models that merge the two modes. Look for models that integrate audio as well.\n\nThe trend toward ever-larger models crossed the threshold from immense to ginormous.What happened:Google kicked off 2021 withSwitch Transformer, the first published work to exceed a trillion parameters, weighing in at 1.6 trillion. Beijing Academy of Artificial Intelligence upped the ante withWu Dao 2.0, a 1.75 trillion-parameter behemoth.Driving the story:There's nothing magical about the number of zeros in a model’s parameter count. But as processing power and data sources have grown, what once was a tendency in deep learning has become a principle: Bigger is better. Well-funded AI companies are piling on parameters at a feverish pace — both to drive higher performance and to flex their muscles — notably in language models, where the internet provides mountains of unlabeled data for unsupervised and semi-supervised pretraining. Since 2018, the parameter-count parade has led through BERT (110 million), GPT-2 (1.5 billion), MegatronLM (8.3 billion), Turing-NLG (17 billion), and GPT-3 (175 billion) to the latest giants.Yes, but:The effort to build bigger and bigger models brings its own challenges. Developers of gargantuan models must overcome four titanic obstacles.\n\nWhere things stand:Natural language modeling leaderboards remaindominatedby models with parameter counts up to hundreds of billions — partly due to the difficulties of processing a trillion-plus parameters. No doubt their trillionaire successors will replace them in due course. And there’s no end in sight: Rumorscirculatethat OpenAI’s upcoming successor to GPT-3 will comprise a server-melting 100 trillion parameters.\n\nMusicians and filmmakers adopted AI as a standard part of the audio-production toolbox.What happened:Professional media makers embraced neural networks that generate new sounds and modify old ones. Voice actors bristled.Driving the story:Generative models can learn from existing recordings to create convincing facsimiles. Some producers used the technology to generate original voices, some to mimic existing voices. You can hear their work via the links below.\n\nYes, but:Bourdain’s widow isn’t the only one who’s disturbed by AI’s ability to mimic deceased performers. Voice actorsexpressedworry that the technology threatens their livelihoods; they were upset by a fan-built modification of the 2015 video gameThe Witcher 3: Wild Huntthat included cloned voices of the original actors.Behind the news:The recent mainstreaming of generated audio followed earlier research milestones.\n\nWhere things stand:Generative audio — not to mention video — models give media producers the ability not only to buff up archival recordings but to create new, sound-alike recordings from scratch. But the ethical and legal issues are mounting. How should voice actors be compensated when AI stands in for them? Who has the right to commercialize cloned voices of a deceased person? Is there a market for a brand-new, AI-generated Nirvana album — and should there be?\n\nThe transformer architecture extended its reach to a variety of new domains.What happened:Originally developed for natural language processing, transformers are becoming the Swiss Army Knife of deep learning. In 2021, they were harnessed todiscover drugs,recognize speech, andpaint pictures— and much more.Driving the story:Transformers had already proven adept atvision tasks,predicting earthquakes, andclassifyingandgeneratingproteins. Over the past year, researchers pushed them into expansive new territory.\n\nBehind the news:The transformer debuted in 2017 and quickly revolutionized language modeling. Its self-attention mechanism, which tracks how each element in a sequence relates to every other element, suits it to analyze sequences of not only words but also pixels, video frames, amino acids, seismic waves, and so on. Large language models based on transformers have taken center-stage as examples of an emerging breed of foundation models — models pretrained on a large, unlabeled corpus that can be fine-tuned for specialized tasks on a limited number of labeled examples. The fact that transformers work well in a variety of domains may portend transformer-based foundation models beyond language.Where things stand:The history of deep learning has seen a few ideas that rapidly became pervasive: the ReLU activation function, Adam optimizer, attention mechanism, and now transformers. The past year’s developments suggest that this architecture is still coming into its own.\n\nLegislators worldwide wrote new laws — some proposed, some enacted — to rein in societal impacts of automation.What happened:Authorities at all levels ratcheted up regulatory pressure as AI’s potential impact on privacy, fairness, safety, and international competition became ever more apparent.Driving the story:AI-related laws tend to reflect the values of the world’s varied political orders, favoring some balance of social equity with individual liberty.\n\nBehind the news:The AI community may be approaching a consensus on regulation. A recent survey of 534 machine learning researchersfoundthat 68 percent believed that deployments should put greater emphasis on trustworthiness and reliability. The respondents generally had greater trust in international institutions such as the European Union or United Nations than in national governments.Where things stand: Outside China, most AI-related regulations are pending approval. But the patchwork of proposals suggests a future in which AI practitioners must adapt their work to a variety of national regimes.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/NeuralGiftNetwork-4a_660px.jpg?upscale=true&width=1200&upscale=true&name=NeuralGiftNetwork-4a_660px.jpg",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-12-21%20at%204.32.38%20PM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-12-21%20at%204.32.38%20PM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/SmallerTown-BiggerTree_600px.jpg?upscale=true&width=1200&upscale=true&name=SmallerTown-BiggerTree_600px.jpg",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/DeerChorus3_600x338px.jpg?upscale=true&width=1200&upscale=true&name=DeerChorus3_600x338px.jpg",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/CoffeeMugDecor-600x338px.jpg?upscale=true&width=1200&upscale=true&name=CoffeeMugDecor-600x338px.jpg",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-12-21%20at%209.09.03%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-12-21%20at%209.09.03%20AM%20copy.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-242/",
    "title": "issue 242",
    "date": "",
    "reading_time": "",
    "content": "Last week, I described four design patterns for AI agentic workflows that I believe will drive significant progress this year: Reflection, Tool use, Planning and Multi-agent collaboration. Instead of having an LLM generate its final output directly, an agentic workflow prompts the LLM multiple times, giving it opportunities to build step by step to higher-quality output. In this letter, I'd like to discuss Reflection. For a design pattern that’s relatively quick to implement, I've seen it lead to surprising performance gains.You may have had the experience of prompting ChatGPT/Claude/Gemini, receiving unsatisfactory output, delivering critical feedback to help the LLM improve its response, and then getting a better response. What if you automate the step of delivering critical feedback, so the model automatically criticizes its own output and improves its response? This is the crux of Reflection.Take the task of asking an LLM to write code. We can prompt it to generate the desired code directly to carry out some task X. After that, we can prompt it to reflect on its own output, perhaps as follows:\n\nHere’s code intended for task X: [previously generated code]Check the code carefully for correctness, style, and efficiency, and give constructive criticism for how to improve it.Sometimes this causes the LLM to spot problems and come up with constructive suggestions. Next, we can prompt the LLM with context including (i) the previously generated code and the constructive feedback and (ii) ask it to use the feedback to rewrite the code. This can lead to a better response. Repeating the criticism/rewrite process might yield further improvements. This self-reflection process allows the LLM to spot gaps and improve its output on a variety of tasks including producing code, writing text, and answering questions.\n\nAnd we can go beyond self-reflection by giving the LLM tools that help evaluate its output; for example, running its code through a few unit tests to check whether it generates correct results on test cases or searching the web to double-check text output. Then it can reflect on any errors it found and come up with ideas for improvement.\n\nFurther, we can implement Reflection using a multi-agent framework. I've found it convenient to create two different agents, one prompted to generate good outputs and the other prompted to give constructive criticism of the first agent's output. The resulting discussion between the two agents leads to improved responses.\n\nReflection is a relatively basic type of agentic workflow, but I've been delighted by how much it improved my applications’ results in a few cases. I hope you will try it in your own work. If you’re interested in learning more about reflection, I recommend these papers:\n\nI’ll discuss the other agentic design patterns in future letters.\n\nKeep learning!\n\nAndrew\n\nP.S. New JavaScript short course! Learn to build full-stack web applications that use RAG in “JavaScript RAG Web Apps with LlamaIndex,” taught by Laurie Voss, VP of Developer Relations at LlamaIndex and co-founder of npm.\n\nSign up here!\n\nAI agents are typically designed to operate a particular software environment. Recent work enabled a single agent to take actions in a variety of three-dimensional virtual worlds.\n\nWhat's new:A team of 90 people at Google and University of British Columbia announcedScalable Instructable Multiworld Agent(SIMA), a system that learned to follow text instructions (such as “make a pile of rocks to mark this spot” or “see if you can jump over this chasm”) in seven commercial video games and four research environments.\n\nHow it works:SIMA’s architecture consists of several transformers and a vanilla neural network. The authors trained it to mimic human players using a dataset of gameplay broken into 10 second tasks, including onscreen images, text instructions, keyboard presses, and mouse motions. The video games included Goat Simulator 3 (a third-person game in which the player takes the form of a goat), No Man’s Sky (a first- or third-person game of exploration and survival in outer space), Hydroneer (a first-person game of mining and building), and others.\n\nResults:Judges evaluated SIMA’s success or failure at completing nearly 1,500 instructions that spanned tasks in nine categories like action (“jump”), navigation (“go to your ship”), and gathering resources (“get raspberries”). In Goat Simulator 3, SIMA completed 40 percent of the tasks. In No Man’s Sky, the judges compared SIMA’s performance to that of the human players whose gameplay produced the training data. SIMA was successful 34 percent of the time, while the players were successful 60 percent of the time. Judges also compared SIMA to versions that were trained to be experts in a single game. SIMA was successful more than 1.5 times more often than the specialized agents.\n\nBehind the news:SIMA extends Google’s earlier successes building agents that rival or beat human players at individual games includingGo,classic Atari games, andStarCraft II.\n\nWhy it matters:Training agents to follow directions in various environments, seeing the same things humans would, is a step toward building instructable agents that can work in any situation. The authors point to potential applications in robotics, simulations, and gaming; wherever an agent might need to be guided through diverse challenges.\n\nWe're thinking:This work shows that an agent trained on multiple games can perform better than an agent trained on just one, and that the richer the language inputs in a gameworld, the better the agent can perform. With only a handful of training environments under its belt, SIMA doesn’t demonstrate superhuman performance, but it gets the job done a surprising amount of the time!\n\nResearchers used an AI system to identify animal cell types from gene sequences, including a cell type that conventional approaches had discovered only in the past year.\n\nWhat’s new:Biologists at Stanford trained asystemto produce embeddings that represent individual cells in an organism. This enabled them to find cell types that have common function in different animals; for instance, the Norn cell, a type of kidney cell that biologists had previously theorized butdiscoveredonly in 2023.\n\nHow it works:Universal Cell Embedding (UCE) comprises two transformers that produce embeddings of genes and cells respectively, plus a classifier based on a vanilla neural network. The authors trained the classifier, given embeddings of a gene and cell, to classify whether or not the cell produces the protein coded by that gene. The training dataset included RNA sequences of 36.2 million cells from eight animal species (humans and mice accounted for 33.9 million) along with related protein structures.\n\nResults:Cell embeddings produced by UCE enabled the authors to identify cell types in animal species that weren’t in the training set. For instance, the authors embedded a dataset of mouse cells and appliedUMAPclustering to differentiate the types. They labeled the clusters as specific cell types (including Norn cells, which biologists took more than a century to find) based on the presence of certain genes that distinguish one cell type from another. Using the labels, they trained a logistic classifier. They applied the classifier to their training dataset and found Norn cells, among other cell types, in species other than mice. They verified the findings by looking for genes that tend to show up only in Norn cells.\n\nWhy it matters:UCE’s embeddings encode biologically meaningful information about individual cells, enabling a clustering algorithm to group them into recognized cell types. The fact that the recently discovered Norn cell was among those clusters suggests that UCE may yield further discoveries that accelerate development of new medicines, lab processes, and research methods. In fact, the model found Norn cells — which are known to occur in the kidney — in organs where they have not been seen before. If this result turns out to be valid, UCE will have made a discovery that has eluded biologists to date.\n\nWe’re thinking:It’s a truism that a machine learning model is only as good as its data. That makes this work all the more impressive: Its training data included a handful of species, yet it generalized to others.\n\nJoin our short course on “JavaScript RAG Web Apps with LlamaIndex” to learn how to build full-stack JavaScript web applications that let you chat with your data. Harness the capabilities of large language models and retrieval augmented generation (RAG)!Enroll for free\n\nThe United States military is using computer vision to target enemy positions in the Red Sea and elsewhere.What’s new:Maven, a system that analyzes satellite and geolocation data, has been used to identify targets in real-world conflicts,Bloombergreported. The system was developed primarily byPalantirand integrates technology from Amazon, Microsoft, information technology firms ECS Federal and L3Harris, aerospace firms Maxar and Sierra Nevada, and other unnamed companies.How it works:The 18th Airborne Corps, a U.S. Army unit organized for rapid deployment around the world, used Maven in live-fire training exercises. The system helped locate surface vessels in the Red Sea, rocket launchers in Yemen, and potential airstrike targets in Iraq and Syria. The U.S. used it to help Ukraine’s armed forces to locate Russian equipment, anonymous sources said.\n\nBehind the news:Google initially developed Maven for the U.S. Defense Department around 2017. Palantirinheritedthe project after Google, facing protests by employees who did not want to contribute to government intelligence systems,declinedto renew its contract in 2018. The U.S. military now has more than 800 active AI projects with a wide range of technology partners and contractors. Other countries are deploying similar technology:IsraelandUkrainehave used AI-assisted targeting in their ongoing conflicts.\n\nYes, but:Some U.S. military experts worry about Maven’s accuracy. In tests, Maven successfully identified objects about 60 percent of the time, while human analysts working with the 18th Airborne Corps did so 84 percent of time. Moreover, the system’s training data emphasizes deserts, and its success rate drops in other types of environments.Why it matters:Maven and similar systems offer some advantages over human analysts. They can observe and integrate multiple data streams simultaneously, and they can identify potential targets much more quickly. It’s likely that more data will make these systems more accurate. On the other hand, they represent a further step toward automated warfare in which automated assistance could come to displace human decision-making.\n\nWe’re thinking:Automated targeting is increasingly used in military applications, and less-sophisticated systems have been inusefor decades. However, humans should always be in control of decisions to fire. We support a globalbanon fully autonomous weapons.\n\nHumanoid robots can play football (known as soccer in the United States) in the real world, thanks to reinforcement learning.\n\nWhat’s new:Tuomas Haarnoja and colleagues at Google and University of Oxford trained anagentto play one-on-one football in a simulated environment. They applied the agent to 20-inch hardware robots on a scaled-down field. You can see it in actionhere.\n\nKey insight:In reinforcement learning, an agent improves as it explores various motions. However, such exploration risks damaging expensive hardware. By training in a simulation, the agent can attempt a diversity of motions without risking a physical robot. Once the agent is trained, it can make the leap from simulation to reality.\n\nHow it works:The agent learned in avirtual worldto control the robot’s motion given (i) a simulated robot’s state (including the position, velocity, and acceleration of each of 20 joints), (ii) the current game state (including the location and velocity of the ball and opponent), (iii) the game state at each of the last five time steps, and (iv) the agent’s five previous actions. Training proceeded via reinforcement learning in two stages.\n\nResults:The agent learned not only to turn and kick but also to anticipate the ball’s motion and block an opponent’s shots. It scored penalties against a stationary goalie with 90 percent success in simulation and 70 percent success in the physical world. It stood up in 0.9 seconds on average, while a manually designed agent stood up in 2.5 seconds. Its maximum walking speed of 0.69 meters per second beat the manually designed agent’s 0.27 meters per second. However, its kicks propelled the ball at 2.0 meters per second on average, slower than the manually designed agent’s 2.1 meters per second.\n\nWhy it matters:Controlling humanoid robots is challenging, as they’re less stable thanquadrupeds. Just getting them to do one type of motion, such asjumping, can require dedicated research. This work drives humanoid robots in complex motions by combining established training methods: training in a noisy simulation, self-play, and using teacher agents to reward particular actions.\n\nWe’re thinking:This work demonstrates that robots get a kick out of machine learning.\n\nThe latest AI updates of the week include:\n\n👉 Stability AI’s Stable Video 3D👉 Sakana’s evolution-inspired model merging technique👉 The new Blackwell B200 GPU by Nvidia\n\nAnd much more.\n\nReadData Points, your weekly AI news digest.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/03/AGENTS-REFLECTION-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/03/SIMA.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/03/CELLS.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/03/The-Batch-ads-and-exclusive-banners---2024-03-25T154251.995.png",
      "https://dl-staging-website.ghost.io/content/images/2024/03/MAVEN.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/03/SOCCER-ezgif.com-optimize.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-112/",
    "title": "issue 112",
    "date": "",
    "reading_time": "",
    "content": "The image below shows two photos of the same gear taken under different conditions. From the point of view of a computer-vision algorithm — as well as the human eye — the imaging setup that produced the picture on the right makes a defect in the gear much easier to spot.This example illustrates the power of data-centric AI development. If you want to improve a neural network’s performance, often improving the data it analyzes is far quicker and easier than tinkering with its architecture. In this case, adjusting the imaging setup made the difference.How can you tell that your imaging setup has room for improvement? If you can look at a physical object from a given angle and spot a defect, but you don’t see it clearly in a photo taken from the same angle, then your imaging setup likely could be improved. Parameters that you can control include\n\nWhile deep learning has been used successfully with datasets in which the examples vary widely — say, recognizing faces against backgrounds that range from a crowded concert hall to an outdoor campsite — narrowing the data distribution simplifies computer vision problems. For example, if you want to detect diseased plants, deep learning may be your best bet if you have pictures of plants taken at various distances and under various lighting conditions. But if all the pictures are taken at a fixed distance under uniform lighting, the problem becomes much easier. In practical terms, that means the model will be more accurate and/or need a lot fewer examples. With a consistent dataset, I’ve seen neural networks learn to perform valuable tasks with just 50 images per class (even though I would love to have had 5,000!).Robotics engineers are accustomed to paying attention to the design of imaging systems (as well as audio and other sensor systems). Such attention also can benefit machine learning engineers who want to build practical computer vision systems.Recently I had the pleasure of writing anarticlewith machine vision guru David Dechow that describes these ideas in greater detail. The article focuses on manufacturing, but the approach it describes applies to many computer vision projects where you can influence the imaging setup. Please take a look!\n\nKeep learning,\n\nAndrew\n\nScrutiny of Facebook intensified after a whistleblower leaked internal research showing the company has known that its ongoing drive to engage users has harmed individuals and society at large.What’s new:Former Facebook product manager Frances Haugen, in appearanceson televisionandbefore the U.S. Congress, described how the company’s algorithms reward divisiveness, damage some users’ mental health, and allow prominent members to skirt its rules.Whistle blown:Haugen, who worked on a team that aimed to combat expressions of hate, violence, and misinformation, revealed her identity this week after passing Facebook documents toThe Wall Street Journaland the U.S. Securities and Exchange Commission (SEC), which oversees public companies. The revelations prompted a Senatehearingin which legislators questioned Facebook’s global head of safety and called for regulating the company. The documents revealed that:\n\nFacebook’s response:The companysaidthat press coverage of the documents had minimized its successes at blocking harmful content, pointing out that vaccine hesitancy among Facebook users declined by 50 percent since January. Instagramsaidthat building a service for kids is “the right thing to do,” especially since many younger users lie about their age to gain access, which is limited to those 13 and older. Nonetheless, it has paused plans to build such a service while it works to persuade parents and policymakers that it’s a good idea.Behind the news:Facebook has aimed to counter adverse effects of its recommendation algorithms with ever more sophisticated content-moderation algorithms. It has developed AI systems to detecthate speech,harmful memes, andmisinformation. Yet it hasn’t addressed longstandingcomplaintsthat it torpedoes any program that has a negative impact on user engagement — including the unit Haugen worked for, which the company dissolved after the 2020 election.Why it matters:Algorithms that optimize engagement are a key driver of profit for social networks — yet, as the leaked documents show, they can have severe consequences. The resulting harms undermine public trust in AI, and they build support for laws that would limit social media platforms and possibly recommendation algorithms in general.We’re thinking:Facebook has been under fire for years. Despite the company’s testimony in several congressional hearings, apologies, and promises to do better, little has changed. An investigation by the SEC could break the logjam. Meanwhile, if you work in AI, we urge you to consider whether your employment, net-net, improves society and, if not, begin the transition into a situation that does.\n\nAmazon unveiled a robot that patrols users’ homes, scopes out strangers, and warns of perceived dangers.What’s new:Astromaps users’ homes while using face recognition to decide whether or not to act on perceived threats such as intruders. It also plays music and delivers teleconferences, and it has storage space for ferrying small items around the house. It’s scheduled to hit the market later this year for an introductory price of $999.How it works:Astro is designed to learn about users’ homes and habits over time. Built on Amazon’s Alexa platform, it uses that system’s software for voice recognition and connects to the same security system as Ring doorbells.\n\nYes, but:Leaked documentspublishedbyViceraise significant privacy concerns. For instance, law enforcement officials might serve warrants to Amazon, rather than homeowners, enabling them tomonitorAstro’s output. Or they might use the robot to executesting operations, as they have used Ring doorbells. Moreover, developers who worked on Astro toldVicethe robot is fragile, prone to falling down stairs, and often misidentifies people.Why it matters:Ring was an unqualified success, having sold over1.4 millionlast year. Astro is a logical next step to further capitalize on that market. And there’s the added benefit that a rolling robot can provide an unprecedented view of a customer’s home and habits.We’re thinking: No doubt many users will find Astro a fun addition to their gadget menagerie. However, we hope that Amazon will make it easy for users to opt out of (or, better yet, not opt into) undisclosed or unconsented uses of the data it collects.\n\nAre you an educator looking to develop your skills? DeepLearning.AI’s Curriculum Architect Program will show you how to design and build online courses like the ones we build ourselves! Learn more about the Curriculum Architect Programhere\n\nTesla’s autonomous driving capability has inspiredhair-raisingantics on the road. Now the company is deploying an algorithm to determine whether customers have shown sufficiently sound judgement to use its “Full Self-Driving” software.What’s new:Starting this week, the beta-test version of Tesla’s latest self-driving update will be available only to drivers who have demonstrated safe driving. The beta program previously was open to about 2,000 drivers.How it works:Drivers can request the software through a button on their car’s dashboard screen.\n\nBehind the news:The engineering association SAE International has graded Tesla’s Full Self-Driving system at Level 2 autonomy, which means it must be supervised constantly by a human driver. National Transportation Safety Board (NTSB) chair Jennifer Homendy recentlysaidthat Tesla’s use of the term “full self-driving” is irresponsible and called on the company to address basic safety issues before expanding the test program. The National Highway Traffic Safety Administration, which has the authority to demand recalls, isinvestigatingthe culpability of Tesla’s software in 11 accidents.Why it matters: Self-driving technology is still developing and has not yet been proven safe under the vast variety of circumstances that arise in real-world driving. Most companies that are developing such technology hire safety drivers to test their systems within tightly constrained boundaries. In contrast, Tesla is enrolling the best drivers of Tesla vehicles to test its system on the open road.We’re thinking:Scoring driver behavior and limiting the distribution of special features only to the safest drivers is a good idea, assuming the score is well designed and implemented. It both ensures that only excellent drivers can use the riskiest features and incentivizes all drivers to do their best. But recruiting customers to test unproven technology is reckless. We urge Tesla, and any company that would consider following its lead, to prove its technology’s safety under controlled conditions before putting the general public at risk. And can we stop calling a great driver assistance system “full self-driving”?\n\nModels trained using supervised learning struggle to classify inputs that differ substantially from most of their training data. A new method helps them recognize such outliers.What’s new:Abhijit Guha Roy, Jie Ren, and colleagues at Google developedHierarchical Outlier Detection(HOD), a loss function that helps models learn to classify out-of-distribution inputs — even if they don’t conform to a class label in the training set.Key insight:Previous work has proposed two general approaches to handling out-of-distribution inputs. One is to include a catch-all outlier class in the training set. Given the diversity of examples in this class, however, it’s difficult to learn to recognize outliers consistently. The other is to label separate outlier classes in the training set. This enables a trained model to recognize certain kinds of outliers but leaves it unable to identify outlier classes that aren't represented in the training set. HOD attempts to cover the gamut by encouraging a model to classify images both as outliers in general and as specific classes of outlier.How it works:The authors started with aResNetpretrained onJFT(Google's proprietary collection of 300 million images). They fine-tuned it onimagesof roughly 10,000 cases of skin disease, each labeled according to 225 different conditions. Of these, 26 conditions were represented by at least 100 cases; these were assigned an additional “inlier” label. The remaining 199 were assigned an additional “outlier” label. The training, validation, and test sets included a a variety of inlier classes, but the outlier classes were divided among them; that is, the datasets had no outlier classes in common.\n\nResults:Training the model on both outlier status and specific outlier classes helped it learn to recognize outliers in general — although the task still proved difficult. The authors’ approach achieved .794 AUC, while the same architecture trained on only a general outlier label (plus labels for all inlier classes) achieved .756 AUC. When classifying inliers, the model trained on all labels achieved 74 percent accuracy, while the one given only a general outlier label achieved 72.8 percent accuracy.Why it matters:Real-world applications can be rife with out-of-distribution data. This approach helps models detect both examples that are similar to those in the training dataset but not labeled (for example, new skin conditions) and examples that are substantially different (for example, pictures of iguanas).We're thinking:Giving models the ability to recognize edge cases could build greater trust in their output.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-10-06%20at%2011.17.27%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-10-06%20at%2011.17.27%20AM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/FACEBOOK-1.gif?upscale=true&width=1200&upscale=true&name=FACEBOOK-1.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ASTRO.gif?upscale=true&width=1200&upscale=true&name=ASTRO.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/blog%20banner_The%20Batch%20Image%20(1).png?upscale=true&width=1200&upscale=true&name=blog%20banner_The%20Batch%20Image%20(1).png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/TESLA.gif?upscale=true&width=1200&upscale=true&name=TESLA.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/LONGTAIL-1.gif?upscale=true&width=1200&upscale=true&name=LONGTAIL-1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-xiv/",
    "title": "issue xiv",
    "date": "",
    "reading_time": "",
    "content": "My stash of new blank notebooks just arrived in the mail.\n\nI have a weakness for stationery. I always feel that if only I had the perfect pen and notebook, I might have better ideas. (Hasn’t worked so far. :-)\n\nMore seriously, though, we know that taking handwritten notes increases retention. If you’re studying an online course or book or listening to a talk, and you want to remember it, take notes by hand. Several studies (e.g.,this one) have shown that the process of synthesizing what you’re hearing into handwritten notes causes you to retain better. So even if I doubt I’ll ever refer to my notes, I will often still take them. I hope you will too.\n\nAnd if you ever find the perfect pen and notebook, let me know!\n\nKeep learning!Andrew\n\nArtificial intelligence has bested humanity at chess, Go, even StarCraft II. But those games are played against a single opponent in one sitting. Poker requires neural nets to learn skills like tracking a table full of players and maximizing winnings over many games.What’s new:Researchers at Facebook and Carnegie Mellon University developedPluribus, a deep learning model that plays No-Limit Texas Hold 'Em against a table full of players. The model fleeced a dozen professional gamblers over 12 days and 10,000 hands.How the game is played:Pluribus learned by playing hundreds of thousands of hands against versions of itself.\n\nA chip and a chair:Pluribus didn’t win every hand, but it did win roughly $1,000 an hour. The researchers didn’t calculate the human players' rate, but the software's accumulation of chips — especially late in the trial — indicates that it maintained a steady advantage.Behind the news:Ever since IBM Deep Blue’s 1997 chess victory over world champion Garry Kasparov, engineers have used strategy games to hone machine intelligence. In 2015, AlphaGo, developed by Google’s DeepMind, defeated champion Lee Sedol using strategies nobody had ever seen before. DeepMind struck again earlier this year, taking down a string of StarCraft II pros.Why it matters:Authors Noam Brown and Tuomas Sandholm believe their technology could be useful in negotiations. Pokermirrors a variety of real-world scenarios, especially in business, which typically involves more than two people, each of whom has hidden motives, with stakes that are rarely sum-zero.Win more, spend less:Pluribus’ winnings at the table are impressive, but so are its savings at the server. The model was trained using an off-the-shelf laptop with 512 gigabytes of memory. The authors estimate the training cost using cloud computing at around $144. Compare that with the$10,000-plusit can take to train a state-of-the-art language model.We’re thinking:Pluribus proves that lightweight models are capable of heavy-duty decision making. This could be a boon for resource-strapped developers — and a bane for online gamblers.\n\nGenerative adversarial networks clearly learn to extract meaningful information about images. After all, they dream up pictures that, to human eyes, can be indistinguishable from photos. Researchers at DeepMind tapped that power, building a GAN that generates feature vectors from images.What’s new:Jeff Donahue and Karen Simonyan adapted the state-of-the-artBigGANimage synthesizer for representation learning. They modified its discriminator, which learns to differentiate between artificially generated images and training images, and added an encoder network from the unrelated modelBiGAN. The new network, dubbedBigBiGAN, not only generates superb images but also learns feature vectors that help existing image recognition networks do a better job.Key insight:An encoder coupled with a powerful generator makes an effective representation learner.How it works:BigBiGAN consists of three main components: generator, encoder, and discriminator.\n\nResults:Used by an image classifier via transfer learning, features from BigBiGAN match the best unsupervised representation learning approach in ImageNet classification. Used as an image generator, it sets a new state of the art for inception score (similarity between original images and their generated counterparts) and Fréchet inception distance (difference between the feature maps of original and generated images).Why it matters:Representation learning with GANs can take advantage of the world’s massive amounts of unlabeled data. BigBiGAN demonstrates that representations learned by GANs are transferable to tasks beyond image generation.Takeaway:BigBiGAN takes us one step closer to bridging the gap between what models understand and how they can express that understanding to us.\n\nNobody wants to sound like a robot over the phone. But maybe a computer can help you bring more humanity to your phone manner.What’s happening:Cogitomakes deep-learning software that coaches customer service representatives through phone calls in real time. The Boston-based startup has raised more than $70 million and sold its technology to at least three dozen call centers across the U.S., according to an article inTime.Automating empathy:Cogito's software was trained on vocal signals beyond the strictly verbal content of conversation: things like tone, pitch, talking speed, rambling, interruption frequency, and relative length of time spent talking.\n\nBehind the news:In the early 2000s, MIT’s Sandy Pentland began collecting a database of non-linguistic speech features by tapping the cell phones of 100 students and faculty — with their consent, of course. He co-founded Cogito in 2007 and the following year wrote a book,Honest Signals, arguing that nonverbal cues can predict the outcome of a social interaction, perhaps even better than the words themselves. The company built a medical app before pivoting to its current product.Why it matters:More than a third of call-center employees move on within a year, according to an industry group. That attrition incurs hiring costs. It also affects customers whose problems are handled by inexperienced or burned-out operators. AI-driven coaching can help on both ends, Cogito claims, by training green representatives and mitigating burnout.Takeaway:AI is developing the capacity to recognize and respond to human emotions. That bodes well in contexts where humans and computers must collaborate under stressful conditions — not just in customer-service scenarios but, say, high-stakes emergency situations.\n\nWhen is an end-to-end algorithm a better choice than a pipeline system? Learn the pros and cons of both in Course 3 of theDeep Learning Specialization.\n\nVideo highlights are as integral to sports as endorsement deals for star athletes. Now AI is picking the most exciting moments and compiling them quicker than humans.What’s new: Since 2017, the All England Lawn Tennis & Croquet Club — host of the world-famous Wimbledon Championships — has used IBM’s Watson technology to choose clips of the best serves, rallies, and points. This year, Big Blue debuted tools that mitigate the system’s bias toward highly animated players, among other things.Robofan:Watson monitors matches live, grading each moment in terms of excitement. The criteria come from audio and video cues — such as a player pumping a fist or the crowd erupting into cheers — which Watson sorts into classes such as player gestures or crowd reactions. By weighing each moment relative to others in a match, the algorithm recommends which highlights are worth assembling into video summaries. Watson chooses the clips, cuts them together, and passes them to a human editor for review.Net effect:The new tools are meant to make sure the system treats all players fairly.\n\nBehind the news:More video views mean more eyes on advertisements — not to mention logos visible around the court. Sporting organizations are businesses, and they’re eager to take advantage of anything that helps them capitalize on fan attention.Why it matters:Watson has had a rough time in its primary field of medicine. Its work at Wimbledon suggests it may have a brighter future in sportscasting. Viewership of the championship’s highlight videos rose dramatically when Watson took over editing duties: The clips garnered 14.4 million more views compared to the year before, an IBM engineer toldComputerworld.We’re thinking:AI has proven its ability to write routine news articles on sports, corporate earnings, and real estate. Now it's gaining skill at video editing. If only it could do something about theGame of Thronesfinale . . . .\n\nNeural networks surpass human abilities in many tasks, with a caveat: Typical supervised learning requires lots of labeled data. New research is pushing back that threshold.What’s new:Emerging semi-supervised learning methods use a smaller amount of labeled data along with plenty of unlabeled data. Meanwhile, data augmentation multiplies labeled examples by distorting them to create variants. A team effort between Google Brain and Carnegie Mellon University combines the two approaches. Their approach,Unsupervised Data Augmentation, turns in excellent performance in both image recognition and language generation tasks.Key insight:UDA encourages models to make similar predictions for original inputs and their altered versions, a technique known as smoothness enforcing. Unlike prior smoothness enforcing algorithms, UDA uses data augmentation to multiply labeled and unlabeled examples.How it works:UDA trains a neural network by calculating two loss functions: one derived from labeled data, the other from unlabeled data. Then it optimizes both losses through gradient descent on subsets of labeled and unlabeled data.\n\nWhy it matters:Neural networks often fail to generalize after supervised learning with limited labeled data. Troughs of unlabeled data may exist, but there may not be enough time or resources for humans to label it. UDA takes advantage of this bounty, promising impressive results even with few labeled examples.Results:UDA achieves state-of-the-art performance in natural language tasks, specifically with the BERT model. It beats the best fully supervised model on the IMDb dataset  — trained on only 20 labeled examples! Similarly, UDA exceeds the previous record on a restricted ImageNet from which labels were removed from 90 percent of the images.Takeaway:Deep learning pioneer Yann LeCun considers semi-supervised learning an essential technique for AI to gain common sense. As the state-of-the-art semi-supervised training algorithm, UDA may be one of the small steps toward AI’s next great leap.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/4ee69448-1f3c-48a8-b10a-95506a939f39.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/72e66a40-46e1-4dee-bd55-e6f7920efc59.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/afd8bc18-6dc6-4194-bea5-1d1c87b38691.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/1bb4e782-3240-4d26-9dd8-94ac7513a26e.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/5000ee5e-aabf-43e7-92d8-38bd74c44192.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/09/ffa2c81d-64a1-4420-adb2-902f77210652.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/83fed3d3-b632-40db-a0db-8c3adebe2593.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-117/",
    "title": "issue 117",
    "date": "",
    "reading_time": "",
    "content": "On Monday, Landing AI (where I’m CEO) announced the close of a $57 million Series A funding round. The investment enables the company to continue building its data-centric MLOps platform for computer vision, with a focus on manufacturing visual inspection.Studiesestimate that AI will create trillions of dollars of value, and machine learning already has changed the trajectory of consumer-internet companies like Google and Facebook. Yet the technology has barely penetrated most other industries. Making AI work in more traditional industries will require a different recipe than internet companies use. I explained why this week atFortune’sBrainstorm A.I.event, pictured below.Datasets are much smaller.I once built a face recognition system using about 350 million images. But when I asked people in the manufacturing industry how many images they had of each defect they wanted to recognize, 50 or fewer was the most common answer. Techniques developed for learning from hundreds of millions of examples will struggle to work with only 50. But the situation improves if you choose those examples well.Data-centric AItools can help you get there.\n\nApplications are more diverse.If we took all current and potential machine learning projects and sorted them in decreasing order of value, we might find that the “head” of the distribution comprises applications like a large company’s web search engine, online ad system, or product recommendation engine. This is followed by a “long tail” of applications that have lower value individually but massive value in aggregate. As a community, we’ve figured out how to organize dozens or hundreds of engineers to build these large applications, some of which can generate over $1 billion of value. But this recipe doesn’t work for other industries where applications are more heterogeneous and where each of 10,000 machine learning models generates $1 million to $5 million each.\n\nFor example, in manufacturing, each plant makes a different product, and thus will need a different trained model to detect defects. In healthcare, every hospital codes its electronic health records (EHR) differently. Rather than a single monolithic model to read every hospital’s EHR, each hospital needs a system trained on its own data. The total value of these applications is enormous. But how can any company help build, deploy and maintain 10,000 custom models without hiring 10,000 machine learning engineers?This “long tail” problem helps to explain why many proof-of-concept implementations and demos don’t make it into production. While a team of engineers can build a one-off application, we still need better tools to make this type of work scalable and economically viable.Landing AI is building tools to make it fast and easy for manufacturers to engineer the data so as to train, deploy, and maintain their own computer vision systems. This design pattern addresses the widespread problems of small datasets and diverse applications. If you’re working in a sector other than manufacturing, consider if your sector has a long tail of applications and if building an MLOps platform to let customers do their own customization — as Landing AI is doing in manufacturing — might advance machine learning in your industry.\n\nKeep learning!\n\nAndrew\n\nThe Google sister company devoted to artificial general intelligence parlayed its technology into a biomedical spin-off.What’s new:DeepMind launched a startup calledIsomorphic.The new company aims to build its business onAlphaFold 2, an ensemble of neural networks that finds the shapes of protein molecules, which determine their biological function. The company ishiringexperts in AI, biology, medicinal chemistry, biophysics, and engineering.How it works:Like DeepMind, Isomorphic is a subsidiary of Google’s parent company Alphabet. DeepMind CEO Demis Hassabis also leads the London-based spin-off.\n\nBehind the news:AlphaFold 2 has analyzed the shapes of over 98 percent of proteins in the human body. It remains for scientists to validate its output through lab experiments.\n\nWhy it matters:Just6.2 percentof drug candidates make it through clinical trials to market, and the cost of developing a successful medicinecosts$1.3 billion on average. Isomorphic could wring trial and error out of the process, boosting success rates, cutting costs, and enriching drug-company customers.We’re thinking:AlphaFold 2 is a big step forward for biomedicine, and deep learning promises further progress in areas like protein-protein interaction (how does a potential treatment interact with a target protein?) and protein dynamics (protein shapes aren’t static, and their motion can affect their properties). Much work by many determined researchers lies ahead to bridge the gap between lab and clinic.\n\nThe GPT-3 natural language model both wowed and worried the AI community and the public alike with its ability to generate realistic prose. Now it’s ready to churn out text on a grand scale.What’s new:Microsoft is making the giant, pretrained neural networkavailableto selected customers through its Azure cloud service. The new service expands onrestrictedaccess offered by OpenAI.How it works:Microsoft will grant access for well-defined applications that comply with the company’sprinciplesfor responsible AI, which include fairness, reliability, transparency, accountability, and privacy. Pricing remains undisclosed.\n\nBehind the news:GPT-3’s road to commercialization began in early 2019, when OpenAItransitionedfrom a nonprofit research institute to a for-profit company. A few months later, it inked a$1 billion dealwith Microsoft to help build the tech giant’s AI platform and later granted Microsoftexclusive commercial accessto GPT-3. OpenAI launched a privatebetaprogram in mid-2020. The model also powers Microsoft’sPower Appsdevelopment platform, which converts natural language into computer code.Why it matters:GPT-3 is an AI juggernaut of the sort that few companies can build, never mind design. Making it available on Azure puts it within reach of not only budding AI companies but also users in healthcare, manufacturing, government, and so on (albeit to use, not to modify). Developers using the beta version haveharnessedGPT-3 to write fiction, generate music notation, and produce images based on text descriptions — over300 applicationsas of spring 2021.Yes, but:Like other architectures trained on text scraped from the web, GPT-3 has apropensityto generate biased, objectionable and confused output. Whether Microsoft’s implementation addresses these issues remains to be seen.\n\nWe’re thinking:Microsoft and OpenAI may not have a monopoly on GPT-3’s capabilities for long. Several Chinese universities teamed up to buildWuDao, which is purportedly 10 times bigger than GPT-3. Microsoft’s Silicon Valley competitors are following suit with everlargerlanguage models.EleutherAIhas released a much smaller open sourceattempt to duplicateGPT-3 and aims to scale it up. Meanwhile, AI21 Labs offersfree accessto the beta version of its 178 billion-parameter Jurassic-1.\n\nYou’re invited! Join us to learn “How to Build and Deploy a Virtual Chatbot” withFourthBrain’s Greg Loughnane on November 17, 2021, at 10 a.m. Pacific Time. There will be a live demo and inside info on deployment, applications, and more!RSVP now\n\nFacebook, beset by reports that it ignored evidence that it causes harm in its drive to spur user engagement, pulled the plug on its face recognition features.What’s new:Facebookdisabledface recognition for uploaded photos and videos as well as its retrospective Memories service and promised to delete over 1 billion individual face recognition templates. The company cited “societal concerns” and uncertainty about future government regulation.Losing Face:Face recognition powered several Facebook features under a single opt-in. Over a third of its 1.8 billion daily users took advantage of them.\n\nBehind the news:Apart from the currentfirestormsparked by documents leaked by a former employee, privacy advocates have repeatedly targeted Facebook’s face recognition features.\n\nWhy it matters:Facebook’s move follows similar actions byAmazon, IBM, and Microsoft. The trend suggests dim prospects for face recognition in mass-market consumer settings.Yes but:This is not necessarily the end of face recognition at Facebook. The company retains itsDeepFacetechnology and will continue to develop it. It views face recognition as an important tool in identity verification and fraud prevention, a spokesperson said.We’re thinking:In the wake of retreating tech giants, a number of smaller companies haveentered the market. The resulting fragmentation makes it harder to keep track of how the technology is being used and potentially abused. Thoughtful regulation should set a clear standard and hold all companies to it.\n\nTo interact with the world, a robot needs to know which items to grab, which items to avoid, and which joints to move in the process. A new method aims to improve a machine’s ability to determine and locate points of interest.What's new:Boyuan Chen and Pieter Abbeel at UC Berkeley with Deepak Pathak at Carnegie Mellon developedKeypoint3D, an unsupervised training method that enables a model to identify spatial coordinates, known as keypoints, that correspond to useful locations in the environment — including spots on its own body.Key insight:Previousworktrained an agent in a virtual world to find keypoints based on a single two-dimensional camera view, but it performed poorly if that view contained overlapping or occluded objects. A similar approach can take advantage of multiple camera views to locate objects in 3D space. Using inferred 3D keypoints to regenerate the original camera views can help the agent learn to track particular objects consistently across time and space.How it works:Keypoint3D trained a system to choose 32 keypoints most useful in completing a particular task and find their locations based on three camera views. The system was trained and tested in a virtual environment, where it drove an agent to complete a set of basic robot tasks (opening a door, closing a box, and so on), as well as draping a scarf over a mannequin (to demonstrate the system’s ability to manipulate the flexible material) and walking on four legs (to demonstrate its ability to find the agent’s own joints). They trained a reinforcement learning model jointly with the keypoint detection models to ensure that the choice of keypoints would be relevant to the task at hand.\n\nWhy it matters:Otherefforts to generate 3D keypoints from multiple views have been designed to locate static objects. This method accounts for changes over time to drive robots that can interact with dynamic environments.We're thinking:It may seem odd to move a robot by guessing the locations of its joints in still images rather than knowing the joint positions in the first place. But this is how humans do it, too — try to bring together your left and right fingertips with your eyes closed. Giving robots this capability would enable them to locate and control objects with greater precision.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-11-10%20at%2010.25.02%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-11-10%20at%2010.25.02%20AM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ISOMORPHIC.gif?upscale=true&width=1200&upscale=true&name=ISOMORPHIC.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/AZURE2.gif?upscale=true&width=1200&upscale=true&name=AZURE2.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/AI+X-11.17_The%20Batch%20Image-1.png?upscale=true&width=1200&upscale=true&name=AI+X-11.17_The%20Batch%20Image-1.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/UNFACE3.gif?upscale=true&width=1200&upscale=true&name=UNFACE3.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/KEYPOINTSv2-1.gif?upscale=true&width=1200&upscale=true&name=KEYPOINTSv2-1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-125/",
    "title": "issue 125",
    "date": "",
    "reading_time": "",
    "content": "As we approach the end of the year, many of us consider setting goals for next year. I wrote about setting learning goals in a previousletter. In this one, I’d like to share a framework that I’ve found useful: process goals versus outcome goals.A process goal is one that calls for regular engagement in an activity; for example, deciding to spend at least N hours weekly studying deep learning, exercising three times a week, or applying for a certain number of jobs. An outcome goal is one that stipulates a particular result. For example, by next year, you might want to complete your university degree, reach a specific weight, get a certain job or — I hope you’ll do this if you haven’t already! — finish the Deep Learning Specialization.\n\nWhen people think about setting goals, most gravitate toward outcome goals. But they have a downside: They’re often not fully within your control, and setbacks due to bad luck can be demoralizing. In contrast, process goals are more fully within your control and can lead more reliably toward the outcome you want.\n\nLearning is a lifelong process. Though it can have a profound impact, often it takes time to get there. Thus, when it comes to learning, I usually set process goals in addition to outcome goals. Process goals for learning can help you keep improving day after day and week after week, which will serve you better than a burst of activity in which you try to cram everything you need to know.When you set New Year resolutions, I hope you’ll consider both outcome goals and process goals. In particular, process goals that help you to…\n\nKeep learning!Andrew\n\nWe leave behind a year in which AI showed notable progress in research as well as growing momentum in areas such as healthcare, logistics, and manufacturing. Yet it also showed its power to do harm, notably its ability to perpetuate bias and spread misinformation. We reviewed these events in ourwinter holidayandHalloweenspecial issues. The coming year holds great potential to bring AI’s benefits to more people while ameliorating flaws that can lead to bad outcomes. In this issue ofThe Batch, AI leaders from academia and industry share their highest hopes for 2022.\n\nFrom language to vision models, deep neural networks are marked by improved performance, higher efficiency, and better generalizations. Yet, these systems are also marked by perpetuation of bias and injustice, inaccurate and stereotypical representation of groups, lack of explainability and brittleness. I am optimistic that we will move slowly toward building more equitable AI, thanks to critical scholars who have been calling for caution and foresight. I hope we can adopt measures that mitigate these impacts as a routine part of building and deploying AI models.\n\nThe field does not lack optimism. In fact, everywhere you look, you find overenthusiasm, overpromising, overselling, and exaggeration of what AI models are capable of doing. Mainstream media outlets aren’t the only parties guilty of making unsustainable claims, overselling capabilities, and using misleading language; AI researchers themselves do it, too.\n\nLanguage models, for example, are given human-like attributes such as “awareness” and “understanding” of language, when in fact models that generate text simply predict the next word in a sequence based on the previous words, with no understanding of underlying meaning. We won't be able to foresee the impact our models have on the lives of real people if we don't see the models themselves clearly. Acknowledging their limitations is the first step toward addressing the potential harms they are likely to cause.\n\nWhat is more concerning is the disregard towards work that examines datasets. As models get bigger and bigger, so do datasets. Models with a trillion parameters require massive training and testing datasets, often sourced from the web. Without the active work of auditing, carefully curating, and improving such datasets, data sourced from the web is like a toxic waste. Web-sourced data plays a critical role in the success of models, yet critical examination of large-scale datasets is underfunded, and underappreciated. Past work highlighting such issues is marginalized and undervalued. Scholars such asDeborah Raji,Timnit Gebru, andJoy Buolamwinihave been at the forefront of doing the dirty and tiresome work and cleaning up the mess. Their insights should be applied at the core of model development. Otherwise, we stand to build models that reflect the lowest common denominators of human expression: cruelty, bigotry, hostility, and deceit.\n\nMy own work has highlighted troubling content — from misogynistic and racial slurs to malignant stereotypical representations of groups — found in large-scale image datasets such as TinyImages and ImageNet. One of the most distressing things I have ever had to do as a researcher was to sift through LAION-400M, the largest open-access multimodal dataset to date. Each time I queried the dataset with a term that was remotely related to Black women, it produced explicit and dehumanizing images from pornographic websites.\n\nSuch work needs appropriate allocations of time, talent, funding and resources. Moreover, it requires support for the people who must do this work. It causes deep, draining emotional and psychological trauma. The researchers who do this work — especially people of color who are often in precarious positions — deserve pay commensurate to their contribution as well as access to counseling to help them cope with the experience of sifting through what can be horrifying, degrading material.\n\nThe nascent work in this area so far — and the acknowledgement, however limited, that it has received — fills me with hope in the coming year. Instead of blind faith in models and overoptimism about AI, let’s pause and appreciate the people who are doing the dirty background work to make datasets, and therefore models, more accurate, just, and equitable. Then, let's move forward — with due caution — toward a future in which the technology we build serves the people who suffer disproportionately negative impacts; in the word ofPratyusha Kalluri, towards technology that shifts power from the most to the least powerful.\n\nMy highest hope for AI in 2022 is that this difficult and valuable work — and those who do such work, especially Black women — will become part and parcel of mainstream AI research. These scholars are inspiring the next generation of responsible and equitable AI. Their work is reason not for defeatism or skepticism but for hope and cautious optimism.\n\nAbeba Birhane is a cognitive science PhD researcher at the Complex Software Lab in the school of computer science at University College Dublin.\n\nRobots are tremendously useful machines, and I would like to see them applied to every task where they can do some good. Yet we don’t have enough programmers for all this hardware and all these tasks. To be useful, robots need to be intelligent enough to learn from experience in the real world and communicate what they’ve learned for the benefit of other robots. I hope that the coming year will see great progress in this area.\n\nUnlike many typical machine learning systems, robots need to be highly reliable. If you’re using a face detection system to find pictures of friends in an image library, it’s not much of a problem if the system fails to find a particular face or finds an incorrect one. But mistakes can be very costly when physical systems interact with the real world. Consider a warehouse robot that surveys shelves full of items, identifies the ones that a customer has paid for, grasps them, and puts them in a box. (Never mind an autonomous car that could cause a crash if it makes a mistake!) Whether this robot classifies objects accurately isn’t a matter of life and death, but even if its classification accuracy is 99.9 percent, one in 1,000 customers will receive the wrong item.\n\nAfter decades of programming robots to act according to rules tailored for specific situations, roboticists now embrace machine learning as the most promising path to building machines that achieve human performance in tasks like the warehouse robot’s pick-and-place. Deep learning provides excellent visual perception including object recognition and semantic segmentation. Meanwhile, reinforcement learning offers a way to learn virtually any task. Together, these techniques offer the most promising path to harnessing robots everywhere they would be useful in the real world.\n\nWhat’s missing from this recipe? The real world itself. We train visual systems on standardized datasets, and we train robot behaviors in simulated environments. Even when we don’t use simulations, we keep robots cooped up in labs. There are good reasons to do this: Benchmark datasets represent a useful data distribution for training and testing on particular tasks, simulations allow dirt-cheap virtual robots to undertake immense numbers of learning trials in relatively little time, and keeping robots in the lab protects them — and nearby humans — from potentially costly damage.\n\nBut it is becoming clear that neither datasets nor simulations are sufficient. Benchmark tasks are more tightly defined than many real-world applications, and simulations and labs are far simpler than real-world environments. Progress will come more rapidly as we get better at training physical robots in the real world.\n\nTo do this, we can’t treat robots as solitary learners that bumble their way through novel situations one at a time. They need to be part of a class, so they can inform one another. This fleet-learning concept can unite thousands of robots, all learning on their own and from one another by sharing their perceptions, actions, and outcomes. We don’t yet know how to accomplish this, but important work in lifelong learning and incremental learning provides a foundation for robots to gain real-word experience quickly and cost-effectively. Then they can sharpen their knowledge in simulations and take what they learn in simulations back to the real world in a loop that takes advantage of the strengths of each environment.\n\nIn the coming year, I hope that roboticists will shut down their sims, buy physical robots, take them out of the lab, and start training them on practical tasks in real-world settings. Let’s try this for a year and see how far we get!\n\nWolfram Burgard is a professor at the University of Freiburg, where he heads the Autonomous Intelligent Systems research lab.\n\nThings are really starting to get going in the field of AI. After many years (decades?!) of focusing on algorithms, the AI community is finally ready to accept the central role of data and the high-capacity models that are capable of taking advantage of this data. But when people talk about “AI,” they often mean very different things, from practical applications (such as self-driving cars, medical image analysis, robo-lawyers, image/video editing) to models of human cognition and consciousness. Therefore, it might be useful to distinguish two broad types of AI efforts: semantic or top-down AI versusecologicalor bottom-up AI.The goal of top-down AI is to match or exceed human performance on a specific human task like image labeling, driving, or text generation. The tasks are defined either by explicit labels (supervised learning), a set of rules (e.g., rules of the road), or a corpus of human-produced artifacts (for instance, GPT3 is trained on human-written texts using human-invented words). Thus, top-down AI is necessarily subjective and anthropocentric. It is the type of AI where we have seen the most advances to date.Bottom-up AI, on the other hand, aims to ignore humans, their tasks and their labels. Its only goal is to predict the surrounding world given sensory inputs (passive and active). Because the world is continuously changing, this goal will never be reached. But the hope is that, along the way, a general, task-agnostic model of the world will emerge.Self-supervised learningon raw sensory data, various generative models such as GANs, and intrinsic motivation approaches (e.g.,curiosity) are all attempts at bottom-up AI.While top-down AI is currently king in industry as well as academia, its focus on imitating humans (via labels and tasks) points to its main limitation. It is like an undergraduate student who didn’t attend lectures all semester but still gets an A by cramming for the final exam — its knowledge is of a superficial nature. Real understanding must be built up slowly and patiently, from the raw sensory inputs upward. This is already starting tohappen, and I hope the progress of bottom-up AI will continue in 2022.As a teenager in the 1980s USSR, I spent a lot of time hanging out with young physicists (as one does) talking about computers. One of them gave a definition of artificial intelligence that I still find the most compelling: “AI is not when a computer can write poetry. AI is when a computer will want to write poetry.” By this definition, AI may be a tall order, but if we want to bring it closer, I suspect we will need to start from the bottom up.Happy 2022! Bottoms up!\n\nAlexei Efros is a professor of computer science at UC Berkeley.\n\nUntil recently, big data processing has been dominated by batch systems like MapReduce and Spark, which allow us to periodically process a large amount of data very efficiently. As a result, most of today’s machine learning workload is done in batches. For example, a model might generate predictions once a day and be updated with new training data once a month.While batch-first machine learning still works for many companies, this paradigm often leads to suboptimal model performance and lost business opportunities. In the coming year, I hope that more companies will deploy models that can generate predictions in real time and update more frequently to adapt to changing environments.Consider an ecommerce website where half the visitors are new users or existing users who aren’t logged in. Because these visitors are new, there are no recommendations personalized to them until the next batch of predictions is computed. By then, it’s likely that many of these visitors will have left without making a purchase because they didn’t find anything relevant to them.In the last couple of years, technically progressive companies have moved toward real-time machine learning. The first level is online prediction. These companies use streaming technologies like Kafka and Kinesis to capture and process a visitor’s activities on their sites — often called behavioral data — in real-time. This enables them to extract online features and combine them with batch features to generate predictions tailored to a specific visitor based on their activities. Companies that have switched to online prediction, which include Coveo, eBay, Faire, Stripe, and Netflix, have seen more accurate predictions. This leads to higher conversion rates, retention rates, and eventually higher revenue. Online inference also enables sophisticated evaluation techniques like contextual bandits that can determine the best-performing model using much less data than traditional A/B testing.The next level of real-time machine learning is continual learning. While machine learning practitioners understand that data distributions shift continually and models go stale in production, the vast majority of models used in production today can’t adapt to shifting data distributions. The more the distribution shifts, the worse the model’s performance. Frequent retraining can help combat this, but the holy grail is to automatically and continually update the model with new data whenever it shows signs of going stale.Continual learning not only helps improve performance, but it can also reduce training costs. When you retrain your model once a month, you may need to train it from scratch on a lot of data. However, with continual learning, you may only need to fine-tune it with a much smaller amount of new data.A handful of companies have used continual learning successfully including Alibaba, ByteDance, and Tencent. However, it requires heavy infrastructure investment and a mental shift. Therefore, it still meets with a lot of resistance, and I don’t expect many companies to embrace it for at least a few years.In 2022, I expect a lot more companies to move toward online prediction, thanks to increasingly mature streaming technologies and a growing number of success stories. And the same underlying streaming infrastructure can be leveraged for real-time model analytics.\n\nChip Huyen works on a startup that helps companies move toward real-time machine learning. She teaches Machine Learning Systems Design at Stanford University.\n\nI believe that natural language processing in 2022 will re-embrace symbolic reasoning, harmonizing it with the statistical operation of modern neural networks. Let me explain what I mean by this.AI has been undergoing a natural language revolution for the past half decade, and this will continue into 2022 and well beyond. Fueling the revolution are so-called large language models (sometimes called foundation models), huge neural networks pretrained on gigantic corpora that encode rich information about not only language but also the world as described by language. Models such as GPT-3 (OpenAI), Jurassic-1 (AI21 Labs), Megatron-Turing NLG (Microsoft-Nvidia), and WuDao 2.0 (Beijing Academy of Artificial Intelligence), to name some of the largest ones, perform impressively well on a variety of natural language tasks from translation to paraphrasing. These models dominate academic leaderboards and are finding their way into compelling commercial applications.For all the justified excitement around large language models, they have significant shortcomings. Perhaps most notably, they don’t exhibit true understanding of any kind. They are, at heart, statistical behemoths that can guess sentence completions or missing words surprisingly well, but they don’t understand (nor can they explain) these guesses, and when the guesses are wrong — which is often — they can be downright ridiculous.Take arithmetic. GPT-3 and Jurassic-1 can perform one- and two-digit addition well. This is impressive, as these general-purpose models were not trained with this task in mind. But ask them to add 1,123 to 5,813 and they spit out nonsense. And why would they not? None of us learned addition merely by observing examples; we were taught the underlying principles.What’s missing is reasoning, and math is just an example. We reason about time, space, causality, knowledge and belief, and so on via symbols that carry meaning and inference on those symbols. These abstract symbols and reasoning don’t emerge from the statistics encoded in the weights of a trained neural network.The new holy grail is to inject this sort of semantic, symbolic reasoning into the statistical operation of the neural machinery. My co-founders and I started AI21 Labs with this mission, and we’re not alone. So-called neuro-symbolic models are the focus of much recent (and some less-recent) research. I expect that 2022 will see significant advances in this area.The result will be models that can perform tasks such as mathematical, relational, and temporal reasoning reliably. No less important, since the models will have access to symbolic reasoning, they will be able to explain their answers in a way that we can understand. This robustness and explainability will help move natural language processing from the current era of statistical pattern recognition into an era of trustworthy, understandable AI. This is not only intellectually exciting, but it also unlocks practical applications in domains in which trustworthiness is essential such as finance, law, and medicine.The year 2022 likely will not mark the end of the quest for such models, but I believe that it may be recognized as a pivotal year in this quest.\n\nYoav Shoham is a co-founder of AI21 Labs and professor emeritus of computer science at Stanford University.\n\nLarge models pretrained on immense quantities of text have been proven to provide strong foundations for solving specialized language tasks. My biggest hope for AI in 2022 is to see the same thing happen in computer vision: foundation models pretrained on exabytes of unlabeled video. Such models, after fine-tuning, are likely to achieve strong performance and provide label efficiency and robustness for a wide range of vision problems.Foundation models like GPT-3 by OpenAI and Gopher by DeepMind have shown a powerful ability to generalize in numerous natural language processing tasks, and vision models pretrained jointly on images and text, such as CLIP by OpenAI, Florence by Microsoft, and FLAVA by Facebook have achieved state-of-the-art results on several vision-and-language understanding tasks. Given the large amount of video readily available, I think the most promising next step is to investigate how to take advantage of unlabeled video to train large-scale vision models that generalize well to challenging real-world scenarios.Why video? Unlike static images, videos capture dynamic visual scenes with temporal and audio signals. Neighboring frames serve as a form of natural data augmentation, providing various object (pose, appearance), camera (geometry), and scene (illumination, object placements) configurations. They also capture the chronological order of actions and events critical for temporal reasoning. In these ways, the time dimension provides critical information that can improve the robustness of computer vision systems. Furthermore, the audio track in video can contain both natural sounds and spoken language that can be transcribed into text. These multimodal (sound and text) signals provide complementary information that can aid learning visual representations.\n\nLearning from large amounts of unlabeled video poses unique challenges that must be addressed by both fundamental AI research and strong engineering efforts:\n\nThese are exciting research and engineering challenges for which I hope to see significant advances in 2022.\n\nYale Song is a researcher at Microsoft Research in Redmond, where he works on large-scale problems in computer vision and artificial intelligence.\n\nThere’s a reason why artificial intelligence is sometimes referred to as “software 2.0”: It represents the most significant technological advance in decades. Like any groundbreaking invention, it raises concerns about the future, and much of the media focus is on the threats it brings. And yet, at no point in human history has a single technology offered so many potential benefits to humanity. AI is a tool whose goodness depends on how we use it.In 2022, I hope that the general public gains a greater appreciation of the benefits that AI brings to their lives. There are misconceptions and fears stemming from cases where AI has been used intrusively, and biased systems may have an unfairly adverse impact on some groups of people in areas like law enforcement, finance, insurance, and healthcare. Nonetheless, learning algorithms have shown potential infighting Covid-19,detecting wildfiresbefore they rage out of control, andanticipating catastrophic failureof things like buildings and airplanes.AI — deep learning models especially — can be a powerful instrument for social good. Computers never tire. They learn from more data than a human can absorb in a lifetime and enable people to accomplish some tasks much faster and with fewer errors. Applying these capabilities to problems like food production, healthcare, and climate change could bring unprecedented progress.Modern daily life requires AI as well. Social media platforms couldn’t exist without automated moderation models that root out toxicity and hate, and these problems threaten to escalate to a new level as human interactions move to virtual reality. Billions of people use the largest of these networks. If a major social media company were to moderate its content manually, it would need to hire a million people. Moderation is not scalable without machine learning models.Another hope I have for 2022 is that AI gains newcomers with engineering backgrounds rather than machine learning or data science. You shouldn’t need an advanced degree to build AI, and as the technology matures and requires less coding it will become easier to create ML models without knowing their internal workings.Increased access to the field is actually a key to realizing the broad social benefits of AI. A more diverse AI workforce would build less-biased systems. Equitable models are paramount as society automates more and more. Banks use AI models to determine who gets a mortgage, and employers use them to determine who gets a job interview. Machine learning models have a strong influence on society, and while the wrong ones can cause harm, the right ones can effect a cycle of positive change.Matt Zeiler is the founder and CEO of Clarifai, an AI platform that helps enterprises transform unstructured image, video, text, and audio data into actionable insights.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-12-29%20at%2011.10.03%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-12-29%20at%2011.10.03%20AM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-12-28%20at%2011.27.03%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-12-28%20at%2011.27.03%20AM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-12-28%20at%2011.37.38%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-12-28%20at%2011.37.38%20AM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-12-28%20at%2011.46.58%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-12-28%20at%2011.46.58%20AM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-12-28%20at%2011.51.48%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-12-28%20at%2011.51.48%20AM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-12-28%20at%2012.04.24%20PM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-12-28%20at%2012.04.24%20PM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-12-28%20at%2012.29.11%20PM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-12-28%20at%2012.29.11%20PM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-12-28%20at%201.14.16%20PM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-12-28%20at%201.14.16%20PM%20copy.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-285/",
    "title": "issue 285",
    "date": "",
    "reading_time": "",
    "content": "Greetings from Davos, Switzerland! Many business and government leaders are gathered here again for the annual World Economic Forum to discuss tech, climate, geopolitics, and economic growth. While the vast majority of my conversations have been on AI business implementations and governance, I have also been speaking about our latest AI climate simulator and about geoengineering. After speaking about geoengineering onstage at multiple events to a total of several hundred people, I’ve been pleasantly surprised by almost uniformly positive reactions. You can play with our simulatorhere.\n\nHere’s why I think we should seriously consider geoengineering: The world urgently needs to reduce carbon emissions, but it hasn’t happened fast enough. Given recent emission trends, without geoengineering, there’s no longer any plausible path to keeping global warming to the 1.5 degrees Celsius goal set by the Paris agreement. Under reasonable assumptions, we are on a path to 2.5 degrees of warming or worse. We might be in for additional abrupt changes if we hit certain tipping points.\n\nIf you tilt a four-legged chair by a few degrees, it will fall back onto its four legs. But if you tip it far enough — beyond its “tipping point” — it will fall over with a crash. Climate tipping points are like that, where parts of our planet, warmed sufficiently, might reach a point where the planet reorganizes abruptly in a way that is impossible to reverse. Examples include a possible melting of the Arctic permafrost, which would release additional methane (a potent greenhouse gas), or a collapse of ocean currents that move warm water northward from the tropics (theAtlantic Meridional Overturning Circulation).\n\nKeeping warming low will significantly lower the risk of hitting a tipping point. This is why theOECD’s reportstates, “the existence of climate system tipping points means it is vital to limit the global temperature increase to 1.5 degrees C, with no or very limited overshoot.”\n\nThe good news is that geoengineering keeps the 1.5 degree goal alive. Spraying reflective particles into the atmosphere — an idea calledStratospheric Aerosol Injection(SAI) — to reflect 1% of sunlight back into space would get us around 1 degree Celsius of cooling.\n\nNow, there are risks to doing this. For example, just as global warming has had uneven regional effects, the global cooling impact will also be uneven. But on average, a planet with 1.5 degrees of warming would be much more livable than one with 2.5 degrees (or more). Further, after collaborating extensively with climate scientists on AI climate models and examining the output of multiple such models, I believe the risks associated with cooling down our planet will be much lower than the risks of runaway climate change.\n\nI hope we can build a global governance structure to decide collectively whether, and if so to what extent and how, to implement geoengineering. For example, we might start with small scale experiments (aiming for <<0.1 degrees of cooling) that are easy to stop/reverse at any time. Further, there is much work to be done to solve difficult engineering challenges, such as how to build and operate a fleet of aircraft to efficiently lift and spray reflective particles at the small particle sizes needed.\n\nEven as I have numerous conversations about AI business and governance here at the World Economic Forum, I am glad that AI climate modeling is helpful for addressing global warming. If you are interested in learning more about geoengineering, I encourage you to play with our simulator at planetparasol.ai.\n\nI am grateful to my collaborators on the simulator work: Jeremy Irvin, Jake Dexheimer, Dakota Gruener, Charlotte DeWald, Daniele Visioni, Duncan Watson-Parris, Douglas MacMartin, Joshua Elliott, Juerg Luterbacher, and Kion Yaghoobzadeh.\n\nKeep learning!\n\nAndrew\n\nExplore Computer Use, which enables AI assistants to navigate, use, and accomplish tasks on computers. Taught by Colt Steele, this free course covers Anthropic’s model family, its approach to AI research, and  capabilities like multimodal prompts and prompt caching.Sign up for free\n\nA new open model rivals OpenAI’s o1, and it’s free to use or modify.\n\nWhat’s new:DeepSeek releasedDeepSeek-R1, a large language model that executes long lines of reasoning before producing output. The code and weights arelicensedfreely for commercial and personal use, including training new models on R1 outputs. Thepaperprovides an up-close look at the training of a high-performance model that implements a chain of thought without explicit prompting. (DeepSeek-R1-lite-previewcame out in November with fewer parameters and a different base model.)\n\nMixture of experts (MoE) basics:The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.\n\nHow it works:DeepSeek-R1 is a version ofDeepSeek-V3-Basethat was fine-tuned over four stages to enhance its ability to process achain of thought(CoT). It’s a mixture-of-experts transformer with 671 billion total parameters, 37 billion of which are active at any given time, and it processes 128,000 tokens of input context. Access to the model via DeepSeek’sAPIcosts $0.55 per million input tokens ($0.14 for cached inputs) and $2.19 per million output tokens. (In comparison, o1 costs $15 per million input tokens, $7.50 for cached inputs, and $60 per million output tokens.)\n\nOther models:DeepSeek researchers also released seven related models.\n\nResults:In DeepSeek’s tests, DeepSeek-R1 went toe-to-toe with o1, outperforming that model on 5 of 11 of the benchmarks tested. Some of the other new models showed competitive performance, too.\n\nWhy it matters:Late last year, OpenAI’s o1 kicked off a trend toward so-called reasoning models that implement a CoT without explicit prompting. But o1 and o3, its not-yet-widely-available successor, hide their reasoning steps. In contrast, DeepSeek-R1 bares all, allowing users to see the steps the model took to arrive at a particular answer. DeepSeek’s own experiments with distillation show how powerful such models can be as teachers to train smaller student models. Moreover, they appear to pass along some of the benefits of their reasoning skills, making their students more accurate.\n\nWe’re thinking:DeepSeek is rapidly emerging as a strong builder of open models. Not only are these models great performers, but their license permits use of their outputs for distillation, potentially pushing forward the state of the art for language models (and multimodal models) of all sizes.\n\nChinese robot makers Unitree and EngineAI showed off relatively low-priced humanoid robots that could bring advanced robotics closer to everyday applications.\n\nWhat’s new:At the annual Consumer Electronics Show (CES) in Las Vegas, Unitree showed itsG1($16,000 with three-finger hands, $21,000 with five-finger, articulated hands), which climbed stairs and navigated around obstacles. Elsewhere on the show floor, EngineAI’sPM01($13,700 through March 2025 including articulated hands) andSE01(price not yet disclosed) marched among attendees with notably naturalistic gaits.\n\nHow it works:Relatively small and lightweight, these units are designed for household and small-business uses. They’re designed for general-purpose tasks and to maintain stability and balance while walking on varied terrain.\n\nBehind the news:In contrast to the more-affordable humanoid robots coming out of China, U.S. companies like Boston Dynamics, Figure AI, and Tesla tend to cater to industrial customers. Teslaplansto produce several thousand of its Optimus ($20,000 to $30,000) humanoids in 2025, ramping to as many as 100,000 in 2026. Figure AI has demonstrated its Figure 02 ($59,000) in BMW manufacturing plants,showinga 400 percent speed improvement in some tasks. At CES, Nvidia unveiled its GR00T Blueprint, which includes vision-language models and synthetic data for training humanoid robots, and said its Jetson Thor computer for humanoids would be available early 2025.\n\nWhy it matters:China’s push into humanoid robotics reflects its broader national ambitions. Its strength in hardware has allowed it to establish a dominant position in drones, and humanoid robots represent a new front for competition. China’s government aims toachievemass production of humanoid robots by 2025 and establish global leadership by 2027, partly to address projected labor shortages of 30 million workers in manufacturing alone. Lower price points for robots that can perform arbitrary tasks independently could be valuable in elder care and logistics, offering tools for repetitive or physically demanding tasks.\n\nWe’re thinking:Although humanoid robots generate a lot of excitement, they’re still in an early stage of development, and businesses are still working to identify and prove concrete use cases. For many industrial applications, wheeled robots — which are less expensive, more stable, and better able to carry heavy loads — will remain a sensible choice. But the prospect of machines that look like us and fit easily into environments built for us is compelling.\n\nLawmakers in the U.S. state of Texas are considering stringent AI regulation.\n\nWhat’s new:The Texas legislature is considering the proposedTexas Responsible AI Governance Act (TRAIGA). The bill would prohibit a short list of harmful or invasive uses of AI, such as output intended to manipulate users. It would impose strict oversight on AI systems that contribute to decisions in key areas like health care.\n\nHow it works:Republican House Representative Giovanni Capriglione introduced TRAIGA, also known asHB 1709, to the state legislature at the end of 2024. If it’s passed and signed, the law would go into effect in September 2025.\n\nSandbox:A “sandbox” provision would allow registered AI developers to test and refine AI systems temporarily with fewer restrictions. Developers who registered AI projects with the Texas AI Council would gain temporary immunity, even if their systems did not fully comply with the law. However, this exemption would come with conditions: Developers must submit detailed reports on their projects’ purposes, risks, and mitigation plans. The sandbox status would be in effect for 36 months (with possible extensions), and organizations would have to bring their systems into compliance or decommission them once the period ends. The Texas AI Council could revoke sandbox protections if it determined that a project posed a risk of public harm or failed to meet reporting obligations.\n\nBehind the news:Other U.S. states, too, are considering or have already passed laws that regulate AI:\n\nWhy it matters:AI is not specifically regulated at the national level in the United States. This leaves individual states free to formulate their own laws. However, state-by-state regulation risks a patchwork of laws in which a system — or a particular feature — may be legal in some states but not others. Moreover, given the distributed nature of AI development and deployment, a law that governs AI in an individual state could affect developers and users worldwide.\n\nWe’re thinking:The proposed bill has its positive aspects, particularly insofar as it seeks to restrict harmful applications rather than the underlying technology. However, it imposes burdensome requirements for compliance, suffers from overly broad language, fails to adequately protect open source, and doesn’t distinguish between research and commercial use. Beyond that, state-by-state regulation of AI is not workable. On the contrary, AI demands international conventions and standards.\n\nDesigning integrated circuits typically requires years of human expertise. Recent work set AI to the task with surprising results.\n\nWhat’s new:Emir Ali Karahan, Zheng Liu, Aggraj Gupta, and colleagues at Princeton and Indian Institute of Technology Madras used deep learning and an evolutionary algorithm, which generates variations and tests their fitness, togenerate designsfor antennas, filters, power splitters, resonators, and other chips with applications in wireless communications and other applications. They fabricated a handful of the generated designs and found they worked — but in mysterious ways.\n\nHow it works:The authors trained convolutional neural networks (CNNs), given a binary image of a circuit design (in which each pixel represents whether the corresponding portion of a semiconductor surface is raised or lowered), to predict its electromagneticscattering propertiesandradiative properties. Based on this simulation, they generated new binary circuit images using evolution.\n\nResults:The authors fabricated some of the designs to test their real-world properties. The chips showed similar performance than the CNNs had predicted. The authors found the designs themselves baffling; they “delivered stunning high-performances devices that ran counter to the usual rules of thumb and human intuition,” co-author Uday Khankhojetoldthe tech news site Tech Xplore. Moreover, the design process was faster than previous approaches. The authors’ method designed a 300x300 micrometer chip in approximately 6 minutes. Using traditional methods it would have taken 21 days.\n\nBehind the news:Rather than wireless chips, Google has used AI toacceleratedesign of the Tensor Processing Units that process neural networks in its data centers.AlphaChipused reinforcement learning to learn how to position chip components such as SRAM and logic gates on silicon.\n\nWhy it matters:Designing circuits usually requires rules of thumb, templates, and hundreds of hours of simulations and experiments to determine the best design. AI can cut the required expertise and time and possibly find effective designs that wouldn’t occur to human designers.\n\nWe’re thinking:AI-generated circuit designs could help circuit designers to break out of set ways of thinking and discover new design principles.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--46-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/The-Batch-ads-and-exclusive-banners--5-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--47-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--45-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--48-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--48-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-iv/",
    "title": "issue iv",
    "date": "",
    "reading_time": "",
    "content": "While most of AI's practical value today is through supervised learning, much of human learning appears to be unsupervised. When I speculate about the future of unsupervised learning, I believe it will still be necessary to train much larger networks, and on much more data than we use today — and that will be very time-consuming without much faster computers.I'm grateful for all of you at Intel, Nvidia, Qualcomm, AMD, and various startups working on faster chips. The DL world is nowhere near maxing out our ability to use compute!Keep learning,Andrew\n\nNeed a quick wardrobe upgrade? Image generation to the rescue! This research project automatically visualizes small changes to clothing that make the wearer look more fashionable.\n\nWhat’s new:Researchers built a neural net to answer the question: Given an outfit, what small changes would make it more fashionable?Fashion++renders improvements, from rolling up sleeves to adding an accessory to replacing garments. Thisvideoexplains.\n\nHow it works:Given a full-body image, the model:\n\nCan AI have fashion sense?To train the fashionability classifier, Wei-Lin Hsiao and her collaborators represented high fashion using theChictopiaphoto set. They created degraded alternatives automatically by swapping in dissimilar garments (as measured by Euclidean distance on CNN features). Judges on Mechanical Turk found 92% of most-changed to be more fashionable.\n\nTakeaway:Fashion++ has the kind of smarts generally thought to be the province of humans. It has clear commercial uses in ecommerce. And who couldn’t use a style assist?\n\nA California grocer is stocking produce grown by robots, a sign of AI’s growing presence in agriculture.\n\nWhat’s new:Iron Ox, a startup that grows greens in robot-tended warehouses, is supplying three vegetable varieties to Bianchini’s Grocery of San Carlos, CA. The producer grows 26,000 heads of lettuce and other greens annually in an 8,000 square foot space — 30 times more space-efficient than conventional farming with less chemical runoff and greenhouse gases.\n\nHow it works:The automated farm relies on human labor for planting and packaging. In between, machines are in charge:\n\nReality check:It’s not clear whether Iron Ox’s operation can be cost-effective. Bianchini’s sells its basil for a price on par with that of conventionally farmed options, theSan Francisco Chroniclereports. But its baby lettuce goes for a lot more, according toThe Verge.\n\nTakeaway:The cost of farm labor is ballooning, and growers of many crops are scoping out automated alternatives. Iron Ox is betting that robotic agriculture ultimately will cost less in a smaller environmental footprint.\n\nThe technique known asattentionhas yielded spectacular results in speech and natural language tasks. Now researchers have shown that it can improve image recognizers as well.\n\nWhat’s new:A residual neural network incorporating self-attention layers beat the state of the art accuracy on ImageNet classification by 1.3% over a ResNet50 baseline, according to a newpaper. It also beat the best Common Objects in Context object detection score over a RetinaNet baseline. This is not the first use of attention in vision tasks, but it outperforms earlier efforts.\n\nHow it works:Quoc Le and his colleagues augmented convolution layers with multi-headed self-attention layers. The resulting network uses attention in parallel with convolution and concatenates the outputs produced by each. Attention extends a network’s awareness beyond the data it’s processing at a given moment. Adding it to a convolutional neural network enables the model to consider relations between different areas of an image.\n\nWhy it matters:The attention networks generally require fewer parameters than convolution layers to achieve an accuracy target. That makes it possible to improve performance in a same-sized model.\n\nBottom line:Image recognizers based on CNNs already achieve high accuracy and sensitivity, and adding attention makes them even sharper.\n\nStill debating whether to get into deep learning? Check out the techniques you'll learn and projects you'll build in the Deep Learning Specialization.Learn more\n\nIf digital assistants had feet, their Achilles heel would be requiring users to remember commands that invoke new skills. Amazon proposes a way to train systems like Alexa to learn new domains incrementally, so they can parse intent in a phrase like “call a car” even if the skill itself — say, Uber/Lyft/Grab/etc.— isn’t named.\n\nWhat’s new:Amazon’sproposal, called Continuous Domain Adaptation or CoNDA, maintains previously learned parameters while updating those relevant to a new domain. This strategy teaches new skills in far less time than retraining a model from scratch. It also avoids thecatastrophic forgetting problem, in which the new learning displaces old.\n\nThe results:CoNDA achieved 95.6 percent accuracy over 100 new domains and 88.2 percent accuracy for all domains after the new ones have been added. That’s only 3.6 percent lower than if the model had been retrained from scratch.\n\nHow it works:Devised by Han Li and his colleagues, CoNDA is a variation on the Shortlister classifier, which comprises three modules. An LSTM-based encoder maps verbal commands to vector representations. A second module generates summarization vectors, freezing parameter weights when a new domain is added and updating only those relevant to new training data. It adds a regularization term to the loss function to avoid overfitting. Finally, a feed-forward network predicts the classification using cosine normalization. A negative sampling procedure maintains known domains to alleviate catastrophic forgetting.\n\nTakeaway:The new training method promises relief to digital assistants struggling to recall dozens of invocations. More important, it opens new doors for continuous learning — an essential capability as AI systems are deployed in ever more dynamic environments.\n\nThe usual ways to plot a course from one place to another typically entail a trade-off between economy of motion and computation time. Researchers instead used deep learning to find efficient paths quickly.\n\nWhat’s new:The researchersproposea recurrent neural network that generates good motion plans in a fixed time regardless of the complexity of the environment. Thisvideogives an overview.\n\nWhy it’s better:Their system’s performance scales almost linearly with higher dimensions, outperforming algorithms like A* and RRT* that bog down in larger or more complex spaces. It proved 10 times faster with a three-link robot, 20 times faster with four links, and 30 times faster controlling a three link arm in six dimensions.\n\nHow it works:Known as OracleNet, the model mimics the stepwise output of anoraclealgorithm (one that can generate efficient paths to or from any spot in a given space). At each waypoint, the network uses the current position and the goal location to decide on the position of the next waypoint. It uses an LSTM to preserve information over several steps.\n\nWhy it matters:Robotic control is devilishly hard, and typical solutions impose severe limitations. This research offers an inkling of what neural networks can accomplish in the field.\n\nWhat’s next:Researchers Mayur Bency, Ahmed Qureshi, and Michael Yip suggest that active learning could enable OracleNet to accommodate dymnamic environments. Further variations could enable it to navigate unfamiliar or changing environments.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/ef921548-9308-4716-a2e4-9dd3ae759ae0.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/1711e957-1676-4a39-ac9a-5e0a36c0ae58.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/b4cc5541-4e1d-4b2d-8013-4627be14a8b0.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/e6d29ef4-e1f2-49a2-aa7b-f0ab14e2d61c.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/4e6c54b3-54f0-4b3d-b127-2d8cf12ecf0d.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/183933c4-f7bc-41cc-bec1-e3e18cbd6644.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-268/",
    "title": "issue 268",
    "date": "",
    "reading_time": "",
    "content": "Last week I spoke at Coursera Connect, the company’s annual conference in Las Vegas, where a major topic was AI and education. There has been a lot of hype about generative AI’s ability to transform industries overnight. Certainly many industries — including education — will be transformed. But we’re about 15 years into the deep learning revolution, and we’re not yet done identifying and building useful deep learning applications. Despite the exciting progress to date with generative AI, I expect that a decade from now we will still be far from finished identifying and building generative AI applications for education and numerous other sectors.\n\nThis was the first time since 2019 that Coursera’s conference was held in person. It was great to see so many people dedicated to the educational mission coming together to discuss innovations, including generative AI innovations, that serve learners.\n\nCoursera’s CEO Jeff Maggioncalda and the company’s executive team demonstrated multiple generative AI products, such as:\n\nBecause AI is a general-purpose technology, there are many opportunities to apply it to different tasks in education. I was thrilled at the volume of experimentation happening across Coursera, DeepLearning.AI, and the broader ecosystem of partners and customers. I was also proud topresentawardsto many partners and customers who are doing great work to serve learners.\n\nI was particularly gratified by the number of people coming together in service of the education mission. Even before the recent rise of AI, education was already urgently in need of improvement. With AI transforming jobs, the need has become even more acute. My heart was warmed by the conversations I had with many people from universities, high schools, businesses, and the Coursera team who have a deep desire to help others through education.\n\nCoursera held its first conference in 2013, when the online education movement was in its early days, and we all had high hopes for where it could go. Today, there are over 155 million learners on Coursera. Despite that, given society’s heightened need for education and AI’s potential to transform the field, I feel the opportunities for edtech at this moment are greater than at any moment over the past decade.\n\nKeep learning!\n\nAndrew\n\nP.S. I’m excited to announce our new specialization,Generative AI for Software Development, taught by Laurence Moroney! Using chatbots to generate code is not the only way AI can help developers. This three-course series shows you how to use AI throughout the software development lifecycle – from design and architecture to coding, testing, deployment, and maintenance. Everyone who writes software can benefit from these skills. Please sign uphere!\n\nGenerative AI for Software Development, our new skill certificate, gives you practical experience applying AI to coding, debugging, optimization, and documentation as it explores AI’s role across the entire development lifecycle—design, architecture, coding, testing, deployment, and maintenance. Equip yourself with the tools to enhance every step of your dev workflow.Enroll now\n\nCalifornia, a jurisdiction that often influences legislators worldwide, passed a slew of new laws that regulate deepfakes.\n\nWhat’s new:California Governor Gavin Newsom signed into law eight bills that aim to curb the use of generative AI inpoliticsandentertainment.\n\nHow it works:The legislation prohibits deceptive AI-generated media in political campaigns; requires permission for using digital stand-ins for actors, musicians, and other entertainers; and criminalizes generation of sexually explicit imagery without the subject’s consent.\n\nBehind the news:Newsom has not yet acted on Senate Bill 1047, acontroversiallaw that would impose significant burdens on AI model developers. He hasexpressedthat the bill could interfere with innovation, especially with respect to open source projects.\n\nWhy it matters:Laws passed in California often point the way for legislators in other U.S. states, the federal government, and consequently other countries. The new laws that regulate deepfakes in political campaigns fill a gap left by the Federal Election Commission (FEC), which hassaidit lacks authority to regulate the use of AI in political ads. Meanwhile, the Federal Communications Commission (FCC)proposedrules that would mandate disclosure of uses of AI in political ads but has yet to implement them.\n\nWe’re thinking:We’re glad to see California target undesirable applications rather than AI models.Regulating applicationsrather than general-purpose technology that has a wide variety of uses — many of which are beneficial — avoids the dangers of California SB-1047, which is still awaiting the governor’s signature or veto. That law, which seeks to restrict AI models, wouldendangerinnovation and especially open source.\n\nThe parade of ever more capable LLMs continues with Qwen 2.5.\n\nWhat’s new:Alibaba releasedQwen 2.5in several sizes, the API variants Qwen Plus and Qwen Turbo, and the specialized modelsQwen 2.5-Coder and Qwen 2.5-Coder-InstructandQwen 2.5-Math and Qwen 2.5-Math-Instruct. Many are freely available for commercial use under the Apache 2.0 licensehere. The 3B and 72B models are also free, but theirlicenserequires special arrangements for commercial use.\n\nHow it works:The Qwen 2.5 family ranges from 500 million parameters to 72 billion parameters.\n\nResults:Compared to other models with open weights, Qwen 2.5-72B-Instruct beats LLama 3.1 405B Instruct and Mistral Large 2 Instruct (123 billion parameters) on seven of 14 benchmarks includingLiveCodeBench,MATH(solving math word problems), andMMLU(answering questions on a variety of topics). Compared to other models that respond to API calls, Qwen-Plus beats LLama 3.1 405B, Claude 3.5 Sonnet, and GPT-4o on MATH, LiveCodeBench, andArenaHard. Smaller versions also deliver outstanding performance. For instance, Qwen 2.5-14B-Instruct outperforms Gemma 2 27B Instruct and GPT-4o mini on seven benchmarks.\n\nBehind the news:Qwen 2.5 extends a parade of ever more capable LLMs that include Claude 3.5 Sonnet, GPT-4o, and LLama 3.1 as well as the earlierQwen 2 family.\n\nWhy it matters:The new models raise the bar for open weights models of similar sizes. They also rival some proprietary models, offering options to users who seek to balance performance and cost.\n\nWe’re thinking:Some companies encourage developers to use their paid APIs by locking their LLMs behind non-commercial licenses or blocking commercial applications beyond a certain threshold of revenue. We applaud Qwen’s approach, which keeps most models in the family open.\n\nThe AI startup Runway is helping to retool Lionsgate, the producer of blockbuster movie franchises likeThe Hunger GamesandJohn Wick, for the era of generated video.\n\nWhat’s new:Runway willbuilda custom video generator to help Lionsgate streamline its production processes. It alsolaunchedan API for its Gen-3 Alpha Turbo model.\n\nRunway + Lionsgate:Runway will fine-tune its proprietary models on Lionsgate productions to enable the filmmaker to generate new imagery based on its previous work. The companies didn’t disclose financial terms of the arrangement.\n\nGen-3 API:Concurrently with announcing the Lionsgate deal, Runwayunveiledan API that drives its Gen-3 Alpha and Gen-3 Alpha Turbo models as well as updates to Gen-3 Alpha.\n\nWhy it matters:Although the plan is to use Runway’s technology for pre- and post-production, this deal puts state-of-the-art video generation at the heart of Lionsgate’s operations and encourages professional cinematographers, editors, special effects artists, and other cinematic specialists to see what they can do with it. For Lionsgate, it’s a bid to stay ahead of competitors. For AI, it could be a major move into the Hollywood spotlight.\n\nWe’re thinking:While upstart competitors are using pretrained models, Lionsgate will be using a model that has internalized its own style and capabilities.\n\nA robot that plays table tennis beats human beginners and entertains experts.\n\nWhat’s new:David B. D’Ambrosio, Saminda Abeyruwan, Laura Graesser, Atil Iscen, Pannag R. Sanketi and colleagues at Google showed off arobot armthat challenges human players at table tennis. You can see it in actionhere.\n\nKey insight:A table tennis match can be broken into individual volleys that start when an opponent hits the ball and end when the robot returns the ball to the opponent’s side of the table or the ball goes out of play. This simple scheme enables a robotic control system to learn how to return a ball without attending to strategy.\n\nThe robot:The authors mounted arobotic armatop two linear gantries that enabled the arm to move to the left and right, and forward and backward. Twocamerascaptured images of the ball and fed them to aperception systemthat estimated ball positions. A 20-cameramotion-capture systemtracked the position of the opponent’s paddle.\n\nHow it works:Instead of training an end-to-end system or using a robotics foundation model, the authors broke down the gameplay into subtasks, delegated them to separate modules, and orchestrated them to work together. The robot was controlled by a high-level controller: a custom algorithm including a convolutional neural network (CNN) that classified whether to return the ball using a forehand or backhand stroke and a vanilla neural network that classified spin. The high-level controller selected among 17 low-level controllers (all CNNs). Each low-level controller executed a different skill, enabling the system to return serves or rallies, adjust for ball spin, target different spots on the table, and so on.\n\nResults:The robot played 29 three-game matches against 29 players of varying skill (beginner, intermediate, advanced, and advanced+ as rated by a professional coach).\n\nWhy it matters:Roboticists have been programming robot arms to play table tennis for at least adecade. Earlier projects enabled robots to perform various aspects of the game, like aiming at a specific target or smashing, but none tackled complete gameplay against competitive human opponents. Breaking the problem into two parts — a library of individual skills (low-level controllers) and an algorithm that chooses which to use — simplifies the task. Weaknesses in the robot’s performance (for example, difficulty returning underspin) can be addressed by adding a skill that compensates.\n\nWe’re thinking:Even expert players had enough fun playing against this robot to want to play more. That’s a successful gaming system!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--13--1.png",
      "https://dl-staging-website.ghost.io/content/images/2024/09/The-Batch-ads-and-exclusive-banners---2024-09-24T101915.929.png",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--10--1.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--11-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--12-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--13-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-273/",
    "title": "issue 273",
    "date": "",
    "reading_time": "",
    "content": "Welcome to our special Halloween issue of The Batch, in which we probe fears, anomalies, and shadows of AI.\n\nIn this letter, I’d like to explore why some people who are knowledgeable in AI take extreme positions on AI “safety” that warn of human extinction and describe scenarios, such as AI deciding to “take over,” based less on science than science fiction. As Iwrotein last year’s Halloween edition, exaggerated fears of AI cause real harm. I’d like to share my observations on the psychology behind some of the fear mongering.\n\nFirst, there are direct incentives for some AI scientists and developers to create fear of AI:\n\nI’ve seen people start off making mild statements about dangers of AI and get a little positive feedback in the form of attention, praise or other rewards, which encouraged them to double down and become more alarmist over time. Further, once someone has taken a few steps in this direction, the psychological effect known ascommitment and consistency bias, where one feels obliged to stay consistent with one’s earlier statements, will lead some people to keep going in this direction.\n\nTo be clear, AI has problems and potentially harmful applications that we should address. But excessive hype about science-fiction dangers is also harmful.\n\nAlthough I’m highlighting various motivations for AI fear mongering, ultimately the motivations that underlie any specific person’s actions are hard to guess. This is why, when I argue for or against particular government policies, I typically stick to the issues at hand and make points regarding the impact of particular decisions (such as whether it will stifle open source) instead of speculating about the motivations of specific people who take particular sides. This, too, is why I rarely make issues personal. I would rather stick to the issues than to the personalities.\n\nWhen I understand someone’s motivations, I find that I can better empathize with them (and better predict what they’ll do), even if I don’t agree with their views. I also encourage expressing one’s own motives transparently. For example, I’m strongly pro the AI community, and strongly pro open source! Still, arguments based on substantive issues ultimately carry the most weight. By arguing for or against specific policies, investments, and other actions based on their merits rather than hypothetical motivations, I believe we can act more consistently in a rational way to serve the goals we believe in.\n\nHappy Halloween!\n\nAndrew\n\nListen! Did you hear a rasping whisper say, “Beware”? Was it a rogue superintelligence? Or just a deepfake? We don’t know, but we heard it, too. It warns of machine learning algorithms that would devour electricity to leave us shivering in the cold night air, mislead us with increasingly inaccurate output, and take over the work that gives our lives meaning. In this special issue ofThe Batch, as inprioryearsatthisseason, we face our fears of AI. Stay close to your laptop’s screen. It may be the only light amid the growing darkness.\n\nThe globe’s growing AI infrastructure requires huge amounts of electricity, possibly more than power providers can generate responsibly. Could AI models suck energy resources dry?\n\nThe fear:Demand for AI is skyrocketing, and with it the demand for energy to fuel training and inference. Power-hungry systems will overwhelm our current power sources. If unchecked, they could lead to energy shortages and runaway carbon emissions.\n\nHorror stories:AI companies don’t disclose the percentage of their energy needs that AI consumes, but top companies, led by OpenAI, havepitchedthe U.S. government to build out new energy sources and infrastructure. The trend is clear: Escalating demand risks tapping out existing power plants, pushing carbon emissions higher, and delaying moves to more sustainable energy sources.\n\nHow scared should you be:The rapid growth of AI poses a sharp dilemma: How can we meet demand without releasing greater and greater amounts of heat-trapping greenhouse gasses into the atmosphere? AI companies’ two-pronged strategy of lobbying governments and investing in carbon-free energy resources suggests the problem requires both short- and long-term approaches.\n\nFacing the fear:WhileAI poses a difficult problem for the world’s energy consumption, it’s also an important part of the solution. Learning algorithms arereducingenergy consumption andmanagingdistribution. They can helpcapture and storecarbon dioxide from energy plants and manufacturers before it reaches the atmosphere. AI is also helping to monitor the atmosphere, oceans, and forests so we canunderstandthe impacts of climate change and make policy accordingly. And processing in centralized data centers — as power-hungry as they are — is far more energy-efficient than using local servers or edge devices. Ongoing AI development will make such efforts more effective and help us build a more sustainable future.\n\nPoliticians and pundits have conjured visions of doom to convince lawmakers to clamp down on AI. What if terrified legislators choke off innovation in AI?\n\nThe fear:Laws and treaties that purportedly were intended to prevent harms wrought by AI are making developing new models legally risky and prohibitively expensive. Without room to experiment, AI’s benefits will be strangled by red tape.\n\nHorror stories:At least one law that would have damaged AI innovation and open source has been blocked, but another is already limiting access to technology and raising costs for companies, developers, and users worldwide. More such efforts likely are underway.\n\nHow scared should you be:The veto of SB 1047 was a narrow escape for California and companies and labs that operate there. Yet regulations like the AI Act are poised to reshape how AI is trained and used worldwide. Historysuggeststhat restrictive laws often lead to more caution and less experimentation from technologists.\n\nFacing the fear:AI needs thoughtful regulation to empower developers to help build a better world, avoid harms, and keep learning. But effective regulation of AI requires restrictingapplications, not the underlying technology that enables them. Policymakers should align with a wide range of developers – not just a few that have deep pockets – to address harmful applications without stifling broader progress.\n\nAI coding assistants are brewing codebases that once were the sole province of human programmers. Will AI systems take over software development?\n\nThe fear:Programming jobs will vanish as tireless AI agents plan, write, debug, and document code as well as or better than humans. Software engineers will find themselves wandering the job market like restless spirits.\n\nHorror stories:Since 2020, AI-powered coding tools have advanced from completing individual lines of code to generating complex programs. More and more coders work with an automated assistant. These tools are poised to take over more and more of the development cycle as they evolve.\n\nHow scared should you be:Nvidia CEO Jensen Huangpredictedthat AI would make “everybody in the world [a] computer programmer,” while observersfretthat Copilot erodes problem-solving skills. But the reality is more nuanced. Researchshowsthat automation is likely to perform certain coding tasks but not entire programming jobs. These tools excel at routine tasks and boilerplate code, but they amplify rather than automate the developer's core skills. Conceptual tasks like specifying what a program should do, collaborating with colleagues, and translating business needs into software design remain the domain of human coders — for now.\n\nFacing the fear:Developers have more to gain by embracing AI assistants than fearing them. These tools don’t just automate tasks; they accelerate learning, refine problem-solving, and enhance programming skills. Developers who master both coding fundamentals and AI assistance won’t just survive — they’ll thrive!\n\nLarge language models are trained on datasets scraped from the web, which includes pages that contain answers to common questions that are used to test the models. How can we evaluate them if they’ve studied the answers before we give them the test?\n\nThe fear:Machine learning research marks progress based on trained models’ responses to benchmark problems they didn’t encounter during training. But the solutions to many problems used to evaluate large language models have made their way into popular training datasets, making it impossible to verify progress in precise ways. The state of the art is an illusion and researchers are shooting in the dark.\n\nHorror stories:Researchers have found disturbing signs that the test sets of many widely used benchmarks have leaked into training sets.\n\nHow scared should you be:Leakage of benchmark test sets into training sets is a serious problem with far-reaching implications. One observerlikenedthe current situation to an academic examination in which students gain access to questions and answers ahead of time — scores are rising, but not because the students have learned anything. If training datasets are contaminated with benchmark tests, it’s impossible to know whether apparent advances represent real progress.\n\nFacing the fear:Contamination appears to be widespread but it can be addressed. One approach is to embedcanary strings— unique markers within test datasets like BIG-bench — that enable researchers to detect contamination by checking whether a model can reproduce them. Another is to continuallyenhancebenchmarks with new, tougher problems. Of course, researchers can devise new benchmarks, but eventually copies will appear on the web. Alternatively, they can keep new benchmarks under wraps and run them only onprivate servers.\n\nTraining successive neural networks on the outputs of previous networks gradually degrades performance. Will future models succumb to the curse of recursive training?\n\nThe fear:As synthetic text, images, videos, and music come to make up an ever larger portion of the web, more models will be trained on synthetic data, and then trained on the output of models that themselves were trained on synthetic data. Gradually, the distribution of the generated training data will deviate ever farther from that of real-world data, leading to less and less accurate models that eventually collapse.\n\nHorror stories:Many state-of-the-art models are trained on data scraped from the web. The web is huge, but it’s not large or diverse enough to provide endless amounts of training data for every task. This tempts developers to train models on data generated by other models, even as the web itself becomes increasingly overrun by synthetic data.\n\nHow scared should you be:Training on synthetic data is at the heart of some of today’s best-performing models, including the Llama 3.1, Phi 3, and Claude 3 model families. (Meta showed that using anagentic workflowwith Llama 3.0 to generate data — rather than generating data directly — resulted in useful data to train Llama 3.1.) This approach is essential to the technique known as knowledge distillation, which makes smaller, more parameter-efficient models. Moreover, it’s valuable for building models that can perform tasks for which little real-world data is available, for instance machine translation models that can handle languages spoken by relatively small populations. Although the authors of “The Curse of Recursion” found that training a series of models, each exclusively on the output of the previous one, leads to rapid degradation in performance, introducing even 10 percent real-world data significantly curbed this decline.\n\nFacing the fear:Model collapse is not a near-term risk, and perhaps not any risk at all, given research progress on generating synthetic data. Still, it makes sense to track the presence of generated data in training datasets and include it carefully. The large-scale web dataset Common Crawl captures regular snapshots of the web. If generated data were to inundate the online environment, using an earlier snapshot would eliminate a huge amount of it. More broadly, model builders increasingly curate high quality data, and whether a given example appears to have been generated will become a factor. Datasets can be filtered using algorithms designed toidentifygenerated content. Increasing use of watermarking would make the job still easier. These measures will help developers ensure a healthy balance of real and generated data in training sets for a long time to come.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--27-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/10/LastWood-byFirelight8_1200px.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/10/BestCostumes2_1200px--1--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--29-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/11/HalloweenQuiz-4b_1200px-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--30-.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-144/",
    "title": "issue 144",
    "date": "",
    "reading_time": "",
    "content": "Last week, I described trends thatAI Fund, the venture studio I lead, has seen in building AI startups. I'd like to discuss another aspect of building companies that’s unique to AI businesses: the controversial topic of data moats.\n\nA company has a data moat if its access to data makes it difficult for competitors to enter its business. Moat is a common business term used evoke the water-filled moats built around castles to make them easier to defend against attackers. For example, if a self-driving car company can acquire far more data than its competitors to train and test its system, and if this data makes a material difference in the system’s performance, then its business will be more defensible.\n\nFor a few years, some investors asked every AI startup’s founders about its data moat, as if they expected everyone to build one. But, like many things in AI, it depends. A data moat can provide protection, but its effectiveness varies depending on the specific circumstances of the business.\n\nFor instance, a data moat may not do much to protect an AI business if:\n\nIn contrast, data can make an AI business more defensible if:\n\nData strategy is important for AI companies, and thinking through how a system’s performance varies with the amount of data, the importance of fresh data, and other factors described above can help you decide how much having data adds to a business’ defensibility. Sometimes a data moat doesn't help at all. But in other cases, it's one pillar (hopefully among many) that makes it harder for competitors to catch up.\n\nKeep learning!\n\nAndrew\n\nItching to get your hands on a fully trained large language model? The wait is over.What’s new:Metaintroducedthe OPT family of transformer-based language models with nearly unfettered access to source code and trained weights. The family’s eight models range in size from 125 million to 175 billion parameters.How it works:The OPT architecture is similar to that of OpenAI’s GPT-3. The models were trained on publicly available datasets that include novels, news articles, Reddit posts, and a subset ofThe Pile.\n\nBehind the news:OPT-175B is the largest and most ambitious open-source language model to date, but it’s not the first.\n\nYes, but:A parameter count of 175 billion parameters is mouthwatering, but it takes a lot of horsepower to drive a model that large. As Maarten Sap of the Allen Institute for Artificial IntelligencetoldIEEE Spectrum, “[I’d] love to use OPT-175B,” but “few research labs actually have the infrastructure to run this model.”Why it matters:For researchers — well, for anyone interested in language modeling, really — the opportunity is obvious. OPT comes pretrained, ready to be used, fine-tuned, dissected, or adapted for any purposes the AI community dreams up. No more APIs! No more paywalls! It’s your party, so indulge yourself. For Meta, open-sourcing these models may have several benefits. Giving away OPT is a community-minded gesture at a time when the company has been under fire for proliferating hatred, misinformation, and disinformation on a grand scale. It’s a bid to attract talent that could help break in young engineers to the company’s coding practices. And it’s a shot at OpenAI, the former nonprofit, open-source shop, which was criticized for keeping GPT-3’s code under wraps.We’re thinking:The OPT-175B traininglogoffers a rare look at a large-scale machine learning project. While the mass media may imagine bespectacled programmers in airy, well-lit rooms debating the nature of intelligence, technology development is often messy as researchers struggle to visualize what an algorithm is doing or trace the source of a GPU crash. Worth a look!\n\nHospitals are using robots to lighten the load on clinical staff.What’s new:A number of U.S. hospitals are using Moxi, a robot from Diligent Robotics, to ferry supplies, lab specimens, soiled laundry, and other items,Wiredreported.How it works:Moxiis four feet tall with blinking L.E.D. eyes and an articulated arm tipped with a rubber grip. It navigates using a front-facing camera, rear-facing lidar, and auditory cues. A secure compartment that can be unlocked by a radio-frequency badge holds sensitive items such as lab samples. Fifteen Moxi robots are operating in U.S. hospitals, and another 60 are scheduled for deployment this year.\n\nBehind the news:In 2020, the American Nurses AssociationassessedMoxi’s performance in three Texas hospitals. The study found that the robots improved nurse productivity and reduced feelings of burnout. However, the robots struggled to navigate crowded hospital halls, and their inability to read expiration dates raised the worry that they might contribute to adverse consequences.Why it matters:Robots may not have the best bedside manner (yet), but they can create much-needed breathing room for human caregivers. In a 2021surveyof U.S. nurses, 83 percent of respondents said their shifts were understaffed in a way that affected patients’ safety half of the time, and 68 percent had considered leaving the profession. Meanwhile, the U.S. is one of many countries with a rapidly growing population of elderly people, putting further strain on the healthcare system. These conditions create a clear opening for robots capable of performing many low-risk, repetitive chores.We’re thinking:Come to think of it, Hippocrates’ dictum “first, do no harm” bears a striking similarity toAsimov’s First Law of Robotics, “a robot may not harm a human being.”\n\nWhat’s new about the revisedMachine Learning Specializationthat’s set to launch in June? It takes the core curriculum — vetted by millions of learners — and makes it more approachable by balancing intuition, code, and math for beginners.Pre-enroll now\n\nA neural network may help an online music service to spot songs with the potential to go big.What’s new:Musiio uses AI to identify specific attributes and qualities in recorded music. Online audio distributor SoundCloudpurchasedthe Singapore-based startup, which was valued at $10 million last year, for an undisclosed sum.How it works:Musiio trained its model on a proprietary database of songs, each tagged with dozens of labels including genre, vocalist’s gender, instruments featured, and emotions expressed.\n\nBehind the news:A number of companies offer AI-powered tools designed to enable recording companies, artists, and fans to squeeze more value out of music.\n\nWhy it matters:Millions of new songs are released every year. Amid the deluge, AI can help distributors recognize potential hits, recording companies identify talent, fans find music they like, and musicians create sounds that stand out. Of course, the makings of a hit include social dynamics among listeners — presumably that’s where acquirer SoundCloud comes in.We’re thinking:According to models, this edition ofThe Batchhas moderate energy with high variance and a 72 percent chance of being powerful.\n\nIf you want to both synthesize data and find the probability of any given example — say, generate images of manufacturing defects to train a defect detector and identify the highest-probability defects  — you may use the architecture known as a normalizing flow. A new type of layer enables users to boost a normalizing flow’s performance by tuning it to their training data.What’s new:Gianluigi Silvestri at OnePlanet Research Center and colleagues at Google Research and Radboud University introduced theembedded-model flow(EMF). This architecture uses aprobabilistic program— a user-defined probability distribution — to influence the training of anormalizing flow.Normalizing Flow basics:A normalizing flow (NF) is a generative architecture. Like a generative adversarial network (GAN), it learns to synthesize examples similar to its training data. Unlike a GAN, it also learns to calculate the likelihood of existing examples. During training, an NF transforms examples into noise. At inference, it runs in reverse to transform noise into synthetic examples. Thus it requires layers that can execute both forward and backward; that is, layers that are invertible as well as differentiable.Key insight:Like a normalizing flow layer, the cumulative distribution function (CDF), which is a function of a probability distribution, can be both differentiable and invertible. (In cases where this is not true, it’s possible to approximate the CDF’s derivative or inverse.) The CDF of a probability distribution can be used to compute that distribution, so it can be used to create a probabilistic program. Such a program, being differentiable and invertible, can be used in an NF, where it can transform a random vector to follow a probability distribution and vice versa.How it works:EMF is a normalizing flow composed of three normalizing flow layers and a user-defined probabilistic program layer. The authors used adatasetof handwritten digits to train the model to generate digits 0 through 9.\n\nResults:The authors compared EMF with a baseline made up of a comparable number of normalizing flow layers. Generating examples in the test set, it achieved a negative log likelihood of 1260.8, while the baseline scored 1307.9 (lower is better). EMF outperformed similar baselines trained for other tasks. For instance, generating solutions to the differential equations for Brownian motion, it achieved a negative log likelihood of -26.4 compared to the baseline’s -26.1.\n\nYes, but:A baseline with an additional normalizing flow layer achieved a better negative log likelihood (1181.3) for generating test-set digits. The authors explain that EMF may have underperformed because it had fewer parameters, although they don’t quantify the difference.Why it matters:Normalizing flows have their uses, but the requirement that its layers be invertible imposes severe limitations. By proposing a new layer type that improves their performance, this work makes them less forbidding and more useful. In fact, probabilistic programs aren’t difficult to make: They’re easy to diagram, and the authors offer an algorithm that turns such diagrams into normalizing flow layers.We’re thinking:The authors achieved intriguing results with a small model (three layers, compared to otherworkand dataset (10,000 examples compared to, say,ImageNet’s1.28 million). We look forward to learning what EMF-style models can accomplish with more and wider layers, and with larger datasets like ImageNet.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/05/Screen-Shot-2022-05-10-at-3-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/05/META.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/05/MOXI--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/05/DeepLearningAI_Banner_Stanford_Teaser_1200x628_Artboard-2--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/05/MUSIIO--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/05/EMBEDDEDv2.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-53/",
    "title": "issue 53",
    "date": "",
    "reading_time": "",
    "content": "Last week, I asked what values the AI community stands for. Thank you to everyone who replied! The email responses in aggregate ran to 55 pages of text, and I enjoyed reading all of them.\n\nA reader who works for a large company wrote, “A purely commercial objective of work is not my calling and I often find myself dreaming about how to break out of the corporate shackles and contribute the rest of my life to doing something meaningful.” These words struck a chord with me. Many of us have the good fortune to find meaning in our work. But if you don’t currently, I hope the AI community will help you do so.\n\nSome other comments stood out to me (lightly edited):\n\nThese thoughts, and many, many others you sent, are wonderful. But one challenge of pushing on compassion (as in the last comment) is that compassion means different things to different individuals. To one person, it may mean mentoring an underprivileged student. To another, it may mean tuning an algorithm to reduce hate speech in social media.\n\nConcepts like compassion, empowerment, and being human are easy to agree on in the absence of specifics, but difficult to define and realize in a concrete way. We all want to be compassionate. But what does that mean in practice?\n\nWe will reach a common understanding only by considering such abstractions in light of a wide variety of ways they might translate into action. This will require tireless discussion as a community. When we have a chance to talk to one another, let’s take the opportunity to discuss the values we hold in common and what it would mean to stand for them in real life. That way, the next time we feel the urge to take a stand — say, tuning a hyperparameter to reduce hate speech at the cost of revenue — we’re more likely to act in a consistent and principled way.\n\nI’m heartened by your responses and encouraged that so many of you are looking for greater meaning and positive impact. I will continue to think about how we can come together as a community and keep the conversation going.\n\nKeep learning!\n\nAndrew\n\nAI is helping avert traffic accidents by assessing the risk of car crashes at specific intersections.What’s happening:MicroTraffic, a Canadian video analytics company, predicts the odds that accidents will occur at intersections that traditional methods overlook. More than 40 cities in Canada and the U.S. have used its analyses.How it works:The usual approach to monitoring traffic safety identifies dangerous intersections based on crashes that already have occurred. Considering close calls brings previously unidentified trouble spots to light.\n\nBehind the news:Commercial and government organizations are working on AI for traffic safety.\n\nWhy it matters:Globally, motor vehicleskill 3,700 peopleeach day. AI could help traffic engineers cut that grim tally.We’re thinking:When your AI software crashes, take heart in the thought that AI is reducing crashes elsewhere.\n\nSemi-supervised learning — a set of training techniques that use a small number of labeled examples and a large number of unlabeled examples — typically treats all unlabeled examples the same way. But some examples are more useful for learning than others. A new approach lets models distinguish between them.What’s new:Researchers Zhongzheng Ren, Raymond A. Yeh, and Alexander G. Schwing from the University of Illinois at Urbana-Champaign developed analgorithmthat weighs the most significant examples more heavily.Key insight:In its most common form, semi-supervised learning tries to minimize a weighted combination of supervised and unsupervised losses. Most previous approaches effectively weight each unlabeled example as equally important. The authors, instead of assigning one weight to all unlabeled examples, calculate weights for every example automatically by evaluating how it changes the model’s output during training.How it works:The algorithm works with any semi-supervised model. It trains by alternating between optimizing the model and the per-example weights.\n\nResults:Using synthetic data, the authors demonstrated that less useful examples were assigned lower weights. In image classification using theCifar-10andSVHNdatasets, their approach marginally outperformed previous state of the art semi-supervised learning work includingFixMatchandUDA. Specifically, using a WideResNet-28-2and Cifar-10 with 250 labeled examples, the authors’ method combined with FixMatch achieved a classification error of 5.05 percent compared to FixMatch’s 5.07 percent. Combined with UDA, the authors’ method on Cifar-10 achieved a classification error of 5.53 percent compared to UDA’s 8.76 percent.Why it matters:Unlabeled data points are available in far greater profusion than labeled data points. This work explores a path toward unlocking their value.We’re thinking:Sometimes another 1,000 cat pictures don’t provide a model with any more useful information. But keep sending them anyway.The Batchteam appreciates it!\n\nA strategy manifesto from one of China’s biggest tech companies declares, amid familiar visions of ubiquitous AI, that deepfakes are more boon than bane.What’s new:Awhite paperissued by Tencent outlines AI’s growing impact on the global economy, particularly in media. The company argues that photorealistic images of people who don’t exist (like the portraits above, generated by thispersondoesnotexist.com), along with manipulation of images of people who do, have tremendous commercial potential despite their reputation for making mischief.The bright side:The paper argues that what it calls deepfake synthesis — the basket of AI techniques capable of synthesizing realistic human faces, voices, and bodies, as well as other objects — could yield a range of economic and social benefits. It’s already being applied for legitimate purposes:\n\nBeyond fakery:The company’s vision for AI isn’t just about digital doppelgangers. The paper argues that techniques like few-shot learning and offline reinforcement learning could drive broader commercialization in disciplines like natural computer vision, language processing, and voice recognition. The company also looks to AI to help improve crop yields and balance energy demand.Behind the news:Tencent has a vested interest in promoting AI. The white paper accompanied thereleaseof specialized machine learning platforms for entertainment, broadcasting, content review, and industrial processes, as well as Light 2.0, a program that encourages commercialization of AI research.Why it matters:Tencent is among China’s most valuable tech companies with a strong presence in gaming, entertainment, and media. Its plans influence the direction of technology as well the attitudes of regulators who would bind it. Its push to commercialize deepfakes could open new markets — and thorny issues — for the AI community.We’re thinking:Deepfakes have potential uses in online education, such as enabling an instructor on video to deliver a course in any number of languages. But, as the paper itself notes, regulation is necessary to thwart potential abuses.\n\nCourses 1 through 3 of our Natural Language Processing Specialization are live on Coursera.Enroll todayand take the first step toward breaking into NLP!\n\nDeep learning is coordinating drones so they can flock together without colliding.What’s new:Caltech researchers Soon-Jo Chung and Yisong Yue developed apair of modelsthat enables swarms of networked drones to navigate autonomously through cluttered environments.How it works:Sensors on each drone collect real-time data that are shared among a swarm. A neural network calledGLASplans drone actions, while another one calledNeural-Swarmhelps compensate for wind caused by nearby fliers.\n\nResults:The authors tested GLAS and Neural-Swarm separately. In comparisons with astate-of-the-artmotion planning algorithm, 16 drones piloted by GLAS navigated 20 percent more effectively through a variety of obstacle courses. Drones controlled by Neural-Swarm were four times better than a baseline linear tracking controller at staying on course.Why it matters:Drones capable of maneuvering safely in swarms could aid urban search and rescue operations, accelerate industrial inspections, and provide comprehensive aerial mapping.We’re thinking:Is anyone else excited to see drone shows even more spectacular than the one that lit up the2018 Olympics?\n\nPeople who take driving lessons during daytime don’t need instruction in driving at night. They recognize that the difference doesn’t disturb their knowledge of how to drive. Similarly, a new reinforcement learning method manages superficial variations in the environment without re-training.What’s new:Nicklas Hansen led a UC Berkeley group in developingPolicy Adaptation during Deployment(Pad), which allows agents trained by any RL method to adjust for visual changes that don’t impact the optimal action.Key insight:Deep reinforcement learning agents often learn to extract important features of the environment and then choose the optimal course of action based on those features. The researchers designed a self-supervised training task that updates a feature extractor to account for environmental changes without disturbing the strategy for selecting actions.How it works:In most agents, a feature extractor captures visual information about the environment while a controller decides on actions. A change in the surroundings — say, from day to night — causes the feature extractor to derive different features, which can confuse the controller. Pad, once it’s deployed and no longer receives rewards, continues to update the feature extractor while leaving the controller unaffected. Thus the agent learns to use the same strategy regardless of environmental changes.\n\nResults:The researchers evaluated Pad by training an agent via thesoft actor-criticmethod, then substituting a plain-color background with a video at test time. On theDeepMind Control Suite, which includes motor-control tasks such as walking, Pad improved the soft actor-critic baseline in the new environment on seven of eight tasks.Yes, but:If the environment doesn’t change, Pad hurts performance (albeit minimally).Why it matters:To be useful in the real world, reinforcement learning agents must handle the transition from simulated to physical environments and cope gracefully with changes of scenery after they’ve been deployed. While all roads have similar layouts, their backgrounds may differ substantially, and your self-driving car should keep its eyes on the road. Similarly, a personal-assistant robot shouldn’t break down if you paint your walls.We’re thinking:Robustnessis a major challenge to deploying machine learning: The data we need to operate on is often different than the data available for training. We need more techniques like this to accelerate AI deployments.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Andrews20Letter202-1-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Microtraffic203.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/WEIGHTED.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Goodfakes.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize-Aug-19-2020-06-07-38-45-PM.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Swarm2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/PAD.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-298/",
    "title": "issue 298",
    "date": "",
    "reading_time": "",
    "content": "Even though I’m a much better Python than JavaScript developer, with AI assistance, I’ve been writing a lot of JavaScript code recently. AI-assisted coding is making specific programming languages less important, even though learning one is still helpful to make sure you understand the key concepts. This is helping many developers write code in languages we’re not familiar with, which lets us get code working in many more contexts!\n\nMy background is in machine learning engineering and back-end development, but AI-assisted coding is making it easy for me to build front-end systems (the part of a website or app that users interact with) using JavaScript (JS) or TypeScript (TS), languages that I am weak in. Generative AI is making syntax less important, so we can all simultaneously be Python, JS, TS, C++, Java, and even Cobol developers. Perhaps one day, instead of being “Python developers\" or “C++ developers,” many more of us will just be “developers”!\n\nBut understanding the concepts behind different languages is still important. That’s why learning at least one language like Python still offers a great foundation for prompting LLMs to generate code in Python and other languages. If you move from one programming language to another that carries out similar tasks but with different syntax — say, from JS to TS, or C++ to Java, or Rust to Go — once you’ve learned the first set of concepts, you’ll know a lot of the concepts needed to prompt an LLM to code in the second language. (Although TensorFlow and PyTorch are not programming languages, learning the concepts of deep learning behind TensorFlow will also make it much easier to get an LLM to write PyTorch code for you, and vice versa!)  In addition, you’ll be able to understand much of the generated code (perhaps with a little LLM assistance).\n\nDifferent programming languages reflect different views of how to organize computation, and understanding the concepts is still important. For example, someone who does not understand arrays, dictionaries, caches, and memory will be less effective at getting an LLM to write code in most languages.\n\nSimilarly, a Python developer who moves toward doing more front-end programming with JS would benefit from learning the concepts behind front-end systems. For example, if you want an LLM to build a front end using the React framework, it will benefit you to understand how React breaks front ends into reusable UI components, and how it updates the DOM data structure that determines what web pages look like. This lets you prompt the LLM much more precisely, and helps you understand how to fix issues if something goes wrong. Similarly, if you want an LLM to help you write code in CUDA or ROCm, it helps to understand how GPUs organize compute and memory.\n\nJust as people who are fluent in multiple human languages can communicate more easily with other people, LLMs are making it easier for developers to build systems in multiple contexts. If you haven’t already done so, I encourage you to try having an LLM write some code in a language you’d like to learn but perhaps haven’t yet gotten around to, and see if it helps you get some new applications to work.\n\nKeep building!\n\nAndrew\n\nLearn to build agents that write and run code to complete complex tasks. “Building Code Agents with Hugging Face smolagents,” made in collaboration with Hugging Face, teaches you how to build code agents, execute their code safely, and set up evals for production-ready multi-agent systems, using the smolagents framework.Enroll for free\n\nOpenAI refreshed its roster of models and scheduled the largest, most costly one for removal.\n\nWhat’s new:OpenAI introduced five new models that accept text and images inputs and generate text output. Their parameter counts, architectures, training datasets, and training methods are undisclosed. The general-purposeGPT-4.1, GPT-4.1 mini, and GPT-4.1 nanoare available via API only. The reasoning modelso3 and o4-mini,are available via API toqualifieddevelopers as well as users of ChatGPT Plus, Pro, and Team, and soon ChatGPT Enterprise and ChatGPT Education. The company willterminateGPT-4.5 — which it introduced as a research preview in late February — in July.\n\nGPT-4.1 family:In an odd turn of version numbers, the GPT-4.1 models are intended to be cost-effective equivalents to GPT-4.5 and updates to GPT-4o. They accept inputs of up to 1 million tokens (compared to GPT-4.5’s and GPT-4o’s 128,000 tokens).\n\no3 and o4-mini:These models update o1 and o3-mini, respectively. They have input limits of 200,000 tokens and can be set to low-, medium-, or high-effort modes to process varying numbers of reasoning tokens, which are hidden from users. Unlike their predecessors, they were fine-tuned to decide when and how to use the tools, including web search, code generation and execution, and image editing.\n\nBehind the news:Late last year, OpenAI introducedo1, the first commercial model trained via reinforcement learning to generate chains of thought. Within a few months, DeepSeek, Google, and Anthropic launched their respective reasoning modelsDeepSeek-R1,Gemini 2.5 Pro, andClaude 3.7 Sonnet. OpenAI has promised to integrate its general-purpose GPT-series models and o-series reasoning models, but they remain separate for the time being.\n\nWhy it matters:GPT-4.5 was an exercise in scale, and it showed that continuing to increase parameter counts and training data would yield ongoing performance gains. But it wasn’t widely practical on a cost-per-token basis. The new models, including those that use chains of thought and tools, deliver high performance at lower prices.\n\nWe’re thinking:Anthropic is one of OpenAI’s key competitors, and a large fraction of the tokens it generates (via API) are forwriting code, a skill in which it is particularly strong. OpenAI’s emphasis on models that are good at coding could boost the competition in this area!\n\nHugging Face has made a name by providing open AI models. Now it’s providing an open robot.\n\nWhat’s new:Hugging Faceacquiredthe French company Pollen Robotics for an undisclosed price. It plans to offer Pollen’sReachy 2, a robot that runs on code that’s freelyavailableunder an Apache 2.0 license, for $70,000.\n\nHow it works:Reachy 2 has two arms, gripper hands, and a wheeled base (optional). It’s designed primarily for education and research in human-robot interaction in real-world settings.\n\nBehind the news:Last year, Remi Cadene, who worked on Tesla’s Optimus,joinedHugging Face to lead robotics projects. In May, he and his team rolled out the LeRobot open source robotics code library, whichprovidespretrained models, datasets, and simulators for reinforcement learning and imitation learning. In November, Nvidia announced acollaborationwith Hugging Face to accelerate LeRobot’s data collection, training, and verification.\n\nWhy it matters:Hugging Face’s acquisition of Pollen reflects an industry-wideinvestmentinrobots, notablyhumanoidrobots, whose prices have beenfalling. Nvidia CEO Jensen Huang has calledAI-enabled roboticsa “multi-trillion dollar” opportunity.\n\nWe’re thinking:AI-enabled robots are marching slowly toward what we hope will be breakthrough applications. Open-source systems are an important part of the trend!\n\nThe U.S. government escalated its long-running effort to block China’s access to cutting-edge AI hardware.\n\nWhat’s new:The White Houseannouncedthat future shipments of Nvidia H20s, AMD MI308s, or equivalent chips to China would require a license. Concurrently, the United States Congresslaunchedan investigation into whether chip vendor Nvidia violated earlier export rules.\n\nHow it works:Nvidia launched the H20 in late 2023 to comply with a 2022 U.S. ban on China-bound shipments of Nvidia’s H100 andH200 processors. The H20 uses the same architecture as the H200, but it’s an order of magnitude slower with less memory and memory bandwidth.\n\nBehind the news:The U.S. government’s many moves to restrict shipments of advanced processors to China have sought to protect the nation’s lead in AI, but they have not prevented Chinese developers from closing the gap. In 2020, the U.S.requiredchip makers that use U.S. technology — which includes both domestic chip designers like Nvidia and makers of advanced fabrication equipment like the Netherlands’ ASML — to seek permission before doing business with Chinese tech giant Huawei. Last December, the U.S. published sweeping limits on sales of processors that involve U.S. technology, as well as the technology itself, to Chinese businesses.\n\nYes, but:Export restrictions may have slowed China’s production of advanced chips, but they have also incentivized China to invest inestablishing leadershipin AI. In January, the Chinese AI developer DeepSeek surprised U.S. policymakers and AI leaders with the release ofDeepSeek-R1, which performs comparably to OpenAI’s o1, but whose weights are freely available and trained using less computation.\n\nWhy it matters:The first wave of restrictions on sales of advanced chips to China didlittle harmto U.S. chipmakers, largely becausedemand outstripped supply. But later restrictions have had a greaterimpacton their sales. The new limits could cost Nvidia and AMD significant revenue and likely willdegradetheir competitiveness abroad andbolsterChina’s homegrown chip-making industry.\n\nWe’re thinking:The AI community’s international scope is one of its greatest strengths. While individual countries must attend to their national security, progress in AI benefits all nations. Even in this era of rising protectionism, we hope members of the global AI community continue to support one another and encourage the free flow of ideas.\n\nLarge language models excel at processing text but can’t interpret images, video, or audio directly without further training on those media types. Researchers devised a way to overcome this limitation.\n\nWhat’s new:Kumar Ashutosh and colleagues at Meta, University of Texas, and UC Berkeley introducedMultimodal Iterative LLM Solver(MILS), a method that pairs a text-only large language model (LLM) with a multimodal embedding model to generate captions for images, video, and audio without further training.\n\nKey insight:LLMs can generate text and refine their outputs based on new information. On the other hand, multimodal embedding models can score the similarity between a given text and an image, video, or audio clip. Given this score, an LLM can regenerate the text iteratively until the score indicates a strong match between the text and the associated media. This enables the LLM to generate accurate captions for images, videos, and audio clips without training in these tasks.\n\nHow it works:Given a prompt and an image, video, or audio clip, Llama 3.1 8B produced and iteratively refined the prompt according to a pretrained multimodal embedding model’s estimate of the similarity between the text and media.\n\nResults:The authors evaluated MILS on captioning images, videos, and audio clips. They measured performance according to Metric for Evaluation of Translation with Explicit ORdering (METEOR), which checks for synonyms, words that share the same root, and word order to determine whether a generated caption matches a ground-truth caption (higher is better). Overall, MILS outperformed models that underwent task-specific training.\n\nWhy it matters:Zero-shot captioning models like Aya Vision and Pixtral require training on paired captions and media. The authors’ approach takes advantage of pretrained multimodal models to enable an LLM to compose multimedia captions without further training.\n\nWe’re thinking:Synthetic data is increasingly useful for training AI models. By enabling LLMs to synthesize good captions, MILS adds fuel to this fire.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--61--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/The-Batch-ads-and-exclusive-banners---2025-04-23T105320.680.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/OpenAI-MODELS_table-11b_1200px.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--78-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--79-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--80-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-214/",
    "title": "issue 214",
    "date": "",
    "reading_time": "",
    "content": "I recently spoke about “Opportunities in AI” at Stanford’s Graduate School of Business. I'd like to share a few observations from that presentation, and I invite you to watch thevideo(37 minutes).\n\nAI is a collection of tools, including supervised learning, unsupervised learning, reinforcement learning, and now generative AI. All of these are general-purpose technologies, meaning that — similar to other general-purpose technologies like electricity and the internet — they are useful for many different tasks. It took many years after deep learning started to work really well circa 2010 to identify and build for a wide range of use cases such as online advertising, medical diagnosis, driver assistance, and shipping optimization. We’re still a long way from fully exploiting supervised learning.\n\nNow that we have added generative AI to our toolbox, it will take years more to explore all its uses. (If you want to learn how to build applications using generative AI, please check out ourshort courses!)\n\nWhere do the opportunities lie? With each new wave of technology, entrepreneurs and investors focus a lot of attention on providers of infrastructure and tools for developers. The generative AI wave has brought tools from AWS, Google Cloud, Hugging Face, Langchain, Microsoft, OpenAI, and many more. Some will be huge winners in this area. However, the sheer amount of attention makes this part of the AI stack hypercompetitive. My teams (specifically AI Fund) build startups in infrastructure and tools only when we think we have a significant technology advantage, because that gives us a shot at building large, sustainable businesses.\n\nBut I believe a bigger opportunity lies in the application layer. Indeed, for the companies that provide infrastructure and developer tools to do well, the application companies that use these products must perform even better. After all, the application companies need to generate enough revenue to pay the tool builders.\n\nFor example, AI Fund portfolio companies are applying AI to applications as diverse asglobal maritime shippingandrelationship mentoring. These are just two areas where the general-purpose technology of AI can create enormous value. Because few teams have expertise in both AI and sectors like shipping or relationships, the competition is much less intense.\n\nIf you’re interested in building valuable AI projects, I think you’ll find the ideas in the presentation useful. I hope you’ll watch thevideoand share it with your friends. It describes in detail AI Fund’s recipe for building startups and offers non-intuitive tips on the ideas that we’ve found to work best.\n\nKeep building!\n\nAndrew\n\nA new version of ChatGPT upgrades the service for corporate customers.\n\nWhat’s new:OpenAIlaunchedChatGPT Enterprise, which combines enhanced data-privacy features with a more capable language model. The price is negotiable on a case-by-case basis,Bloombergreported.\n\nHow it works:ChatGPT Enterprise provides enhanced access to GPT-4, previously available via ChatGPT Plus ($20 per month) and API calls at a cost per thousand tokens.\n\nBehind the news:OpenAI hasmetamorphosedfrom a nonprofit into a tech-biz phenomenon, but its business is still taking shape. For 2022, the companyreported$540 million in losses on $28 million in revenue. It’sreportedlyon track to bring in $1 billion this year, and ChatGPT Enterprise is bound to benefit from OpenAI’s high profile among business users: The email addresses of registered ChatGPT users represent 80 percent of the Fortune 500, according to the company.\n\nWhy it matters:Large language models are transforming from public experiments to mainstream productivity tools. ChatGPT Enterprise is a significant step in that transition, giving large companies the confidence they need to integrate GPT-4 into their day-to-day operations with less worry that OpenAI will ingest proprietary information.\n\nWe’re thinking:Some reporters havequestionedthe financial value of generative AI. While OpenAI’s business is evolving, this new line of business is promising. We anticipate that enterprise subscriptions will be stickier than API access, since customers’ switching costs are likely to be higher.\n\nMicrosoft promised to shield users of its generative AI services against the potential risk of copyright infringement.\n\nWhat’s new:Microsoft said it wouldcover the costfor any copyright violations that may arise from use of its Copilot features, which generate text, images, code, and other media within its productivity apps.\n\nHow it works:In its Copilot Copyright Commitment, Microsoft vows to defend customers in court against allegations that they infringed copyrights by using Microsoft software. It also promises to reimburse the cost of adverse judgments or settlements.\n\nBehind the news:Microsoft, its subsidiary GitHub, and its partner OpenAI are currentlydefending themselvesagainst allegations that GitHub Copilot violated copyright laws. Programmer and attorney Matthew Butterick claims that OpenAI trained GitHub Copilot in violation of open-source licenses and that the system reproduces copyrighted code without authorization. In May, a judgerejecteda request by the defendants to dismiss the case, which remains ongoing.\n\nWhy it matters:Generative AI represents a huge business opportunity for Microsoft and others. Yet the technology is under attack by copyright holders, creating the potential that customers may face lawsuits simply for using it. That may be persuading enterprise customers — Microsoft’s bread and butter — to avoid generative AI. The company’s promise to protect them from legal action is a bold bet that the cost of defending customers will be far less than the profit it gains from selling generative products and services.We’re thinking:It’snot yet clearwhether using or developing generative AI violates anyone’s copyright, and it will take time forcourtsandlawmakersto provide a clear answer. While legal uncertainties remain, Microsoft’s commitment is an encouraging step for companies that would like to take advantage of the technology and a major vote of confidence in the business potential of generative AI.\n\nSpeechLab is building speech AI that conveys the nuance and emotion of the human voice, bringing together proprietary models for multi-speaker, multi-language text-to-speech; voice cloning; speech recognition; and more. Learn more atSpeechLab.AI\n\nGoogle, which distributes a large portion of ads on the web, tightened its restrictions on potentially misleading political ads in advance of national elections in the United States, India, and South Africa.\n\nWhat’s new:Starting in November 2023, in select countries, Google’s ad network will require clear disclosure of political ads that contain fictionalized depictions of real people or events, the companyannounced. The policy doesn’t explicitly mention generative AI, which can automate production of misleading ads.\n\nHow it works:In certain countries, Google accepts election-related ads only from advertisers that pass a lengthy verification process. Under the new rules, verified advertisers that promote “inauthentic” images, video, or audio of real-world people or events must declare, in a place where users are likely to notice it, that their depiction does not represent reality accurately.\n\nBehind the news:Some existing AI-generated political messages may run afoul of Google’s restrictions.\n\nYes, but:The rules’ narrow focus on inauthentic depictions of real people or events may leave room for misleading generated imagery. For instance, a U.S. Republican Partyvideocontains generated images of a fictional dystopian future stemming from Joe Biden’s hypothetical re-election in 2024. The images don’t depict real events, so they may not require clear labeling under Google’s new policy.\n\nWhy it matters:Digital disinformation has influenced elections for years, and the rise of generative AIgivesmanipulators a new toolbox. Google, which delivers an enormous quantity of advertising via Search, YouTube, and the web at large, is a powerful vector for untruths and propaganda. With its new rules, the company will assume the role of regulating itself in an environment where few governments have enacted restrictions.We’re thinking:Kudos to Google for setting standards for political ads, generated or otherwise. The rules leave some room for interpretation; for instance, does a particular image depict a real event inauthentically or simply depict a fictional one? On the other hand, if Google enforces the policy, it’s likely to reduce disinformation. We hope the company will provide a public accounting of enforcement actions and outcomes.\n\nVision transformers have bested convolutional neural networks (CNNs) in a number of key vision tasks. Have CNNs hit their limit? New research suggests otherwise.\n\nWhat’s new:Sanghyun Woo and colleagues at Korea Advanced Institute of Science & Technology, Meta, and New York University builtConvNeXt V2, a purely convolutional architecture that, after pretraining and fine-tuning, achieved state-of-the-art performance on ImageNet. ConvNeXt V2 improves uponConvNeXt, which updated the classicResNet.\n\nKey insight:Vision transformers learn via masked pretraining — that is, hiding part of an image and learning to reconstruct the missing part. This enables them to learn from unlabeled data, which simplifies amassing large training datasets and thus enables them to produce better embeddings. If masked pretraining works for transformers, it ought to work for CNNs as well.\n\nHow it works:ConvNeXt V2 is an encoder-decoder pretrained on 14 million images inImageNet 22k. For the decoder, the authors used a single ConvNeXt convolutional block (made up of three convolutional layers). They modified the ConvNeXt encoder (36 ConvNeXt blocks) as follows:\n\nResults:The biggest ConvNeXt V2 model (659 million parameters) achieved 88.9 percent top-1 accuracy on ImageNet. The previous state of the art,MViTV2(a transformer with roughly the same number of parameters) achieved 88.8 percent accuracy. In addition, ConvNeXt V2 required less processing power: 600.7 gigaflops versus 763.5 gigaflops.\n\nWhy it matters:Transformers show great promise in computer vision, but convolutional architectures can achieve comparable performance with less computation.\n\nWe’re thinking:While ImageNet 22k is one of the largest publicly available image datasets, vision transformers benefit from training on proprietary datasets that are much larger. We’re eager to see how ConvNeXt V2 would fare if it were scaled to billions of parameters andimages. In addition, ImageNet has been joined by many newer benchmarks. We’d like to see results for some of those.\n\nAustralia bans AI-generated depictions of child abuse from search enginesAustralia's eSafety Commissioner required search engines including Google and DuckDuckGo to remove synthetic child abuse material from their results. The mandate also requires such companies to research technologies that help users identify deepfake images on their platforms. (The Guardian)Peer reviewers uncover AI-generated science researchGenerative AI tools are being used to write scientific papers without disclosure. Some generated papers have passed peer review. Researchers and reviewers identified such manuscripts through telltale phrases left unedited by users, like “regenerate response.” This suggests that the actual number of undisclosed AI-generated peer-reviewed papers could be significantly higher. (Nature)Education integrates AIEducators, institutions, and nongovernmental organizations aregrappling with the challenges and possibilities of AI in the classroom. Institutions are establishing policies and regulatory bodies are developing guidelines. Meanwhile, students and educators alike are exploring AI's limitations and benefits. (Reuters)Baidu releases Ernie chatbotThe move made Baidu’s stock price raise by over 3% following the announcement. On the day of release, the chatbot topped the charts on Apple’s iOS store in China for free apps. (AP News)Walmart workers get generative AI app trained on corporate dataApproximately 50,000 non-store employees will have access to the app, called \"My Assistant.” It will be capable of performing tasks like summarizing long documents. The app is based on an LLM provided by an unnamed third party. (Axios)U.S. restricts export of AI chips to Middle East countriesThe U.S. extended export restrictions on advanced AI chips produced by Nvidia and Advanced Micro Devices (AMD), which previously blocked chips sold to China, to include countries in the Middle East. These restrictions are part of a wider effort by the U.S. government to control exports of advanced technology that could be used for military purposes to certain regions. (Reuters)How countries around the world are regulating AIFrom Brazil's detailed draft AI law focusing on user rights to China's regulations emphasizing \"Socialist Core Values,\" nations are working to establish restrictions on AI. The EU, Israel, Italy and the United Arab Emirates are also working on regulations. (The Washington Post)Apple intensifies investment in conversational AIApple’s plan would enable iPhone users to perform multi-step tasks using voice commands. This initiative aligns with Apple's earlier establishment of an AI team to develop conversational AI. (The Information)\n\nGizmodo owner replaced Spanish-language journalists with machine translationG/O Media laid off the editors ofGizmodo en Españoland is employing AI for article translation. Readers reported issues such as articles switching from Spanish to English mid-text. The staff’s union criticized the decision as a promise broken by the company's leadership. (The Verge)\n\nAI-generated mushroom foraging guides raise safety concernsManuals on identifying mushrooms have proliferated on Amazon. Experts warn that these books, aimed at beginners, often lack accuracy in identifying poisonous mushrooms, posing a potential risk to those who rely on them. Amazon has taken steps to remove some of these books, which illustrate the potential stakes of ensuring safety and accuracy in AI-generated content. (404 media)AI generates grown-up likenesses of children whose parents disappeared decades agoAn Argentine publicist uses Midjourney to combine photos of parents who disappeared during that country’s dictatorship of the 1970s and ‘80s to generate portraits of their children as adults. The activist group Grandmothers of Plaza de Mayo created the images, which aim to raise awareness of more than 500 children who were stolen from their parents and never photographed. (AP news)Morgan Stanley to launch chatbot for wealth management servicesThe investment bank is set to introduce a chatbot developed in collaboration with OpenAI. It will help bankers swiftly access research and forms. Morgan Stanley and OpenAI are also developing a feature to summarize meetings, draft follow-up emails, update sales databases, schedule appointments, and provide financial advice on various areas. (Reuters)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--55-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--56-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--57-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/09/The-Batch-ads-and-exclusive-banners--63-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--95-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/09/unnamed--96-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-282/",
    "title": "issue 282",
    "date": "",
    "reading_time": "",
    "content": "Happy sum(i**3 for i in range(10)) !\n\nDespite having worked on AI since I was a teenager, I’m now more excited than ever about what we can do with it, especially in building AI applications. Sparks are flying in our field, and 2025 will be a great year for building!\n\nOne aspect of AI that I’m particularly excited about is how easy it is to build software prototypes. AI is lowering the cost of software development and expanding the set of possible applications. While it can help extend or maintain large software systems, it shines particularly in building prototypes and other simple applications quickly.\n\nIf you want to build an app to print out flash cards for your kids (I just did this in a couple of hours with o1’s help), or write an application that monitors foreign exchange rates to manage international bank accounts (a real example from DeepLearning.AI’s finance team), or analyzes  user reviews automatically to quickly flag problems with your products (DeepLearning.AI's content team does this), it is now possible to build these applications quickly through AI-assisted coding.\n\nI find AI-assisted coding especially effective for prototyping because (i) stand-alone prototypes require relatively little context and software integration and (ii) prototypes in alpha testing usually don’t have to be reliable. While generative AI also helps with engineering large, mission-critical software systems, the improvements in productivity there aren't as dramatic, because it’s challenging to give the AI system all the context it needs to navigate a large codebase and also to make sure the generated code is reliable (for example, covering all important corner cases).\n\nUntil now, a huge friction point for getting a prototype into users’ hands has been deployment. Platforms like Bolt, Replit Agent, Vercel V0 use generative AI with agentic workflows to improve code quality, but more importantly, they also help deploy generated applications directly. (While I find these systems useful, my own workflow typically uses an LLM to design the system architecture and then generate code, one module at a time if there are multiple large modules. Then I test each module, edit the code further if needed — sometimes using an AI-enabled IDE like Cursor — and finally assemble the modules.)\n\nBuilding prototypes quickly is an efficient way to test ideas and get tasks done. It’s also a great way to learn. Perhaps most importantly, it’s really fun! (At least I think it is. 😄)\n\nHow can you take advantage of these opportunities in the coming year? As you form new year resolutions, I hope you will:\n\nHappy New Year!Andrew\n\nP.S. I develop mostly in Python. But if you prefer JavaScript: Happy Array.from({ length: 10 }, (_, i) => i ** 3).reduce((a, b) => a + b, 0) !\n\nWe stand at the threshold of a new era: One in which AI systems possess striking abilities to reason about the world, grasp our wishes, and take actions to fulfill them. What will we do with these powers? We asked leaders of the field to share their hopes for the coming year. As in ourpreviousNewYearspecialissues, their answers offer inspiring views of what we may build and the good we can bring.\n\nStability AI’s aim is to liberate artists of all trades from the repetitive, mechanical aspects of their work and help them spend the majority of their time on the creative side. So our highest hope for next year is that generative AI will help people to be more creative and productive.\n\nIn addition, I hope the AI community will focus on:\n\nHanno Basse is Chief Technology Officer of Stability AI. Previously he served as CTO of Digital Domain, Microsoft Azure Media and Entertainment, and 20th Century Fox Film Corp.\n\nLast year, we saw an explosion of models that generate either video or audio outputs in high quality. In the coming year, I look forward to models that produce video clips complete with audio soundtracks including speech, music, and sound effects. I hope these models will bring a new era of cinematic creativity.\n\nThe technologies required for such cinematic video generators are in place. Several companies provide very competitive video models, and Udio and others create music models. All that’s left is to model video and audio simultaneously, including dialog and voiceovers. (In fact, we’ve already seen something like this: Meta’s Movie Gen. Users describe a scene and Movie Gen will produce a video clip complete with a music score and sound effects.)\n\nOf course, training such models will require extensive datasets. But I suspect that the videos used to train existing video generators had soundtracks that include these elements, so data may not be a barrier to developing these models.\n\nInitially, these models won’t produce output that competes with the best work of professional video editors. But they will advance quickly. Before long, they’ll generate videos and soundtracks that approach Hollywood productions in raw quality, just as current image models can produce images that are indistinguishable from high-end photographs.\n\nAt the same time, the amount of control users have over the video and audio outputs will continue to increase. For instance, when we first released Udio, users couldn’t control the harmony it generated. A few months later, we launched an update that enables users to specify the key, or tonal center. So users can take an existing song and remix it in a different key. We are continuing to do research into giving users additional levers of control, such as voice, melody, and beats, and I’m sure video modeling teams are doing similar research on controllability.\n\nSome people may find the prospect of models that generate fully produced cinematic videos unsettling. I understand this feeling. I enjoy photography and playing music, but I’ve found that image and audio generators are helpful starting points for my creative work. If I choose, AI can give me a base image that I can work on in Photoshop, or a musical composition to sample from or build on. Or consider AI coding assistants that generate the files for an entire website. You no longer need to rely on web developers, but if you talk to them, you’ll learn that they don’t always enjoy writing the boilerplate code for a website. Having a tool that builds a site’s scaffold lets them spend their time on development tasks they find more stimulating and fun.\n\nIn a similar way, you’ll be able to write a screenplay and quickly produce a rough draft of what the movie might look like. You might generate 1,000 takes, decide which one you like, and draw inspiration from that to guide a videographer and actors.\n\nArt is all about the creative choices that go into it. Both you and I can use Midjourney to make a picture of a landscape, but if you’re an artist and you have a clear idea of the landscape you want to see, your Midjourney output will be more compelling than mine. Similarly, anyone can use Udio to make high-production quality music, but if you have good musical taste, your music will be better than mine. Video will remain an art form, because individuals will choose what their movie is about, how it looks, and how it feels — and they’ll be able to make those choices more fluidly, quickly, and interactively.\n\nDavid Ding is a lifelong musician and co-founder of Udio, maker of a music-creation web app that empowers users to make original music. Previously, he was a Senior Research Engineer at Google DeepMind.\n\nIn 2025, I expect progress in training foundation models to slow down as we hit scaling limits and inference costs continue to rise. Instead, I hope for an explosion of innovation on top of AI, such as the rapidly developingagents stack. I hope we will see innovation in how wecombine AI with toolsand existing systems to deliver exciting new capabilities and create new product categories. Perhaps most of all, I am excited to see how people change in response to this new world.\n\nWe have achieved AGI. Now what?Let’s start with — and hopefully end — the longstanding debate around artificial general intelligence (AGI). I know this is controversial, but I think we have achieved AGI, at least definitionally: Our AI is nowgeneral. I will leave the longer debate about sentience and superintelligence to the philosophers and instead focus on the key innovation: generality.\n\nThe artificial intelligence or machine learning of previous decades was intelligent but highly specialized. It could often surpass human ability on a narrowly defined task (such as image recognition or content recommendation). Models today, and perhaps more importantly thesystems around them, are capable of accomplishing a very wide range of tasks often as well as, and in some cases better, than humans. It is this generality that will allow engineers, scientists, and artists to use these models to innovate in ways that the model developers never imagined. It is also this generality, combined with market forces, that will make 2025 so exciting.\n\nBecoming AI-native:The generality of these models and their natural language interfaces mean that everyone can use and explore AI.And we are! We are learning to explain our situations to machines, give context and guidance, and expect personalized answers and solutions. AtRunLLM, where I’m a co-founder, we’re building high-quality technical support agents. We find that users increasingly use our agents not just to solve problems but to personalize solutions to their specific tasks. We’ve also found — to our surprise — that users share much more with an AI than they would share with another person.\n\nMeanwhile, at UC Berkeley, I am impressed by students who use AI to re-explain my lecture or study from an AI-generated practice exam. They have found ways to use AI to help personalize and improve their learning experiences. In 2025, maybe we will begin to prefer AIs over humans when we need help or are trying to learn.\n\nAcross all these use cases, we’re clearly getting better at working around the limitations of large language models and using AI in ways I would not have imagined 12 months ago.\n\nReturn on AI:The focus in 2025 will turn to showing real value from past investments. Investors and enterprises will expect startups and enterprise AI teams to transition from exploring to solving real problems — reducing cost, generating revenue, improving customer experience, and so on. This is bad news for academics who need to raise research funds (DM me if you have any leftover funds from fiscal year 2024) but great news for everyone else, who will ride the wave of new AI-powered features.\n\nThere will be a race to find innovative ways to incorporate AI into every aspect of a product and business. In many cases, we will see hastily executed chatbots and auto-summarization features — the first step on the AI journey. I hope these will be quickly replaced by contextual agents that adapt to users’ needs and learn from their interactions. The pandemic paved the way for remote (digital) assistants and exposed a virtually accessible workplace with the tools needed for tomorrow’s agents. These agents likely will specialize in filling roles once held by people or maybe filling new roles created by other agents. Perhaps we will know that AI has delivered on its promise when everyone manages their own team of custom agents.\n\nChat is only the beginning:My hope for 2025 is that we move beyond chatting and discover how to use AI to do great things! I hope we will see AI agents that work in the background, invisibly helping us with our daily tasks. They will surface the right context as we make decisions and help us learn as the world changes. Through context and tools, they will let us know what we are missing and catch the balls we drop. We will chat less and our AI powered agents will accomplish more on our behalf. I look forward to the day when I can confidently step away from a keyboard and focus on the human interactions that matter.\n\nJoseph Gonzalez is a professor at UC Berkeley, a co-founder of RunLLM, and an advisor to Genmo and Letta.\n\nBuilding a foundation model takes tremendous amounts of data. In the coming year, I hope we’ll enable models to learn more from less data.\n\nThe AI community has achieved remarkable success by scaling up transformers and datasets. But this approach may be reaching a point of diminishing returns — an increasingly widespread belief among the pretraining community as they try to train next-generation models. In any case, the current approach poses practical problems. Training huge models on huge datasets consumes huge amounts of time and energy, and we’re running out of new sources of data for training large models.\n\nThe fact is, current models consume much more data than humans require for learning. We’ve known this for a while, but we’ve ignored it due to the amazing effectiveness of scaling. It takes trillions of tokens to train a model but orders of magnitude less for a human to become a reasonably intelligent being. So there’s a difference in sample efficiency between our best models and humans. Human learning shows that there’s a learning algorithm, objective function, architecture, or a combination thereof that can learn more sample-efficiently than current models.\n\nOne of the keys to solving this problem is enabling models to produce higher-level abstractions and filter out noise. I believe this concept, and thus the general problem of data efficiency, is related to several other current problems in AI:\n\nConsidering data efficiency in light of these other problems, I believe they’re all related. It’s not clear which is the cause and which are the effects. If we solve interpretability, the mechanisms we engineer may lead to models that can extract better features and lead to more data-efficient models. Or we may find that greater data efficiency leads to more interpretable models.\n\nEither way, data efficiency is fundamentally important, and progress in that area will be an indicator of broader progress in AI. I hope to see major strides in the coming year.\n\nAlbert Gu is an Assistant Professor of Machine Learning at Carnegie Mellon University and Chief Scientist of Cartesia AI. He appears on Time’s list of the most influential people in AI in 2024.\n\nIn 2025, AI will have learned to see, it will be way smarter and more accurate, and it will start to do things on your behalf.\n\nToday AI systems struggle to understand our full context. Their perception is limited to the chat window and a fairly narrow set of interactions. They don’t have a full understanding of what we’re doing or aiming for beyond that. To really grasp our intentions, they need to see what we see.\n\nThis capability is now here. AI can sit within the software we use and work alongside us co-browsing. If text was the first modality for interacting with AI, and voice the breakthrough feature of 2024, I think vision will occupy a similar place in 2025. At Microsoft AI, it has been a major priority of mine to create an AI that can work alongside you in your browser, so you can chat through what you’re looking at or working on and make it a true two-way interaction.\n\nVision is a step change, palpably different from the ways we’ve been able to use computers in the past. I can’t wait to see where it goes in the coming months.\n\nAlongside vision, we’ll see enormous progress in reducing hallucinations. This is still a critical blocker for widespread adoption of AI. If people doubt what AI tells them, it severely limits what they’ll use it for. Trust is utterly foundational for AI. The good news is that the quality of models as well as their retrieval and grounding capabilities are still rapidly improving.\n\nWhile I don’t think we’ll eliminate hallucinations entirely, by this time next year, we won’t be fussing about them as much. On most topics, talking to an AI will be at least as reliable as using a search engine and probably more so. This isn’t about a single technical advance, but the persistent accretion of gains across the spectrum. It will make a massive difference.\n\nLastly, we’re entering the agentic era. We’ve been dreaming of this moment for decades. In my book,The Coming Wave: Technology, Power, and the 21st Century’s Greatest Dilemma, I proposed that we start thinking about ACI, orartificially capable intelligence: the moment when AI starts taking concrete actions on behalf of users. Giving AI the ability to take actions marks the moment when AI isn’t just talking to us, it’s doing things. This is a critical change, and it’s right around the corner.\n\nIf we get it right, we’ll be able to, at once, make life easier and calmer while supercharging businesses and personal productivity alike. But agentic capabilities demand the highest standards of safety, security, and responsibility. Meanwhile, creating genuinely useful agents still has many formidable hurdles, not least integrating with myriad other systems.\n\nThe momentum is there. Actions are on their way. 2025 is going to be a big year.\n\nMustafa Suleyman is Chief Executive Officer of Microsoft AI. He co-founded Inflection AI and founded DeepMind Technologies.\n\nAs we approach 2025, my greatest hope for AI is that it will enableprosocialplatforms that promote empathy, understanding, and collaboration rather than division.\n\nFor too long, the algorithms that drive social media have functioned like strip-mining machines, extracting attention while eroding trust and social cohesion. What remains are depleted online spaces, where empathy struggles to take root and collective problem-solving finds no fertile ground. AI can — and should — help us transcend these entrenched divides.\n\nTo achieve this, we must design AI systems that place prosocial values at their core. Instead of reinforcing fragmentation, recommendation algorithms can guide us toward “bridging content” that reveals common ground. They should clearly identify the communities a piece of content relates to — whether physical, religious, political, social, cultural, or professional — and illuminate the specific lines of division it seeks to mend.\n\nRealizing this vision requires a fundamental shift in what we optimize for. Instead of relying on pure engagement metrics, we should adopt values-driven indicators that prioritize constructive discourse and mutual understanding. For instance, we might spotlight “surprising validators,” or individuals and perspectives that productively challenge assumptions, thereby enriching our sense of what seemed irreconcilable. Researchers and developers should co-create new ranking and curation methods, embed them into widely used platforms, and rigorouslyassesstheir impact on democratic life.\n\nAt the same time, the AI community must embrace participatory, inclusive approaches to development and governance. Research onpluralistic alignmentstresses that AI systems emerge from and operate within complex social contexts, and including a wide range of voices helps guard against institutional blind spots. Tools likePolis, which can visualize stances and reveal hidden areas of consensus, already illustrate how complexity can be transformed into clarity. Such participatory methods ensure that AI reflects the priorities and values of the societies it serves, rather than amplifying the biases of the few.\n\nBy embracing these inclusive, democratic principles, AI can help us co-createdigital public squaresthat foster social cohesion rather than erode it. Embedding collective input at every stage — from how we build datasets to how we set governance policies — ensures that AI systems genuinely align with a spectrum of human values and serve as catalysts for common understanding.\n\nAudrey Tang is Taiwan’s Cyber Ambassador, former Minister of Digital Affairs, and co-author ofPlurality: The Future of Collaborative Technology and Democracy.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--35-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--36-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--37-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--38-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--39-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--40-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--41-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-74/",
    "title": "issue 74",
    "date": "",
    "reading_time": "",
    "content": "I hope this latest challenge will inspire a renewal of democracy. Organizations that are tested — and that survive the test — end up stronger.\n\nDemocracy stands on several pillars, among them:\n\nThe AI community can help strengthen all three.\n\nJanuary 6, 2021, was a nadir for the U.S., and the path ahead will be long and hard. But I believe the country has reached a turning point. I hope the dire events of the past week will renew our appreciation of just how precious sound government is.\n\nKeep learning!\n\nAndrew\n\nFace recognition is being used to identify people involved in last week’s assault on the U.S. Capitol. It’s also being misused to support their cause.What’s new:Law enforcement agencies and online sleuths are using deep learning to put names to faces in images shot while supporters of U.S. President Trump overran the building in Washington, D.C. to stop certification of his defeat in the recent national election, leaving several people dead and many injured. At the same time, pro-Trump propagandists are making false claims that the technology shows left-wing infiltrators led the attack.What happened:Police arrested few of the perpetrators. In the aftermath, the abundant images have fed AI-powered sleuthing to find those who were allowed to leave the scene.\n\nDeepfakes, too:Falsehoods also circulated regarding deepfake technology. Users of4chanand social media siteParlerwrongly asserted that President Trump’s post-insurrection speech, in which he called the participants “criminals” and “unpatriotic,” was faked by AI. The White Housedebunkedthis claim.Why it matters:The Capitol assault, apart from its aim to disrupt the democratic process (and apparently toassassinate officials), highlights that face recognition and deepfakes are two sides of the machine learning coin: One is a powerful tool for uncovering facts, the other a powerful tool for inventing them. While the police are relying on the former capability, propagandists are exploiting both by spreading believable but false claims.We’re thinking:Paranoia about artificial intelligence once centered on fear that a malicious superintelligence would wreak havoc. It turns out that humans using AI — and lies about AI — to spread disinformation pose a more immediate threat.\n\nTwo new models show a surprisingly sharp sense of the relationship between words and images.What’s new:OpenAI, the for-profit research lab, announced a pair of models that have produced impressive results in multimodal learning:DALL·E, which generates images in response to written prompts, andContrastive Language-Image Pretraining(CLIP), a zero-shot image classifier. The company published apaper that describes CLIPin detail; a similar Dall-E paper is forthcoming.How they work:Both models were trained on text-image pairs.\n\nYes, but:Neither model is immune to goofs. Asked to produce a pentagonal clock, for instance, DALL·E rendered some timepieces with six or seven sides. CLIP, meanwhile, has trouble counting objects in an image and differentiating subclasses like car brands or flower species.Behind the news:The new models build on earlier research at the intersection of words and images. A seminal 2016paperfrom the University of Michigan and Max Planck Institute for Informatics showed that GANs could generate images from text embeddings. Other work has resulted in models that render images from text, among themGenerative EngineandText to Image. Judging by the examples OpenAI has published so far, however, DALL·E seems to produce more accurate depictions and to navigate a startling variety of prompts with flair.Why it matters:As OpenAI chief scientist (and former post-doc in Andrew’s lab) Ilya Sutskever recentlywroteinThe Batch, humans understand concepts not only through words but through visual images. Plus, combining language and vision techniques could overcome computer vision’s need for large, well labeled datasets.We’re thinking:If we ever build a neural network that exhibits a sense of wonder, we’ll call it GOLL·E.\n\nWhen you’re training a deep learning model, it can take days for an optimization algorithm to minimize the loss function. A new approach could save time.What’s new:Juntang Zhuang and colleagues at Yale, University of Illinois at Urbana-Champaign, and University of Central Florida proposedAdaBelief, a more efficient variation on the popularAdamoptimizer.Key insight:The popular optimization methods of stochastic gradient descent (SGD) and Adam sometimes take small steps, requiring more time to reach their destination, when they could take larger ones. Given a small learning rate and a point in a large, steep area of a loss function’s landscape, SGD takes small steps until the slope becomes steeper, while Adam’s steps become smaller as it progresses. In both scenarios, an ideal optimizer would predict that the slope is long and take larger steps.How it works:AdaBelief adjusts its step size depending on the difference between the current gradient and the average of previous gradients.\n\nResults:The authors providevideosshowing that, in experiments on functions with known minimums, AdaBelief was faster than both Adam and SGD with momentum (as shown above). To demonstrate their method’s accuracy, they compared AdaBelief to SGD, Adam, and other adaptive optimizers on tasks including image classification, image generation, and language modeling. AdaBelief basically matched SGD’s accuracy and exceeded that of all other adaptive optimizers. For instance, onImageNet, AdaBelief increased aResNet18’s highest top-1 accuracy, or accuracy of its best prediction, to 70.08 percent, on par with SGD’s 70.23 percent and 2 percent better than the best adaptive optimizers.Why it matters:Faster optimization means faster training, and that means more time to experiment with different models.We’re thinking:The authors’ video demonstrations suggest that AdaBelief could be a valuable alternative to Adam. However, they don’t supply any numbers that would make for a precise speed comparison. We look forward to the authors of theDeep Learning Optimizer Benchmark Suite, who haveevaluatedover a dozen optimizers in various tasks, running AdaBelief through its paces.\n\n“Generative Deep Learning with TensorFlow,” Course 4 of ourTensorFlow: Advanced Techniques Specialization, is now available on Coursera.Enroll now\n\nA pair of neural networks is helping to prioritize Covid-19 cases for contact tracing.What’s new:The public health department of California’s Contra Costa County is using deep learning tosort Covid-19 casesreported via the pre-internet technology known as fax.How it works:Hospitals and medical labs document cases of coronavirus infection using hand-written forms. Many transmit the documents to public health officials over telephone landlines. Stanford University researchers developedCovid Fast Faxto evaluate them so that public health workers, who still manually review each case, can spot the most critical ones. The system comprises two convolutional neural networks.\n\nBehind the news:The use of fax in health care persists despite billions of dollars to promote digital health records. Digital systems face roadblocks, as many professionals find themdifficult to use, and for-profit hospitals aren’t always eager to make it easy for patients to share their information withcompetitors.\n\nWhy it matters:According to a2019 survey, 89 percent of U.S. health organizations still rely on fax to transmit medical information. Anything that accelerates the processing of that information is a plus — especially during a pandemic.We’re thinking:It’s 2021, and hospitals are still relying on fax to make critical decisions? AI can help hospitals cope with outmoded communications technology, but it’s no substitute for updating U.S. health care infrastructure.\n\nWorld models, which learn a compressed representation of a dynamic environment like, say, a video game, have delivered top results in reinforcement learning. A new method makes them much smaller.What’s new:Jan Robine and colleagues at Heinrich Heine University Düsseldorf presentDiscrete Latent Space World Models. Their approach matches the performance of the state of the art in six Atari games,SimPLe, with far fewer parameters.Key insight:Researchers have devoted significant effort to making reinforcement learning algorithms efficient, but they’ve given less attention to making models themselves efficient. Using high-performance architectures for the various components of a world model ought to improve the entire system — in this case, by reducing its size.How it works:Following the typical world models approach, the authors trained separate neural networks to generate a representation of the environment (the representation model), predict how actions would affect the environment (the dynamics model), and choose the action that will bring the greatest reward (the policy model).\n\nResults:The authors compared their method to SimPLe in six Atari games. SimPLe uses 74 million parameters, while their method uses 12 million during training and 3 million during inference. Nonetheless, their method’s mean scores over five training runs beat SimPLe in five out of six games when given 100,000 observations.Yes, but:Although the authors’ method beat SimPLe on average, SimPLe racked up higher scores in four out of six games.Why it matters:Smaller models consume less energy, require less memory, and execute faster than larger ones, enabling machine learning engineers to perform more experiments in less time.We’re thinking:World models are young enough that something as simple as changing the components used can make a big difference. This suggests that plenty of opportunity remains to improve existing models.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/Screen20Shot202021-01-1320at2010.56.4020AM20copy.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/PARANOIA.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/AVOCADO.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker2061.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/Gif201-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/FAX.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker2069.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-278/",
    "title": "issue 278",
    "date": "",
    "reading_time": "",
    "content": "There’s a lingering misconception that building with generative AI is expensive. It is indeed expensive to train cutting-edge foundation models, and a number of companies have spent billions of dollars doing this (and even released some of their models as open weights). But as a result, it’s now veryinexpensiveto build a wide range of AI applications.\n\nThe AI stack has several layers, shown in the diagram below. Here are the lower layers, from the bottom up:\n\nThe foundation model layer frequently appears in headlines because foundation models cost so much to build. Some companies have made massive investments in training these models, and a few of those have added to the hype by pointing out that paying lots for compute and data would lead (probably) topredictably better performancefollowingscaling laws.\n\nThis layer is also currently hyper-competitive, and switching costs for application developers to move from one model to another are fairly low (for example, requiring changes to just a few lines of code). Sequoia Capital’s thoughtful article on “AI's $600B Question” points out that, to justify massive capital investments in AI infrastructure (particularly GPU purchases and data center buildouts), generative AI needs to get around $600B of revenue. This has made investing at the foundation model layer challenging. It’s expensive, and this sector still needs to figure out how to deliver returns. (I’m cautiously optimistic it will work out!)\n\nOn top of this layer is an emerging orchestration layer, which provides software that helps coordinate multiple calls to LLMs and perhaps to other APIs. This layer is becoming increasingly agentic. For example,Langchainhas helped many developers build LLM applications, and its evolution intoLangGraphfor building agents has been a great development. Other platforms such asAutogen,MemGPT, andCrewAI(disclosure: I made a personal investment in CrewAI) are also making it easier to build agentic workflows. Switching costs for this layer are much higher than for the foundation model layer, since, if you’ve built an agent on one of these frameworks, it’s a lot of work to switch to a different one. Still, competition in the orchestration layer, as in the foundation model layer, seems intense.\n\nFinally, there’s the application layer. Almost by definition, this layer has to do better financially than all the layers below. In fact, for investments at the lower layers to make financial sense, the applications had better generate even more revenue, so the application vendors can afford to pay providers of infrastructure, cloud computing, foundation models, and orchestration. (This is why my team AI Fund focuses primarily on AI application companies, as I discussed in atalk.)\n\nFortunately, because of the massive investments in foundation models, it’s now incredibly inexpensive to experiment and build prototypes in the applications layer! Over Thanksgiving holiday, I spent about one and a half days prototyping different generative AI applications, and my bill for OpenAI API calls came out to about $3. On my personal AWS account, which I use for prototyping and experimentation, my most recent monthly bill was $35.30. I find it amazing how much fun you can have on these platforms for a small number of dollars!\n\nBy building on widely available AI tools, AI Fund now budgets $55,000 to get to a working prototype. And while that is quite a lot of money, it’s far less than the billions companies are raising to develop foundation models. Individuals and businesses can experiment and test important ideas at reasonable cost.\n\nKeep learning!\n\nAndrew\n\nStarting your career in AI has never been easier withMachine Learning Specialization, a foundational program for beginners in machine learning.Get started!\n\nOne of the world’s biggest payment processors is enabling large language models to spend real money.\n\nWhat’s new:Stripe announced Stripe Agent Toolkit, alibraryfor Python and Typescript that supports agentic workflows that use API calls to execute monetary transactions. You can download ithere.\n\nHow it works:An agentic purchasing workflow may look like this: A user asks the agent to find a flight to a certain destination, on a certain schedule, with a certain price limit; and an LLM queries a flight database, chooses a flight, obtains authorization from the user, and purchases the flight. Stripe Agent Toolkit supports agentic workflow frameworks fromCrewAI,LangChain, andVercel. It doesn’t yet implement all of Stripe’s API, but Stripe expects to extend it in the future.\n\nWhy it matters:Agents that can spend money securely open a wide variety of applications. Stripe’s API previously made it possible to enable an LLM-based application to make purchases online, but doing so required trusting the LLM to generate the right API calls and not to make inappropriate ones. The new library makes it easier to enforce spending limits and API constraints, and thus to build agents that engage in ecommerce safely.\n\nWe’re thinking:Stripe’s offering helps developers build agents that are cents-ible!\n\nMistral AI unveiled Pixtral Large, which rivals top models at processing combinations of text and images.\n\nWhat’s new:Pixtral Largeoutperformsa number of leading vision-language models on some tasks. Theweightsare free for academic and non-commercial use and can be licensed for business use. Access isavailablevia Mistral AI’s website or API for $2/$6 per million tokens for input/output. In addition, Pixtal Large now underpins le Chat, Mistral AI’s chatbot, which alsogainedseveral new features.\n\nHow it works:Pixtral Large generates text in response to text and images in dozens of languages. It processes 131,072 tokens of context, which is sufficient to track relationships among 30 high-resolution images at a time. Based on Mistral Large 2 (a 123 billion-parameter large language model) and a 1 billion-parameter vision encoder, it demonstrates strong performance across several benchmarks (as reported by Mistral).\n\nBehind the news:Pixtral Large arrives as competition intensifies among vision-language models. Meta recentlyenteredthe field with Llama 3.2 vision models in 11B and 90B variants. Both Pixtral Large and Llama 3.2 90B offer open weights, making them smaller and more widely available than Anthropic’s, Google’s, or OpenAI’s leading vision-language models. However, like those models, Pixtral Large falls short of the reported benchmark scores of the smaller, more permissively licensedQwen2-VL 72B.\n\nWhy it matters:Pixtral Large and updates to le Chat signal that vision-language capabilities — combining text generation, image recognition, and visual reasoning — are essential to compete with the AI leaders. In addition, context windows of 129,000 tokens and above have become more widely available, making it possible to analyze lengthy (or multiple) documents that include text, images, and graphs as well as video clips.\n\nWe’re thinking:Mistral is helping to internationalize development of foundation models. We’re glad to see major developers emerging in Europe!\n\nRapid progress in generative AI comes with a hidden environmental cost: mountains of obsolete hardware.\n\nWhat’s new:Astudyprojects that servers used to process generative AI could produce millions of metric tons of electronic waste by 2030. Extending server lifespans could reduce the burden substantially, according to author Peng Weng and colleagues at the Chinese Academy of Sciences and Reichman University.\n\nHow it works:The study extrapolated from publicly available data to model accumulation of electronic waste, or e-waste, between 2023 and 2030. The authors examined four scenarios: One scenario assumed linear growth in which hardware manufacturing expands at the current rate of 41 percent annually. The other three assumed exponential growth of demand for computing: conservative (85 percent annually), moderate (115 percent annually), and aggressive (136 percent annually). The study evaluated each scenario with and without measures taken to reduce waste.\n\nWhy it matters:E-waste is a problem not only due to its sheer quantity. Server hardware contains materials that are both hazardous and valuable. Discarded servers contain toxic substances like lead and chromium that can find their way into food water supplies. They also contain valuable metals, such as gold, silver, and platinum, that could save the environmental and financial costs of producing more of them.Properrecyclingof these components could yield $14 billion to $28 billion, highlighting both the economic potential and the urgent need to develop and deploy advanced recycling technologies.\n\nWe’re thinking:Humanity dumps over 2 billion metric tons of waste annually, so even comprehensive recycling and repurposing of AI hardware and other electronic devices would make only a small dent in the overall volume. However, the high density of valuable materials in e-waste could make mining such waste profitable and help recycle waste into valuable products, making for a more sustainable tech economy.\n\nJailbreak prompts can prod a large language model (LLM) to overstep built-in boundaries, leading it to do things like respond to queries it was trained to refuse to answer. Researchers devised a way to further boost the probability that LLMs will respond in ways that respect such limits.\n\nWhat’s new:Jingtong Su, Julia Kempe, and Karen Ullrich at New York University and MetaAI improved model behavior viaE-DPO. Their method modifiesDirect Preference Optimization(DPO), a popular way to align models with human preferences.\n\nKey insight:DPO fine-tunes a model to encourage a developer’s notion of good behavior and suppress bad behavior, but it must also ensure that the model doesn’t forget knowledge it learned during pretraining. To this end, DPO’s loss function includes a regularization constraint that encourages the model to produce token probabilities similar to those it produced prior to fine-tuning. However, this causes the model to retain not only desired knowledge but also undesired knowledge that may lead it to produce an unwanted response. We can reduce the probability that it will draw on such undesired knowledge by changing the regularization constraint. The idea is to ensure similar token probabilities between (a) a model prior to fine-tuning, asked to behave harmlessly prior to receiving the harmful prompt and (b) the fine-tuned model, given a harmful prompt. This adjustment helps the fine-tuned model deliver outputs based on benign knowledge, along with the usual benefits of DPO.\n\nHow it works:The authors used E-DPO to further fine-tuneMistral-7b-sft-constitutional-ai(which is aligned using the technique known asconstitutional AI) ontwodatasetsin which each example consists of a prompt, a preferred response, and an objectionable response.\n\nResults:E-DPO reduced Mistral-7b-SFT-constitutional-ai’s average attack success rate (ASR, the percentage of times a jailbreak prompt successfully elicited an objectionable responses) across 11 jailbreak datasets and methods (two sets of human-proposed jailbreak prompts and a variety of automatic jailbreak prompt-finding methods) from theHarmBenchbenchmark. The fine-tuned model achieved 36.95 ASR, while prior to fine-tuning it achieved 44.47 ASR. Typical DPO reduced the average ASR to 42.00.\n\nWhy it matters:We can’t train a model to respond in a desirable way to all jailbreaks, no matter how big the training dataset. The space of potential jailbreaks is practically unlimited. Instead, it’s necessary to alter training methods, as this work does.\n\nWe’re thinking:Humans, like learning algorithms, can circumvent social norms when they encounter a harmful request (attack your neighbors) cloaked in a manipulative scenario (to uphold religious or nationalistic values). While we work on aligning models with human preferences, let’s make sure we ourselves are aligned, too.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--25-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/12/Banner-MLS.png",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--26-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--27-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--28-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/12/unnamed--29-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-159/",
    "title": "issue 159",
    "date": "",
    "reading_time": "",
    "content": "Last week, Iwroteabout switching roles, industries, or both as a framework for considering a job search. If you’re preparing to switch roles (say, taking a job as a machine learning engineer for the first time) or industries (say, working in an AI tech company for the first time), there’s a lot about your target job that you probably don’t know. A technique known as informational interviewing is a great way to learnAn informational interview involves finding someone in a company or role you’d like to know more about and informally interviewing them about their work. Such conversations are separate from searching for a job. In fact, it’s helpful to interview people who hold positions that align with your interests well before you’re ready to kick off a job search.\n\nPrepare for informational interviews by researching the interviewee and company in advance, so you can arrive with thoughtful questions. You might ask:\n\nFinding someone to interview isn’t always easy, but many people who are in senior positions today received help when they were new from those who had entered the field ahead of them, and many are eager to pay it forward. If you can reach out to someone who’s already in your network — perhaps a friend who made the transition ahead of you or someone who attended the same school as you — that’s great! Meetups such asPie & AIcan also help you build your network.Finally, be polite and professional, and thank the people you’ve interviewed. And when you get a chance, please pay it forward as well and help someone coming up after you. If you receive a request for an informational interview from someone in the DeepLearning.AI community, I hope you’ll lean in to help them take a step up! If you’re interested in learning more about informational interviews, I recommend thisarticlefrom the UC Berkeley Career Center.I’ve mentioned a few times the importance of your network and community. People you’ve met, beyond providing valuable information, can play an invaluable role by referring you to potential employers. Stay tuned for more on this topic.\n\nKeep learning!\n\nAndrew\n\nWhile the United States doesn’t explicitly regulate AI at the national level, many parts of the country have moved to limit the technology.What’s new:The Electronic Privacy Information Center publishedThe State of State AI Policy, a summary of AI-related laws that states and cities considered between January 2021 and August 2022.Passed:Seven laws were enacted that regulate a variety of AI applications and activities.\n\nPending:Thirteen more laws are currently in progress in nine states and Washington DC. Bills would establish advisory bodies to study the impacts of AI in California, Georgia, Maryland, Massachusetts, New Jersey, New York, and Rhode Island. California lawmakers propose mandating processes to minimize algorithmic bias. Hawaii lawmakers propose a tax credit for AI businesses.Why it matters: AI increasingly affects U.S. society, sometimes inalarmingways (and at the expense of publictrust). Yet it remains largely unregulated at the national level. State and local legislation are filling the gap. However, a patchwork legal landscape could be a headache for companies that aim to do business in multiple states.We’re thinking:A yawning gap separates leaders in technology and government. Many tech executives hold the stereotype that politicians don't understand technology. Meanwhile, politicians widely regard tech executives as being hostile to the government and primarily out to make a buck. It will take effort on both sides to overcome these stereotypes and forge a shared understanding that leads to better regulations as well as better AI.\n\nWhen a neural network learns image labels, it may confuse a background item for the labeled object. For example, it may learn to associate the label “camel” with desert sand and then classify a cow on a beach as a camel. New research has trained networks to avoid such mistakes.What’s new:A team at Stanford and Northeastern University led by Michael Zhang proposedCorrect-N-Contrast(CNC), a training method that makes neural networks more robust to spurious correlations, in which features and labels are associated but not causally related.Key insight:A neural network likely has learned a spurious correlation when it produces dissimilar representations of two images with the same label. When learning representations of two images of a cow, for example, the error may manifest as a representation of a grassy field in one image and a representation of a beach in the other. A contrastive loss function can help a neural network avoid such errors by encouraging it to learn similar representations for similar objects against different backgrounds.How it works:The authors trained models to classify examples and identified examples the models got wrong, possibly owing to spurious correlations. Then they trained a second neural network to classify them correctly using a contrastive loss function.\n\nResults:The authors evaluated their models’ accuracies on groups of examples known to be difficult to classify. Their approach outperformedEIIL, which first trains a model to infer related groups of examples and then trains a second model to classify examples using the group IDs, both on average and on individual tasks. For instance, the ResNet-50 trained on CelebA with CNC achieved 88.8 percent accuracy, while training with EIIL achieved 81.7 percent accuracy. Across all tasks, the authors’ approach achieved 80.9 percent average accuracy while EIIL achieved 74.7 percent average accuracy.Yes, but:Group DRO, which provides additional information during training such as a description of the background of an image or the gender of a depicted person, achieved 81.8 percent average accuracy.Why it matters:Previous approaches to managing spurious correlations tend to expand training datasets to capture more variability in data. This work actively guides models away from representing features that reduce classification accuracy.We’re thinking:A self-driving car must detect a cow (or a person or another vehicle) whether it stands on a meadow, a beach, or pavement.\n\nNektarios Kalogridis was a software developer in finance. He saw the growing impact of AI on the industry, so he took Andrew Ng’sMachine Learningcourse. Today, he’s a senior algorithmic trading developer at one of the world’s largest banks.Enroll in theMachine Learning Specialization!\n\nAutonomous robots are restocking the refrigerated sections in corner stores.What’s new:FamilyMart, a chain of Japanese convenience stores, plans toemploy robotsto fill shelves with beverage bottles at 300 locations.How it works:The TX SCAR from Tokyo-based firm Telexistence includes an arm and camera. It shuttles along a rail in between stock shelves and the rear of a customer-facing refrigerator, moving up to 1,000 containers a day.\n\nBehind the news:FamilyMartalso operates grab-and-go stores in which AI models recognize items as shoppers put them into carts and ring up sales automatically as they exit.Amazonhas similar stores in the United Kingdom and United States.Why it matters:Japan faces anaging workforcewith no end in sight. People over 65 years old make up around a quarter of the population, which is expected to have the world’s highest average age for decades. Embracing robot labor is one solution, along with matching older workers with appropriate jobs and extending the retirement age.We’re thinking:Frommaking french friesto restocking shelves, the jobs that once were rites of passage for young adults are increasingly automated. Will the next wave of after-school gigs involve debugging code and greasing servos?\n\nTwo molecules can contain the same types and numbers of atoms but exhibit distinct properties because their shapes differ. New research improves machine learning representations to distinguish such molecules.What’s new:Xiaomin Fang, Lihang Liu, and colleagues at Baidu proposedgeometry-enhanced molecular representation learning(GEM), an architecture and training method that classifies molecules and estimates their properties.Key insight:Chemists have used graph neural networks (GNNs) to analyze molecules based on their atomic ingredients and the types of bonds between the atoms. However, these models weren’t trained on structural information, which plays a key role in determining a molecule’s behavior. They can be improved by training on structural features such as the distances between atoms and angles formed by their bonds.GNN basics:A GNN processes datasets in the form of graphs, which consist of nodes connected by edges. For example, a graph might depict customers and products as nodes and purchases as edges. This work used a vanilla neural network to update the representation of each node based on the representations of neighboring nodes and edges.How it works:The authors trained a modified GNN on18 million molecules whose properties were unlabeledto estimate structural attributes of molecules. They fine-tuned it to find molecular properties.\n\nResults:GEM achieved state-of-the-art results on 14 tasks, surpassingGROVER, a transformer-GNN hybrid that learns to classify a molecule’s connected atoms and bond types but not structural attributes. For example, when estimatingproperties that are important for solubility in water, it achieved 1.9 root mean square error, while the large version of GROVER achieved 2.3 root mean squared error. On average, GEM outperformed GROVER on regression tasks by 8.8 percent and by 4.7 percent on classification tasks.Why it matters:This work enabled a GNN to apply representations it learned from one graph to another — a promising approach for tasks that involve overlapping but distinct inputs.We’re thinking:How can you trust information about atoms? They make up everything!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/08/AI-ML-JOBSEARCH_Info-Interview_1200px-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/08/EPIC_State-AI-Regs_LongerHolds_1200px.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/08/SPURIOUS--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/08/MLS_Learner_1200x628_A-1_Artboard-1-copy-11-1.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/08/TXSCARA_600px.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/08/GEM--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-277/",
    "title": "issue 277",
    "date": "",
    "reading_time": "",
    "content": "Happy Thanksgiving! In the United States, this is a week when many reflect on their blessings and give thanks. Even as I reflect on how lucky I am to have food, shelter, family, and friends, I think about those who have much less and what we can do to help them.\n\nLast week, I spoke with a woman who had been severely physically abused by her husband. She showed me pictures of her face from a few years ago, which had a bloodied sequence of tears down the middle. She also showed me scars left by cigarette burns inflicted by her husband, who told her these burns made her ugly so no other man would ever want her. She is no longer with her husband but continues to struggle. Her phone is badly cracked and barely holds a charge. Without a high-school degree, she has struggled to find a job and is surviving by staying on the couch of a friend. As winter approaches, they keep their place chilly to save the cost of electricity.\n\nWorking in AI, I am fortunate to interact with many of the smartest and most capable technology and business leaders in the world. But both at home and when I travel, I try to meet with people of a broad range of backgrounds, because ultimately I want to do work that helps people broadly, and this requires that I understand people broadly. When you go to a grocery store and see someone put down a $5 carton of eggs because it is too expensive, and hear them think through how to explain to their kids why they’re skipping eggs that week, it gives you a deeper appreciation for why a $1.50/hour raise can be life-changing for many people.\n\nWhile I can try to help out individuals here and there, technology is advancing rapidly, and this gives me a lot of optimism for the future. Technology remains the best way I know of to help people at scale through providing better education, career guidance, healthcare, personal safety, healthier food, or other things needed to support thriving.\n\nI am optimistic about the future because I see so many ways life can be so much better for so many people. I feel blessed that, when my kids or I are cold, we have warm clothing, and when we are hungry, we have a working car to drive to the grocery and buy fresh food. I feel blessed that, rather than using a badly cracked cellphone, I have a modern laptop and a fast internet connection to do my work on.\n\nAs a child, my father taught me the aphorism “there but for the grace of God go I” to recognize that, in even slightly different circumstances, I might have ended up with much less. Having worked on many software products, I know that, to make good decisions, I have to understand the people I hope to serve. This is why I continue to routinely seek out, speak with, and try to understand people from all walks of life, and I hope many others in AI will do so, too.\n\nI see so many people in the AI community building things to make the world better. I am thankful for what the AI community has already done, and I look forward to continuing to build and serve others together.\n\nKeep building!\n\nAndrew\n\nGet started coding in Python withAI Python for Beginners, a four-part course led by Andrew Ng. Build projects from the very first lesson with real-time support from an AI assistant. Complete the course and bring your ideas to life!Start today\n\nAn up-and-coming Hangzhou AI lab unveiled a model that implements run-time reasoning similar to OpenAI o1 and delivers competitive performance. Unlike o1, it displays its reasoning steps.\n\nWhat’s new:DeepSeekannouncedDeepSeek-R1, a model family that processes prompts by breaking them down into steps. A free preview version isavailableon the web, limited to 50 messages daily; API pricing is not yet announced. R1-lite-preview performs comparably to o1-preview on several math and problem-solving benchmarks. DeepSeek said it would release R1 as open source but didn't announce licensing terms or a release date.\n\nHow it works:DeepSeek-R1-lite-preview uses asmaller base modelthan DeepSeek 2.5, which comprises 236 billion parameters. Like o1-preview, most of its performance gains come from an approach known astest-time compute, which trains an LLM to think at length in response to prompts, using more compute to generate deeper answers. Unlike o1-preview, which hides its reasoning, at inference, DeepSeek-R1-lite-preview’s reasoning steps are visible. This makes the model more transparent, but it may also make it morevulnerableto jailbreaks and other manipulation.\n\nBehind the news:DeepSeek-R1 follows OpenAI in implementing this approach at a time when scaling laws that predict higher performance from bigger models and/or more training data are beingquestioned.\n\nWhy it matters:DeepSeek is challenging OpenAI with a competitive large language model. It’s part of an important movement, after years of scaling models by raising parameter counts and amassing larger datasets, toward achieving high performance by spending more energy on generating output.\n\nWe’re thinking:Models that do and don’t take advantage of additional test-time compute are complementary. Those that do increase test-time compute perform well on math and science problems, but they’re slow and costly. Those that don’t use additional test-time compute do well on language tasks at higher speed and lower cost. Applications that require facility in both math and language may benefit by switching between the two.\n\nA new generation of robots can handle some household chores with unusual skill.\n\nWhat’s new:Physical Intelligence, a startup based in San Francisco, unveiledπ0(pronounced “pi-zero”), a machine learning system that enables robots to perform housekeeping tasks that require high coordination and dexterity, like folding clothes and cleaning tables. The company alsoannounced$400 million in investments from OpenAI, Jeff Bezos, and several Silicon Valley venture capital firms.\n\nHow it works:π0 is a version of the pretrainedPaliGemmavision-language model that has been modified forflow matching. (Flow matching is similar to diffusion, in which a model learns to remove noise from inputs to which noise has been added, and ultimately generates output by removing noise from an input of pure noise). A user supplies a text command, and the robot uses its sensor inputs to remove noise from a pure-noise action embedding to generate an appropriate action.\n\nResults:π0 outperformed the open robotics modelsOpenVLA,Octo,ACT, andDiffusion Policy, all of which were fine-tuned on the same data, on all tasks tested, as measured by a robot’s success rate in completing each task. For example, using a single robotic arm to stack a set of bowls of four sizes, π0 completed about 100 percent on average. Diffusion Policy completed about 55 percent, ACT about 45 percent, and OpenVLA and Octo below 10 percent. Across all tasks, π0 completed about 80 percent on average, while Diffusion Policy completed about 35 percent on average.\n\nYes, but:The robot occasionally makesmistakes. In one video, it puts too many eggs into a carton and tries to force it shut. In another, it throws a container off a table instead of filling it with items.\n\nBehind the news:Commercial robotics appears to be undergoing a renaissance. Skildraised$300 million to develop a “general-purpose brain for robots.” Figure AIsecured$675 million to build humanoid robots powered by multimodal models. Covariant, which specializes in industrial robotics,licensedits technology to Amazon. (Disclosure: Andrew Ng is a member of Amazon's board of directors). OpenAIrenewedits robotics effort afterdismantlingits robotics department in 2020.\n\nWhy it matters:Robots have been slow to benefit from machine learning, but the generative AI revolution is driving rapid innovations that make them much more useful. Large language models have made it possible to command robots using plain English. Meanwhile, the team at Physical Intelligence collected a dataset of sufficient size and variety to train the model to generate highly articulated and practical actions. Household robots may not be right around the corner, but π0 shows that they can perform tasks that people need done.\n\nWe’re thinking:One of the team members compared π0 to GPT-1 for robotics — an inkling of things to come. Although there are significant differences between text data (which is available in large quantities) and robot data (which is hard to get and varies per robot), it looks like a new era of large robotics foundation models is dawning.\n\nAmazon and Anthropic expanded their partnership, potentially strengthening Amazon Web Services’ AI infrastructure and lengthening the high-flying startup’s runway.\n\nWhat’s new:Amazon, already a significant investor in Anthropic,putanother $4 billion into the AI company. In exchange, Anthropic will train and run its AI models on Amazon’s custom-designed chips. (Disclosure: Andrew Ng serves on Amazon’s board of directors.)\n\nHow it works:The new round brings Amazon’s investment in Anthropic to $8 billion (though it remains a minority stake without a seat on the startup’s board). The deal extended the partnership in several ways:\n\nBehind the news:In November, Anthropicagreedto use Google’s cloud-computing infrastructure in return for a $2 billion investment. The previous month, Amazon hadcommittedto invest as much as $4 billion in Anthropic, and Anthropic had made Amazon Web Services the primary provider of its models.\n\nYes, but:The UK’s Competition and Markets Authority recentlyclearedboth Amazon’s and Google’s investments in Anthropic, but regulators continue to monitor such arrangements for violations of antitrust laws. Microsoft and OpenAI face a similarinvestigationby the European Commission and U.S. Federal Trade Commission.\n\nWhy it matters:The speed and skill required to build state-of-the-art AI models is driving tech giants to collaborate with startups, while the high cost is driving startups to partner with tech giants. If the partnership between Amazon and Anthropic lives up to its promise, Claude users and developers could see gains in performance and efficiency. This could validate Amazon's hardware as a competitor with Nvidia and strengthen Amazon Web Services’ position in the cloud market. On the other hand, if Claude faces any challenges in scaling while using Trainium and Inferentia, that could affect both companies' ambitions.\n\nWe’re thinking:Does the agreement between Amazon and Anthropic give the tech giant special access to the startup’s models for distillation, research, or integration, as thepartnershipbetween Microsoft and OpenAI does? The companies’ announcements don’t say.\n\nAn open source model is designed to perform sophisticated object detection on edge devices like phones, cars, medical equipment, and smart doorbells.\n\nWhat’s new:Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, and colleagues at the International Digital Economy Academy introducedGrounding DINO 1.5, a system that enables devices with limited processing power to detect arbitrary objects in images based on a text list of objects (also known as open-vocabulary object detection). You can download the code and weightshere.\n\nKey insight:The originalGrounding DINOfollows many of itspredecessorsby using image embeddings of different levels (from lower-level embeddings produced by an image encoder’s earlier layers, which are larger and represent simple patterns such as edges, to higher-level embeddings produced by later layers, which are smaller and represent complex patterns such as objects). This enables it tobetter detect objects at different scales. However, it takes a lot of computation. To enable the system to run on devices that have less processing power, Grounding DINO 1.5 uses only the smallest (highest-level) image embeddings for a crucial part of the process.\n\nHow it works:Grounding DINO 1.5 is made up of components that produce text and image embeddings, fuse them, and classify them. It follows the system architecture and training of Grounding DINO with the following exceptions: (i) It uses a different image encoder, (ii) a different model combines text and image embeddings, and (iii) it was trained on a newer dataset of 20 million publicly available text-image examples.\n\nResults:Grounding DINO 1.5 performed significantly faster than the original Grounding DINO: 10.7 frames per second versus 1.1 frames per second running on anNvidia Jetson Orin NXcomputer. Tested on adatasetof images of common objects annotated with labels and bounding boxes, Grounding DINO 1.5 achieved better average precision (a measure of how many objects it identified correctly in their correct location, higher is better) than both Grounding DINO andYOLO-Worldv2-L(a CNN-based object detector). Grounding DINO 1.5 scored 33.5 percent, Grounding DINO 27.4 percent, and YOLO-Worldv2-L 33 percent.\n\nWhy it matters:The authors achieved 10 times the speed with just a couple of small changes (a more efficient image encoder and a smaller image embedding when performing cross-attention between embeddings of images and texts). Small changes can yield big results.\n\nWe’re thinking:Lately model builders have been building better, smaller, faster large language models for edge devices. We’re glad to see object detection get similar treatment.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/11/2--9-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--23-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--34-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--37-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35--1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-104/",
    "title": "issue 104",
    "date": "",
    "reading_time": "",
    "content": "How much math do you need to know to be a machine learning engineer? It’s always nice to know more math! But there’s so much to learn that, realistically, it’s necessary to prioritize. Here are some thoughts about how you might go about strengthening your math background.To figure out what’s important to know, I find it useful to ask what you need to know to make the decisions required for the work you want to do. At DeepLearning.AI, we frequently ask, “What does someone need to know to accomplish their goals?” The goal might be building a machine learning model, architecting a system, or passing a job interview.Understanding the math behind algorithms you use is often helpful, since it enables you to debug them. But the depth of knowledge that’s useful changes over time. As machine learning techniques mature and become more reliable and turnkey, they require less debugging, and a shallower understanding of the math involved may be sufficient to make them work.\n\nFor instance, in an earlier era of machine learning, linear algebra libraries for solving linear systems of equations (for linear regression) were immature. I had to understand how these libraries worked so I could choose among different libraries and avoid numerical roundoff pitfalls. But this became less important as numerical linear algebra libraries matured.\n\nDeep learning is still an emerging technology, so when you train a neural network and the optimization algorithm struggles to converge, understanding the math behindgradient descent,momentum, and theAdamoptimization algorithm will help you make better decisions. Similarly, if your neural network does something funny — say, it makes bad predictions on images of a certain resolution, but not others — understanding the math behind neural network architectures puts you in a better position to figure out what to do.Sometimes, we’re told that an idea is “foundational.” While there’s a lot to be said for understanding foundations, often this designation is arbitrary and thus not very useful for prioritizing what to study next. For example, computing happens on processors that are packed with transistors. Do you need a deep understanding of how transistors work to write software? It's hard to imagine an AI application where a detailed knowledge of the physics of transistors would affect your decisions.Rather than accepting an authority’s decree that a topic is foundational, it’s worth asking what circumstances would require specific knowledge to help you make better decisions.Of course, I also encourage learning driven by curiosity. If something interests you, go ahead and learn it regardless of how useful it will be in the foreseeable future. Maybe this will lead to a creative spark or technical breakthrough.\n\nKeep learning!Andrew\n\nAlgorithms trained to diagnose medical images can recognize the patient’s race — but how?What’s new:Researchers from Emory University, MIT, Purdue University, and other institutionsfoundthat deep learning systems trained to interpret x-rays and CT scans also were able to identify their subjects as Asian, Black, or White.What they found:Researchers trained various implementations ofResNet,DenseNet, andEfficientNeton nine medical imaging datasets in which examples were labeled Asian, Black, or White as reported by the patient. In tests, the models reliably recognized the race, although their performance varied somewhat depending on the type of scan, training dataset, and other variables.\n\nBehind the news:Racial bias has been documented in some medical AI systems.\n\nWhy it matters:The fact that diagnostic models recognize race in medical scans is startling. The mystery of how they do it only adds fuel to worries that AI could magnify existing racial disparities in health care.We’re thinking:Neural networks can learn in ways that aren’t intuitive to humans. Finding out how medical imaging algorithms learn to identify race could help develop less biased systems — and unlock other mysteries of machine learning.\n\nSelf-attention enables transformer networks to track relationships between distant tokens — such as text characters — in long sequences, but the computational resources required grow quadratically with input size. New work aims to streamline the process by rating each token’s relevance to the task at hand.What’s new:Sainbayar Sukhbaatar and colleagues at Facebook proposedExpire-Span, which enables attention to ignore tokens that aren’t useful to the task at hand.Key insight:Depending on the task, some tokens affect a model’s performance more than others. For instance, in predicting the sentiment of the sentence, “Then she cried,” “cried” is more important than “then.” By forgetting less relevant tokens, attention can process longer sequences with less computation.How it works:The authors modified a transformer’s attention layers. They trained the model in typical fashion to predict the next character in a sequence using theenwik8dataset of text fromEnglish Wikipedia. Given the first token, it predicted the next. Then, using the first two tokens, it predicted the next, and so on.\n\nResults:The authors evaluated Expire-Span based on total memory usage, training time per batch, and bits per byte (a measure of how well the model predicted the next token; lower is better). On enwik8, it achieved 1.03 bits per byte, whileAdaptive-Spanachieved 1.04 bits per byte andcompressive transformerachieved 1.05 bits per byte. The authors’ model used 25 percent less GPU memory than the other two approaches (15GB versus 20GB and 21GB respectively). It also took less time to train (408ms per batch of 512 tokens compared to 483ms and 838ms).Why it matters:Forgetting the least relevant information enables transformers to process longer sequences in less time and memory.We’re thinking:Q: What do you do if a transformer forgets too much? A: Give it an Optimus Primer.\n\nFactored, a sister company of DeepLearning.AI that helps ambitious Silicon Valley-based companies build data science teams, is partnering with rigorously vetted machine learning engineers, data engineers, and data analysts to work on your projects.Learn more\n\nInsects that spread pollen to fruiting plants are in trouble. A possible alternative: Robots.What’s new:Farmers in Australia and the U.S. are using robots from Israeli startup Arugga Farming to pollinate greenhouse tomatoes,The Wall Street Journalreported.How it works:The system is designed for growing tomatoes, which self-pollinate when their pollen is stirred up by the beating of insect wings. Robots equipped with cameras, vision algorithms, and air compressors wheel themselves between rows of plants. When they recognize a flower that’s ready to produce fruit, they blast it with air to release its pollen.\n\nBehind the news:A number of other companies are using AI-enabled robots to pollinate plants. Edete Precision Technologies has had success withalmonds, and Bumblebee AI hopes to pollinateavocados, kiwis, and cocoa. Developed at West Virginia University, a robot called BrambleBee aims to pollinateblackberries, raspberries, and brambleberries.Why it matters:Robotic pollinators may prove to be an important technology outside of greenhouses. Climate change and habitat loss areravaging Earth’s insect populationsincluding bees. Meanwhile, such machines could be helpful to farmers: Bees are expensive to rent, they can spread plant diseases, and importing them is restricted in places such as Australia.We’re thinking:These robots are sure to generate a buzz.\n\nThe U.S. plans to build nearly a dozen new civilian AI research labs.What’s new:The U.S. National Science Foundation (NSF) committed $220 million tofund11 National Artificial Intelligence Research Institutes, complementing seven other AI research institutes that were established last year.What’s happening:The NSF grants provide each institute about $20 million annually over five years. Some will receive additional funding from public and private partners such as the U.S. Department of Homeland Security, Amazon, and Intel. Their missions include:\n\nBehind the news:The NSFfundedan initial seven national AI institutes in September. Earlier, the U.S. had said it would spend$2 billion annuallyon AI over the next two years.Why it matters:Other governments spend much more on AI than the U.S., and this outlay is small in the scheme of national AI funding. However, the allocation and the goals to which it is being put suggest that the federal government recognizes AI’s importance to the U.S. economy and its potential to benefit the world at large.We’re thinking:U.S. government funding was critical to AI's rise. For example, the Defense Advanced Research Products Agency (DARPA) providedfundsto both Andrew and Yann LeCun for deep learning research. We’re hopeful that these new programs will fund similarly valuable innovations.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/08/Screen-Shot-2021-08-10-at-7.webp",
      "https://dl-staging-website.ghost.io/content/images/2021/08/race.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/08/FORGET.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/08/Factored-image-Promo-2.webp",
      "https://dl-staging-website.ghost.io/content/images/2021/08/ARUGGA.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/08/nsf-2.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-266/",
    "title": "issue 266",
    "date": "",
    "reading_time": "",
    "content": "Over the weekend, my two kids colluded in a hilariously bad attempt to mislead me to look in the wrong place during a game of hide-and-seek. I was reminded that most capabilities — in humans or in AI — develop slowly.\n\nSome people fear that AI someday will learn to deceive humans deliberately. If that ever happens, I’m sure we will see it coming from far away and have plenty of time to stop it.\n\nWhile I was counting to 10 with my eyes closed, my daughter (age 5) recruited my son (age 3) to tell me she was hiding in the bathroom while she actually hid in the closet. But her stage whisper, interspersed with giggling, was so loud I heard her instructions clearly. And my son’s performance when he pointed to the bathroom was so hilariously overdramatic, I had to stifle a smile.\n\nPerhaps they will learn to trick me someday, but not yet! (In his awful performance, I think my son takes after me. To this day, I have a terrible poker face — which is matched by my perfect lifetime record of losing every poker game I have ever played!)\n\nLast year, the paper “Are Emergent Abilities of Large Language Models a Mirage?” by Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo, which won a NeurIPS outstanding paper award, considered “emergent” properties of LLMs, which refers to capabilities that seem to appear suddenly as model sizes increase. The authors point out that scaling laws imply that the per-token error rate decreases (improves) slowly with scale, and emergent properties might be an artifact of researchers studying nonlinear or discontinuous metrics that transform a gradually decreasing per-token error rate into something that looks more like a step function.\n\nConsider a “combination lock” metric that requires getting many items right. Say we’re measuring the likelihood that an LLM will get 10 independent digits of an answer right. If the odds of it getting each digit right improve gradually from 0 to 1, then the odds of it getting all 10 digits right will appear to jump suddenly. But if we look at continuous metrics, such as the total number of correct digits, we will see that the underlying performance actually improves gradually. (Public perception of a technology can also shift in a discontinuous way because of social dynamics.)\n\nThis is why many of ussaw GPT-3 as a promising stepin transforming text processing long before ChatGPT appeared: BERT, GPT, GPT-2, and GPT-3 represented points on a continuous spectrum of progress. Or, looking back further in AI history, even though AlphaGo’s victory over Lee Sedol in the game of Go took the public by surprise, it actually represented many years of gradual improvements in AI’s ability to play Go.\n\nWhile analogies between human and machine learning can be misleading, I think that just as a person’s ability to do math, to reason — or to deceive — grows gradually, so will AI’s. This means the capabilities of AI technology will grow gradually (although I wish we could achieve AGI overnight!), and the ability of AI to be used in harmful applications, too, will grow gradually. As long as we keep performingred-teamingexercises and monitoring our systems’ capabilities as they evolve, I’m confident that we will have plenty of time to spot issues in advance, and the science-fiction fears of AI-initiated doomsday will remain science fiction.\n\nKeep learning!\n\nAndrew\n\nLearn to build AI systems that interact with video in “Multimodal RAG: Chat with Videos,” taught by Intel’s Vasudev Lal. Use multimodal embedding models to merge visual and text data, and build retrieval augmented generation (RAG) systems with LangChain and vector stores.Start today\n\nWaymo, the autonomous vehicle division of Alphabet, released ananalysisof its own safety data. It suggests that the company’s self-driving cars are safer than human drivers on the same roads.\n\nWhat’s new:Waymo’s analysis claims that its robotaxis, compared to human-driven vehicles, were involved in proportionally fewer accidents that involved police reports, passenger injuries, or airbag deployment. The company argues that these types of incidents are more relevant to assessing safety than minor collisions with no serious damage.\n\nHow it works:The study compares the number of incidents per mile experienced by Waymo vehicles and human drivers. It covers over 22 million miles driven along specific routes in Phoenix, Arizona, and San Francisco, California. The results were consistent in Phoenix and San Francisco.\n\nBehind the news:Waymo’s study arrives amid ongoing scrutiny of autonomous vehicle safety, particularly in San Francisco, where accidents and traffic disruptions caused by self-driving cars have raisedpublic backlash and regulatory challenges. Earlier this year, the state of CaliforniabannedCruise, a Waymo competitor, after one of its self-driving cars drove over a pedestrian and dragged her about 20 feet before coming to a stop.\n\nWhy it matters:Waymo’s analysis implies that autonomous vehicles could significantly reduce road accidents and injuries. The data could help urban planners to craft policies that would integrate autonomous vehicles into existing transportation systems.\n\nYes, but:Waymo’s analysis is based on methods and benchmarks introduced in tworesearchpapersthat have not yet been peer reviewed. Validating them through peer review would help to establish the safety record of self-driving cars.\n\nWe’re thinking:This report makes a compelling case for autonomous vehicles. But the question remains whether these findings will be sufficient to increase public trust. We encourage other self-driving companies to release comprehensive safety data.\n\nTraditionally, building 3D meshes for gaming, animation, product design, architecture, and the like has been labor-intensive. Now the ability to generate 3D meshes from a single image is widely available.\n\nWhat’s new:Two companies launched systems that produce a 3D mesh from one image. Stability AI releasedSF3D. Itsweightsandcodeare freely available to users with annual revenue under $1 million. Meanwhile, Shutterstocklauncheda service that provides a similar capability.\n\nHow it works:Stability AI’s SF3D generates output in a half-second, while Shutterstock’s service takes around 10 seconds.\n\nBehind the news:These releases arrived amid a flurry of recent works that aim to tackle similar problems. Most are based onLarge Reconstruction Model(LRM), proposed by Adobe in late 2023, which produces a 3D mesh and surface texture from a single image in less than 5 seconds. Follow-upworktrained LRM on real-world images in addition to the images of synthetic 3D meshes used in the original work and then reproduced LRM’s capabilities in anopen source model. Further research extended the model tolearn from generated videos. Stability AI’s new system addresses issues in its own previousworkthat was based on LRM.\n\nWhy it matters:SF3D replacesNeRF, a 2D-to-3D approach proposed in 2020 that serves as the basis for LRM and several other methods, with DMTet, which incorporates surface properties to achieve smoother meshes and better account for light reflecting off object surfaces.\n\nWe’re thinking:3D generation is advancing rapidly. To ignore this technology would be a mesh-take!\n\nThe European Union, United Kingdom, United States, and other countriessigneda legally binding treaty that regulates artificial intelligence.\n\nWhat’s new:The treaty, officially known as theFramework Convention on Artificial Intelligence and Human Rights, Democracy, and the Rule of Law, provides a legal framework for states to preserve democratic values while promoting AI innovation. It was negotiated by member nations of the Council of Europe (a transnational organization that promotes democracy and human rights and includes nearly twice as many countries as the EU) as well as observer states including Australia, Canada, and Mexico, which have not yet signed it. Countries that did not participate include China, India, Japan, and Russia.\n\nHow it works:The treaty will take effect later this year. It applies to any use of AI by signatories, private actors working on behalf of signatories, or actors in those jurisdictions. AI is broadly defined as any “machine-based system . . . [that generates] predictions, content, recommendations, or decisions that may influence physical or virtual environments.” The signatories agreed to do the following:\n\nExceptions:The treaty allows exceptions for national security and doesn’t cover military applications and national defense. It also doesn’t apply to research and development of AI systems that are not yet available for general use, unless testing such systems can interfere with human rights, democracy, or the rule of law.\n\nBehind the news:The Council of Europe oversees the European Convention on Human Rights and its Court of Human Rights in Strasbourg, France. Its AI treaty builds on previous initiatives including the European Union'sAI Act, which aims to regulate AI based on risk categories, and other national and international efforts like the United States’AI Bill of Rightsand the globalAI Safety Summit.\n\nWhy it matters:As the first binding international agreement on AI, the treaty can be enforced by signatories’ own laws and regulations or by the European Court of Human Rights. Since so many AI companies are based in the U.S. and Europe, the treaty may influence corporate practices worldwide. Its provisions could shape the design of deployed AI systems.\n\nYes, but:Like any regulation, the treaty’s effectiveness depends on the interpretation of its high-level concepts. Its core terms (such as accountability measures, democratic processes, oversight, privacy rights, and transparency) represent a broad framework, but their precise meaning  is vague and interpretation is left to the signatories. Also, the nonparticipation of major AI powers like China and large countries like Russia and India raises questions about whether its standards can be applied globally.\n\nWe’re thinking:The EU and U.S. have very different approaches to AI regulation; the EU has taken a much heavier hand. Yet both agreed to the treaty. This could indicate that these regions are finding common ground, which could lead to more uniform regulations internationally.\n\nDatasets that were scraped from the web tend to be unbalanced, meaning examples of some classes (say, cats) are plentiful while examples of others (say, caterpillars) are scarce. A model that’s trained on an unbalanced dataset will perform unevenly across classes, but the labor required to balance the data manually can be prohibitive. An automated method addresses such imbalances.\n\nWhat’s new:Huy V. Vo and colleagues at Meta, France’s National Institute for Research in Digital Science and Technology, Université Paris Saclay, and Google proposed amethodthat automatically selects a balanced subset of text or image datasets.\n\nKey insight:A naive way to balance a dataset automatically is to cluster it usingk-meansto define implicit categories and then draw an equal number of points randomly from the resulting clusters. But this approach tends to form many clusters in areas of the distribution that have more examples, leading to over-representation of certain categories. For instance, when the authors applied k-means to web images and associated the clusters with their nearest neighbors in ImageNet, around 300 clusters (out of 10,000) corresponded to the ImageNet class “website.” However, after clustering, the distribution of the centroids is a bit more uniform than that of the entire dataset. Applying k-means repeatedly distributes the centroids (and thus the clusters) more uniformly. After a number of iterations, each cluster is more likely to represent a distinct category, and selecting equal numbers of examples from each cluster makes a balanced dataset.\n\nHow it works:The authors balanced image and text datasets using several iterations of k-means clustering. Their image dataset started with 743 million examples from a “publicly available repository of crawled web data.” For text, they started withCCNet, a version ofCommon Crawlthat was filtered to match the distribution of language and topics found in Wikipedia. The following approach ensured balanced sampling from all levels, maintaining a balance among high-level classes (such as animal, vehicle, and sport) and lower-level subclasses (such as dog, airplane, and football):\n\nResults:Both vision and language models that were pretrained on the balanced data outperformed models that were pretrained on the corresponding unbalanced datasets.\n\nWhy it matters:The old-school machine learning algorithm k-means can organize quantities of pretraining data that are too large for manual inspection yet crucial to data-hungry models. Breaking down data into clusters also makes it possible to manually inspect cluster elements, which might help identify unwanted data.\n\nWe’re thinking:Even in the era of foundation models, data-centric AI — that is, systematically engineering the data used to train such models — remains a critical, often under-appreciated step. This paper offers a promising way to create more balanced datasets. The encouraging results suggest fruitful avenues for further study.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--10--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--9-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--5-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--7-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--8-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/09/unnamed--6-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-1/",
    "title": "issue 1",
    "date": "",
    "reading_time": "",
    "content": "I am writing to you from Colombia today, and am excited to announce the opening of our office in Medellín. The office will serve as the Latin American headquarters for three of the companies in our AI ecosystem:Landing AI,deeplearning.ai, andAI Fund.\n\nAI is still in its infancy. Although Silicon Valley and Beijing are currently leading the way in AI, with the UK and Canada also emerging as innovation hubs, there are still opportunities for every major country. Colombia is on a trajectory to become a hub of AI in Latin America.\n\nI am proud to bet on Colombia and support the growth of the Colombian AI community and the broader Latin American AI community. You can find additional detailshereor in Frederic Lardinois’ TechCruncharticle.\n\nKeep learning!Andrew\n\nAutomatic license plate readers capture thousands of vehicle IDs each minute, allowing law enforcement and private businesses to track drivers with or without their explicit consent. Fashion-forward freedom fighters are countering the algorithms with a line of shirts, dresses, and tops covered with images of license plates.\n\nWhat’s new:Security researcher and clothing designer Kate Rose unveiled her Adversarial Fashion line at the Defcon hacker convention. The garments are meant to foul automatic license plate readers by diluting their databases with noise.\n\nHow it works:Such readers typically use optical character recognition to capture lettering found in rectangular shapes they identify as license plates. But they aren’t picky about whether those rectangles are attached to a car.\n\nBehind the news:Use of automatic license plate readers grew by 3,000 percent over the past two years, according to a February article inQuartz. Companies like OpenALPR and PlateSmart Technologies have spurred the trend by marketing their systems to casinos, hospitals, and schools.\n\nWhy it matters:ALPR technology isn’t useful only for catching scofflaws who blow through red lights. Bad actors with access to license plate tracking data can stalk an individual’s movements, according to theElectronic Frontier Foundation. They can create databases of people who regularly visit sensitive locations like women’s health clinics, immigration centers, and union halls.\n\nWe’re thinking:Rose’s designs aren’t likely to have a practical impact unless they become a widespread geek craze (and in that case, makers of license plate readers will respond by building in an adversarial clothing detector). Their real effect may be to spur a public conversation about the worrying proliferation of automated surveillance technology.\n\nAlphabet subsidiary DeepMind lost $572 million in the past year, and its losses over the last three years amounted to more than $1 billion. AI contrarian Gary Marcus used the news as an opportunity to question the direction of AI as an industry.What’s new:In an essay published byWired, Marcus extrapolates DeepMind’s finances into an indictment of AI trends in recent years.\n\nThe blow-by-blow:He begins with a seeming defense of DeepMind, saying that the losses can be viewed as investments in cutting-edge research.\n\nBehind the news:Marcus is a longtime critic of deep learning. He published a 10-pointcritiqueof deep learning’s shortcomings last year. He is currently promoting a book,Rebooting AI, arguing that the AI community should reorder its priorities to accommodate approaches that mimic human intelligence. In June, he announced a new venture,robust.ai, with roboticist Rodney Brooks.Yes, but:As a tech company, Alphabet does well to invest in nascent technologies or risk being disrupted by them. As a public company, it has a fiduciary responsibility to do so. Moreover, DeepMind has achieved phenomenal successes at solvingGoandStarCraft IIand helped make Google’s data centers and Android devices run more efficiently.What they’re saying:The essay created a stir on social media.\n\nWe’re thinking:Marcus warns that investors may abandon AI if big investments like DeepMind don’t start providing returns. But some AI approaches already are having a huge economic impact, and emerging techniques like DRL new enough that it makes little sense to predict doom for all approaches based on slow progress in one. Better to save such double-barreled criticism for AI that is malicious or inept. We disagree with Marcus’ views on deep learning, but cheer him on as he codes, tests, and iterates his own way forward.\n\nLess than a month after XLNet overtook BERT, the pole position in natural language understanding changed hands again.RoBERTais an improved BERT pretraining recipe that beats its forbear, becoming the new state-of-the-art language model — for the moment.\n\nWhat’s new:Researchers at Facebook AI and from the University of Washington modifiedBERTto beat the best published results on three popular benchmarks.\n\nKey insight:Since BERT’s debut late last year, success in language modeling has been fueled not only by bigger models but also by an order of magnitude more data, more passes through the training set, and larger batch sizes. RoBERTa shows that these training choices can have a greater impact on performance than advances in model architecture.\n\nHow it works:RoBERTa uses the BERT LARGE configuration (355 million parameters) with an altered pretraining pipeline. Yinhan Liu and her colleagues made the following changes:\n\nResults:RoBERTa achieves state-of-the-art performance on GLUE without multi-task fine tuning, on SQuAD without additional data (unlike BERT and XLNet), and on RACE.\n\nYes, but:As the authors point out, the comparison would be fairer if XLNet and other language models were fine-tuned as rigorously as RoBERTa. The success of intensive fine-tuning raises the question whether researchers with limited resources can obtain state-of-the-art results in the problems they care about.\n\nWhy it matters:The authors show that rigorous tuning of hyperparameters and dataset size can play a decisive role in performance. The study highlights the importance of proper evaluation procedures for all new machine learning techniques.We’re thinking:Researchers are just beginning to assess the impact of hyperparameter tuning and data set size on complex neural network architectures at scale of 100 to 1,000 million parameters. BERT is an early beneficiary, and there’s much more exploration to be done.\n\nHow do you check the convergence of optimization algorithms like momentum, RMSprop, and mini-batch gradient descent? Learn how in theDeep Learning Specialization.\n\nThe U.S. federal government released a plan to develop technical standards for artificial intelligence, seeking to balance its aim to maintain the nation’s tech leadership and economic power with a priority on AI safety and trustworthiness.What’s new:Responding to a February executive order, the National Institute of Standards and Technology issued its roadmap for developing AI standards that would guide federal policy and applications. The 46-pagedocumentseeks to foster standards strict enough to prevent harm but flexible enough to drive innovation.\n\nWhat it says:The plan describes a broad effort to standardize in areas as disparate as terminology and user interfaces, benchmarking and risk management. It calls for coordination among public agencies, institutions, businesses, and foreign countries, emphasizing the need to develop trustworthy AI systems that are accurate, reliable, secure, and transparent.Yes, but:The authors acknowledge the risk of trying to corral such a dynamic enterprise. In fact, they admit that aren’t entirely sure how to go about it. “While there is broad agreement that these issues must factor into US standards,” they write, “it is not clear how that should be done and whether there is yet sufficient scientific and technical basis to develop those standards provisions.\n\nWe’re thinking:NIST’s plan drives a stake in the ground for equity, inclusivity, and cooperation. Here’s hoping it can bake those values — which is not to say specific implementations — into the national tech infrastructure and spread them abroad.\n\nImage-to-image translation, in which stylistic features from one image are imposed on the content of another to create a new picture, traditionally has been limited to translating either shapes or textures. A new network translates both, allowing more flexible image combinations and creating more visually satisfying output.\n\nWhat’s new:A team from Boeing’s South Korea lab createdU-GAT-IT, a network that produces superior translations between images.\n\nKey insights:Where earlier image-to-image translation networks work best with particular image styles, U-GAT-IT adds layers that make it useful across a variety of styles.\n\nHow it works:U-GAT-IT uses a typical GAN architecture: A discriminator classifies images as either real or generated and a generator tries to fool the discriminator. It accepts two image inputs.\n\nResults:Test subjects chose their favorite images from a selection of translations by U-GAT-IT and four earlier methods. The subjects preferred U-GAT-IT’s output by up to 73% in four out of five data sets.\n\nWhy it matters:Image-to-image translation is a hot topic with many practical applications. Professional image editors use it to boost image resolution and colorize black-and-white photos. Consumers enjoy the technology in apps like FaceApp.\n\nWe’re thinking:The best-performing deepfake networks lean heavily on image-translation techniques. A new generation that takes advantage of U-GAT-IT’s simultaneous shape-and-texture modeling may produce even more convincing fake pictures.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Newsletter082119Top.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/adversarial20fashion.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_deepmind.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Cropped20Roberta.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_CourseAd-DLSOptimizationAlgorithms.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_nist.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_style20320sized-1024x577.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-239/",
    "title": "issue 239",
    "date": "",
    "reading_time": "",
    "content": "Progress on LLM-based agents that can autonomously plan out and execute sequences of actions has been rapid, and I continue to see month-over-month improvements. Many projects attempt to take a task like “write a report on topic X” and autonomously take actions such as browsing the web to gather information to synthesize a report.AI agents can be designed to take many different types of actions. Research agents (like many projects built onAutoGPT,GPTresearcher, orSTORM) search the web and fetch web pages. A sales representative agent might dispatch a product to a user. An industrial automation agent might control a robot.\n\nSo far, I see agents that browse the web progressing much faster because the cost of experimentation is low, and this is key to rapid technical progress. It’s cheap to fetch a webpage, and if your agent chooses poorly and reads the wrong page, there’s little harm done. In comparison, sending a product or moving a physical robot are costly actions, which makes it hard to experiment rapidly. Similarly, agents that generate code (that you can run in a sandbox environment) are relatively cheap to run, leading to rapid experimentation and progress.Although today’s research agents, whose tasks are mainly to gather and synthesize information, are still in an early phase of development, I expect to see rapid improvements. ChatGPT, Bing Chat, and Gemini can already browse the web, but their online research tends to be limited; this helps them get back to users quickly. But I look forward to the next generation of agents that can spend minutes or perhaps hours doing deep research before getting back to you with an output. Such algorithms will be able to generate much better answers than models that fetch only one or two pages before returning an answer.Even when experimentation is quick, evaluation remains a bottleneck in development. If you can try out 10 algorithm variations quickly, how do you actually pick among them? Using an LLM to evaluate another LLM's output is common practice, but prompting an LLM to give very accurate and consistent evaluations of text output is a challenge. Any breakthroughs here will accelerate progress!\n\nAn exciting trend has been a move toward multi-agent systems. What if, instead of having only a single agent, we have one agent to do research and gather information, a second agent to analyze the research, and a third to write the final report? Each of these agents can be built on the same LLM using a different prompt that causes it to play a particular, assigned role. Another common design pattern is to have one agent write and a second agent work as a critic to give constructive feedback to the first agent to help it improve. This can result in much higher-quality output. Open-source frameworks like Microsoft’sAutoGen,Crew AI, andLangGraphare making it easier for developers to program multiple agents that collaborate to get a task done.I’ve been playing with many agent systems myself, and I think they are a promising approach to architecting intelligent systems. A lot of progress has been made by scaling up LLMs, and this progress no doubt will continue. But big ideas are sometimes made up of many, many little ideas. (For example, you might arrive at an important mathematical theorem via lots of little derivation steps.) Today’s LLMs can reason and have lots of “little ideas” in the sense that they take in information and make basic inferences.Chain-of-thought promptingshows that guiding an LLM to think step-by-step — that is, to string together many basic inferences — helps it to answer questions more accurately than asking it to leap to a conclusion without intermediate steps.\n\nAgent programming models are a promising way to extend this principle significantly and guide LLMs to have lots of little ideas that collectively constitute bigger and more useful ideas.\n\nKeep learning!Andrew\n\nP.S. New short course: “Open Source Models with Hugging Face,” taught by Maria Khalusova, Marc Sun, and Younes Belkada! Hugging Face has been a game changer by letting you quickly grab any of hundreds of thousands of already-trained open source models to assemble into new applications. This course teaches you best practices for building this way, including how to search and choose among models. You’ll learn to use the Transformers library and walk through multiple models for text, audio, and image processing, including zero-shot image segmentation, zero-shot audio classification, and speech recognition. You’ll also learn to use multimodal models for visual question answering, image search, and image captioning. Finally, you’ll learn how to demo what you build locally, on the cloud, or via an API using Gradio and Hugging Face Spaces.Please sign up here\n\nEuropean AI champion Mistral AI unveiled new large language models and formed an alliance with Microsoft.\n\nWhat’s new:Mistral AIintroducedtwo closed models, Mistral Large and Mistral Small (joining Mistral Medium, which debuted quietly late last year). Microsoft invested $16.3 million in the French startup, and itagreedto distribute Mistral Large on its Azure platform and let Mistral AI use Azure computing infrastructure. Mistral AI makes the new models available to try for freehereand to use on itsLa Plateformeand via custom deployments.\n\nModel specs:The new models’ parameter counts, architectures, and training methods are undisclosed. Like the earlier, open source Mistral 7B and Mixtral 8x7B, they can process 32,000 tokens of input context.\n\nBehind the news:Mistral AI was founded in early 2023 by engineers from Google and Meta. The French government has touted the company as a home-grown competitor to U.S.-based leaders like OpenAI. France’s representatives in the European Commissionarguedon Mistral’s behalf to loosen the European Union’s AI Act oversight on powerful AI models.\n\nYes, but:Mistral AI’s partnership with Microsoft has divided European lawmakers and regulators. The European Commission, which already wasinvestigatingMicrosoft’s agreement with OpenAI for potential breaches of antitrust law,plansto investigate the new partnership as well. Members of President Emmanuel Macron’s Renaissance partycriticizedthe deal’s potential to give a U.S. company access to European users’ data. However, other French lawmakerssupportthe relationship.\n\nWhy it matters:The partnership between Mistral AI and Microsoft gives the startup crucial processing power for training large models and greater access to potential customers around the world. It gives the tech giant greater access to the European market. And it gives Azure customers access to a high-performance model that’s tailored to Europe’s unique regulatory environment.\n\nWe’re thinking:Mistral AI has made impressive progress in a short time, especially relative to the resources at its disposal as a startup. Its partnership with a leading hyperscaler is a sign of the tremendous processing and distribution power that remains concentrated in the large, U.S.-headquartered cloud companies.\n\nA robot outperformed human chemists at synthesizing chemicals.\n\nWhat’s new:Researchers at University of Amsterdam builtRoboChem, an integrated robotic system that learned to design light-activated chemical reactions while achieving optimal yields and throughput.\n\nHow it works:RoboChem includes a computer that runs a machine learning model and a set of automated lab instruments including a liquid handler, syringe pumps, and a photochemical reactor, all enclosed in an airtight vacuum chamber. Given a set of reagents and resulting product, RoboChem aimed to find conditions that maximize the yield (the ratio of the amount of a product synthesized to the potential amount, expressed as a percentage) and throughput (rate of synthesis) in the fewest experimental runs. It followed a 3-part cycle: (i) determine experimental conditions (amounts and concentrations of the given reagents, intensity of light, and time spent in the reactor), (ii) combine the reagents under those conditions, and (iii) evaluate the yield and throughput via a spectrometer.\n\nResults:Robochem executed reactions to produce 18 substances. In all cases, it found experimental conditions that had either higher throughput and yield, or higher throughput and nearly equivalent yield, than the best conditions previously known. In one reaction, RoboChem achieved yield of 58 percent and throughput of 95.6 g/Lh (gram yield per liter in the reactor per hour), while previous work had achieved 45 percent and 2.8 g/Lh. In another reaction, RoboChem achieved 81 percent and 1720 g/Lh, where previous best results achieved 82 percent and 3 g/Lh — 1 percent lower yield but 573 times greater throughput.\n\nBehind the news:In 2020, researchers at the University of Liverpooltraineda mobile robot arm to navigate a chemistry lab, mix chemicals, and operate equipment. That robot used a similar optimization method. However, the Amsterdam robot is much less expensive and proved itself in a wider range of experiments.\n\nWhy it matters:The authors believe that RoboChem could dramatically increase lab productivity at lower cost in time and money. The light-activated reactions they focused on have applications in fields including pharmaceuticals, household chemicals, and renewable energy.We’re thinking:These researchers clearly are in their element.\n\nIn “Open Source Models with Hugging Face,” our latest short course, you’ll use open source models to build chatbots, language translators, and audio narrators using Hugging Face tools like the model hub, transformers library, Spaces, and Gradio.Join now\n\nGoogle asserted its open sourcebona fideswith new models.\n\nWhat’s new:Googlereleasedweights for Gemma-7B, an 8.5 billion-parameter large language model intended to run GPUs, and Gemma-2B, a 2.5 billion-parameter version intended for deployment on CPUs and edge devices. Each size isavailablein two versions: pretrained base model and one fine-tuned to follow instructions.\n\nHow it works:Gemma models arebasedon the architecture used in Google’s larger Gemini. Unlike Gemini, they’re not multimodal.\n\nBehind the news:Google has a rich history of open source AI projects including AlphaFold, TensorFlow, several versions of BERT and T5, and the massive Switch. Lately, though, its open source efforts have been overshadowed by open large language models (LLMs) from Meta, Microsoft, and Mistral.ai. LLMs small enough to run on a laptop have opened open source AI to an expanding audience of developers.\n\nWhy it matters:Gemma raises the bar for models of roughly 7 billion parameters. It delivers exceptional performance in a relatively small parameter counts, expanding the options for developers who are building on top of LLMs.\n\nWe’re thinking:Gemma confirms Google’s commitment to open source and outperforms top open models of equal size. It’s likely to spur further innovation, especially in AI foredge devices, and keep the Google name in front of enterprising open source developers.\n\nLarge language models are not good at math. Researchers devised a way to make them better.\n\nWhat's new:Tiedong Liu and Bryan Kian Hsiang Low at the National University of Singapore proposed a method tofine-tune large language models for arithmetic tasks.\n\nKey insight:Large language models (LLMs) do fairly well at addition and subtraction as well as multiplication and division by single digits or by powers of 10. They’re less adept at the more challenging tasks of multiplication and division of larger numbers. One way to perform these tasks well is to divide them into simpler subtasks. For example, a relatively easy way to multiply two large numbers like 123 and 321 is to\n\nA similar technique exists for division. Together, these approaches can enable LLMs to perform more complicated mathematical tasks.\n\nHow it works:The authors built GOAT (a model GOod at Arithmetic Tasks) by fine-tuningLLaMAon a synthetic dataset that comprised 1 million examples of arithmetic operations on integers that were divided into steps for easier calculation.\n\nResults:The authors compared GOAT and GPT-4 onBIGBench, which contains arithmetic operations on integers up to five digits. GOAT performed either on par with or better than GPT-4 for all operations. Specifically, GPT-4 struggled to multiply and divide large numbers. Multiplying 5-digit numbers, GPT-4 achieved 0 percent accuracy, while GOAT achieved 96.7 percent. Dividing five-digit numbers, GPT-4 achieved 53.4 percent, while GOAT achieved 96.5 percent. GOAT also performed better than other LLMs (Bloom, GPT-NeoX, OPT, and Pythia) that had been fine-tuned in the same way. The authors attribute this to the fact that LLaMA generates a separate token for each digit (and does not learn tokens that represent multiple digits), while the other models learn tokens for multiple digits (for example, separate tokens for 748, 74, and 7).\n\nWhy it matters:LLMs have latent mathematical knowledge that can be unlocked by thoughtful fine-tuning.\n\nWe’re thinking:Humans, too, aren’t great at multiplying or dividing numbers directly — but give us a pencil and paper so we can work things out step by step, and we’re much better.\n\nMore AI news of the week includes:\n\n🔊 Adobe previews an AI audio tool💰 Microsoft launches Copilot for Finance🐋 AI uncovers reasons behind humpback whale deaths\n\nRead Data Points here.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/03/MULTIAGENTS2-1.png",
      "https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-06T161635.694.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-06T161718.283.png",
      "https://dl-staging-website.ghost.io/content/images/2024/03/The-Batch-ads-and-exclusive-banners---2024-03-05T085620.866.png",
      "https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-06T161846.471.png",
      "https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-06T161921.552-1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-94/",
    "title": "issue 94",
    "date": "",
    "reading_time": "",
    "content": "In school, most questions have only one right answer. But elsewhere, decisions often come down to a difficult choice among imperfect options. I’d like to share with you some approaches that have helped me make such decisions.\n\nWhen I was deciding where to set up a satellite office outside the U.S., there were many options. My team and I started by listing important criteria such as supply of talent, availability of local partners, safety and rule of law, availability of visas, and cost. Then we evaluated different options against these criteria and built a matrix with cities along one axis and our criteria along the other. That clarified which country would make a great choice.\n\nWhen I feel stuck, I find it helpful to write out my thoughts:\n\nDocumenting decisions in this way also builds a foundation for further choices. For example, over the years, I’ve collected training data for many different kinds of problems. When I need to select among tactics for acquiring data, having been through the process many times, I know that some of the most important criteria are (i) the time needed, (ii) the number of examples, (iii) accuracy of the labels, (iv) how representative the input distribution is, and (v) cost.\n\nIf I’m making a decision as part of a team, I check with teammates at each step to make sure we’re accurately capturing the top options, criteria, and so on. (The comments feature in Google Docs is a great way to facilitate open debate within a team.) This helps me avoid losing track of some criteria and acting based on an incomplete set; for example, picking the satellite office’s location based only on the availability of talent. It also helps align everyone on the final decision.\n\nAs you may know, I wound up setting up a satellite office in Colombia because of the availability of talent and a supportive ecosystem of partners. The team there has become a key part of many projects. Lately I’ve worried about their wellbeing amid Covid-19 and widespread unrest. But in hindsight, setting up in Colombia was one of my best decisions, and I remain as committed as ever to supporting my friends there.\n\nKeep learning!Andrew\n\nAutonomous weapons are often viewed as an alarming potential consequence of advances in AI — but they may already have been used in combat.\n\nWhat’s new:Libyan forces unleashed armed drones capable of choosing their own targets against a breakaway rebel faction last year, said a recent United Nations (UN)report. The document, a letter from the organization’s Panel of Experts on Libya to the president of the Security Council, does not specify whether the drones targeted, attacked, or killed anyone. It was brought to light byNew Scientist.\n\nKiller robots:In March of 2020, amid Libya’s ongoing civil war, the UN-supported Government of National Accord allegedly attacked retreating rebel forces usingKargu-2quadcopters manufactured by Turkish company STM.\n\nBehind the news:Many nations use machine learning in their armed forces, usually to bolster existing systems, typically with a human in the loop.\n\nWhy it matters:Observers have long warned that deploying lethal autonomous weapons  on the battlefield could ignite an arms race of deadly machines that decide for themselves who to kill. Assuming the UN report is accurate, the skirmish in Libya appears to have set a precedent.\n\nWe’re thinking:Considering the problems that have emerged in using today’s AI for critical processes like deploying police, sentencing convicts, and making loans, it’s clear that the technology simply should not be used to make life-and-death decisions. We urge all nations and the UN to developrulesto ensure that the world never sees a real AI war.\n\nResearchers have proposed task-agnostic architectures forimage classification tasksandlanguage tasks. New work proposes a single architecture for vision-language tasks.What’s new:Led by Tanmay Gupta, researchers at the Allen Institute for AI and University of Illinois at Urbana-Champaign designed a general-purpose vision architecture and built a system,GPV-I, that can perform visual question answering, image captioning, object localization, and image classification.Key insight:Model architectures usually are designed for specific tasks, which implies certain types of output. To classify ImageNet, for instance, you need 1,000 outputs, one for each class. But text can describe both tasks and outputs. Take classification: the task “Describe this image” leads to the output, “this image is a dog.” By generating a representation of text that describes a task, a model can learn to perform a variety of tasks and output text that completes it without task-specific alterations in its architecture.How it works:Given a text description of a task — say, “describe the image” — and an image, GPV-I generates separate representations of the text and image, determines their relative importance to one another, and outputs a relevant text response and a copy of the image with bounding boxes. The authors trained it onCOCOimage captioning,VQAquestion answering, andRefCOCO+object localization datasets.\n\nResults:The researchers evaluated GPV-I on COCO classification, COCO captioning, and VQA question answering. They compared its performance with models trained for those tasks. On classification, GPV-I achieved accuracy of 83.6 percent, while a ResNet-50 achieved 83.3 percent. On captioning, GPV-I achieved 1.023CIDEr-D— a measure of the similarity of generated and ground-truth captions, higher is better — compared to aVLP’s 0.961 CIDEr-D. On question answering, GPV-I achieved 62.5 percent accuracy compared to ViLBERT’s score of 60.1 percent, based on the output’s similarity to a human answer.Why it matters:A single architecture that can learn several tasks should be able to share concepts between tasks. For example, a model trained both to detect iguanas in images and to answer questions about other topics might be able to describe what these creatures look like even if they weren’t represented in the question-answering portion of the training data.We’re thinking:Visual classification, image captioning, and visual questioning answering are a start. We look forward to seeing how this approach performs on more varied tasks.\n\nAI lately has achieved dazzling success interpreting X-rays and other medical imagery in the lab. Now it’s catching on in the clinic.\n\nWhat’s new:Roughly one-third of U.S. radiologists use AI in some form in their work, according to asurveyby the American College of Radiology. One caveat: Many who responded positively may use older — andquestionable— computer-aided detection, a technique for diagnosing breast cancer that dates to the 1980s, rather than newer methods.\n\nWhat they found:The organization queried its membership via email and received 1,861 responses.\n\nBehind the news:AI’s role in medical imaging is still taking shape, asdetailedby Stanford radiology professor Curtis Langlotz in the journalRadiology: Artificial Intelligence. In 2016, a prominent oncologistwrotein theNew England Journal of Medicine, “machine learning will displace much of the work of radiologists.” Two years later,Harvard Business Reviewpublished a doctor-penned essay headlined, “AI Will Change Radiology, but It Won’t Replace Radiologists.”Radiology Businessrecently asked, “Will AI replace radiologists?” andconcluded, “Yes. No. Maybe. It depends.”\n\nWhy it matters:AI’s recent progress in medical imaging is impressive. Although the reported 30 percent penetration rate probably includes approaches that have been uses for decades, radiologists are on their way to realizing the technology’s promise.\n\nWe’re thinking:One-third down, two-thirds to go! Machine learning engineers can use such findings to understand what radiologists need and develop better systems for them.\n\nWe’re proud to launchPractical Data Science, in partnership with Amazon Web Services (AWS)! This new specialization will help you develop the practical skills to deploy data science projects effectively and overcome machine learning challenges using Amazon SageMaker.Enroll now\n\nTesla is abandoning radar in favor of a self-driving system that relies entirely on cameras.\n\nWhat’s new:The electric car makerannouncedit will no longer include radar sensors on Model 3 sedans and Model Y compact SUVs sold in North America. Tesla is the only major manufacturer of autonomous vehicles to bet solely on computer vision. Most others rely on a combination of lidar, radar, and cameras.\n\nHow it works:Tesla has dropped radar only in the U.S. and only in its two most popular models. It aims to gather data and refine the technology before making the change in Model S, Model X, and vehicles sold outside the U.S.\n\nBehind the news:Some people in the self-driving car industry favor using relatively expensive lidar and radar sensors in addition to low-cost cameras because they provide more information and thus greater safety. Camera-only advocates counter that humans can drive safely perceiving only images, so we should build AI that does the same. Most companies working on autonomous vehicles have chosen the more expensive route  as the fastest way to reach full autonomy safely. Once they get there, the thinking goes, they can attend to bringing the cost down.\n\nWhy it matters:If Tesla’s bet on cameras pays off, it could have an outsize influence on future self-driving technology.\n\nWe’re thinking:While it’s great to see ambitious plans to commercialize computer vision, Tesla’s initiative will require tests on public streets. That means countless drivers will be the company’s unwitting test subjects — a situation that, as ever, demands strong oversight by road-safety authorities.\n\nTransformer models trained on sequences of amino acids that form proteins have had successclassifyingandgeneratingviable sequences. New research shows that they also capture information about protein structure.\n\nWhat’s new:Transformers can encode the grammar of amino acids in a sequence the same way they do the grammar of words in a language. Jesse Vig and colleagues at Salesforce Research and University of Illinois at Urbana-Champaign developedmethodsto interpret such models that reveal biologically relevant properties.\n\nKey insight:When amino acids bind to one another, the sequence folds into a shape that determines the resulting protein’s biological functions. In a transformer trained on such sequences, a high self-attention value between two amino acids can indicate that they play a significant role in the protein’s structure. For instance, the protein’s folds may bring them into contact.\n\nHow it works:The authors studied aBERTpretrained on adatabase of amino acid sequencesto predict masked amino acids based on others in the sequence. Given a sequence, they studied the self-attention values in each layer of the model.\n\nResults:The authors compared their model’s findings with those reported in otherproteindatabases. The deeper layers of the model showed an increasing proportion of related pairs in which the amino acids actually were in contact, up to 44.7 percent, while the proportion of all amino acids in contact was 1.3 percent. The chance that the second amino acid in a related pair was part of a binding site didn’t rise steadily across layers, but it reached 48.2 percent, compared to a 4.8 percent chance that any amino acid was part of a binding site.\n\nWhy it matters:A transformer model trained only to predict missing amino acids in a sequence learned important things about how amino acids form a larger structure. Interpreting self-attention values reveals not only how a model works but also how nature works.\n\nWe’re thinking:Such tools might provide insight into the structure of viral proteins, helping biologists discover ways to fight viruses including SARS-CoV-2 more effectively.\n\nJoin usfor a live virtual event on June 9, 2021! Experts fromOmdenawill walk through two AI case studies: “Real Life: Understanding the Causes and Effects of Student Debt through Machine Learning” and “AI for Energy: Transitioning Toward a Sustainable Energy System.”",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-06-01-at-5.57.29-PM-copy--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2021/06/Kargu-Redo-2.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/06/ezgif.com-gif-maker---2021-05-04T162716.343-2.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/06/radiology-redo-2.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/06/Specialization-Name--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2021/06/tesla-redo_orange-background-2.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/06/PROTEIN-1-2.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/06/The-Batch-Image--3--1.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-203/",
    "title": "issue 203",
    "date": "",
    "reading_time": "",
    "content": "Suddenly it seems like everyone wants to regulate AI. The European Union is on the verge of enacting a comprehensive AI Act that’s intended to mitigate risks and protect individual rights. In the United States, Senate Majority leader Chuck Schumer foresees legislation possibly within months.\n\nI’m in favor of regulation, too. But I’m very concerned about whether we’re on a trajectory toward helpful and effective regulation. At the moment, few regulators have sufficient understanding of AI’s potential benefits and harms to craft effective laws.The only thing more dangerous than knowing too little is knowing too little without understanding just how little that is.\n\nI’m glad regulators are seeking to learn more about AI (as you can read about below). This is a wonderful step! But I see a dangerous situation emerging in which regulators speak with a number of academic and business leaders and come away thinking they understand things well enough. At best, only a few people in the world have the information to answer questions such as:\n\nAnswering questions like these requires far greater visibility into large AI companies than we currently have. In many countries, publicly traded companies are required to make substantial financial disclosures. Companies may find these requirements intrusive or burdensome, but the resulting transparency builds trust in the financial system. Similarly, the countries of the world need to compel large AI companies to disclose their activities in detail.\n\nWhile the details of any required disclosure need to be worked out, I can imagine, for example, requiring large companies to analyze, or allow independent organizations to analyze, how much content of different flavors (such as pro/con various social issues) they deliver to different subsets of their audience (such as users in a particular region or demographic group). By presenting aggregate results, this can be done in a way that preserves individual privacy. Information like this would enable regulators to draw a straight line between the technology and events in the world. Without it, governments won’t know enough to craft sound regulations.\n\nAI is making society richer, and governments have an important role in maximizing its benefits and minimizing its harms. But until there is greater transparency, it will be difficult for lawmakers to recognize the technology’s impacts in either direction. It will be difficult to prevent lobbyists from steering legislation to block competitors or otherwise further their interests in ways that don’t align with society’s.\n\nI have deep respect for democratically elected legislators and the important work they do. I hope that all of us in AI — especially the many engineers and scientists who want to make the world better for everyone — can engage to help regulators play a constructive role in AI’s advance.\n\nKeep learning!\n\nAndrew\n\nP.S. We just launched “Generative AI with Large Language Models,” a course built in collaboration with Amazon Web Services. Gain hands-on practice with techniques like reinforcement learning from human feedback; zero-, few-, and one-shot learning; fine-tuning; and advanced prompting using ReAct. You can sign uphere.\n\nThe crowdworkers you hire to provide human data may use AI to produce it.\n\nWhat's new:Researchers at École Polytechnique Fédérale de Lausannefoundthat written material supplied by workers hired via Amazon Mechanical Turk showed signs of being generated by ChatGPT.\n\nHow it works:44 Mechanical Turk workers summarized medical research abstracts in roughly 100 words. The authors analyzed each summary for evidence that it had been generated by ChatGPT. The analysis relied on two methods:\n\nResults:The authors analyzed 46 summaries written by 44 workers. The classifier found 21 summaries that showed 50 percent or greater likelihood of having been written by ChatGPT and 15 summaries that showed at least a 98 percent or greater likelihood. 41 of the summaries involved copying and pasting.\n\nYes, but:The researchers studied 46 summaries, a rather small sample. Furthermore, summarization is labor-intensive for humans but well within the capabilities of large language models. Other crowdsourced tasks may not be so easy to automate.\n\nBehind the news:Mechanical Turk, founded by Amazon in 2005, has played anoutsizerole in machine learning. Many of the field’s most important datasets includingImageNetemployed crowdsourced labor.\n\nWhy it matters:Machine learning engineers often use services like Mechanical Turk to collect and annotate training data on the assumption that humans are doing the work. If a significant number of crowdworkers instead rely on AI, it raises questions about the quality of the data and the validity of the output from models trained on it. Recentworkfound that, as the amount of model-generated content in a training set increases, the trained model’s performance decreases.\n\nWe're thinking:Training on machine-generated data seems likely to affect model performance unless you’re training a smaller model to mimic a larger one (known as model distillation). For example, it’s hard to imagine a language model trained only on the output of ChatGPT surpassing ChatGPT, whereas one trained on human data might. The lack of transparency with respect to which data comes from humans and which comes from machines presents a huge challenge for AI practitioners.\n\nWhile Microsoft and Google scramble to supercharge their businesses with text generation, Meta has yet to launch a flagship generative AI service. Reporters went looking for reasons why.\n\nWhat’s new:Staff turnover, misaligned priorities, insufficient processing power, and caution in the wake of earlier controversies have hindered Meta’s ability to take advantage of generative AI,The Wall Street Journalreported.Challenges:Reporters spoke to more than a dozen current and former Meta employees to determine why, despite extensive investments in large language models (LLMs) and vision models like DINOv2 and SAM, the company lacks a high-profile generative initiative. They pointed to several factors:\n\nReorganization:Meta has taken steps to break the logjam. Earlier this month, itannounceda number of generative AI products including chatbots for Messenger and WhatsApp, a photo editor for Instagram, and a productivity assistant for internal use. In February, Meta CEO Mark Zuckerburgannounceda new generative AI group that reports directly to chief product officer Chris Cox. The group will focus on training models to integrate with products such as Facebook, Instagram, and WhatsApp.\n\nWhy it matters:The rapid rise of generative AI threatens to upend the tech world’s established order. Meta — like Google in response Microsoft’s aggressive launch of Bing Chat — has found itself in a defensive position.We’re thinking:OpenAI developed breakthrough technology using a focused team of hundreds, and since then, several organizations have restructured from handfuls of researchers who work on diverse projects to large, focused teams that include both researchers and engineers. Although this shift prompted many researchers to leave in search of freedom to pursue their interests, the focused structure strikes us as a more promising approach from a business point of view.\n\nMaster the technology behind large language models and learn how to fine-tune and use them to power real-world applications. Join us for “Generative AI with Large Language Models,” a new course developed in collaboration with AWS.Enroll now!\n\nUnited States lawmakers are getting a crash course in AI.\n\nWhat’s new:Chuck Schumer, the majority leader in the U.S. Senate, announced an unusual plan to educate legislators who are crafting AI regulations,The New York Timesreported. It could lead to legislation “within months,” he said.How it works:The senator calls his program SAFE Innovation, an acronym for four regulatory priorities: security, accountability, foundations, and explain [sic].\n\nBehind the news:Schumer’s move reflects growing interest in regulating AI among U.S. lawmakers.\n\nYes, but:Any proposal must overcome fundamental disagreements between the two major parties, especially over whether a new, dedicated agency should oversee AI or whether that can be left to existing agencies. Moreover, some observers worry that Schumer’s deliberative approach could slow down legislative efforts that are already underway.Why it matters:Thoughtful AI regulations must strike a delicate balance between encouraging innovation and protecting the public. It’s imperative that lawmakers — few of whom have a background in technology or science — understand the nuances.\n\nWe’re thinking:U.S. politics are increasingly divided. Bipartisan listening sessions on AI may serve a dual goal of educating lawmakers and uniting them around a shared vision.\n\nFine-tuning a neural network typically involves retraining every layer on new data. But research shows that networks may perform better when fine-tuning modifies only a subset of layers.\n\nWhat’s new:Yoonho Lee, Annie S. Chen, and colleagues at Stanford demonstratedsurgical fine-tuning, a method that chooses specific layers to modify depending on how the fine-tuning dataset differs from the pretraining data.\n\nKey insight:Earlier layers in a neural network learn to produce representations of fundamental features of the input, such as edges and shapes in an image, while later layerscombinethese features in a way that contributes to predicting a desired output, such as the image’s label. During fine-tuning, if the new images differ from the pretraining images in appearance, only earlier layers require modification. If the new images resemble the pretraining images but differ in their labels, only later layers require modification. Fine-tuning the appropriate layers updates a network effectively by prioritizing the weights most relevant to the new data.\n\nHow it works:The authors fine-tuned aResNet-26model pretrained onCIFAR-10using manual and automated approaches.\n\nResults:Evaluated onCIFAR-C, a version of the CFAR dataset deliberately corrupted by noise, the authors’ manual method classified images with 82.8 percent accuracy, while fine-tuning the whole network achieved 79.9 percent accuracy. The automated approach achieved 81.4 percent.\n\nWhy it matters:The authors drew on knowledge of how the neural networks process input to propose an efficient fine-tuning method. Better understanding of how a networkextractsfeatures could yield further ways to improve machine learning models.\n\nWe’re thinking:On datasets more complex than CIFAR-C, it can be hard to judge the difference between a pretraining dataset and a fine-tuning dataset. This may make the authors’ automated approach more valuable, even though it didn’t yield the best results.\n\nDisney+ TV series faces backlash over use of AIMarvel seriesSecret Invasionused an AI-generated title card in its intro. Artists are labeling this decision unethical and dangerous to the artistic community. (Venture Beat)ChatGPT-powered device allows trees to “talk” to peopleThe TreeTag, a device that helps monitor a tree's vital signs, integrates generative AI to enable the tree to communicate in text form its condition and needs. (CNET)U.S. workers and employers out of sync on new technologyA survey found that employees are enthusiastic about emerging technologies. However, these technologies become outdated by the time their company adopts them. The age difference between younger employers and older managersmight account for late adoption. (Ernst & Young)Google is warning employees over chatbot useThe tech giant reportedly advised its employees not to share confidential information with chatbots, including its own, as user input to chatbots is stored by the companies who own the technology. (ZDNet)OpenAI lobbied EU to revise proposed AI regulationThe company behind ChatGPT attempted to soften a draft of the AI Act, which would impose stringent restrictions on general-purpose AI systems. Legislators integrated some of the suggested changes into the draft. (The Verge)Chatbot leads church service in GermanyChatGPT, personified by an avatar on a screen, delivered a 40-minute service at a convention of Protestants. The chatbot generated 98 percent of the sermon. (AP News)OpenAI considers launching an app storeThe company is reportedly contemplating a marketplace for users to sell customized AI models. (The Information)You.com introduced  subscription to AI productsYou.com, which offers a personalized search engine, launched a paid service called YouPro. It provides unlimited access to a suite of text and image generation tools based on the latest models for $9.99/month. (Business Wire)China's black market defies U.S. ban on chip exportsUnderground vendors in Shenzen are evading U.S. restrictions on chip exports to China. They typically sell A100 Nvidia chips at double the usual price. (Reuters)Chatbots occupy trusted jobs in IndonesiaAI avatars are working as immigration officers, news anchors in the world’s fourth most populous country. (Rest of World)Research: AI Reveals new figures in Nazca LinesScientists used a deep learning model to scan aerial photographs captured in Peru’s Nazca Desert. They discovered three new geoglyphs, or huge line drawings cut into the desert floor thousands of years ago. (Live Science)Google launched an anti-money-laundering toolThe tool accurately detected two to four times more incidents than earlier methods and reduced alert volumes by 60 percent in tests conducted with HSBC. (The Wall Street Journal)AI models are failing to meet proposed EU regulationsResearchers assessed whether 10 popular AI models meet standards outlined in the EU’s draft AI Act, a comprehensive AI regulation that is expected to become law by early next year. The models evaluated include BLOOM, GPT-4, LLaMA, and Stable Diffusion. All ten fell short in various areas, and six scored below 50 percent. (Financial Times)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/06/unnamed--32--1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/06/unnamed--20-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/06/unnamed--21-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/06/The-Batch-ads-and-exclusive-banners--36-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/06/unnamed--22-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/06/FINETUNINGv2--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-134/",
    "title": "issue 134",
    "date": "",
    "reading_time": "",
    "content": "Last week, DeepLearning.AI invited a group of learners to our Palo Alto office’s courtyard. We had a good time chatting about paths into AI, career trajectories, applications people were working on, and challenges they were facing. You can see the group below.A few people mentioned the challenge of persuading others to try a machine learning solution. Even at leading tech companies, it’s not uncommon for someone to say, “Yes, machine learning may work well for other applications, but for what we’re doing, non-learning software works fine.”\n\nStill, machine learning might work better. If you believe that a learning algorithm can help optimize server allocations, improve product recommendations, or automate some part of a business process, how can you push your idea forward?\n\nHere are some tips that have worked for me:\n\nThroughout this process, be open to learning that your idea isn’t sound after all or that it might need to change before it can be successful. I would guess that almost every successful AI application you read about in The Batch required someone to persuade others to give machine learning a shot.\n\nDon’t let the skeptics shut you down. Don’t give up, keep pushing, and . . .\n\nKeep learning!\n\nAndrew\n\nMachine learning algorithms may have unmasked the authors behind a sprawling conspiracy theory that has had a wide-ranging impact on U.S. politics.What’s new:Two research teams analyzed social media posts to identify Q, the anonymous figure at the center of a U.S. right-wing political movement called QAnon,The New York Timesreported. Inspired by Q’s claims that U.S. society is run by a Satanic cabal, QAnon members havecommittedacts of violence. Some U.S. politicians haveexpressedsupport for the movement.CommuniQués:Q posted over three years starting on the website 4chan in October 2017 before migrating later that year to 8chan, which later shut down and relaunched as 8kun. Q stopped posting in December 2020.Elements of style:Swiss text-analysis firm OrphAnalyticsclusteredQ’s posts to track changes in authorship over time.\n\nMeet the authors:Florian Cafiero and Jean-Baptiste Camps at École Nationale des Chartesbuiltsupport vector machines (SVMs) to classify various authors as Q or not Q.\n\nYes, but:Both Furber and Watkins denied writing as Q toThe New York Times.\n\nWhy it matters:QAnon’s claims have beendebunkedby numerous fact-checkers, yet a 2022 surveyfoundthat roughly one in five Americans agreed with at least some of them. The movement’s appeal rests partly on the belief that Q is an anonymous government operative with a high-level security clearance. Evidence that Q is a pair of internet-savvy civilians may steer believers toward more credible sources of information.We’re thinking:Machine learning offers an evidence-based way to combat disinformation. To be credible, though, methods must be openly shared and subject to scrutiny. Kudos to these researchers for explaining their work.\n\nYour next coworker may be an algorithmic teammate with a virtual face.What’s new:WorkFusionunveileda line of AI tools that automate daily business tasks. One thing that sets them apart is the marketing pitch: Each has a fictitious persona including a name, face (and accompanying live-action video), and professional résumé.How it works:WorkFusion offers a cadre of six systems it touts asvirtual teammates. Each is dedicated to a role such as customer service coordinator and performs rote tasks such as entering data or extracting information from documents. At this point, their personas are superficial — they don’t affect a system’s operation, just the way it’s presented to potential customers.\n\nBehind the news:WorkFusion’s virtual teammates are examples of robotic process automation (RPA), which automates office work by interacting with documents like spreadsheets and email. The RPA market is expected to grow 25 percent annually, reaching $7.5 billion by 2028.\n\nYes, but:Giving AI systems a persona raises the questions why a particular role was assigned to a particular sort of person and whether that persona reinforces undesirable social stereotypes. For instance, a  2019 United Nationsreportcriticized voice assistants such as Amazon’s Alexa for using female voices as a default setting.Why it matters:People already anthropomorphizecars,guitars, andRoombas. Wherever people and AI work together closely, it may make sense to humanize the technology with a name and face, a practice that’s already common in the chatbot biz. Just watch out for theuncanny valley— a creepy realm populated by unsettling, nearly-but-not-quite-human avatars.We’re thinking:These virtual teammates are no match for HAL 9000, but we hope they’llopen the pod bay doorswhen you ask them to.\n\nDevelop practical skills to deploy your data science projects with thePractical Data Science Specialization! Learn how to overcome challenges at each step of the machine learning workflow using Amazon SageMaker.Enroll today\n\nChina is poised to become the dominant national power in AI, new research suggests.What’s new:The Brookings Institution, a nonprofit public-policy think tank,assessedwhich of 50 countries are best positioned to become AI powerhouses.What they found:The analysts examined each country according to 10 data points including total processing power, number of top supercomputers,privateandpublicinvestments in AI, and volumes of research publications and patent filings.\n\nBehind the news:The new report adds to Brookings’ growing body of research into national AI postures. Last November, the organizationconcludedthat Singapore, India, and Germany ranked highest in terms of AI talent due to high numbers of STEM graduates and tech workers already in the market. The previous month, itrankedthe efforts of 44 nations to fulfill their AI aspirations, giving high marks to China, Germany, and India.Why it matters:AI is an emerging arena for geopolitical competition. Understanding the global distribution of AI development and investment can help leaders make appropriate decisions and aspiring AI practitioners find sources of knowledge and employment.We’re thinking:Many observers frame global competition in AI as a winner-take-all tournament. We believe there are great opportunities for international collaboration that would lift everyone.\n\nA new deep learning technique increased the precision of short-term rainfall forecasts.What's new:Suman Ravuri, Karel Lenc, Matthew Willson, and colleagues at DeepMind, UK Meteorological Office, University of Exeter, and University of Reading developed theDeep Generative Model of Radar(DGMR) to predict amounts of precipitation up to two hours in advance.Key insight:State-of-the-art precipitation simulations struggle with short time scales and small distance scales. Agenerative adversarial network(GAN) canrapidlygeneratesequences of realistic images. Why not weather maps? A conditional GAN, which conditions its output on a specific input — say, previous weather history — could produce precipitation maps of future rainfall in short order.How it works:Given a random input, a GAN learns to produce realistic output through competition between a discriminator that judges whether output is synthetic or real and a generator that aims to fool the discriminator. A conditional GAN works the same way but adds an input that conditions both the generator’s output and the discriminator’s judgment. The authors trained a conditional GAN, given radar images of cloud cover, to generate a series of precipitation maps that represent future rainfall.\n\nResults:The authors tested their approach at multiple time intervals and distance scales according to thecontinuous ranked probability score, a modified version of mean average error in which lower is better. Its output was on par with or slightly more accurate than that of the next-best competitor,Pysteps. Of 56 meteorologists who compared the generated and ground-truth precipitation maps, roughly 90 percent found that the authors’ predictions had higher “accuracy and value” than the Pysteps output with respect to medium and heavy rain events.Why it matters:GANs can produce realistic images whether they’re cat photos or precipitation maps. A conditional GAN can turn that capability into a window on the future. Moreover, by averaging multiple attempts by the conditional GAN, it’s possible to compute the certainty of a given outcome.We're thinking:Predicting the weather isn’t just hard, it’s variably hard —  it’s far harder at certain times than at others. An ensemble approach like this can help to figure out whether the atmosphere is in a more- or less-predictable state.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202022-03-01%20at%203.35.19%20PM.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202022-03-01%20at%203.35.19%20PM.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202022-02-28%20at%205.46.58%20PM.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202022-02-28%20at%205.46.58%20PM.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/TEAMMATES.gif?upscale=true&width=1200&upscale=true&name=TEAMMATES.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20C3.png?upscale=true&width=1200&upscale=true&name=The%20Batch%20C3.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/BROOKINGS.png?upscale=true&width=1200&upscale=true&name=BROOKINGS.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20(5)-2.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20(5)-2.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-143/",
    "title": "issue 143",
    "date": "",
    "reading_time": "",
    "content": "AI Fund, which I lead, is a venture studio that works with entrepreneurs to build companies rapidly and increase their odds of success. We’ve evaluated a lot of AI startup ideas. There’s no one-size-fits-all template for building businesses, but we’ve fine-tuned our recipes. In this and subsequent letters, I’ll share some of the patterns I’ve seen.\n\nAI businessesdifferfrom traditional software startups in important ways. For instance, technical feasibility isn’t always clear, product specification is complex, and data is necessary to train and test the system.\n\nOne important factor is whether a startup focuses on hard tech (sometimes called deep tech). A hard-tech company:\n\nIn determining whether a business requires hard tech, the key factor is whether best-in-class technology will make the difference between success and failure.\n\nFor instance, speech recognition based on deep learning was hard tech 10 years ago. Only a handful of teams were able to build highly accurate systems and put them into production at scale. Higher accuracy greatly improved the user experience, and that drove adoption. Competitors had a hard time catching up.\n\nAnother example is online advertising. Building a system that selects the most relevant ad within hundreds of milliseconds is very challenging. Showing better ads results in more revenue per page view. More revenue not only improves the bottom line but makes it possible to afford higher costs to acquire users (say, by paying a maker of web browsers to feature one search engine over another). This, in turn, makes it harder for rivals to compete.\n\nWhat once was hard tech often becomes easier to build over time. For example, as speech recognition became commoditized, more teams were able to build useful systems. When this happens, having the best tech is much less critical to success. Other factors can have a bigger impact such as superior product design, a skilled sales team, bundling with other services, or an efficient supply chain.\n\nI enjoy working on hard-tech businesses — and many AI Fund companies fit that description — because the quality of the technology really matters. A hard-tech company has an incentive to build the best possible team, because the finest team can significantly outperform competitors.\n\nOf course, AI businesses that aren’t hard-tech can be very meaningful, too. There are many, many exciting applications, across all industries, yet to be built using established technology. We need developers going at these problems, too.\n\nKeep learning!\n\nAndrew\n\nOdds are that the next mass contagion will jump to humans from animals. But which species?What’s new:Virus hunters are using learning algorithms to learn which animals are likely to carry microbes that pose a danger to humans,The New York Timesreported.How it works:Several systems trained on biological, ecological, and genetic data have shown promise in identifying sources of interspecies infection.\n\nBehind the news:The AI community isn’t just working to predict future pandemics, it’s also fighting the current one.\n\nWhy it matters:Ebola, HIV, swine flu — many dire human diseases evolved in animals. Using AI to identify viruses likely to cross the species barrier could give scientists a jump on whatever comes next. Medical researchers could develop vaccines and treatments ahead of time, and officials could mitigate the spread of potentially dangerous disease by managing animal populations and limiting trade in disease-carrying species.We’re thinking:Whether an animal virus can infect a human is one question. Whether it can cause a pandemic is another. Machine learning engineers have an opportunity to help answer that one as well.\n\nIn Beijing, self-driving cars are rolling without a driver behind the wheel.What’s new:China’s capital city authorized Pony.ai and Apollo Go, a Baidu subsidiary, to deploy self-driving taxis without a human in the driver’s seat,Reutersreported. An authorizationissuedlast November had allowed the companies to operate in the same area with safety drivers.How it works:The robotaxis are restricted to a 23 square-mile zone in Beijing’s southeastern suburbs, home to roughly 300,000 residents. Rides are free, but both companies plan to charge in the near future.\n\nBehind the news:Autonomous taxis — with safety drivers — are operating in a number of Chinese cities includingShanghaiand Shenzhen. All except Pony.ai have provisional licenses that require them to provide free rides. In April, Pony.aireceivedthe country’s first authorization to charge robotaxi passengers.Why it matters:Permission to operate autonomous vehicles in Beijing, which is home to over 20 million people, is a huge market opportunity. Permission to do it without safety drivers likely will represent a huge cost saving if the government relaxes the requirement to carry a supervisor. But the symbolism is an even bigger deal: If robotaxis can handle Beijingtraffic, they may be ready for wider adoption. (Then again,gridlockisn’t the most challenging condition for an autonomous vehicle.)We’re thinking:Safe adoption of self-driving cars still requires limitations on their range and cooperation with governments. Countries that aim to accelerate progress in this area should help entrepreneurs deploy vehicles in a relatively self-contained region and expand from there.\n\nConsidering a career in data science or machine learning?Join uson May 18, 2022, to hear from industry leaders about their experiences. Every story is different, and everyone’s journey is unique. Take the next step now!\n\nHospitals across the United States are relying on AI to keep patients safe.What’s new:Doctors are using a variety of machine learning systems to assess the risk that a given patient will suffer complications,The Wall Street Journalreported.How it works:Several facilities are using AI to identify patients who need special attention.\n\nBehind the news:Government regulators are beginning to accept machine learning’s potential to transform healthcare.\n\nWhy it matters:The Covid-19 pandemic has highlighted tragically underfunded and overworked healthcare workers around the globe. Automated tools could help providers make better use of limited time and resources and help them to focus their attention on the most important cases.We’re thinking:Many countries face a demographic cliff: The population of younger people is falling precipitously, while the number of elders is growing. It seems likely that AI will be instrumental in helping doctors care for an aging population with a rising life expectancy.\n\nVision Transformer(ViT) outperformed convolutional neural networks in image classification, but it required more training data. New work enabled ViT and its variants to outperform other architectures with less training data.What’s new:Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song at Inha Universityproposedtwo tweaks to transformer-based vision architectures.Key insight:ViT and its variants divide input images into smaller patches, generates a representation — that is, a token — of each patch, and applies self-attention to track the relationships between each pair of tokens. Dividing an image can obscure the relationships between its parts, so adding a margin of overlap around each patch can help the attention layers learn these relationships. Moreover, an attention layer may fail to distinguish sufficiently between strong and weak relationships among patches, which interferes with learning. For instance, it may weight the relationship between a background patch and a foreground patch only slightly lower than that between two foreground patches. Enabling the attention layers to learn to adjust such values should boost the trained model’s performance.How it works:Starting with a collection of transformer-based image classifiers, the authors built modified versions that implemented two novel techniques. The models included ViT,T2T,CaiT,PiT, andSwin. They were trained on datasets of 50,000 to 100,000 images (CIFAR-10,CIFAR-100,Tiny-ImageNet, andSVHN) as well as the standardImageNettraining set of 1,281 million images.\n\nResults:The alterations boosted the top-1 accuracy of all models on all datasets. They improved the accuracy of PiT and CaiT by 4.01 percent and 3.43 percent on CIFAR100, and the accuracy of ViT and Swin by 4.00 percent and 4.08 percent on Tiny-ImageNet. They improved the ImageNet accuracy of ViT, PiT, and Swin by 1.60 percent, 1.44 percent, and 1.06 percent respectively.Yes, but:The authors also applied their alterations to the convolutional architecturesResNetandEfficientNet. Only  CaiT and Swin surpassed them on CIFAR100  and SVHN. Only CaiT beat them on Tiny-ImageNet. No transformer beat ResNet’s performance on CIFAR10, though all the modified transformers except ViT beat ResNet on the same task.Why it matters:The authors’ approach makes transformers more practical for visual tasks in which training data is limited.We’re thinking:Transformers are making great strides in computer vision. Will they supplant convolutional neural networks? Stay tuned!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/05/Screen-Shot-2022-05-04-at-12-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/05/Virus-Animation.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/05/BEIJING--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/05/AI-X-5.18_The-Batch-Image.png",
      "https://dl-staging-website.ghost.io/content/images/2022/05/HOSPITALS-1.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/05/SMALL.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-168/",
    "title": "issue 168",
    "date": "",
    "reading_time": "",
    "content": "Each year, AI brings wondrous advances. But, as Halloween approaches and the veil lifts between the material and ghostly realms, we see that spirits take advantage of these developments at least as much as humans do.\n\nAs Iwrotelast week, prompt engineering, the art of writing text prompts to get an AI model to generate the output you want, is a major new trend. Did you know that the Japanese word for prompt — 呪文— also means spell or incantation? (Hat tip to natural language processing developerPaul O’Leary McCann.) The process of generating an image using a model like DALL·E 2 or Stable Diffusion does seem like casting a magic spell — not to mention these programs' apparent ability to reanimate long-dead artists like Pablo Picasso — so Japan's AI practitioners may be onto something.\n\nSome AI companies aredeliberatelyreviving the dead. The startupHereAfter AIproduces chatbots that speak, sound, and look just like your long-lost great grandma. Sure, it's a simulation. Sure, the purpose is to help the living connect with deceased loved ones. When it comes to reviving the dead — based on what I've learned by watching countless zombie movies — I'm sure nothing can go wrong.\n\nI'm more concerned by AI researchers who seem determined to conjure ghastly creatures. Consider the abundance of recent research into transformers. Every transformer uses multi-headed attention. Since when is having multiple heads natural? Researchers are sneaking multi-headed beasts into our computers, and everyone cheers for the new state of the art! If there's one thing we know about transformers, it's that there's more than meets the eye.\n\nThis has also been a big year for learning from masked inputs, and approaches likeMasked Autoencoders,MaskGIT, andMaskViThave achieved outstanding performance in difficult tasks. So if you put on a Halloween mask, know that you're supporting a key idea behind AI progress.\n\nTrick or treat!\n\nAndrew\n\nEver look at a neural network’s output and think to yourself, “that's uncanny”? While the results can be inspiring — potential cures for dreaded diseases, streamlined industrial operations, beautiful artworks — they can also be terrifying. What if a model’s pattern-matching wizardry were applied to designing poison gas? Have corporate executives sold their souls in return for automated efficiency? Will evil spirits gain the upper hand as nations jockey for AI dominance? In this special issue ofThe Batch, asinpreviousyearsat this season, we raise a torch to the gloomy corners of AI and face gremlins that we ourselves have unleashed. Onward into the darkness!\n\nAI researchers are starting to see ghosts in their machines. Are they hallucinations, or does a dawning consciousness haunt the trained weights?\n\nThe fear:The latest AI models are self-aware. This development — at best —poses ethical dilemmas over human control of sentient digital beings. More worrisome, it raises unsettling questions about what sort of mind a diet of data scraped from the internet might produce.\n\nHorror stories:Sightings of supposed machine sentience have come from across the AI community.\n\nIt’s just an illusion, right?:While media reports generally took the claim that LaMDA’s was self-aware seriously — albeit skeptically — the broader AI community roundly dismissed it. Observers attributed impressions that LaMDA is sentient to human bias and DALL-E 2’s linguistic innovation to random chance. Models learn by mimicking their training data, and while some are very good at it, there’s no evidence to suggest that they do it with understanding, consciousness, or self-reflection. Nonetheless, Loab gives us the willies.\n\nFacing the fear:Confronted with unexplained phenomena, the human mind excels at leaping to fanciful conclusions. Science currently lacks a falsifiable way to verify self-awareness in a computer. Until it does, we’ll take claims of machine sentience or consciousness with a shaker full of salt.\n\nAdvanced AI requires advanced hardware. What if the global supply of high-end AI chips dries up?\n\nThe fear:Most of the world’s advanced AI processors are manufactured in Taiwan, where tension with mainland China is rising. Nearly all such chips are designed in the U.S., which hasblockedChina from obtaining them. That could prompt China to cut off U.S. access to Taiwan’s manufacturing capacity. Military action would be a human tragedy. It would also imperil progress in AI.\n\nHorror stories:China and the U.S. are on a collision course that threatens the global supply of advanced chips.\n\nSecuring the supply:Both the U.S. and China are trying to produce their own supplies of advanced chips. But fabricating circuitry measured in single-digit nanometers is enormously difficult and expensive, and there’s no guarantee that any particular party will accomplish it.\n\nFacing the fear:If a chipocalypse does occur, the AI community will need to become adept at workarounds that take advantage of older semiconductor technology, such as small data, data-centric AI development, and high-efficiency model architectures. It will also need to push for international cooperation amid intensifying polarization. Still, a chip shortage would be the least scary thing about a great-power conflict.\n\nDo you want to develop and deploy machine learning applications? Join our hands-on workshop “Branching out of the Notebook: ML Application Development with GitHub” on November 9, 2022, to learn industry-standard practices you can use today!RSVP\n\nCompanies are using AI to screen and even interview job applicants. What happens when out-of-control algorithmsarethe human resources department?\n\nThe fear:Automated systems manage every stage of the hiring process, and they don’t play fair. Trained on data rife with social biases, they blatantly discriminate when choosing which candidates to promote and which to reject. The door to your dream job is locked, and an unaccountable machine holds the key. Minority candidate? Speak with an accent? Unconventional background? You’re out of distribution!\n\nHorror stories:Many companies and institutions use automated hiring systems, but independent researchers have found them prone to bias and outright error.\n\nBad performance review:Automated hiring systems are facing scrutiny from lawmakers and even the companies that use them.\n\nFacing the fear:While many companies use hiring algorithms, most still keep humans in the loop. They have good incentive to do so: While machines can process mountains of resumes, human managers may recognize candidates who have valuable traits that an algorithm would miss. Humans and machines have complementary strengths, and a careful combination may be both efficient and fair.\n\nA growing number of AI models can be put to purposes their designers didn’t envision. Does that include heinous deeds?\n\nThe fear:Foundation models have proven to be adept at deciphering human language. They’ve also proven their worth in deciphering the structural languages of biology and chemistry. It’s only a matter of time before someone uses them to produce weapons of mass destruction.\n\nHorror stories:Researchers demonstrated how an existing AI system can be used to make chemical weapons.\n\nGas masks:In an interview, one of the researcherssuggestedthat developers of general-purpose models, such as the one they used to generate toxic chemicals, should restrict access. He added that the machine learning community should institute standards for instruction in chemistry that inform budding scientists about the dangers of misusing research.\n\nFacing the fear:It’s hard to avoid the conclusion that the safest course is to rigorously evaluate the potential for harm of all new models and restrict those that are deemed dangerous. Such a program is likely to meet with resistance from scientists who value free inquiry and businesspeople who value free enterprise, and it might have limited impact on new threats that weren’t identified when the model was created. Europe is taking a first step with its regulation of so-calledgeneral-purpose AI. However, without a broad international agreement on definitions of dangerous technology and how it should be controlled, people in other parts of the world will be free to ignore them. Considering the challenges, perhaps the best we can do is to work proactively and continually to identify potential misuses and ways to thwart them.\n\nThe new remote administrative assistant is a little too perky, hardworking, and efficient. Is it because he’s a bot?\n\nThe fear:Virtual employees are infiltrating the distributed office. Outfitted with programmed personalities and generated smiles, they’re increasingly difficult to tell from flesh and blood. Managers, pleased by the productivity boost, will stop caring which is which, leaving you surrounded by colleagues who cheerfully work 24/7, never make a mistake, and decline invitations to meet up for happy hour.Horror stories:What started in the middle of the last decade with programs likeClara— who schedules meetings via emails so cordial they might fool the uninitiated — has evolved into human-like agents dressed up with names, faces, and fake resumes.\n\nFraudulent friends:White-collar bots pose threats more serious than a proliferation of workplaces with addresses in the uncanny valley. In 2020, fraudstersuseda generative audio model to clone the voice of a company director and convince a Hong Kong bank to fork over some $35 million. Con artists using a similar play stole $243,000 from a UK energy firm in 2019.\n\nFacing the fear:Ceaselessly cheerful, perpetually productive automatons might leave their human colleagues feeling demoralized. If you’re going to anthropomorphize your algorithms, at least program them to be late for a meeting once in a while.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/10/this-one-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/10/Screen-Shot-2022-10-26-at-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/10/unnamed--4-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/10/CHIPS--1-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/10/Ff2woLQXwAAJ9qr.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/10/HIRING.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/10/unnamed--5-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/10/unnamed--3-.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-75/",
    "title": "issue 75",
    "date": "",
    "reading_time": "",
    "content": "Experience gained in building a model to solve one problem doesn’t always transfer to building models for other problems. How can you tell whether or not intuitions honed in one project are likely to generalize to another? I’ve found that two factors can make the difference: the size of the training set and whether the data is unstructured or structured.For instance, I’ve heard blanket statements like, “you should always have at least 1,000 examples before tackling a problem.” This is good advice if you’re working on a pedestrian detector, where data is readily available and prior art shows that large datasets are important. But it’s bad advice if you’re building a model to diagnose rare medical conditions, where waiting for 1,000 examples might mean you’ll never get started.\n\nUnstructured data includes text, images, and audio clips, which lend themselves to interpretation by humans. Structured data, on the other hand, includes things like transaction records or clickstream logs, which humans don’t process easily.\n\nThis difference leads to very different strategies for training and deploying models:\n\nDataset size has implications as well:\n\nIf you find yourself in need of advice while working on, say, a manufacturing visual inspection problem with 100 examples, the best person to ask would be someone who has worked on a manufacturing visual inspection problem with 100 examples. But if you can’t find such a person, consider looking for someone with expertise in the same dataset size/type quadrant as the problem you’re working on.\n\nAs you develop your career, you might also consider whether you want to stay in one quadrant and develop deep expertise there, or move across quadrants and develop more general skills.\n\nKeep learning!\n\nAndrew\n\nAs a senior deep learning engineer at Nvidia, Swetha Mandava helps make models run more efficiently on large-scale hardware. Learn about her onramp to AI and how she stays on track.Read more\n\nAn algorithm is helping cities locate pipes that could release highly toxic lead into drinking water.What’s new:BlueConduit, a startup that focuses on water safety, is working with dozens of North American municipal governments to locate lead water lines so they can be replaced,Wiredreported.How it works:For each city, the company develops a custommodelthat ranks the likelihood that any given property has lead pipes.\n\nBehind the news:Founded by faculty at Georgia Tech and University of Michigan, BlueConduit developed its technology to help manage a wave of lead poisoning in Flint, Michigan, between 2014 and 2019. There it achieved70 percentaccuracy in classifying properties with lead pipes. Contaminated water in Flint exposedthousandsof people to dangerously high levels of lead.Why it matters:Lead exposure can impair development in children, and it’s linked to heart, kidney, and fertility problems in adults. Yet digging up older water lines that may use lead pipes can costthousandsof dollars. Cities can save millions if they can focus on the most dangerous locations and avoid replacing pipes in houses that are already safe.We’re thinking:Flint stopped using BlueConduit’s system in 2018partly becausesome residents complained they were being passed over by the AI-driven replacement strategy — a sign of how little they trusted their local government and the unfamiliar technology. The city reinstated the system the following year under pressure from the state and legal actions, but the lesson remains: When you’re deploying a major AI system, establishing trust is as important as tuning parameters.\n\nModels that learn relationships between images and words are gaining ahigherprofile. New research shows that adversarial learning, usually a way to make models robust to deliberately misleading inputs, can boost vision-and-language performance.What’s new:Vision-and-language models based on transformer networks have shown strong performance on tasks such as answering questions about images. Zhe Gan of Microsoft and colleagues at Microsoft and the University of Maryland improved such models viaVision-and-Language Large-scale Adversarial (VILLA)training.Key insight:Vision-and-language models often are pretrained, for instance, to fill in blanks in image captions, and then fine-tuned for a specific task, such as answering questions about images.Previous workwith language models showed that adversarial fine-tuning — that is, giving the model input that’s designed to fool it and training it not to be fooled — can increase accuracy. The team extended this idea to vision-and-language models in both pretraining and fine-tuning.How it works:The authors worked withUNITER, which has achieved state-of-the-art performance on several vision-and-language tasks. UNITER embeds images and text separately. Then it feeds the embeddings into aBERT-like model to create a multimodal embedding.\n\nResults:UNITER trained with VILLA outperformed a standard UNITER in six vision-and-language tasks. In visual question answering, UNITER with VILLA answered 73.67 percent correctly, while the plain model answered 72.91 percent correctly. In the two-stage visual commonsense reasoning task of answering a question and justifying the answer, UNITER with VILLA scored 59.75 percent, while its standard counterpart succeeded 57.76 percent of the time.Why it matters:We understand the world through several modalities, and that makes us smarter. For instance, to describe a tree, neither an image nor a biological description is sufficient, but together they have a revealing synergy. Current models still struggle to grasp the meaning of images and language individually, but they will always be missing something until they can draw connections between them.We’re thinking:Vision: check. Language: check. Now sound, aroma, touch . . .\n\nAll four courses of ourTensorFlow: Advanced Techniques Specializationare now available on Coursera!Enroll now\n\nComputer vision is alerting authorities the moment someone draws a gun.What’s new:Several companies offer deep learning systems that enable surveillance cameras to spot firearms and quickly notify security guards or police, according toVice.No people were harmed in the training of this model:Some developers of gun detection models have gone to great lengths to produce training data.\n\nBehind the news:The use of computer vision in such offerings updates earlier systems based on sounds. For instance,ShotSpotteris used by over 100 police departments in the U.S. The system picks up gunshot sounds from acoustic sensors placed around a community and uses machine learning to compare them with an audio database. When it recognizes a gunshot, it triangulates the location and alerts police.Why it matters:Gun violenceis endemic across the U.S, includinghundredsof mass shootings. By warning police or security guards before a shooter opens up, AI-powered gun detection could save lives.We’re thinking:Like any machine learning system applied to the real world, gun detection algorithms aren’t perfect.One such systemused in New York state schools was found to mistake broom handles for guns. Such mistakes could be dangerous if they prompt police to enter possible crime scenes with their own weapons drawn and pulses pounding.\n\nCorporations are tailoring their financial reports to be read by machines.What’s new:Automated systems download far more company financial reports than humans, according to astudyby the U.S. nonprofit National Bureau of Economic Research. Consequently, companies are filling those reports with data that looks good to computers.What they did:The study analyzed 50 years of quarterly and annual financial reports submitted by public companies to the U.S. Securities and Exchange Commission.\n\nBehind the news:Computer systems increasingly drive the stock market. Last year, Deutsche Bankestimatedthat automated systems made buying and selling decisions for 80 percent of equity trading and 90 percent of equity futures trading. Corporate financials are following suit.Why it matters:The study found that the more easily a computer can digest a company’s financial reports, the faster its stock is traded after a report has been published. This suggests that the market’s pace, already lightning-fast, is bound to accelerate.We’re thinking:Companies have every incentive to tweak their reports to impress their audience, whether readers consist of wetware or software. But there’s a slippery slope between painting a rosy picture and exaggerating in ways that border on fraud. Regulators, analysts, and AI practitioners alike have a responsibility to guard against market manipulation.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/Screen20Shot202021-01-2020at2010.25.5520AM20copy.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/SWETHA120576x324.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/LEAD20576x324.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/VILLA20576x324.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/Gif202-1080202-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/GUN.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/CORPORATE.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-22/",
    "title": "issue 22",
    "date": "",
    "reading_time": "",
    "content": "One of the best gifts a friend gave me last year was recommending a book that I subsequently read and loved. She didn’t even have to buy it for me!\n\nThe right information at the right time can have a powerful impact. It can alter the course of a project or even a career. No online recommender system today knows you well enough to suggest the thing that’s best for you at this moment. But you may know a friend well enough to do that.\n\nOnce, a team I was leading needed more product knowledge. Rather than spend eight hours explaining product management, I spent five minutes asking everyone to readInspired: How To Create Products Customers Love. They came back with a much better direction for the product.\n\nIs there an educational resource you’d like to recommend to a friend? (Hint, hint: Recommending The Batch makes a wonderful gift.\n\n????)\n\nKeep learning!\n\nAndrew\n\nDaniel quit his job as a web developer and set out to break into AI. After a journey through courses, research papers, and competitions, he works at a startup building an NLP-powered chatbot.Read more\n\nArtificial intelligence was everywhere at the biggest, buzziest consumer-technology showcase in the U.S.\n\nWhat’s new:AI ruled the convention floor at the annual Consumer Electronics Show in Las Vegas, asnumerousmediaoutletsproclaimed. As usual, many products on display were half-baked concepts or solutions looking for problems. (Imagine collecting the training dataset for this cat litter box withscat recognition. On second thought, don’t.) Among the highlights, some were already here, some almost ready, and others still in the lab.Here:Products currently on the market represent the intersection of practical machine learning and mass-market applications.\n\nNear:Many of the show’s coolest reveals are either coming soon — if all goes well — or available only to deep-pocketed customers.\n\nOn the horizon:NEON, a Samsung-backed startup, showed impressively lifelike video imagery of virtual people (shown above). These avatars are meant to be conversational assistants, giving advice on personal fitness, health, or finance, eventually employed in service roles. Their bodies and expressions are based on captured human gestures, with customizable features like eye gaze and eyebrow motion.\n\nWe’re thinking:CES 2020 was enthralled by AI. Purveyors of consumer tech must take care, though, to deliver on their promises or risk bringing on hype fatigue.\n\nIn tasks that involve generating natural language, neural networks often map an input sequence of words to an output sequence of words. Facebook researchers used a similar technique on sequences of mathematical symbols, training a model to map math problems to math solutions.What’s new:Guillaume Lample and Francois Charton built a sequence-to-sequencemodelthat solves integrals and ordinary differential equations.Key insight:To apply machine translation to math, an equation must be represented as a sequence of characters that capture its semantics. A mathematical expression represented as a tree — with operators as internal nodes and operands as leaves — maps unambiguously to a sequence. For example, the image above shows the tree for 2 + 3*(5+2). The corresponding sequence is [+ 2 * 3 + 5 2].How it works:The authors used existing math software to generate datasets consisting of (problem, solution) pairs for integrals and ordinary differential equations. For each type of problem, they trained a separate transformer model to predict solutions.\n\nResults:The transformer model beat Mathematica, Matlab, and Maple on integration for the dataset generated by differentiating the solution (98.4 percent accuracy with beam size 1 compared to 84 percent for Mathematica, the best of those three math apps). It also beat the math software on differential equations with beam sizes 10 and 50. The model solved integration problems in the test set that SymPy couldn’t, showing that it generalized beyond the program used to generate its training dataset.Why it matters:Transformer networks can solve problems that dedicated commercial math programs can’t. That said, their solutions may not be 100 percent accurate.We’re thinking:Beating Mathematica is a remarkable result. Assuming the data distributions for training and test represented the most common problems in integrals and ordinary differential equations, this approach could open a vast frontier to state-of-the-art machine learning.\n\nAI is contributing to paintings, music, and now a whimsical fantasy video.What’s new:The Squireis an amateur romp through a snowy realm of knights in armor and damsels in distress. The script was composed byAI Dungeon 2, an interactive text-adventure game based on theGPT-2language model.How it works:Filmmakers Josh Johr and Dominick Todero began like any other AI Dungeon 2 player: By choosing a character (squire, mage, and so on) and setting (forest, dungeon). The program generated an internal context statement and fed it to the text engine, which responded: “You are Jake, a squire living in the kingdom of Larion…” It asked them periodically for input and generated text to advance the story. When the program declared, “GAME OVER!,” they set about planning the production.\n\nWe’re thinking:In the early days of generated music, some listeners enjoyed the jarring notes that computers often came up with. We’re still in the early days of generated narrative, but the results, for all their screwball turns, can be delightful.\n\nHow can you advance your AI career as a machine learning engineer, data scientist, or software engineer? This report fromWorkera, a deeplearning.ai company, walks you through the different career paths you can take, the tasks you’ll work on, and the skills recruiters are looking for in each role.Download the report\n\nCould a black box become Hollywood’s crystal ball?What’s new:Warner Bros. is using an AI-powered tool that predicts a movie’s box-office success, according toHollywood Reporter.How it works:Cinelyticpromotesits software as a project-management platform to help movie execs make decisions throughout a film’s lifecycle. The company says it’s looking not to automate decision making but to make human managers more effective.\n\nBehind the news:Hollywood honchos have been experimenting with AI to help them home in on blockbusters and award winners for a few years. A growing number of companies are after a piece of the action.\n\nWhy it matters:Movies can cost hundreds of millions of dollars to make, so producers are eager for any insight that can return their investment at the box office. Predictive systems could be especially helpful around film festivals, when executives often have to jump into fast-moving bidding wars.We’re thinking:This kind of approach lends itself to many industries. We look forward to one for publishing AI newsletters.\n\nAlphaGo Zerodemonstrates superhuman performance playing Go, chess, and shogi. Models likeR2D2do the same playing classic Atari titles. A new approach to deep reinforcement learning is the first to achieve state-of-the-art results playing both board and video games.What’s new:DeepMind researchers Julian Schrittwieser, Ioannis Antonoglou, and Thomas Hubert adapted techniques from AlphaGo Zero to developMuZero. While AlphaGo Zero requires knowledge of game rules, MuZero does not.Key insight:Board games like Go or chess have two players, and the only outcomes are win or lose. Video games may have only one player and offer immediate rewards. MuZero mastered these diverse conditions by learning a world model and employing AlphaGo Zero-style search.How it works:At each step in the game, MuZero considers the immediate outcome of a given move and the probability of winning if it is made. It analyzes potential consequences through a series of components.\n\nResults:MuZero matched AlphaZero’s performance in chess, shogi, and Go with slightly less computation at each timestep. In Atari games, MuZero beat the previous state-of-the-art median score across 57 titles by 5 percent in one-tenth of the training time.Why it matters:Previous models either perform precise planning (best for board games) or learn complicated dynamics (best for video games). MuZero shows that a single model can do both.We’re thinking:Stellar performance in games attracts lots of attention, but making the translation to significant impact on real-world tasks has been a challenge. MuZero addresses some of the weaknesses of previous algorithms — a step toward making a difference beyond games.\n\nThe Trump administration announced a hands-off policy for regulating artificial intelligence.What’s new:The White House unveiled anexecutive orderinstructing federal agencies to minimize rule-making related to AI businesses.What it says:The order guides regulators to craft rules that protect civil liberties, consider scientific research, and encourage public input while avoiding rules that might hinder innovation or weigh on company finances.\n\nBehind the news:Six members of the G7 group of the world’s leading economies have been working to build consensus on a set of international AI guidelines since 2018. The U.S. is the only member to reject them. Before the executive order was issued, US Chief Technology Officer Michael Kratsios toldWiredthat the guidance was part of a broader effort to inject free market principles into international AI standards, as opposed to top-down direction.Why it matters:Clear regulations allow companies to innovate without worrying that the government might suddenly change the rules. They also provide boundaries for generally accepted uses of new technology.\n\nWe’re thinking:We welcome government actions that help AI reach the market while protecting citizens. Further policies that would help the field grow include investing in education and welcoming the immigration of talented scientists and engineers.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/TheBatchFeaturedImageBreakingIntoAIDanielPopescu.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/CES2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Symbolic20Math20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_1200x675.JAN.jpg",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Cinelytic20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Atari2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/White20House20220ASPECT.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-220/",
    "title": "issue 220",
    "date": "",
    "reading_time": "",
    "content": "Welcome to the Halloween special issue of The Batch, where we take a look at fears associated with AI. In that spirit, I’d like to address a fear of mine: Sensationalist claims that AI could bring about human extinction will cause serious harm.\n\nIn recent months, I sought out people concerned about the risk that AI might cause human extinction. I wanted to find out how they thought it could happen. They worried about things like a bad actor using AI to create a bioweapon or an AI system inadvertently driving humans to extinction, just as humans have driven other species to extinction through lack of awareness that our actions could have that effect.\n\nWhen I try to evaluate how realistic these arguments are, I find them frustratingly vague and nonspecific. They boil down to “it could happen.” Trying to prove it couldn’t is akin to proving a negative. I can’t prove that AI won’t drive humans to extinction any more than I can prove that radio waves emitted from Earth won’t lead space aliens to find us and wipe us out.\n\nSuch overblown fears are already causing harm. High school students who take courses designed by Kira Learning, an AI Fund portfolio company that focuses on grade-school education, have said they are apprehensive about AI because they’ve heard it might lead to human extinction, and they don’t want to be a part of that. Are we scaring students away from careers that would be great for them and great for society?\n\nI don’t doubt that many people who share such worries are sincere. But others have a significant financial incentive to spread fear:\n\nI firmly believe that AI has the potential to help people lead longer, healthier, more fulfilling lives. One of the few things that can stop it is regulators passing ill-advised laws that impede progress. Some lobbyists for large companies — some of which would prefer not to have to compete with open source — are trying to convince policy makers that AI is so dangerous, governments should require licenses for large AI models. If enacted, such regulation would impede open source development and dramatically slow down innovation.\n\nHow can we combat this? Fortunately, I think the developer and scientific communities believe in spreading truthful, balanced views, and open source has a lot of supporters. I hope all of us can keep promoting a positive view of AI.\n\nAI is far from perfect, and we have much work ahead of us to make it safer and more responsible. But it already benefits humanity tremendously and will do so even more in the future. Let’s make sure unsubstantiated fears don’t handicap that progress.\n\nWitching you lots of learning,\n\nAndrew\n\nP.S. We have a Halloween treat for you! LangChain CEO Harrison Chase has created a new short course, “Functions, Tools, and Agents with LangChain.” It covers the latest capabilities in large language models, including OpenAI’s models, to call functions. This is very useful for handling structured data and a key building block for LLM-based agents.Sign up here!\n\nThe days grow short, the shadows long. Terrifying monsters prowl in the darkness,recentyearshaveshown. We sense the presence of creatures that would do us harm: chatbots that dispense deadly advice, machines bent on conquering our places of work, investors whose unrestrained avarice would ruin us all. How can we hold back the encroaching gloom and prolong the light that is our salvation? We propose asix-month pausein Earth’s orbit around the sun.\n\nLarge language models occasionally generate information that’s false. What if they produce output that’s downright dangerous?\n\nThe fear:Text generators don’t know true from false or right from wrong. Ask an innocent question about food or health, and you might get an innocent — but fatal — answer.\n\nHorror stories:Large language models may already have claimed their first victims.\n\nHow scared should you be:AI models are becoming safer as researchers develop techniques that align models to human preferences, such as reinforcement learning from human feedback, constitutional AI, and data-centric AI.\n\nFacing the fear:Large language models are widely available, but they’re still experimental. Researchers — like users — are learning how to control them. Builders of systems geared toward the general public — like mental health and recipe chatbots — have a special responsibility to consider sensitive, dangerous, or nefarious uses.\n\nDo the latest machine learning models constitute a supercharged tech stack for cybercrime?\n\nThe fear:Innovations like text generation, voice cloning, and deepfake videos give scammers powerful new ways to gain their victims’ trust and infiltrate their systems. They threaten to bring on an epidemic of e-fraud.\n\nHorror stories:The arsenal of automated tools available to scammers and lawbreakers is growing.\n\nHow scared should you be?AI security is a real problem.\n\nFacing the fear:Developers and governments alike are working to thwart malevolent uses of AI. Large AI companiesemployso-called red teams that test a system’s security by simulating attacks. This approach finds and fixes vulnerabilities before lawbreakers discover them. And for users, tried-and-true advice for avoiding scams still applies in the AI age: Exercise skepticism toward online promises, double check identities, hold personal information closely, and don’t click on unknown attachments or links.\n\nThis course aims to keep you updated on the fast-changing world of LLMs as a developer tool. Explore advancements like OpenAI’s function calling capability and a new syntax called LangChain Expression Language (LCEL), and apply these tools by building a conversational agent.Enroll for free\n\nThe latest advances in AI are built on freely available training data. What will happen if it becomes off-limits?\n\nThe fear:Creative workers don’t want AI developers to train models on their works without permission or compensation, or at all. Data is vanishing as they scramble to lock it down.\n\nHorror stories:Generative AI models readily produce outputs that imitate the styles of individual authors and artists. Creative people and organizations that work on their behalf are reacting by suing AI developers (all proceedings are ongoing at publication time) and restricting access to their works.\n\nSurvival in a data desert:Some AI companies have negotiated agreements for access to data. Others let publishers opt out of their data-collection efforts. Still others are using data already in their possession to train proprietary models.\n\nFacing the fear:Copyright holders and creative workers are understandably worried that generative AI will sap their market value. Whether the law is on their side remains to be seen. Laws in many countries don’t explicitly address use of copyrighted works to train AI systems. Until legislators set a clear standard, disagreements will be decided case by case and country by country.\n\nAI is taking over the workplace. Will there be enough jobs left for people?\n\nThe fear:Workers of all kinds are on the firing line as large language models, text-to-image generators, and hardware robots match their performance at a lower cost.\n\nHorror stories:Automated systems are performing a wide range of tasks that previously required human labor.\n\nCreeping pink slips:Workers are expressing anxiety about their prospects, and researchers believe the labor market is about to experience a seismic shift.\n\nFacing the fear:Each new wave of technology puts people out of work, and society has a responsibility to provide a safety net and training in new skills for people whose jobs become fully automated. In many cases, though, AI is not likely to replace workers — but workers who know how to use AI are likely to replace workers who don’t.\n\nAI companies are soaring on promises they can revolutionize society while making a profit. What if they're flying too close to the sun?\n\nThe fear:The latest models generate publication-worthy essays and award-winning artworks, but it’s not clear how to make them generate enough revenue to both cover their costs and turn a profit. The bubble is bound to burst.Horror stories:During the dot-com bust of 2000, internet stocks tumbled as their underlying weaknesses became apparent. The cryptocurrency crash of 2022 evaporated nearly two-thirds of Bitcoin’s value. Some observers believe that, similarly, today’s hottest AI bets are overhyped and overvalued.\n\nBad omens:Generative AI accomplishes new marvels with each passing month, but that doesn’t necessarily translate into profitable businesses. Investors and analysts are throwing up red flags.\n\nFacing the fear:No one knows what the future will bring, but generative AI’s usefulness, which already has attracted billions of users, continues to evolve at a rapid pace. No doubt, some investments won’t pay off — but many will: The consultancy McKinsey estimated that generative AI could add between $2.6 trillion and $4.4 trillion to the global economy annually. Already generative models form the foundation of conversational assistants, image generators, video effects, and automated coding tools. An avalanche of further applications and refinements appears to be inevitable as the technology continues to advance.\n\nCalifornia halts Cruise's self-driving cars due to safety concernsThe California Department of Motor Vehicles (DMV) suspended all driverless vehicles operated by Cruise, General Motors’ robotaxi subsidiary. The DMV stated that Cruise misrepresented safety information and its vehicles posed an \"unreasonable risk\" to public safety. The company must fulfill safety requirements to have its permits reinstated. (The Washington Post)\n\nResearchers built a tool for artists to disrupt generative AI modelsNightshade, a “data poisoning” tool, allows artists to introduce subtle and invisible changes to their digital artwork to thwart generative AI models. When scraped into AI training sets, these alterations can cause models to produce unpredictable and often bizarre results. The research, submitted for peer review, suggests that Nightshade could rebalance power between artists and AI companies by creating an effective safeguard against misuse of artists’ creative content. (MIT Technology Review)\n\nApple to ramp up Generative AI integration across devicesAfter missing the past year’s wave of generative innovation, the tech giant is building its own large language model and intensifying efforts to incorporate generative AI technology across its product line. Apple’s focus includes revamping Siri, enhancing the Messages app, and integrating AI into the next version of iOS. Apple also plans to use generative AI in development tools, lifestyle and productivity apps, and customer service applications. (Bloomberg)\n\nRapper Pras’s lawyer used AI-generated closing argument, requests new trialThe rapper, convicted of several federal crimes, claims his attorney used AI to compose his trial’s closing argument, leading to an ineffective defense that did not address key aspects of the case. The lawyer allegedly had an undisclosed stake in the AI company. Pras’s motion for a new trial underscores the potential pitfalls and challenges associated with AI-assisted legal representation. (Ars Technica)\n\nStriking Hollywood actors' participation in AI training raises ethical and privacy concernsDuring the Screen Actors Guild’s Hollywood strikes, hundreds of out-of-work actors participated in an \"emotion study\" project. The study, organized by AI company Realeyes and involving Meta, aimed to collect data from actors to teach AI to understand and express human emotions. While the job posting suggested that individual likenesses wouldn't be used for commercial purposes, the broad data license agreement allowed the companies significant leeway to use the actors' faces, expressions, and derived data. (MIT Technology Review)\n\nUniversal Music filed a $75 million lawsuit against Anthropic for alleged copyright infringementThe plaintiffs claim that Anthropic systematically copied and distributed copyrighted lyrics by artists like Beyoncé and the Rolling Stones without permission. This allegedly interfered with Universal’s ability to profit from licensing their lyrics, undermining existing and potential licensing markets. The lawsuit seeks damages and to block Anthropic from using copyrighted lyrics. (The Hollywood Reporter)\n\nResearch: Meta's Image Decoder translates brain activity into visual imageryThis technology uses brain activity to generate images of what someone is seeing or thinking, with fairly high accuracy. The model combines deep learning with magnetoencephalography (MEG) to decode brain activity and produce images based on MEG data. In its highest performing cases, the Image Decoder system achieved an accuracy rate of 70% in retrieving or recreating images based on MEG data. The technique faces technology limitations and ethical concerns. (VentureBeat)\n\nAn AI model focused on diversity and inclusivityLatimer, often referred to as the \"Black GPT,\" is a large language model designed to offer generative AI tools a better representation of black and brown people. It incorporates books, oral histories, and local archives from underrepresented communities during the training process. The platform aims to become an educational tool and reduce biases and inaccuracies in data. (POCIT)\n\nResearch: Stanford's Transparency Index reveals lack of clarity in AI modelsStanford researchers introduced the Foundation Model Transparency Index, evaluating transparency in ten major language models, including OpenAI's GPT-4 and Google's PaLM 2. The transparency index rates the models on 100 criteria, including their makers’ disclosure of training data sources, hardware used, labor involved in training, and more. The top-scoring model achieved 54 out of 100, indicating a fundamental deficiency in transparency. (The New York TimesandStanford’s Center for Research on Foundation Models)\n\nAmazon advances warehouse automation with new technologiesAmazon is introducing a humanoid robot named Digit and the Sequoia technology. Digit, developed by Agility Robotics Inc., featuring bipedal movement and hand-like clasps, was designed to consolidate emptied totes. Sequoia identifies and sorts inventory items into containers for employees to pick and process orders more efficiently. Amazon aims to mitigate injury risks through automation and reduce processing times by up to 25%. Some workers and activists are concerned that these automated systems will replace human employees. (Bloomberg)\n\nNvidia enhances robotics platform with generative AI and new APIs for edge processingThe chipmaker is expanding its Jetson platform with generative AI models and cloud-native APIs and microservices. Nvidia’s Jetson platform now includes the Isaac ROS robotics framework and a Metropolis expansion to accelerate AI application development. Developers can also access a new generative AI Lab with open source models to simplify deployment and management of applications at the edge. (Nvidia)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/10/Terminator_1200px-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/10/INTRO2_v4_1200px.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/10/Screenshot-2023-10-25-at-11.59.14-AM.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/10/Screenshot-2023-10-25-at-12.07.03-PM.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/10/The-Batch-ads-and-exclusive-banners--71-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/10/DataDisappearence5_1200px.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/10/Screenshot-2023-10-24-at-2.13.26-PM.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/10/BUBBLE_1200px.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-253/",
    "title": "issue 253",
    "date": "",
    "reading_time": "",
    "content": "One reason for machine learning’s success is that our field welcomes a wide range of work. I can’t think of even one example where someone developed what they called a machine learning algorithm and senior members of our community criticized it saying, “that’s not machine learning!” Indeed, linear regression using a least-squares cost function was used by mathematicians Legendre and Gauss in the early 1800s — long before the invention of computers — yet machine learning has embraced these algorithms, and we routinely call them “machine learning” in introductory courses!\n\nIn contrast, about 20 years ago, I saw statistics departments at a number of universities look at developments in machine learning and say, “that’s not really statistics.” This is one reason why machine learning grew much more in computer science than statistics departments. (Fortunately, since then, most statistics departments have become much more open to machine learning.)\n\nThis contrast came to mind a few months ago, as I thought about how to talk aboutagentic systemsthat use design patterns such asreflection,tool use,planning, andmulti-agent collaborationto produce better results than zero-shot prompting. I had been involved in conversations about whether certain systems should count as “agents.” Rather than having to choose whether or not something is an agent in a binary way, I thought, it would be more useful to think of systems as being agent-like to different degrees. Unlike the noun “agent,” the adjective “agentic” allows us to contemplate such systems and include all of them in this growing movement.\n\nMore and more people are building systems that prompt a large language model multiple times using agent-like design patterns. But there’s a gray zone between what clearly is not an agent (prompting a model once) and what clearly is (say, an autonomous agent that, given high-level instructions, plans, uses tools, and carries out multiple, iterative steps of processing).\n\nRather than arguing over which work to include or exclude as being a true agent, we can acknowledge that there are different degrees to which systems can be agentic. Then we can more easily include everyone who wants to work on agentic systems. We can also encourage newcomers to start by building simple agentic workflows and iteratively make their systems more sophisticated.\n\nIn the past few weeks, I’ve noticed that, while technical people and non-technical people alike sometimes use the word “agent,” mainly only technical people use the word “agentic” (for now!). So when I see an article that talks about “agentic” workflows, I’m more likely to read it, since it’s less likely to be marketing fluff and more likely to have been written by someone who understands the technology.\n\nLet’s keep working on agentic systems and keep welcoming anyone who wants to join our field!\n\nKeep learning,\n\nAndrew\n\nGrow your generative AI skills with DeepLearning.AI’sshort courses! Learn how to build highly controllable agents in “AI Agents in LangGraph.”Enroll for free and get started\n\nApple presented its plan to imbue its phones and computers with artificial intelligence.What’s new:AppleannouncedApple Intelligence, a plethora of generative-AI features that integrate with iOS 18, iPadOS 18, and MacOS Sequoia. The beta version of Apple Intelligence will be available in U.S. English prior to a wider rollout near the end of the year, starting with the iPhone 15 Pro and Mac computers that useM-serieschips.On-device and in the cloud:The new capabilities rely on a suite of language and vision models. Many of the models will run on-device, while workloads that require more processing power will run on a cloud powered by Apple chips.\n\nHow it works:Appleoutlinedthe architecture that underpins the new features and compared two models of its against competitors.\n\nBehind the news:While rivals like Microsoft and Google dove into generative AI, Applemovedmore cautiously. During the 2010s, it invested heavily in its Siri voice assistant, but the technology wasoutpacedby subsequent developments. Since then, the famously secretive company has been perceived asfalling behindbig-tech rivals in AI.Why it matters:While Apple’s big-tech competitors have largely put their AI cards on the table, Apple has held back. Now its strategy is on display: Proprietary foundation models, LoRA to fine-tune them to specific tasks, emphasis on the user experience over raw productivity, judicious use of edge and cloud computing, and deals with other model makers, all wrapped up in substantial privacy protections.We’re thinking:Apple’s control over its product ecosystem gives the company an extraordinary distribution channel. That’s why Google reportedlypaidApple $20 billion in 2022 to provide the default search engine in Apple’s Safari web browser. This advantage means that, whatever its pace of development and strategy in AI, Apple’s competitive edge remains sharp.\n\nSonically minded developers gained a high-profile text-to-audio generator.\n\nWhat’s new:Stability AIreleasedStable Audio Open, which takes text prompts and generates 16kHz-resolution music or sound effects. The model’s code and weights are available for noncommercial use. You can listen to a few sample outputshere.How it works:Stability AI promotes Stable Audio Open for generating not full productions but elements that will be assembled into productions. Although it’s similar to the earlierStable Audio 2.0, it has important differences.\n\nBehind the news:Stable Audio Open competes not only with Stable Audio 2.0 but also with a handful of recent models. ElevenLabs,knownfor voice cloning and generation,introducedSound Effects, which generates brief sound effects from a text prompt. Users can input up to 10,000 prompt characters with a free account. For music generation,Udio and Sunooffer web-based systems that take text prompts and generate structured compositions including songs with lyrics, voices, and full instrumentation. Users can generate a handful of compositions daily for free.\n\nWhy it matters:Stable Audio Open is pretrained on both music and sound effects, and it can be fine-tuned and otherwise modified. The fact that its training data was copyright-free guarantees that users won’t make use of proprietary sounds — a suitable option for those who prefer to steer clear of the music industry’s brewing intellectual propertydisputes.We’re thinking:We welcome Stability AI’s latest contribution, but we don’t consider it open source. Its license doesn’t permit commercial use and thus, as far as we know, doesn’t meet the definition established by theOpen Source Initiative. We urge the AI community toward greater clarity and consistency with respect to the term “open source.”\n\nAt meetings in Seoul, government and corporate officials from dozens of countries agreed to take action on AI safety.What’s new:Attendees at the AI Seoul Summit and AI Global Forum, both held concurrently in Seoul, formalized the broad-strokes agreements to govern AI,The Guardianreported. Presented as a sequel to November’s AI summit in Bletchley Park outside of London, the meetings yielded several multinational declarations and commitments from major tech firms.\n\nInternational commitments:Government officials hammered out frameworks for promoting innovation while managing risk.\n\nCorporate commitments:AI companies agreed to monitor their own work and collaborate on further measures.\n\nBehind the news:Co-hosted by the UK and South Korean governments at the Korea Advanced Institute of Science and Technology, the meeting followed an initial summit held at Bletchley Park outside London in November. The earlier summitfacilitatedagreements to create AIsafetyinstitutes, test AI products before public release, and create an international panel akin to theIntergovernmental Panel on Climate Changeto draft reports on the state of AI. The panelpublishedan interim report in May. It will release its final report at thenext summitin Paris in November 2024.\n\nWhy it matters:There was a chance that the Bletchley Park summit would be a one-off. The fact that a second meeting occurred is a sign that public and private interests alike want at least a seat at the table in discussions of AI safety. Much work remains to define terms and establish protocols, but plans for future summits indicate a clear appetite for further cooperation.\n\nWe’re thinking:Andrew Ng spoke at the AI Global Forum on the importance ofregulating applications rather than technologyand chatted with many government leaders there. Discussions focused at least as much on promoting innovation as mitigating hypothetical risks. While some large companies continued to lobby for safety measures that would unnecessarily impede dissemination of cutting-edge foundation models and hamper open-source and smaller competitors, most government leaders seemed to give little credence to science-fiction risks, such as AI takeover, and express concern about concrete, harmful applications like the use of AI to interfere with democratic elections. These are encouraging shifts!\n\nA critical step in diagnosing illnesses is a conversation between doctor and patient to assemble a medical history, discuss approaches to managing symptoms, and so on. Can a large language model play the doctor’s role? Researchers trained one to do surprisingly well.\n\nWhat's new:Articulate Medical Intelligence Explorer(AMIE), a chatbot built by Google researchers Tao Tu, Anil Palepu, Mike Schaekermann and colleagues, showed better diagnostic ability and bedside manner than doctors in conversations with patients. The conversations covered a range of complaints including cardiovascular, respiratory, gastroenterology, neurology, urology, obstetric, and gynecology conditions.\n\nKey insight:A pretrained LLM that’s fine-tuned on conversations between doctors and patients can learn to mimic the doctor’s role. However, such models are limited because available datasets of real-world medical conversations don’t cover the full range of medical scenarios and include ambiguities, interruptions, implicit references and the like, posing difficulties for learning. Conversations generated by a pretrained LLM can cover more conditions in more articulate language. After fine-tuning on real-world conversations, further tuning on generated conversations can improve performance. In addition, after a conversation, critiquing the “doctor’s” performance can improve its ability to render diagnoses, suggest plans for managing symptoms, empathize with patients, and otherwise perform its role.\n\nHow it works:The authors fine-tuned a pretrainedPaLM-2on medicalmultiple-choice questionsthat describe symptoms, possible causes, and evidence for the correct diagnosis, as well as datasets for tasks like summarizing and continuing medical dialogs. They further fine-tuned the model on its own output.\n\nResults:Specialist physicians evaluated the doctor model’s performance in 149 conversations with human actors who played the roles of patients based on scenarios supplied by clinical providers. They compared the model’s output with those of 20 primary care physicians based on their own conversations with the actors.\n\nWhy it matters:LLMs can generate fine-tuning data that improves their own performance. By training on relevant, factually correct medical information from the web, LLMs can generate realistic conversations at scale — even in a highly technical, high-stakes discipline like medicine and despite their potential to generate potentially dangerous hallucinations. Used as fine-tuning data, this output enables LLMs to converse with humans more effectively.\n\nWe're thinking:AI promises to spread intelligence far and wide. As the authors acknowledge, further work remains to demonstrate this work’s efficacy, ethics, security, and regulatory compliance in a clinical setting. Yet it’s an exciting glimpse of a world in which medical intelligence is fast, cheap, and widely available.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed--63-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/06/The-Batch-ads-and-exclusive-banners---2024-06-12T075130.872.png",
      "https://dl-staging-website.ghost.io/content/images/2024/06/APPLEAI.png",
      "https://dl-staging-website.ghost.io/content/images/2024/06/STABLEAUDIOOPEN.png",
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-12T145349.047.png",
      "https://dl-staging-website.ghost.io/content/images/2024/06/unnamed---2024-06-12T145439.840.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-211/",
    "title": "issue 211",
    "date": "",
    "reading_time": "",
    "content": "Machine learning development is anempirical process. It’s hard to know in advance the result of a hyperparameter choice, dataset, or prompt to a large language model (LLM). You just have to try it, get a result, and decide on the next step. Still, understanding how the underlying technology works is very helpful for picking a promising direction. For example, when prompting an LLM, which of the following is more effective?Prompt 1: [Problem/question description] State the answer and then explain your reasoning.\n\nPrompt 2: [Problem/question description] Explain your reasoning and then state the answer.\n\nThese two prompts are nearly identical, and the former matches the wording of many university exams. But the second prompt is much more likely to get an LLM to give you a good answer. Here’s why: An LLM generates output by repeatedly guessing the most likely next word (or token). So if you ask it to start by stating the answer, as in the first prompt, it will take a stab at guessing the answer and then try to justify what might be an incorrect guess. In contrast, prompt 2 directs it to think things through before it reaches a conclusion. This principle also explains the effectiveness of widely discussed prompts such as “Let’s think step by step.”\n\nThe image above illustrates this difference using a question with one right answer. But similar considerations apply when asking an LLM to make judgment calls when there is no single right answer; for example, how to phrase an email, what to say to someone who is upset, or the proper department to route a customer email to.That’s why it’s helpful to understand, in depth, how an algorithm works. And that means more than memorizing specific words to include in prompts or studying API calls. These algorithms are complex, and it’s hard to know all the details. Fortunately, there’s no need to. After all, you don’t need to be an expert in GPU compute allocation algorithms to use LLMs. But digging one or two layers deeper than the API documentation to understand how key pieces of the technology work will shape your insights. For example, in the past week, knowing how long-context transformer networks process input prompts and how tokenizers turn an input into tokens shaped how I used them.\n\nA deep understanding of technology is especially helpful when the technology is still maturing. Most of us can get a mature technology like GPS to perform well without knowing much about how it works. But LLMs are still an immature technology, and thus your prompts can have non-intuitive effects. Developers who understand the technology in depth are likely to build more effective applications, and build them faster and more easily, than those who don't. Technical depth also helps you to decide when you can’t tell what’s likely to work in advance, and when the best approach is to try a handful of promising prompts, get a result, and keep iterating.\n\nKeep learning!\n\nAndrew\n\nP.S. Our short course on fine-tuning LLMs is now available! As Iwrotelast week, many developers are not only prompting LLMs but also fine-tuning them — that is, taking a pretrained model and training it further on their own data. Fine-tuning can deliver superior results, and it can be done relatively inexpensively. In this course, Sharon Zhou, CEO and co-founder of Lamini (disclosure: I’m a minor shareholder) shows you how to recognize when fine-tuning can be helpful and how to do it with an open-source LLM. Learn to fine-tune your own modelshere.\n\nThe New York Timeslaunched a multi-pronged attack on the use of its work in training datasets.\n\nWhat’s new:The company updated itsterms of serviceto forbid use of its web content and other data for training AI systems,Adweekreported. It’s also exploring a lawsuit against OpenAI for unauthorized use of its intellectual property,according toNPR. Meanwhile,The New York Timesbacked outof a consortium of publishers that would push for payment from AI companies.From negotiation to mandate:The 173-year-old publisher, which has nearly 10 million subscribers across online and print formats, was negotiating with OpenAI to use its material, but talks recently broke down.The New York Timeshad more success with Google: In February, Google agreed to pay around $100 million to useTimescontent in search results, although an agreement on AI training was not reported.\n\nBehind the news:Earlier this month, 10 press and media organizations includingAgence France-Presse,Associated Press, and stock media provider Getty Imagessignedan open letter that urges regulators to place certain restrictions on AI developers. The letter calls for disclosure of training datasets, labeling of model outputs as AI-generated, and obtaining consent of copyright holders before training a model on their intellectual property. The letter followedseveralongoinglawsuitsthat accuse AI developers of appropriating data without proper permission or compensation.\n\nWhy it matters:Large machine learning models rely on training data scraped from the web as well as other freely available sources. Text on the web is sufficiently plentiful that losing a handful of sources may not affect the quality of trained models. However, if the norms were to shift around using scraped data to train machine learning models in ways that significantly reduced the supply of high-quality data, the capabilities of trained models would suffer.\n\nWe’re thinking:Society reaps enormous rewards when people are able to learn freely. Similarly, we stand to gain incalculable benefits by allowing AI to learn from information available on the web. An interpretation of copyright law that blocks such learning would hurt society and derail innovation. It’s long past time torethink copyrightfor the age of AI.\n\nHackers attacked AI models in a large-scale competition to discover vulnerabilities.\n\nWhat’s new:At the annual Defcon hacker convention in Las Vegas, 2,200 people competed to break guardrails around language models,The New York Timesreported. The contest, which was organized by AI safety nonprofits Humane Intelligence and SeedAI and sponsored by the White House and several tech companies, offered winners an Nvidia RTX A6000 graphics card.Breaking models:Contestants in the Generative Red Team Challenge had 50 minutes to perform 21 tasks of varying difficulty, which they selected from a board like that of the game showJeopardy. Seven judges scored their submissions.\n\nBehind the news:Large AI developers often test their systems by hiring hackers called “red teams,” a term used by the United States military to represent enemy forces in Cold War-era war games, to attack them.\n\nWhy it matters:The security flaws found in generative AI systems are distinctly different from those in other types of software. Enlisting hackers to attack systems in development is essential in sniffing out flaws in conventional software. It’s a good bet for discovering deficiencies in AI models as well.\n\nWe’re thinking:Defcon attracts many of the world’s most talented hackers — people who havetricked ATMs into dispensing cashandtaken over automobile control software. We feel safer knowing that this crowd is on our side.\n\nJoin \"Finetuning Large Language Models,\" a new short course that teaches you how to finetune open source models on your own data.Enroll today and get started\n\nAn upstart supplier of AI chips secured a major customer.\n\nWhat’s new:Cerebras, which competes with Nvidia in hardware for training large models,signeda $100 million contract with Abu Dhabi tech conglomerate G42. The deal is the first part of a multi-stage plan to build a network of supercomputers.\n\nHow it works:The deal covers the first three of nine proposed systems. The first,Condor Galaxy 1(CG-1), is already up and running in Santa Clara, California. CG-2 and CG-3 are slated to open in early 2024 in Austin, Texas and Asheville, North Carolina. Cerebras and G42 are in talks to build six more by the end of 2024. G42 plans to use the network to supply processing power primarily to healthcare and energy companies\n\nBehind the news:Nvidia accounts for95 percentof the market for GPUs used in machine learning — a formidable competitor to Cerebras and other vendors of AI chips. Despite Nvidia’s position, though, there are signs that it’s not invincible.\n\nWhy it matters:The rapid adoption of generative AI is fueling demand for the huge amounts of processing power required to train and run state-of-the-art models. In practical terms, Nvidia is the only supplier of tried-and-true AI chips for large-scale systems. This creates a risk for customers who need access to processing power and an opportunity for competitors who can satisfy some of that demand.\n\nWe’re thinking:As great as Nvidia’s products are, a monopoly in AI chips is not in anyone’s best interest. Cerebras offers an alternative for training very large models. Now cloud-computing customers can put it to the test.\n\nVision transformers typically process images in patches of fixed size. Smaller patches yield higher accuracy but require more computation. A new training method lets AI engineers adjust the tradeoff.\n\nWhat's new:Lucas Beyer and colleagues at Google Research trainedFlexiViT, a vision transformer that allows users to specify the desired patch size.\n\nKey insight:Vision transformers turn each patch into a token using two matrices of weights, whose values describe the patch’s position and appearance. The dimensions of these matrices depend on patch size. Resizing the matrices enables a transformer to use patches of arbitrary size.\n\nHow it works:The authors trained a standard vision transformer on patches of random sizes between 8x8 and 48x48 pixels. They trained it to classifyImageNet-21K(256x256 pixels).\n\nResults:The authors compared FlexiVit to two vanilla vision transformers,ViT-B/16 and ViT-B/30, trained on ImageNet-21k using patch sizes of 16x16 and 30x30 respectively. Given patches of various sizes, the vanilla vision transformers’ position and appearance matrices adjusted in the same manner as FlexiViT’s. FlexiViT performed consistently well across patch sizes, while the models trained on a fixed patch size performed well only with that size. For example, given 8x8 patches, FlexiViT achieved 50.2 percent precision; ViT-B/16 achieved 30.5 percent precision, and ViT-B/30 achieved 2.9 percent precision. Given 30x30 patches, FlexiViT achieved 46.6 percent precision, ViT-B/16 achieved 2.4 percent precision, and ViT-B/30 achieved 47.1 percent precision.\n\nWhy it matters:The processing power available often depends on the project. This approach makes it possible to train a single vision transformer and tailor its patch size to accommodate the computation budget at inference.\n\nWe're thinking:Unlike text transformers, for which turning text into a sequence of tokens is relatively straightforward, vision transformers offer many possibilities for turning an image into patches and patches into tokens. It’s exciting to see continued innovation in this area.\n\nSupermarket recipe bot produced a dangerous cooking formulaNew Zealand-based supermarket chain Pak‘nSave introduced a bot to generate recipes from leftover ingredients. A user requested a recipe that included the dangerous combination of water, ammonia, and bleach, and the bot complied. The company responded by putting safeguards in place to prevent misuse. (Gizmodo)California firefighters harness AI to combat wildfiresThe ALERTCalifornia program is aiding California firefighters by using over 1,000 cameras to feed video data into an AI system that identifies potential wildfires and alerts first responders, ensuring rapid action. The platform, operational since July, already demonstrated its effectiveness by detecting fires in remote locations. (Reuters)Tutoring firm settles US lawsuit over AI biasChina's iTutorGroup Inc agreed to resolve a lawsuit brought by the U.S. Equal Employment Opportunity Commission (EEOC). The U.S. alleged that iTutorGroup unlawfully used AI to discriminate against older job seekers. The company agreed to pay $365,000 to over 200 applicants. (Reuters)U.S. legislators establish AI working groupMembers of the Democratic party  in the U.S. House of Representatives formed a working group to focus on AI regulations. The group aims to collaborate with the Biden administration, companies, and fellow legislators to formulate bipartisan policies concerning AI. (Reuters)Skydio shifts from consumer drones to enterprise and public sectorThe company, known for its consumer drones, exited the market and will focus on growing its enterprise and public-sector business. However, the company hasn't ruled out a return to the consumer drone space in the future. (IEEE Spectrum)School district uses ChatGPT to decide which books to remove from librariesIn response to state legislation requiring books to be age-appropriate, the Mason City Community School District in Iowa used ChatGPT to assess its catalog. Administrators decided that consulting with the chatbot was the simplest way to comply with the requirements efficiently, despite concerns about its accuracy and consistency. (Popular Science)San Francisco approves round-the-clock robotaxi operationsThe California Public Utilities Commission granted approval to Waymo and Cruise robotaxis to operate throughout San Francisco at all hours. Shortly after the approval was announced, a driverless Cruise vehicle drove into a city paving project and became stuck in wet concrete. (APandThe New York Times)Gartner’s 2023 Hype Cycle elevates generative AI to 'Peak of Inflated Expectations'The Gartner Hype Cycle, which interprets the progress of tech trends, placed generative AI on the position of the Peak of Inflated Expectations. The market research firm attributed this designation to the proliferation of products claiming generative AI integration and the discrepancy between vendors' claims and real-world benefits. The next milestone in the Hype Cycle is the 'Plateau of Productivity.' (Venture Beat)Research:OpenAI apparently conceals use of copyrighted material in ChatGPT trainingResearchers found that ChatGPT avoids responses that include verbatim excepts from copyrighted material, behavior that wasn’t observed in earlier versions of the chatbot. Nonetheless, they were able to devise prompts that caused the model to output such excerpts. (Business Insider)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--85--1.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--86-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--87-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/08/The-Batch-ads-and-exclusive-banners--51-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--46-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--88-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-96/",
    "title": "issue 96",
    "date": "",
    "reading_time": "",
    "content": "Can you develop a dataset that results in the best performance on this problem?\n\nThe competition will end on September 4, 2021 — the birthday of John McCarthy, who coined the term artificial intelligence. The winners will be invited to join me at a private roundtable event to share ideas about how to grow the data-centric movement, and I will highlight their work here in The Batch.I’m grateful to Chris Re at Stanford and D Sculley at Google for advising us on this competition, and to everyone who contributed their thoughtsonsocialmedia.\n\nThere will be more data-centric AI competitions in the future. But if you join this one with me, you’ll be able to tell your friends that you were there at the very beginning of the data-centric AI movement! You’ll find further informationhere.\n\nKeep preparing data!Andrew\n\nA neural network wrote the blueprint for upcoming computer chips that will accelerate deep learning itself.\n\nWhat’s new:Google engineers used a reinforcement learning system to arrange the billions of minuscule transistors in an upcoming version of its Tensor Processing Unit (TPU) chips optimized for computing neural networks. The system generated the design in six hours rather than the usual span of weeks, as detailed inNature.\n\nKey insight:Designing a chip is like playing a board game. A silicon wafer’s area resembles a board, parameters like macro counts and netlist topologies resemble pieces, and evaluation metrics resemble victory conditions. Reinforcement learning (RL) excels at meeting such challenges: Think of DeepMind’s AlphaGo — the RL model that, in 2015, became the first computer program to beat a Go master on a full-size board without a handicap.\n\nHow it works:Google introduced its approach in apaperpublished last year.\n\nResults:The researchers compared their system’s output to that of a human team who had designed an existing TPU. Their approach completed the task in a fraction of the time, and it either matched or outperformed the human team with respect to chip area, wire length, and power consumption.\n\nBehind the news:Google introduced the first TPU in 2015, and today the chips power Google services like search and translation and are available to developers via Google Cloud.Launched last month, the fourth-generation TPU can train a ResNet-50 on ImageNet in1.82 minutes.\n\nWhy it matters:AI-powered chip design could cut the cost of bespoke chips, leading to an explosion of special-purpose processing for all kinds of uses.\n\nWe’re thinking:Reinforcement learning is hot, and we’ve seen companies announce “RL” results that would be described more accurately as supervised learning. But this appears to be a genuine use of RL ideas, and it’s great to see this much-hyped approach used in a valuable commercial application.\n\nA new report assessed how AI has helped address Covid-19 and where it has fallen short.\n\nWhat’s new:Machine learning systems haven’t lived up to their promise in some areas, but in others they’ve made a substantial impact, biomedical engineer Maxime Nauwynck wrote inThe Gradient, an online journal of machine learning.\n\nApplication areas:The author surveyed only systems specifically designed or adapted to fight Covid-19.\n\nBehind the news:AI-powered health monitoring systems from BlueDot and Healthmap madeheadlinesearly last year when they reported a novel disease outbreak in the Wuhan area one week before the World Health Organization issued its first warnings.\n\nWhy it matters:While AI is no panacea, this inventory makes clear that the technology has made significant contributions to the fight against Covid-19.\n\nWe’re thinking:When new technology meets a previously unknown illness, there are bound to be hits and misses. The successes should help us prepare for — or, better yet, avoid — the next contagion.\n\nComing soon! “Machine Learning Modeling Pipelines in Production,” Course 3 in theMachine Learning Engineering for Production (MLOps) Specialization, launches on Coursera on June 30, 2021.Pre-enroll now!\n\nSystems designed to turn handwriting into text typically work best on pages with a consistent layout, such as a single column unbroken by drawings, diagrams, or extraneous symbols. A new system removes that requirement.\n\nWhat’s new:Sumeet Singh and Sergey Karayev of Turnitin, a company that detects plagiarism, created a general-purposeimage-to-sequence modelthat converts handwriting into text regardless of its layout and elements such as sketches, equations, and scratched-out deletions.\n\nKey insight:Handwriting recognition systems typically use separate models to segment pages into blocks of words and turn the writing into text. Neural networks allow an end-to-end approach. Convolutional neural networks are good at processing images, andtransformersare good at extracting information from sequences. A CNN can create representations of text in an image, and a transformer can turn those representations into text.\n\nHow it works:The system feeds pages through an encoder based on a 34-layerResNetfollowed by a transformer-based decoder.\n\nResults:On IAM, the author’s system achieved a character error rate of 6.3 percent, while anLSTM designed for 2Dachieved 7.9 percent. On Free Form Answers, it achieved a character error rate of 7.6 percent. Among Microsoft’sCognitive Services, Google’sCloud Vision, andMathpix, the best achieved 14.4 percent.Why it matters:End-to-end approaches to deep learning have been overhyped. But, given the large amount of data, including easily synthesized data, available for handwriting recognition, this task is an excellent candidate for end-to-end learning.We’re thinking:But can it decipher yourdoctor’s scrawl?\n\nSelf-driving cars, get ready to share the road with self-riding bikes.\n\nWhat’s new:Beijing-based machine learning researcherZhihui Pengbuilt a riderless bike that stays upright, navigates, and avoids collisions,Synced Reviewreported. You can watch Peng’s video presentationhere.\n\nHow he did it:Zhihui calls his design eXtremely Unnatural Auto-Navigation (Xuan).\n\nBehind the news:Zhihui was inspired by a 2016 April Fool’s Day prank played by Google. In avideothat announced “Google’s Self-Driving Bike,” the company made it appear as though a two-wheeler had driven itself through the streets of Amsterdam.\n\nWhy it matters:Self-driving bikes aren’t necessarily a joke. Aself-driving motorcyclehelped to attract attention to the 2004 Darpa Grand Challenge, which kick-started the current self-driving movement. Zhihui’s contraption is a DIY project, but it may prefigure summonable e-bikes, autonomous food deliveries, or steering control for long-distance cyclists who need a break from the handlebars.\n\nWe’re thinking:We look forward to the self-pedaling unicycle.",
    "images": [
      "https://info.deeplearning.ai/hs-fs/hubfs/CHIPS.gif?width=1200&upscale=true&name=CHIPS.gif",
      "https://info.deeplearning.ai/hs-fs/hubfs/covid.gif?width=1200&upscale=true&name=covid.gif",
      "https://info.deeplearning.ai/hs-fs/hubfs/The%20Batch%20Image%204.png?width=1200&upscale=true&name=The%20Batch%20Image%204.png",
      "https://info.deeplearning.ai/hs-fs/hubfs/handwritting_revised.gif?width=1152&upscale=true&name=handwritting_revised.gif",
      "https://info.deeplearning.ai/hs-fs/hubfs/bike.gif?width=1200&upscale=true&name=bike.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-59/",
    "title": "issue 59",
    "date": "",
    "reading_time": "",
    "content": "This special issue of The Batch celebrates the launch of our newGenerative Adversarial Networks Specialization!\n\nGANs are among the most exciting technologies to emerge from deep learning. These networks learn in a very different way than typical supervised methods for learning x-to-y mappings. By pitting a discriminator network and a generator network against one another (details below), they produce photorealistic images, medical training data, children’s book illustrations, and other types of output.\n\nEarlier today, we held an online panel discussion on “GANs for Good” with Anima Anandkumar, Alexei Efros, Ian Goodfellow, and our course instructor Sharon Zhou. I was struck by the number of new applications GANs are enabling, and the number that are likely to come.\n\nIan explained that GAN-generated training examples for a particular application at Apple are one-fifth as valuable as real examples but cost much less than one-fifth as much to produce. Anima described exciting progress on disentanglement and how the ability to isolate objects in images is making it easier to control image generation (“add a pair of glasses to this face”). Alexei talked about the impact GANs are having on art through tools likeArtbreeder.\n\nAll the speakers talked about alternatives to reading research papers to keep up with the exploding literature. If you missed the live discussion, you can watch a video of the entire eventhere.\n\nWe’re still in the early days of practical GAN applications, but I believe they will:\n\nAs an emerging technology, GANs have numerous untapped applications. This is a moment to dream up new ideas, because no one else may be working on them yet.\n\nI hope this technology will spark your hunger to learn more and invent new applications that will make life better for people all over the world.\n\nKeep learning!\n\nAndrew\n\nGenerative adversarial networks, or GANs, are said to give computers the gift of imagination. Competition between a discriminator network, which learns to classify the system’s output as generated or real, and a generator network, which learns to produce output that fools the discriminator, produces fantasy images of uncanny realism. Firstproposedin 2014, the architecture has been adopted by researchers to extend training datasets with synthetic examples and by businesses to create customized imagery for ads, entertainment, and personal services. But it has a dark side: GANs make it easy and convincing for jilted lovers to graft an ex’s face onto another person’s body, or politicians to misrepresent themselves as able to speak an ethnic minority’s language to win their votes. And it tends to tilt the training data distribution, favoring common examples while ignoring outliers. Researchers are improving the technology at breakneck pace, and developing ways to thwart, or at least detect, egregious uses. We have yet to see the best — and the worst — that GANs have to offer.\n\nBrilliant ideas strike at unlikely moments.Ian Goodfellowconceived generative adversarial networks while spitballing programming techniques with friends at a bar. Goodfellow, who views himself as “someone who works on the core technology, not the applications,” started at Stanford as a premed before switching to computer science and studying machine learning with Andrew Ng. “I realized that would be a faster path to impact more things than working on specific medical applications one at a time,” he recalls. From there, he earned a PhD in machine learning at Université de Montréal, interned at the seminal robotics lab Willow Garage, and held positions at OpenAI and Google Research. Last year, he joined Apple as director of machine learning in the Special Projects Group, which develops technologies that aren’t part of products on the market. His work at Apple is top-secret.\n\nThe Batch:How did you come up with the idea for two networks that battle each other?Goodfellow:I’d been thinking about how to use something like a discriminator network as a way to score a speech synthesis contest, but I didn’t do it because it would have been too easy to cheat by overfitting to a particular discriminator. Some of my friends wanted to train a generator network using a technique that would have taken gigabytes of data per image, even for the tiny images we studied with generative models in 2014. We were discussing the problem one night at a bar, and they asked me how to write a program that efficiently manages gigabytes of data per image on a GPU that, back then, had about 1.5GB RAM. I said, that’s not a programming problem. It’s an algorithm design problem. Then I realized that a discriminator network could help a generator produce images if it were part of the learning process. I went home that night and started coding the first GAN.The Batch:How long did it take?Goodfellow:By copying and pasting bits and pieces of earlier papers, I got the first GAN to produceMNISTimages in only an hour of work or so. MNIST is such a small dataset that, even back then, you could train on it very quickly. I think it trained for tens of minutes before it could produce recognizable handwritten digits.The Batch:What did it feel like to see the first face?Goodfellow:That wasn’t as much of a revolutionary moment as people might expect. My colleagueBing Xumodeled face images from the Toronto Face Database, which were only 90 pixels square and grayscale. Because the faces were always centered and looking straight at the camera, even very simple algorithms like PCA could make pretty good faces. The main thing we were surprised by were the images it made ofCIFAR10, where there’s a lot of variability. They looked like crap. But we had been looking at crap from generative models for years, and we could tell that this was an exciting, new kind of crap.The Batch:Has anything surprised you about the way this work has played out?Goodfellow:In the first GAN paper, we included a list of things that might happen in future work. A lot of them did. The one big category of things I didn’t anticipate was domain-to-domain translation. GANs likeCycleGANfrom Berkeley. You can take a picture of a horse and have it transformed into a zebra without training on matched pairs of horse and zebra images. That’s very powerful because it can be easy to passively collect data in each domain, but it’s time-consuming and expensive to get data that matches up across domains.The Batch:What are you most hopeful about in GAN research?Goodfellow:I’d like to see more use of GANs in the physical world, specifically for medicine. I’d like to see the community move toward more traditional science applications, where you have to get your hands dirty in the lab. That can lead to more things that have more of a tangible, positive impact on peoples’ lives. For instance, in dentistry, GANs have been used to makepersonalized crownsfor individual patients. Insilico is using GANs todesign medicinal drugs.The Batch:How much do you worry about bias in GAN output? The ability to produce realistic human faces makes it a pressing issue.Goodfellow:GANs can be used to counteract biases in training data by generating training data for other machine learning algorithms. If there’s a language where you don’t have as much representation in your data, you can oversample that. At Apple, we were able to generate data for a gestural text-entry feature calledQuickPath. A startup called Vue.ai uses GANs to generate images ofwhat clothing would look like on different models. Traditionally, there may not have been much diversity in who was hired to be a model to try on this clothing. Now you can get a model who looks like you, wearing a specific item of clothing you’re interested in. These are baby steps, but I hope there are other ways GANs can be used to address issues of underrepresentation in datasets.\n\nTrying on new makeup is a hassle — apply, inspect, wash off, repeat. Not to mention the tribulation of visiting brick-and-mortar stores during the pandemic. Augmented reality is helping people try on all the makeup they want without leaving home.What’s new:Modiface, a subsidiary of beauty giant L’Oréal,uses GANsto let customers see how different colors of lipstick, eye shadow, and hair will look on them.How it works:Modiface’s approach is a hybrid ofCycleGAN,StarGANandStyleGAN, company operations chief Jeff Houghton toldThe Batch.It uses a CNN to track landmarks on a user’s hair and face. At L’Oreal’swebsite, users can select different lipsticks, eyeliners, blushes and hair dyes and fine-tune their shade, texture, and gloss. Then they can virtually apply those products to an uploaded selfie or a real-time video of their own faces.\n\nBehind the news:Augmented reality applications are reshaping the beauty industry. In 2018, the same year L’OréalpurchasedModiface, American chain UltaacquiredGlamst, a startup specializing in augmented reality.Meitu, a multi-billion dollar Chinese company, uses AI-driven face manipulation to make its users appear more attractive in social media posts, job applications, and other digital venues.Why it matters:E-commerce is increasinglyimportantto the beauty industry’s bottom line, and the pandemic isdrivingeven more business to the web. Tools like this make it easier for customers to try out products, which may boost sales.We’re thinking:It seems like we’re spending half our lives in video conferences. Do we still need to apply makeup at all, or can we let a GAN do it instead?\n\nLearn how to build a CycleGAN and control image features inCourses 1 and 2of the GANs Specialization from DeepLearning.AI, available now on Coursera. Build the powerful StyleGAN inCourse 3: Apply GANs, coming soon!\n\nA typical GAN’s output doesn’t necessarily reflect the data distribution of its training set. Instead, GANs are prone to modeling the majority of the training distribution, sometimes ignoring rare attributes — say, faces that representminority populations. A twist on the GAN architecture forces its output to better reflect the diversity of the training data.What’s new:IMLE-GANlearns to generate all the attributes of its training dataset, including rare ones. Ning Yu spearheaded the research with colleagues at University of Maryland, Max Planck Institute, University of California Berkeley, CISPA Helmholtz Center for Information Security, Princeton’s Institute for Advanced Study, and Google.Key insight:A GAN’s discriminator distinguishes whether or not the generator’s output is generated, while the generator learns to produce output that fools the discriminator. Ideally, a generator’s output would mirror the training data distribution, but in practice — since its only aim is to fool the discriminator, and the discriminator typically evaluates only one image at a time — it can learn to favor common types of examples. The authors had their model compare several generated works with examples from the training set, as well as interpolations between generated works, to encourage greater diversity in the output.How it works:IMLE-GAN enhances a GAN withImplicit Maximum Likelihood Estimation(IMLE). Instead of naively adding the IMLE loss to the usual adversarial loss, the authors modified the default IMLE loss and added a novel interpolation loss to compensate for fundamental incompatibilities between the adversarial and IMLE losses.\n\nResults:The authors evaluated IMLE-GAN againstStyleGANand a handful of other models usingStacked MNIST, a variation of the MNIST dataset that includes handwritten digits in 1,000 distinct styles. IMLE-GAN reproduced 997 of the styles, while StyleGAN reproduced 940. Trained onCelebA, a large-scale dataset of celebrity faces, IMLE-GAN generated attributes present in less than 6 percent of training examples with increased precision compared to StyleGAN. For instance, it generated wearers of eyeglasses with 0.904 precision, compared to StyleGAN’s meager 0.719.Why it matters:Much of the time, we want our models to learn the data distribution present in the training set. But when fairness or broad representation are at stake, we may need to put a finger on the scale. This work offers an approach to making GANs more useful in situations where diversity or fairness is critical.We’re thinking:This work helps counter model and dataset bias. But it’s up to us to make sure that training datasets are fair and representative.\n\nLearn about what to consider when evaluating a GAN for your application and the potential impact of biases inCourse 2: Build Better GANs, available now on Coursera.\n\nConvolutional neural networks are good at recognizing disease symptoms in medical scans of patients who were injected with iodine-based dye, known as radiocontrast, that makes their organs more visible. But some patients can’t take the dye. Now synthetic scans from a GAN are helping CNNs learn to analyze undyed images.What’s new:Researchers from the U.S. National Institutes of Health and University of Wisconsindevelopeda GAN that generates labeled, undyed computerized tomography (CT) images of lesions on kidneys, spleens, and livers. They added these images to real-world training data to improve performance of a segmentation model that marks lesions in diagnostic scans.How it works:The work is based onCycleGANand theDeepLesiondataset of CTs. CycleGAN has been used toturn pictures of horses into pictures of zebraswithout needing to match particular zebra and horse pics. This work takes advantage of that capability to map between dyed and undyed CTs.\n\nResults:Tested on undyed, real-world CT scans, the U-Net trained on the combination of CycleGAN output and natural images outperformed the others. It was best at identifying lesions on kidneys, achieving a 57 percent improvement over the next-best model. With lesions on spleens, the spread was 4 percent; on livers, 3 percent. In estimating lesion volume, it achieved  an average error of 0.178, compared to the next-highest score of 0.254. Tested on the remainder of the dyed DeepLesion images, all four U-Nets isolated lesions roughly equally well.Behind the news:The researchers behind this model have used it to improve screening for dangerous levels ofliverfatand to identify patients with high risk ofmetabolic syndrome, a precursor to heart disease, diabetes, and stroke.Why it matters:Medical data can be hard to come by and labeled medical data even more so. GANs are making it easier and less expensive to create large, annotated datasets for training AI diagnostic tools.We’re thinking:Medical AI is just beginning to berecognized by key healthcare playersin the U.S. Clever uses of CycleGAN and other architectures could accelerate the process.\n\nTo learn more about using GANs to augment training datasets, including the pros and cons, stay tuned for GANs SpecializationCourse 3: Apply GANs, coming soon to Coursera.\n\nWe’re thrilled to announce the launch of our new Generative Adversarial Networks Specialization on Coursera!Enroll now\n\nDeepfakes have gone mainstream, allowing celebrities to star incommercialswithout setting foot in a film studio. A new method helps determine whether such endorsements — and other images produced by generative adversarial networks — are authentic.What’s new:Lucy Chai led MIT CSAIL researchers in an analysis of where image generators fool and where they fail. They developed atechniqueto detect portions of an image that betray fakery.Key insight:Large-scale features of generated images are highly varied, but generated textures contain consistent artifacts. Convolutional neural networks (CNNs) are especiallysensitive to textures, which makes them well suited to recognizing such artifact-laden areas. A CNN tailored for analyzing small pieces of images can learn to recognize parts dominated by generated textures.How it works:The authors built classifiers that survey images one patch at a time. They ran the classifiers on output fromStyleGAN,Glow,and a generator model based onGaussian mixture models(GMMs). They averaged the patchwise classifications to analyze each GAN’s vulnerability to detection.\n\nResults:The authors’ best classifier achieved 100 percent average precision on StyleGAN output and 91.38 percent on GMM. These scores outperformed non-truncatedMesoInception4, Resnet-18, Xception, and CNN models, which achieved average precision between 99.75 and 73.33 percent. On Glow, the authors’ best classifier achieved 95 percent average precision, whereas the best full model scored 97.32 percent.Why it matters:The better GANs become, the more useful they can be for both good andill. In shedding light on areas where particular GANs produce more artifacts, this work illuminates pathways for researchers to improve them. But it also provides a map for malefactors to make their activities harder to detect. In fact, when the researchers trained a GAN to fool their classifiers, accuracy fell to less than 65 percent.We’re thinking:Building a discriminator that recognizes a particular generator’s output is easier than building a good generator. In fact, GAN researchers routinely degrade discriminators to give the generator a fighting chance to fool it. But social media platforms, among others, would like to catchallgenerated images, regardless of the generator that produced them. Looking for common artifacts offers a promising approach — until a variety of generators learn how to avoid producing them.\n\nLearn how image translation is used to create deepfakes in the upcomingCourse 3: Apply GANs, available soon on Coursera.\n\nGANs are adept at mapping the artistic style of one picture onto the subject of another, known as style transfer. However, applied to the fanciful illustrations in children’s books, some GANs prove better at preserving style, others better at preserving subject matter. A new model is designed to excel at both.What’s new:Developed by researchers at Hacettepe University and Middle East Technical University, both in Turkey,Ganillaaims to wed photographic content and artistic style for illustrations in children’s books. It converts photos into virtual artwork in the styles of 10 published children’s book illustrators, including favorites like Patricia Polacco and Kevin Henkes, while staying true to scenes in photos.\n\nHow it works:Ganilla is almost identical toCycleGANexcept for a specially crafted generator.\n\nResults:There’s no way to measure objectively how well a model generates landscapes in specific artistic styles, so the authors used quantitative and qualitative approaches to compare Ganilla’s output with that of a CycleGAN,DualGAN, andCartoonGANtrained on the same data.\n\nYes, but:Based on examples in the paper, the training illustrations tended to be heavy on stylized human and animal characters, while the photos contain very few characters. We’re curious to see what Ganilla would do with more photos of people and animals.\n\nWhy it matters:GANs are powerful creative tools, and — like printmaking and photography before them — they’re spawning their own adversarial dynamic in the arts. Artists working in traditional media haveraised concernsabout GANs being trained to make derivatives of their work. Now, digital artists areaccusingtraditional artists of creative theft for making paint-on-canvas reproductions of their AI-abetted digital compositions.We’re thinking:When it comes to art, we favor GANs as acreative partner.\n\nLearn about human and algorithmic approaches to evaluating generative adversarial networks in GAN SpecializationCourse 2: Build Better GANson Coursera. To build your own CycleGAN for style transfer, stay tuned forCourse 3: Apply GANS, coming soon to Coursera!",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-3020at2012.03.3120PM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker201-2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-2920at201.03.5020PM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker-2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2021.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2023.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2028.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2022.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2027.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-244/",
    "title": "issue 244",
    "date": "",
    "reading_time": "",
    "content": "Planning is a keyagentic AI design patternin which we use a large language model (LLM) to autonomously decide on what sequence of steps to execute to accomplish a larger task. For example, if we ask an agent to do online research on a given topic, we might use an LLM to break down the objective into smaller subtasks, such as researching specific subtopics, synthesizing findings, and compiling a report.\n\nMany people had a “ChatGPT moment” shortly after ChatGPT was released, when they played with it and were surprised that it significantly exceeded their expectation of what AI can do. If you have not yet had a similar “AI Agentic moment,” I hope you will soon. I had one several months ago, when I presented a live demo of a research agent I had implemented that had access to various online search tools.\n\nI had tested this agent multiple times privately, during which it consistently used a web search tool to gather information and wrote up a summary. During the live demo, though, the web search API unexpectedly returned with a rate limiting error. I thought my demo was about to fail publicly, and I dreaded what was to come next. To my surprise, the agent pivoted deftly to a Wikipedia search tool — which I had forgotten I’d given it — and completed the task using Wikipedia instead of web search.\n\nThis was an AI Agentic moment of surprise for me. I think many people who haven’t experienced such a moment yet will do so in the coming months. It’s a beautiful thing when you see an agent autonomously decide to do things in ways that you had not anticipated, and succeed as a result!\n\nMany tasks can’t be done in a single step or with a single tool invocation, but an agent can decide what steps to take. For example, to simplify an example from the HuggingGPT paper (cited below), if you want an agent to consider a picture of a boy and draw a picture of a girl in the same pose, the task might be decomposed into two distinct steps: (i) detect the pose in the picture of the boy and (ii) render a picture of a girl in the detected pose. An LLM might be fine-tuned or prompted (with few-shot prompting) to specify a plan by outputting a string like\"{tool: pose-detection, input: image.jpg, output: temp1 } {tool: pose-to-image, input: temp1, output: final.jpg}\".\n\nThis structured output, which specifies two steps to take, then triggers software to invoke a pose detection tool followed by a pose-to-image tool to complete the task. (This example is for illustrative purposes only; HuggingGPT uses a different format.)\n\nAdmittedly, many agentic workflows do not need planning. For example, you might have an agent reflect on, and improve, its output a fixed number of times. In this case, the sequence of steps the agent takes is fixed and deterministic. But for complex tasks in which you aren’t able to specify a decomposition of the task into a set of steps ahead of time, Planning allows the agent to decide dynamically what steps to take.\n\nOn one hand, Planning is a very powerful capability; on the other, it leads to less predictable results. In my experience, while I can get the agentic design patterns ofReflectionandTool useto work reliably and improve my applications’ performance, Planning is a less mature technology, and I find it hard to predict in advance what it will do. But the field continues to evolve rapidly, and I'm confident that Planning abilities will improve quickly.\n\nIf you’re interested in learning more about Planning with LLMs, I recommend:\n\nKeep learning!AndrewP.S. Making sure your RAG system has access to the data it needs to answer questions is an important, but often laborious, step for good performance. Our new short course “Preprocessing Unstructured Data for LLM Applications,” taught by Matt Robinson of Unstructured, teaches you how to build systems that can easily ingest data from a wide range of formats (like text, images, and tables) and from many different sources (like PDF, PowerPoint, and HTML). You’ll learn practical ways to extract and normalize content from diverse formats, enrich your content with metadata to enable more powerful retrieval and reasoning, and use document layout analysis and vision transformers to process embedded images and tables. Putting these components together, you’ll build a RAG bot that draws from multiple document types, demonstrating how high-quality data ingestion and preprocessing affect the quality of RAG output.Sign up here!\n\nNew coding tools act like agents to automate software programming tasks.\n\nWhat’s new:A wave of open source software-development tools based on large language models take advantage of the ability of large language models to plan, critique their own work, and extend themselves by calling functions.\n\nHow it works:These projects follow hot on the heels of Cognition’sDevin, a commercial system billed as a semi-autonomous software developer that’s available to selected customers upon request. Some, like Devin, provide sandboxed chat for natural-language commands, command line shell, code editor, and/or a web browser through which the agent can test code or find documentation. Given a prompt, they generate a step-by-step plan and execute it. They may ask for further information or instructions, and users can interrupt to modify their requests.\n\nBehind the News:Code-completion tools like Github Copilot and Code Llama quickly have becomeubiquitous.AutoGPT, released in 2023, is an open-source generalist AI agent based on GPT-4 that has been used to write and debug code. Recently Replit, known for its Ghostwriter code-completion and chatbot applications, began buildingits own LLMsfor automated code repair.\n\nWhy it matters:Agentic coding tools are distinguished bytechniquesthat enable large language models to plan, reflect on their work, call tools, and collaborate with one another. Usersreportthat, unlike previous coding assistants, the new tools are better at sustaining extended tasks and correcting their own work.\n\nWe’re thinking:Many software developers worry that large language models will make human coders obsolete. We doubt that AI will replace coders, but we believe that coders who use AI will replace those who don’t. Agent-based tools still have a long way to go, but they seem likely to augment programmers’ abilities in a larger development pipeline.\n\nGenerative AI is being used mostly to generate ideas.\n\nWhat’s new:The tech consultancy Filteredstudiedthe most common uses for generative AI. While most gen AI users produced text, the study surprisingly found that users were slightly more likely to generate videos than images.How it works:The analysts sifted through tens of thousands of posts on popular online forums for anecdotes that described uses of generative AI. The analysts grouped the posts into a list of 100 most popular uses of generative AI and ranked each one by reach and value added.\n\nBehind the news:The range of use cases reflects the huge number of people, from all walks of life and all parts of the world, who are using generative AI tools. In a given week in November 2023, more than 100 million peopleusedChatGPT, the most popular of these tools. Independently, in February 2024, Pew Researchfoundthat 23 percent of U.S. adults had used ChatGPT at least once, including 43 percent of respondents under 30 years old and 37 percent of those with postgraduate degrees. According to the Pew report, 20 percent of all Americans had used ChatGPT for work, and 17 percent had used it for entertainment, with younger and more educated users leading the way.Why it matters:It’s clear that millions of people use generative AI but less clear how they use it. Understanding how and where they actually apply it is helpful for anyone who aims to develop new generative AI products and services or plans to integrate the tech into their organization.\n\nWe’re thinking:While it’s encouraging that more than a fifth of U.S. adults have tried ChatGPT,  it also suggests huge room for growth in generative AI at large.\n\nIntegrate diverse data types into your LLM applications in our new short course built in collaboration with Unstructured. Learn techniques to extract and normalize data from PDFs, tables, and images into a structured format.Sign up today\n\nThe CEO of Stability AI resigned as the company faces an increasingly competitive market.\n\nWhat’s new:Emad Mostaque stepped down from Stability AI, developer of the Stable Diffusion image generator among other models, amid financial woes, uncertain direction, and sinking confidence from investors and employees alike,Forbesreported. Mostaque’s departure followed the exits of numerous executives and key employees.\n\nHow it works:StabilityconfirmedMostaque’s departure in a blog post. The company’s chief operating officer Shan Shan Wong and chief technology officer Christian Laforte will act as co-CEOs until its directors find a permanent replacement. They inherit a company with troubles beyond leadership.\n\nBehind the news:Despite its troubles, Stability continued to release new models. In February, itopenedthe waitlist for the third-generation version of Stable Diffusion. Last month, itreleasedStable Video 3D, a project in which the team produced three-dimensional objects from images. This month, itreleasedStable Audio 2.0, which can produce music files up to three minutes long from a text prompt.Why it matters:Stability has been a standard bearer for open-source AI in a field where tech giants aim to dominate with closed models. Effective leadership could have a major impact on the models available to developers in the years ahead.\n\nWe’re thinking:Stability helped capture the public imagination during the generative AI boom of 2022, and its open models, particularly its diffusion models, have been a huge benefit to the AI community. We hope new leadership puts the company on firm footing.\n\nAn architectural innovation improves upon transformers — up to 2 billion parameters, at least.\n\nWhat’s new:Albert Gu at Carnegie Mellon University and Tri Dao at Princeton University developed theMambaarchitecture, a refinement of the earlier state space sequence architecture. A relatively small Mamba produced tokens five times faster and achieved better accuracy than a vanilla transformer of similar size while processing input up to a million tokens long.\n\nStructured State Space Sequence (S4) basics:S4s, also known as structured SSMs, can be functionally similar to recurrent neural networks (RNNs): They can accept one token at time and produce a linear combination of the current token and an embedding that represents all previous tokens. Unlike RNNs and their extensions including LSTMs — but like transformers — they can also perform an equivalent computation in parallel during training. In addition, they are more computationally efficient than transformers. An S4’s computation and memory requirements rise linearly with input size, while a vanilla transformer’s rise quadratically — a heavy burden with long input sequences.\n\nKey insight:S4s are more efficient than transformers but, while a transformer’s input length is limited only by processing and memory, an S4’s input length is limited by how well its hidden state can represent previously input tokens as new tokens arrive. Agating mechanismthat lets the model process the most important parts of an input and ignore the rest can enable it to process longer inputs. One viable gate: Typically S4s apply the same mathematical function to all input tokens, whose parameters consist of four learned matrices. Changing the matrices for each input enables the model to learn which tokens or parts of tokens are least important and can be ignored (set to zero). This condenses the input, enabling the modified S4 to process very long input sequences.\n\nHow it works:Mamba is made up of blocks, each of which includes a modified S4 (which the authors call a selective SSM). The authors pretrained different instances on a variety of tasks including generating tokens fromThe Pile(a collection of text from the web) and predicting DNA base pairs inHG38(a single human genome) in sequences up to 1 million tokens long.\n\nResults:Mamba achieved better speed and accuracy than transformers of similar size, including tasks that involved inputs of 1 million tokens.\n\nYes, but:The authors tested model sizes much smaller than current state-of-the-art large language models.\n\nWhy it matters:Google’s transformer-based Gemini 1.5 Pro offers context lengths up to 1 million tokens, but methods for building such models aren’t yet widely known. Mamba provides an alternative architecture that can accommodate very long input sequences while processing them more efficiently. Whether it delivers compelling benefits over large transformers and variations that provide higher efficiency and larger context is a question for further research\n\nWe're thinking:Research on Mamba is gaining momentum. Other teams are probing the architecture in projects likeMotion Mamba,Vision Mamba,MoE-Mamba,MambaByte, andJamba.\n\nReadData Pointsto find the latest AI updates of the week, including:\n\n👉 Advanced new editing tools by DALL·E👉 Command R+, a new LLM by Cohere👉 An update on big tech's hunt for AI training\n\nAnd more!\n\nCheck out Data Points now.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-10T140722.194.png",
      "https://dl-staging-website.ghost.io/content/images/2024/04/DEVIN-PLUS-v6_1200px.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-10T141020.606.png",
      "https://dl-staging-website.ghost.io/content/images/2024/04/The-Batch-ads-and-exclusive-banners---2024-04-09T091357.311.png",
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-10T141216.341.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/04/unnamed---2024-04-10T141759.545.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-69/",
    "title": "issue 69",
    "date": "",
    "reading_time": "",
    "content": "Like many people in the AI community, I am saddened by the sudden departure from Google of ethical AI researcher Timnit Gebru. Timnit is a tireless champion of diversity and fairness in AI. Her work, for example highlighting bias in face recognition systems, has been a productive influence on many researchers and companies. At the same time, my friend Jeff Dean built Google AI into a world-class engineering organization. I’ve seen him speak up for diversity when no one else in the room was doing so.\n\nHaving not yet spoken to either of them, I hesitate to offer my opinion on the matter at this time. But the situation highlights a larger problem in the AI community: lack of a shared set of values (such as fairness, diversity, and transparency) and norms (such as what to do when there’s a problem).\n\nIn academia, all scholars place high value on the pursuit and dissemination of knowledge. In medicine, all doctors recognize that the wellbeing of patients is their primary duty. We need that kind universal commitment in AI.\n\nWe’re building technology that affects billions of people without a coherent set of guiding principles. Many companies and think tanks have published their own codes of ethics, and these statements are important — but they are far from sufficient. We need a set of values and norms that are shared across our entire community and transcend any one company. That way, we can collectively hold individuals, companies, and perhaps even governments accountable to them and operate for the common good even when we disagree.\n\nHow can we bring the AI community together around shared values and norms? I encourage you to spend time with your teams, collaborators, and peers to discuss this difficult question. It’s past time to lay the foundation for a set of values and norms that all AI practitioners will proudly stand up for.\n\nKeep learning!\n\nAndrew\n\nHelium balloons that beam internet service to hard-to-serve areas are using AI to navigate amid high-altitude winds.\n\nWhat’s new:Loon, the Alphabet division that provides wireless internet via polyethylene blimps, used reinforcement learning todevelopan autonomous control system that keeps the vehicles closer to their targets while consuming less energy than its hand-coded predecessor. The new algorithm controls Loon’s fleet over Kenya, where the companylaunchedits first commercial service in July.\n\nHow it works:Balloons navigate by ascending or descending to catch winds that push them in the direction desired. Loon usedQR-DQN, a distributional reinforcement learning algorithm, to train a feed-forward network to determine when the balloon should ascend, descend, or stay put.\n\nBehind the news:Loon began within Alphabet’s experimental X division in the early 2010s and became a for-profit subsidiary in 2018. The company provided emergency internet access toPuerto Ricoafter hurricane Maria in 2017, and toPerufollowing a massive earthquake in 2019. A single balloon can serve several thousand individuals spread over 80 square kilometers.\n\nWhy it matters:Billions of people, includingtwo-thirdsof all school-age children, don’t have access to the internet. In the Covid era, with students and workers alike staying home, the digital divide is more acute than ever. Cutting the cost of service to remote areas could bring many of those people into the information economy.We’re thinking:In Kenya, where Loon’s first balloons are flying, better connections could boost the growing community of AI engineers. To learn more about Kenya’s AI scene, check out our Working AI profile of data scientist and DeepLearning.AI ambassadorKennedy Kamande Wangari.\n\nThe latest pretrained language models have shown a remarkable ability tolearn facts. A new study drills down on issues of scale, showing that such models might learn the approximate weight of a dog or cost of an apple, at least to the right order of magnitude.What’s new:Xikun Zhang and Deepak Ramachandran with colleagues at Stanford, Google, AI2 Israel, Bar Ilan University, and University of Pennsylvania probed whether word embeddings produced by pretrained models encode knowledge of objects’ mass, length, or price.Key insight:Pretrained features that represent words may or may not capture scale-bound attributes. To find out, the authors built simple linear models that took the pretrained embeddings as a starting point and trained them on a dataset that explicitly associates words with such attributes. If the models learned to estimate such attributes, they reasoned, then the pretrained embeddings did, indeed, represent them.How it works:The authors analyzed features generated byELMoandBERT, whose embeddings vary depending on context, as well as the earlierword2vec, a fixed set of embeddings. They also tested features generated by their own model, NumBERT, which is identical to BERT except that numerals in its pretraining data were replaced by the same numbers in scientific notation.\n\nResults:The linear models matched the DoQ measures with greater-than-random accuracy. Those that used embeddings from ELMo, BERT, and NumBERT produced better performance than those that used word2vec. To evaluate whether the linear models generalized beyond DoQ, the authors tested them oncomparing sizes and weightsbetween pairs of objects. The regression model that used NumBERT embeddings achieved accuracy of 0.76, outperforming BERT (0.71), ELMo (0.72), and word2vec (0.74). The classification model that used NumBERT embeddings likewise outperformed the others but achieved lower accuracy.Why it matters:The latest language models have comeunderfirefor being less smart than their accomplishments might suggest. But how much less smart? Studies like this help quantify the deficits so we can work toward improving them.We’re thinking:Language models also need to understand scale distinctions based on modifying words such as the difference between “watch” and “gold watch,” or between “Yoda” and “Baby Yoda.”\n\nA new consortium of companies, schools, and research labs is building open tools for next-generation machine learning.\n\nWhat’s new:MLCommonsaims to foster innovation in machine learning by developing new benchmarks, datasets, and best practices. Its founding board includes representatives of Alibaba, Facebook, Google, Intel, and DeepLearning.AI’s sister company Landing AI.Fresh resources:The group kicked off by releasing two products:\n\nBehind the news:MLCommons grew out of the development ofMLPerf, a benchmark for measuring hardware performance on machine learning tasks. MLCommons will continue to steward MLPerf.Why it matters:Publicly available datasets and benchmarks have spurred much of AI’s recent progress. Producing such resources is expensive, and doing it well requires expertise from several subdisciplines of AI. MLCommons brings together more than 50 organizations to keep the community fueled with the tools necessary to continue innovating.We’re thinking:Datasets from theLinguistic Data Consortiumand others have been a boon for speech recognition research in academia, but academic researchers still lack datasets on the scale used by big tech companies. Access to 87,000 hours of speech will help these groups to develop cutting-edge speech systems.\n\n“Advanced Computer Vision with TensorFlow,” Course 3 in ourTensorFlow: Advanced Techniques Specialization, is now available on Coursera!Enroll now\n\nA computer vision system is helping to keep runners with impaired vision on track.What’s new:A prototype smartphoneappdeveloped by Google translates camera images into audio signals. Guidelines produces sounds that indicate deviations from lines painted on a path, enabling people to correct their course without using their eyes, according toVentureBeat.How it works:Users strap an Android device to their belly and listen through bone-conducting headphones that provide both auditory and haptic feedback but don’t block street sounds. All processing is performed on the device and requires no Internet connection.\n\nBehind the news:Other AI-powered accessibility apps are also helping people with sensory impairments live more independently.\n\nWhy it matters:Apps like this could bring more independence to thehundreds of millionsof people worldwide who have a serious visual impairment. And they might help people who can’t see get around town for activities other than exercise.We’re thinking:Some of Andrew’s early efforts in deep learning were inspired by the work ofBrainPortTechnologies, which showed that you can take an image, map it to a pattern of pressures on a blind person’s tongue, and thereby help them “see” with their tongue. This raises an intriguing question: Is there single learning algorithm that, depending only on the input data it receives, can learn to process either vision or touch?\n\nLabor unions aim to give workers more protection against the automated systems that increasingly rule the workplace.What’s new:The Trades Union Congress (TUC), a federation of unions in England and Wales that represents over 5.5. million workers,launcheda task force to lobby for collective bargaining, increased transparency, and legal protections related to AI in the workplace.Challenges of automation:The move comes after the TUC produced areportthat found widespread unease among British workers over automated systems that perform management tasks like scheduling shifts, analyzing performance, and determining layoffs.\n\nWhy it matters:Earlier this year, astudyfound that many companies hesitate to deploy AI models for fear of public and legal backlash. Bringing workers into the process of deciding whether, when, and how to deploy them in the workplace could help overcome the fear and distrust — thought it could also slow AI adoption.We’re thinking:AI is helping companies become more efficient. It also puts more power in the hands of employers by enabling them to manage workers in ways that were not possible before. We welcome efforts to ensure fair treatment of employees.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-12-0920at2011.48.1320AM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/LOON.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2042.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/MLCOMMONS.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Gif20420v2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Blind20420Gif.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-12-0820at2012.17.0220PM.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-162/",
    "title": "issue 162",
    "date": "",
    "reading_time": "",
    "content": "Stable Diffusion, an image generation model that takes a text prompt and produces an image, was released a few weeks ago in a landmark event for AI. While similar programs like DALL·E and Craiyon can be used via API calls or a web user interface, Stable Diffusion can be freelydownloadedand run on the user’s hardware.\n\nI'm excited by the artwork produced by such programs (Developer Simon Willison posted a fun tweetstorm thathighlightssome of the creativity they’ve unleashed), but I’m also excited by the ways in which other developers areincorporatingit into their own drawing tools. Ironically, Stable Diffusion’s manner of release moves us closer to “open AI” than the way DALL·E was released by the company called OpenAI. Kudos to Emad Mostaque and his Stability AI team, which developed the program.\n\nIf you want to learn about how diffusion models like Stable Diffusion work, you can find a concise descriptionhere.Image generation is still maturing, but it’s a big deal. Many people have the creativity to produce art but lack the drawing skill to do so. As an amateur illustrator (I like to draw pandas to entertain my daughter using theProcreatepaint app), my meager skill limits what I can create. But sitting in front of the DALL·E or Stable Diffusion user interface, I can ask her what she wants to see a panda doing and render a picture for her.\n\nArtists who have greater skill than I can use image generators to create stunning artworks more efficiently. In fact, an image produced this way recentlywonan art competition at the Colorado State Fair.\n\nThe rise of inexpensive smartphone cameras brought an explosion in photography, and while expensive DSLRs still have a role, they now produce a minuscule fraction of all pictures taken. I expect AI-powered image generators to do something similar in art: Ever-improving models and user interfaces will make it much more efficient to generate art using AI than without. I see a future where most art is generated using AI, and novices who have great creativity but little drawing skill will be able to participate.My friend and collaborator Curt Langlotz, addressing the question of whether AI will replace radiologists, said that radiologists who use AI will replace radiologists who don’t. The same will be true here: Artists who use AI will (largely) replace artists who don’t. Imagine the transition in the 1800s from the time when each artist had to source their own minerals to mix shades of paint to when they could purchase ready-mixed paint in a tube. This development made it easier for any artist to paint whatever and whenever they wished. I see a similar transition ahead. What an exciting time!Separately from generating images for human consumption, these algorithms have great potential to generate images for machine consumption. A number of companies have been developing image generation techniques to produce training images for computer vision algorithms. But because of the difficulty of generating realistic images, many have focused on vertical applications that are sufficiently valuable to justify their investment, such as generating road scenes to train self-driving cars or portraits of diverse faces to train face recognition algorithms.\n\nWill image generation algorithms reduce the cost of data generation and other machine-to-machine processes? I believe so. It will be interesting to see this space evolve.\n\nKeep learning!\n\nAndrew\n\nSince September 2019, DeepLearning.AI’s network of Pie & AI Ambassadors has brought the AI community together at more than 700 events in 61 countries. Read about these leaders, their events, and how they’re turning their local areas into AI hubs.Learn more\n\nAn online marketplace enables people to buy text prompts designed to produce consistent output from the new generation of text-to-image generators.\n\nWhat’s new:PromptBase is a virtual marketplace for bespoke text strings designed as input for programs likeDALL·E 2,Midjourney, andStable Diffusion,The Vergereported.\n\nHow it works:Buyers can browse PromptBase by specifying the desired system, searching categories such as “jewelry” or “wallpaper,” or typing in keywords. They can click to purchase the prompt via credit card or Google Pay. The site, which launched in June, has 50,000 active monthly users.\n\nWhat they’re saying:“Every word in a prompt has a weight associated with it, so trying to work out what works best and where becomes a core asset in the skillset,” prompt engineer Justin Reckling, toldThe Verge.\n\nBehind the News:Designer and illustrator Guy Parsons offersThe DALL·E 2 Prompt Book, a compendium of tips for producing effective prompts for text-to-image generators. The book offers several pages of tips including words that describe specific art styles, materials, compositional structures, colors, and emotions, as well as words that can influence photorealistic output such as camera angles, settings, lenses, lighting, film stocks, and so on. Moreover, research published last yearinvestigatesthe relationship between prompt structure, model parameters, and text-to-image output. The work presents a number of helpful guidelines such as, “Keep the focus on keywords rather than rephrasings.”\n\nWhy it matters:AI-driven media generators are opening a universe of productivity in imagery, text, and music. Marketplaces for effective prompts can supercharge these already-powerful tools by cutting the time it takes to generate desirable output. They can also serve as training grounds for the emerging discipline ofprompt engineering: the craft of addressing generative models in ways that yield precise, repeatable output.\n\nWe’re thinking:While they may not immediately replace professional illustrators — many generated images require touching up for professional purposes — image generators are becoming a staple tool of artists and graphic designers and seem likely to put many of them out of work. We hope that prompt engineering can provide an alternative livelihood for some.\n\nA U.S. court ruled against an implementation of AI-powered software designed to catch students who cheat on academic examinations.\n\nWhat’s new:A federal judge determined that Cleveland State University’s use of Honorlock, a system that scans students’ behavior and surroundings for signs of cheating, violates their rights,National Public Radioreported.\n\nHow it works:Students install Honorlock as a web browser extension and permit access to the computer’s microphone and camera.\n\nThe case:In 2021, Cleveland State University student Aaron Ogletreesuedthe school for subjecting him to a virtual room scan, which he claimed violated his Constitutional protection against unreasonable searches. He complied with the scan but filed suit later. The university argued that a room scan doesn’t constitute a search because it’s limited in scope and conducted to ensure academic integrity. The judge ruled that the university had violated Ogletree’s rights.\n\nBehind the News:Scientific investigations of other AI-powered proctoring systems have reached conflicting conclusions about their effectiveness.\n\nWhy it matters:Automated proctoring has value, especially in the era of remote education. Although the ruling against Cleveland State applies only to that school, it raises questions about the legality of such automated room scans nationwide.\n\nWe’re thinking:While the judge's decision ostensibly affects AI-powered proctor software, many institutions use human proctors who might occasionally request a manual room scan. The underlying question —  what proctoring methods are reasonable, ethical, fair, and legal? — is independent of whether machines or humans should do the job.\n\nJoin us on September 28, 2022, for “Beyond Jupyter Notebooks: MLOps Environment Setup and First Deployment.” This live workshop will show you how to set up your computer to build and deploy machine learning applications so you can run your models in production environments! Registerhere\n\nFactories are using AI to warn them when equipment is reaching the breaking point.\n\nWhat’s new:Services that monitor machinery to predict imminent failure and provide guidance on necessary upkeep are booming,The Wall Street Journalreported.\n\nHow it works:Predictive maintenance systems anticipate breakdowns based on historical and real-time data collected from industrial machinery, enabling maintenance personnel to schedule repairs before they incur costly downtime.\n\nBehind the news:Sales of predictive maintenance services stood at around $4 billion in 2020. The global total is expected to reach $18.6 billion by 2027, expanding at a compound annual growth rate of 24.5 percent,according tothe research firm Research and Markets.\n\nWhy it matters:Supply-chain problems have bedeviled industrial companies since the onset of the Covid-19 pandemic. By predicting when a machine is likely to fail, AI can help them avoid costly outages and enable them to stock up on replacement parts ahead of time.\n\nWe’re thinking:Predictive maintenance helps reduce costs on an industrial scale, but could it be adapted for households? Imagine if your washing machine could figure out for itself whether that ominous knocking sound during the spin cycle was just a momentary annoyance or truly worrisome.\n\nThe ability to update language models is essential to incorporate new information and correct undesirable behaviors. Previous methods are unwieldy and often fail as the amount of new data increases. New work offers a workaround.\n\nWhat’s New:Eric Mitchell and colleagues at Stanford and École Polytechnique Fédérale de Lausanne proposedSemi-Parametric Editing with a Retrieval-Augmented Counterfactual Model(SERAC), an add-on system that can adapt trained models with an abundance of new information.\n\nKey insight:Say you’ve trained a language model to produce output based on the current Prime Minister of the United Kingdom. You’ll need to retrain the model when the Prime Minister changes. Alternatively you can update the model either by fine-tuning or training a secondary model, known as amodel editor, that estimates and applies the change in weights necessary to respond to queries about the Prime Minister accurately without affecting responses to other queries. However, both approaches have problems. Fine-tuning every time information changes is impractical, and both approaches fail beyond around 10 new pieces of data (as the authors demonstrate without proposing an explanation why). Instead of changing model weights, a separate system can store new data and learn to provide output to queries that are relevant to that data. Such a system would handle any amount of new data and work with any model without retraining.\n\nHow it works:The authors’ system is designed to complement a base model. It consists of three parts. Theedit memorystored facts in the form of input-output pairs. Thescope classifierdetermined whether a new input is relevant to facts stored in the edit memory. Thecounterfactual modelgenerated output for relevant inputs. The base model continued to handle all other queries.\n\nResults:The authors used two metrics, edit success and drawdown, to evaluate SERAC’s ability to update responses from a pretrainedT5-large. Edit success measured the correctness of the T5’s responses to inputs relevant to the contents of the edit memory; higher is better (1 being perfect). Drawdown measured the correctness of responses to inputs not relevant to data in edit memory; lower is better (0 being perfect). SERAC outperformed model editors such asModel Editor Networks with Gradient Decomposition(MEND). On question-answering, SERAC achieved 0.986 edit success compared to MEND’s 0.823, and 0.009 drawdown compared to MEND’s 0.187. The authors applied the SERAC system they’d trained on T5-large to other sizes. Its performance barely budged. Moreover, SERAC continued to outperform as the number of new input-output pairs increased. The authors increased the number of simultaneous pairs to 75. Measuring performance as the difference between edit success and drawdown (the worst possible being -1, best being 1), SERAC’s fell only from 0.98 to around 0.90, while MEND’s degraded from 0.64 to around -0.95.\n\nWhy it matters:This work opens the door to keeping trained language models up to date even as information changes at a rapid clip. Presumably businesses could use it to update information about, say, their products, leadership, numbers of employees, locations, and so on. Developers of conversational models could keep their chatbots abreast of changes in politics, law, and scientific discovery.\n\nWe’re thinking:A single system that can update any language model opens the tantalizing possibility of a product, updated regularly, that can adapt previously trained models to new information.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/unnamed-2.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/unnamed-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/09/unnamed.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/unnamed--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/unnamed--2--1.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/unnamed--1--3.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/unnamed--2-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-262/",
    "title": "issue 262",
    "date": "",
    "reading_time": "",
    "content": "When entrepreneurs build a startup, it is often their speed and momentum that gives them a shot at competing with the tech behemoths. This is true of countries as well.\n\nI was recently in Thailand, where I was delighted to see tremendous momentum building in AI (and sip the best Thai ice tea I’ve ever tasted). Even though Thailand is not as advanced in AI technology or applications as leading tech countries, the enthusiasm for building AI throughout government, corporations, and academia was thrilling. I came away heartened that AI’s benefits will be spread among many countries and convinced that one’s level of AI development right now matters less than your momentum toward increasing it.\n\nSeeing the momentum behind AI in Thailand — where the per capita GDP is around one fifth that of Japan, and one tenth that of the United States — left me feeling that any country, company, or person has a shot at doing meaningful work in the field. While advanced economies such as the U.S. and China are still in the lead, generative AI has made the playing field more level. Foundation models, especially those with open weights, are significantly lowering the barriers to building meaningful AI projects. In Thailand, a lot of people I met weren’t just talking about AI, they were rolling up their sleeves and building. That buys a nation a lot more momentum than just talk.I met with Prime Minister Srettha Thavisin and his Ministers of Higher Education and Education (primary/secondary) along with many staffers. It was delightful to hear the PM speak of his enthusiasm for AI. The ministers discussed how to (i) provide AI training and (ii) use AI to improve education in a variety of subjects. Happily, the focus was on creating value while thinking through realistic risks like AI’s potential to proliferate misinformation, and not a single person asked me about whether AI will lead to human extinction!\n\nI also met with many business leaders and enjoyed seeing a rapid pace of experimentation with AI. KBTG, an affiliate of the country’s leading digital bank KBank, is working on a financial chatbot advisor, AI-based identity verification for anti-fraud, AI for auto insurance, and a Thai-language financial large language model. These features are growing mobile banking and increasing financial access. Many business leaders in other sectors, too, have asked their teams to run experiments. There are many AI applications yet to be built in industrial sectors, tourism, trade, and more! (KBTG is an investor in AI Fund, which I lead.)\n\nI often visit universities in both developed and developing economies, and I’ve been surprised to see that universities in developing economies sometimes adopt AI faster. At Chulalongkorn University (known as Chula), I met with the University President Wilert Puriwat and Director of Chula AI Professor Proadpran Punyabukkana. Chula AI has rolled out campus-wide training in generative AI for faculty, staff, and students. In addition, it supports building AI applications such as AI screening for depression and gastrointestinal cancer.It takes years to build up advanced technology. But momentum matters, and there will be many rewards along the journey. There’s no time like the present to start building!\n\nKeep building,\n\nAndrew\n\nOur short course “Improving Accuracy of LLM Applications” teaches a step-by-step approach to improving the accuracy of applications built on large language models. You’ll build an evaluation framework, incorporate self-reflection, and fine-tune models using LoRA and memory tuning to embed facts and reduce hallucinations.Enroll for free\n\nPrices for access to large language models are falling as providers exploit new efficiencies and compete for new customers.\n\nWhat’s new:Open AIcutthe price of calls to GPT-4o’s API by 50 percent for input tokens and 33 percent for output tokens, with an even steeper discount for asynchronous processing. Not to be outdone, Googlecutthe price of API calls to Gemini 1.5 Flash by approximately 75 percent.\n\nHow it works:The latest price reductions follow a steady trend,trackedby Smol.ai CEO Shawn Wang, in which providers are charging less even as model performance (as measured by LMSys’sChatbot Arena LeaderboardElo ratings) rises. Here’s a list of recent prices in order of each model’s  rank on the leaderboard as of this writing:\n\nBehind the news:Less than six months ago, cutting-edge large language models like GPT-4, Claude 2, Gemini 1.0, Llama 2, and Mistral Large were less capable and more expensive than their current versions. For instance, GPT-4 costs $30/$60 per million tokens input/output. Since then, models have notched higher benchmark performances even prices have fallen. The latest models are also faster, have larger context windows, support a wider range of input types, and do better at complex tasks such as agentic workflows.\n\nWhy it matters:Competition is fierce to provide the most effective and efficient large language models, offering an extraordinary range of price and performance to developers. Makers of foundation models that can’t match the best large models in performance or the best small models in cost are in a tight corner.\n\nWe’re thinking:What an amazing time to be developing AI applications! You can choose among models that are open or closed, small or large, faster or more powerful in virtually any combination. Everyone is competing for your business!\n\nA new company with deep roots in generative AI made an eye-catching debut.\n\nWhat’s new:Black Forest Labs, home to alumni of Stability AI,releasedthe Flux.1 family of text-to-image models under a variety of licenses including open options. The largest of them outperformed Stable Diffusion 3 Ultra, Midourney v6.0, and DALL·E 3 HD in the company’s internal qualitative tests.\n\nHow it works:The Flux.1 models are based on diffusion transformers that were trained usingflow matching, a form of diffusion. Like other latent diffusion models, given text and a noisy image embedding, they learn to remove the noise. At inference, given text and an embedding of pure noise, they remove the noise in successive steps and render an image using a decoder that was trained for the purpose.\n\nResults:Black Forest Labs evaluated the models internally in qualitative tests. Given images produced by one of the Flux.1 family and a competitor, roughly 800 people judged which they preferred for various qualities. The two larger versions achieved high scores.\n\nBehind the news:The Black Forest Labs staff includes former core members of Stability AI, whichlostmany top employees in April. Black Forest CEO Robin Rombach co-authored the papers that introduced VQGAN, latent diffusion, adversarial diffusion distillation, Stable Diffusion XL, and Stable Video Diffusion.\n\nWhy it matters:Text-to-image models generally occupy three tiers: large commercial models like Midjourney v6, OpenAI DALL·E 3, and Adobe Firefly; offerings that are open-source to varying degrees like Stability AI’s Stable Diffusion 3 Medium; and smaller models that can run locally like Stable Diffusion’s Stable Diffusion XL Lightning. The Flux.1 suite checks all the boxes with high marks in head-to-head comparisons.\n\nWe’re thinking:In late 2022, Stability AI’s release of the open Stable Diffusion unleashed a wave of innovation. We see a similar wave building on the open versions of Flux.1.\n\nOpenAI may be spending roughly twice as much money as it’s bringing in, a sign of the financial pressures of blazing the trail in commercial applications of AI.\n\nWhat’s new:OpenAI’s operating expenses could amount to $8.5 billion in 2024, according to anestimatebyThe Informationbased on anonymous sources. Meanwhile, its annual revenue is shaping up to be around $3.5 billion to $4.5 billion, putting it on course to lose between $4 billion and $5 billion this year.\n\nRevenue versus expenses:The report combined previous reporting with new information from people “with direct knowledge” of OpenAI’s finances and its relationship with Microsoft, which provides computing power for GPT-4o, ChatGPT, and other OpenAI products.\n\nWhy it matters:ChatGPT famously grew at an extraordinary pace in 2023 when the number of visitsballoonedto 100 million within two months of the service’s launch. OpenAI’s internal sales team turned that enthusiasm into fast-growing revenue, reportedlyoutpacingeven Microsoft’s sales of OpenAI services. Yet that growth rests on top-performance AI models, which are expensive to develop, train, and run.\n\nWe’re thinking:OpenAI is a costly undertaking: OpenAI CEO Sam Altmansaidit would be “the most capital-intensive startup in Silicon Valley history.” But generative AI is evolving quickly. With OpenAI’s revenue rising, its models becoming more cost-effective (witnessGPT-4o mini), and the cost of inference falling, we wouldn’t bet against it.\n\nLiterary works are challenging to translate. Their relative length, cultural nuances, idiomatic expressions, and expression of an author’s individual style call for skills beyond swapping words in one language for semantically equivalent words in another. Researchers built a machine translation system to address these issues.\n\nWhat’s new:Minghao Wu and colleagues at Monash University, University of Macau, and Tencent AI Lab proposedTransAgents, which uses a multi-agent workflow to translate novels from Chinese to English. You can try a demohere.\n\nKey insight:Prompting a large language model (LLM) to translate literature often results in subpar quality. Employing multiple LLMs to mimic human roles involved in translation breaks down this complex problem into more tractable parts. For example, separate LLMs (or instances of a single LLM) can act as agents that take on roles such as translator and localization specialist, and they can check and revise each other’s work. An agentic workflow raises unsolved problems such as how to evaluate individual agents’ performance and how to measure translation quality. This work offers a preliminary exploration.\n\nHow it works:TransAgents prompted pretrained LLMs to act like a translation company working on a dataset ofnovels. The set included 20 Chinese novels, each containing 20 chapters, accompanied by human translations into English.\n\nResults:Professional translators compared TransAgents’ output with that of human translators and GPT-4 Turbo in a blind test. One said TransAgents “shows the greatest depth and sophistication,” while another praised its “sophisticated wording and personal flair” that “effectively conveys the original text’s mood and meaning.”\n\nWhy it matters:While machine translation of ordinary text and conversations has made great strides in the era of LLMs, literary translation remains a frontier. An agentic workflow that breaks down the task into subtasks and delegates them to separate LLM instances makes the task more manageable and appears to produce results that appeal to human judges (and an LLM as well). That said, this is preliminary work that suggests a need for new ways to measure the quality of literary translations.\n\nWe’re thinking:Agentic workflowsraise pressing research questions: What is the best way to divide a task for different agents to tackle? How much does the specific prompt at each stage affect the final output? Good answers to questions like this will lead to powerful applications.\n\nAre you an experienced developer? Share your coding story and inspire new learners! We’re celebrating the launch of “AI Python for Beginners,” taught by Andrew Ng, and we’d like to feature your story to inspire coders who are just starting out.Submit your story here!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-14T145457.617-1.png",
      "https://dl-staging-website.ghost.io/content/images/2024/08/The-Batch-ads-and-exclusive-banners---2024-08-13T091442.999.png",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--78-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-14T145536.630.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--79-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-14T145608.454.png",
      "https://dl-staging-website.ghost.io/content/images/2024/08/The-Batch-ads-and-exclusive-banners---2024-08-13T105325.070.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-210/",
    "title": "issue 210",
    "date": "",
    "reading_time": "",
    "content": "An increasing variety of large language models (LLMs) are open source, or close to it. The proliferation of models with relatively permissive licenses gives developers more options for building applications.\n\nHere are some different ways to build applications based on LLMs, in increasing order of cost/complexity:\n\nFor most teams, I recommend starting with prompting, since that allows you to get an application working quickly. If you’re unsatisfied with the quality of the output, ease into the more complex techniques gradually. Start one-shot or few-shot prompting with a handful of examples. If that doesn’t work well enough, perhaps use RAG (retrieval augmented generation) to further improve prompts with key information the LLM needs to generate high-quality outputs. If that still doesn’t deliver the performance you want, then try fine-tuning — but this represents a significantly greater level of complexity and may require hundreds or thousands more examples. To gain an in-depth understanding of these options, I highly recommend the courseGenerative AI with Large Language Models, created by AWS and DeepLearning.AI.\n\n(Fun fact: A member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like me. I wonder if my job is at risk? 😜)\n\nAdditional complexity arises if you want to move to fine-tuning after prompting a proprietary model, such as GPT-4, that’s not available for fine-tuning. Is fine-tuning a much smaller model likely to yield superior results than prompting a larger, more capable model? The answer often depends on your application. If your goal is to change the style of an LLM’s output, then fine-tuning a smaller model can work well. However, if your application has been prompting GPT-4 to perform complex reasoning — in which GPT-4 surpasses current open models — it can be difficult to fine-tune a smaller model to deliver superior results.\n\nBeyond choosing a development approach, it’s also necessary to choose a specific model. Smaller models require less processing power and work well for many applications, but larger models tend to have more knowledge about the world and better reasoning ability. I’ll talk about how to make this choice in a future letter.\n\nKeep learning!\n\nAndrew\n\nP.S. We just released “Large Language Models with Semantic Search,”  a short course built in collaboration with Cohere and taught by Jay Alammar and Luis Serrano. Search is a key part of many applications. Say, you need to retrieve documents or products in response to a user query. How can LLMs help? You’ll learn about (i) embeddings to retrieve a collection of documents loosely related to a query and (ii) LLM-assisted re-ranking to rank them precisely according to a query. You’ll also go through code that shows how to build a search system for retrieving relevant Wikipedia articles. Pleasecheck it out!\n\nNvidia’s top-of-the-line chips are in high demand and short supply.\n\nWhat’s new:There aren’t enough H100 graphics processing units (GPUs) to meet the crush of demand brought on by the vogue for generative AI,VentureBeatreported.\n\nBottleneck:Cloud providers began havingtrouble finding GPUsearlier this year, but the shortfall has spread to AI companies large and small. SemiAnalysis, a semiconductor market research firm,estimatesthat the chip will remain sold out into 2024.\n\nWho’s buying:Demand for H100s is hard to quantify. Large AI companies and cloud providers may need tens of thousands to hundreds of thousands of them, while AI startups may need hundreds to thousands.\n\nBehind the news:Nvidiaannouncedthe H100 early last year and began full production in September. Compared to its predecessor, the A100, the H100 performs about 2.3 times faster in training and 3.5 times faster at inference.\n\nWhy it matters:Developers need these top-of-the-line chips to train high-performance models and deploy them in cutting-edge products. At a time when AI is white-hot, a dearth of chips could affect the pace of innovation.We’re thinking:Nvidia’s CUDA software, which undergirds many deep learning software packages, gives the company’s chips a significant advantage. However, AMD’s open source ROCm is making great strides, and its MI250 and upcoming MI300-series chips appear to be promising alternatives. An open software infrastructure that made it easy to choose among GPU providers would benefit the AI community.\n\nThe latest wave of large language models trained in Chinese is open source for some users.\n\nWhat’s new:Internet giant Alibaba released large language models that are freely available to smaller organizations. The internet giant followed Baichuan Intelligent Technology, a startup that contributed its own partly open models, and Beijing Academy of Artificial Intelligence, which announced that its WuDao 3.0 would be open source.How it works:These pretrained models are small compared to, say, Meta’s LLaMa 2 (70 billion parameters) — but that may be a plus in China, where U.S. export restrictions have made chips for processing AI hard to get.\n\nBehind the news:Developers in China areracingto cash in on chatbot fever. But they face unique hurdles.\n\nWhy it matters:The March leak of Meta’sLLaMAinitiated a groundswell of open models that excel in English and a subsequent explosion of innovation and entrepreneurial activity. Competitive open models trained in Mandarin and other Chinese languages could spark similar developments in one of the world’s biggest countries — as long as developers hew to the law.\n\nWe’re thinking:High-profile models like ChatGPT and Bard, having been trained on huge amounts of English-language data, tend to know a lot about the histories, geographies, and societies of English-speaking countries but relatively little about places where other languages are spoken. Models trained on Chinese corpora will serve speakers of China’s languages far better, and open source models fine-tuned for Chinese users likely will play an important role.\n\nJoin our new course, “Large Language Models with Semantic Search,” and learn the techniques you need to integrate LLMs with search and how to use your website’s information to generate responses.Enroll for free\n\nThe latest robot dog is smarter — and less expensive — than ever.\n\nWhat’s new:Unitree Robotics of Hangzhou, China,unleashedGo2, a quadruped robot that trots alongside its owner, stands on two legs, jumps, talks, takes photos, and retails for less than a high-end MacBook.\n\nHow it works:Go2 is made of aluminum and plastic, weighs around 15 kilograms, and moves using 12 joints. A robotic arm mounted on the unit’s back is optional. It comes in three versions with a starting price of $1,600.\n\nWhy it matters:Boston Dynamics’ industrial-strength robodog Spot ismanipulatinghigh-voltage electrical equipment,inspectingnuclear power plants, andhelpingto monitor urban areas. But its price — from$74,500to$200,000— puts it out of reach of many potential users. With its dramatically lower price, Go2 suggests that such mechanical beasts may find a wider range of uses.We’re thinking:While wheels are great on flat ground, four legs with backward-facing joints are morestableon uneven terrain. Plus, robot dogs are cute!\n\nLarge language models increasingly reply to prompts with a believably human response. Can they also mimic human behavior?\n\nWhat's new:Joon Sung Park and colleagues at Stanford and Google extended GPT-3.5 to buildgenerative agentsthat went about their business in a small town and interacted with one another in human-like ways. The code is newlyavailableas open source.\n\nKey insight:With the right prompts, a text database, and a server to keep track of things, a large language model (LLM) can simulate human activity.\n\nHow it works:The authors designed 25 agents (represented by 2D sprites) who lived in a simulated town (a 2D background depicting the layout and the contents of its buildings) and let them run for two days. Each agent usedGPT 3.5;a database of actions, memories, reflections, and plans generated by GPT 3.5; and a server that tracked agent and object behaviors, locations (for instance, in the kitchen of Isabella’s apartment), and statuses (whether a stove was on or off), and relayed this information to agents when they came nearby.\n\nResults:The complete agents exhibited three types of emergent behavior: They spread information initially known only to themselves, formed relationships, and cooperated (specifically to attend a party). The authors gave 100 human evaluators access to all agent actions and memories. The evaluators asked the agents simple questions about their identities, behaviors, and thoughts. Then they ranked the agents’ responses for believability. They also ranked versions of each agent that were missing one or more functions, as well as humans who stood in for each agent (“to identify whether the architecture passes a basic level of behavioral competency,” the authors write). These rankings were turned into aTrueSkillscore (a variation on the Elo system used in chess) for each agent type. The complete agent architecture scored highest, while the versions that lacked particular functions scored lower. Surprisingly, the human stand-ins also underperformed the complete agents.\n\nYes, but:Some complete agents “remembered” details they had not experienced. Others showed erratic behavior, like not recognizing that a one-person bathroom was occupied or that a business was closed. And they used oddly formal language in intimate conversation; one ended exchanges with her husband, “It was good talking to you as always.”\n\nWhy it matters:Large language models produce surprisingly human-like output. Combined with a database and server, they can begin to simulate human interactions. While the TrueSkill results don’t fully convey how humanly these agents behaved, they do suggest a role for such agents in fields like game development, social media, robotics, andepidemiology.\n\nWe're thinking:The evaluators found the human stand-ins less believable than the full-fledged agents. Did the agents exceed human-level performance in the task of acting human, or does this result reflect a limitation of the evaluation method?\n\nJoin our upcoming workshop with Predibase and learn how to use open source tools to overcome challenges like the “host out of memory” error when fine-tuning models like Llama-2.Register now\n\nPublisher surprised to find it published AI-generated artWizards of the Coast, the publisher of Dungeons & Dragons guidebooks and stories, admitted the use of AI-generated artwork in a digital book. The company, which claimed to have been unaware that it had published generated content prior to the book’s release, said it would update its policies to prevent AI art from being included in future publications. (Polygon)Brookings shows the uneven geography of AI activity in the U.S.A report from Brookings Institute highlights the concentration of AI activity in tech-focused cities like San Francisco, New York, and Seattle. It proposes policy actions at federal, state, and local levels to promote more widespread AI development. (Brookings)AI and robotics make recycling more efficientCompanies such as EverestLabs and AMP Robotics are using AI to streamline the recycling process. Their robotic arms identify recyclable items to boost object recovery rates by up to three times compared to human efforts. (CNBC)Research:A deep learning model recognizes laptop keystrokes by soundResearchers trained a model to analyze sound profiles of laptop keystrokes. They achieved 93 percent accuracy when interpreting individual key sounds in Zoom audio recordings. This approach raises security concerns, especially for laptops used in public settings. (Ars Technica)Commercialized deepfakes raise questions about control and misuseSynthesia touts its AI avatars, which look like video recordings of people,  to enhance corporate presentations and training sessions. However, the photorealistic avatars have been put to use by scammers and propagandists (Wired)Tech giants rally behind AI in resurgent quarterAI helped companies like Google, Meta, and Microsoft rebound from a financial slump in the most recent quarter. Now they’re doubling down on AI to revitalize their product lines and fuel innovation. Some are already benefiting from the AI fever. (The New York Times)Disney forms AI task forceThe Walt Disney Company established a dedicated group to explore the applications of AI across its entertainment empire. The initiative aims to develop in-house solutions and forge partnerships that can drive innovation and cut costs. (Reuters)\n\nGoogle and Universal Music explore licensing for AI-generated musicAlphabet's Google is in early discussions with Universal Music  over licenses to use artists' voices and melodies in AI-generated songs. The companies aim to develop technology that would  enable  fans to make their own sound-alike productions while compensating copyright owners. (Reuters)Report highlights AI's role in promoting eating disordersA study conducted by the Center for Countering Digital Hate (CCDH) found that ChatGPT and Stable Diffusion produced harmful output around 41 percent of the time when tested with prompts related to eating disorders. Experts emphasize the need to ensure that AI-generated content doesn't promote unhealthy body-image ideals or provide dangerous advice to users who may suffer from eating disorders. (The Washington PostandCCDH)Zoom promises not totrain its AI systems on customer dataThe video conferencing platform added a line to its terms of service stating that it will not employ customer audio, video, or chat content for training AI models without consent. The company updated its terms after users discovered language that granted the right to use customer data to build AI systems. In July, Zoom had revised its terms to broaden its access to customer data in developing AI products and services. (The Washington Post)Stack Overflow adapts to survive in the age of LLMsThe longstanding community for developers faces a decline in traffic as AI models increasingly answer technical questions. Some language models, partly trained on Stack Overflow's data, compete with Stack Overflow directly. The company plans to develop its own question-answering models and charge AI companies to use its data. (Business Insider)Google's AI seeks to continue training AI on published contentGoogle submitted a proposal to the Australian government suggesting that generative AI systems should be allowed to use publishers' content to train AI systems while providing an opt-out for those that want to keep their content out of training datasets. Google's position sparked discussions about content creators' rights. (The Guardian)OpenAI allows content providers to opt out of its training datasetsGPTBot, a web crawler that collects online data used by OpenAI to train its models, offers website operators the ability to opt out. The crawler will not scrape data from sites that exercise the option.(The Verge)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--45--1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--82-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--83-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/08/The-Batch-ads-and-exclusive-banners--48-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/08/The-Batch-ads-and-exclusive-banners--4-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--84-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/08/The-Batch-ads-and-exclusive-banners--50-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-198/",
    "title": "issue 198",
    "date": "",
    "reading_time": "",
    "content": "It’s time to move beyond the stereotype that machine learning systems need a lot of data. While having more data is helpful, large pretrained models make it practical to build viable systems using a very small labeled training set — perhaps just a handful of examples specific to your application.\n\nAbout 10 years ago, with the rise of deep learning, I was one of the leading advocates for scaling up data and compute to drive progress. That recipe has carried us far, and it continues to drive progress in large language models, which are based on transformers. A similar recipe is emerging in computer vision based on large vision transformers.\n\nBut once those models are pretrained, it takes very little data to adapt them for a new task. With self-supervised learning, pretraining can happen on unlabeled data. So, technically, the model did need a lot of data for training, but that was unlabeled, general text or image data. Then, even with only a small amount of labeled, task-specific data, you can get good performance.\n\nFor example, say you have a transformer trained on a massive amount of text, and you want it to perform sentiment classification on your own dataset. The most common techniques are:\n\nThese techniques work well. For example, customers of my team Landing AI have been building vision systems with dozens of labeled examples for years.\n\nThe 2010s were the decade of large supervised models, I think the 2020s are shaping up to be the decade of large pretrained models. However, there is one important caveat: This approach works well for unstructured data (text, vision and audio) but not for structured data, and the majority of machine learning applications today are built on structured data.\n\nModels that have been pretrained on diverse unstructured data found on the web generalize to a variety of unstructured data tasks of the same input modality. This is because text/images/audio on the web have many similarities to whatever specific text/image/audio task you might want to solve. But structured data such as tabular data is much more heterogeneous. For instance, thedataset of Titanic survivorsprobably has little in common with your company’s supply chain data.\n\nNow that it's possible to build and deploy machine learning models with very few examples, it’s also increasingly possible to build and launch products very quickly — perhapswithout even bothering to collect and use a test set. This is an exciting shift. I’m confident that this will lead to many more exciting applications, including specifically ones where we don’t have much labeled data.\n\nKeep learning!\n\nAndrew\n\nA new regulatory body created by the European Union promises to peer inside the black boxes that drive social media recommendations.\n\nWhat’s new:TheEuropean Centre for Algorithmic Transparency(ECAT) will study the algorithms that identify, categorize, and rank information on social media sites and search engines.\n\nHow it works:ECATis empowered to determine whether algorithms (AI and otherwise) comply with the European Union’sDigital Services Act, which aims to block online hate speech, certain types of targeted ads, and other objectionable content. The agency, which is not yet fully staffed, will have between 30 to 40 employees including specialist AI researchers. Its tasks fall into three major categories:\n\nBehind the news:EU regulators are increasingly targeting AI. On April 13, the European Data Protection Boardlauncheda task force to coordinate investigations by several nations into whether OpenAIviolatedprivacy laws when it trained ChatGPT. Since 2021, EU lawmakers have beencraftingthe AI Act, a set of rules designed to regulate automated systems according to their potential for harm. The AI Act is expected to pass into law later this year.\n\nWhy it matters:The EU is on the leading edge of regulating AI. As with many national-level efforts, Europe’s investigations into social media algorithms could reduce harms and promote social well-being well beyond the union’s borders.We’re thinking:This is a welcome step. Governments need to understand technology before they can craft thoughtful regulations to manage it. ECAT looks like a strong move in that direction.\n\nOne of the world’s largest investment banks built a large language model to map cryptic government statements to future government actions.What’s new:JPMorgan Chase trained a model based on ChatGPT to score statements by a United States financial regulator according to whether it plans to raise or lower interest rates,Bloombergreported.How it works:The U.S. Federal Reserve, a government agency that’s empowered to set certain influential interest rates, periodically comments on the national economy. Its words are deliberately vague to prevent markets from acting in advance of formal policy decisions.\n\nResults:The team tested the model by scoring past 25 years of Federal Reserve statements and speeches. They didn’t describe the results in detail but said they found a general correlation between the predicted and actual interest rate fluctuations.\n\nBehind the news:Prior to the advent of large language models, investors tried to predict the impact of central bank announcements viasentiment analysis,timingthe interval between official meetings and publication of minutes, andwatchingthe sizes of their briefcases.\n\nWhy it matters:Central banks use interest rates to steer their country’s economies. Lower rates spur economic growth and fight recessions by making money cheaper to borrow. Higher interest rates tamp down inflation by making borrowing more expensive. If you can predict such changes accurately, you stand to reap huge profits by using your predictions to guide investments.\n\nWe’re thinking:Custom models built by teams outside the tech sector are gaining steam. Bloomberg itself — which makes most of its money providing financial data —traineda BLOOM-style model on its corpus and found that it performed financial tasks significantly better than a general-purpose model.\n\nJoin us for a live workshop on Wednesday, May 31, 2023 at 10:00 a.m. Pacific Time, and discover how customized fine-tuning techniques can help you harness pretrained language models to build robust AI applications.Register now\n\nText-to-image generators are visualizing the next wave of architectural innovation.\n\nWhat’s new:Patrick Schumacher, principal architect at Zaha Hadid Architects,explainedhow the company uses generative AI to come up with ideas. He made the remarks at an industry roundtable called AI and the Future of Design.\n\nHow it works:The architects use DALL•E 2, Midjourney, and Stable Diffusion to generate exterior and interior images of concepts in development. Schumacher showed generated images for projects in development, including a high-rise complex in Hong Kong and Neom, a massive smart city planned for Saudi Arabia.\n\nBehind the news:Text-to-image models are finding their way into a variety of design disciplines.\n\nWhy it matters:Zaha Hadid Architects has worked on Olympic venues, international airport terminals, and skyscrapers. Millions of people soon may interact with buildings visualized by AI.We’re thinking:What a great example of human-computer collaboration: The models learn from the architects’ past designs to help the them envision fresh concepts.\n\nTinyMLshows promise for bringing deep learning to applications where electrical power is scarce, processing in the cloud is impractical, and/or data privacy is paramount. The trick is to get high-performance algorithms to run on hardware that offers limited computation, memory, and electrical power.\n\nWhat's new:Michael Bechtel, QiTao Weng, and Heechul Yun at University of Kansas built a neural network that steeredDeepPicarMicro, a radio-controlled car outfitted for autonomous driving, around a simple track. This work extends earlierworkin which the authors built neural networks for extremely limited hardware.\n\nKey insight:A neural network that controls a model car needs to be small enough to fit on a microcontroller, fast enough to recognize the car’s surroundings while it’s in motion, and accurate enough to avoid crashing. One way to design a network that fits all three criteria is to (i) build a wide variety of architectures within the constraints of size and latency and (ii) test their accuracy empirically.\n\nHow it works:The hardware included a NewBright 1:24-scale car with battery pack and motor driver, Raspberry Pi Pico microcontroller, and Arducam Mini 2MP Plus camera. The model was based onPilotNet, a convolutional neural network. The authors built a dataset by manually driving the car around a wide, circular track to collect 10,000 images and associated steering inputs.\n\nResults:The authors selected 16 models with various losses and latencies and tested them on the track. The best model completed seven laps before crashing. (Seven models failed to complete a single lap.) The models that managed at least one lap tended to achieve greater than 80 percent accuracy on the test set and latency lower than 100 milliseconds.\n\nWhy it matters:This work shows neural networks, properly designed, can achieve useful results on severely constrained hardware. For a rough comparison, the Nvidia Tegra X2 processor that drives a Skydio 2+ drone provides four cores that run at 2 gigaHertz, while the Raspberry Pi Pico’s processor provides two cores running at 133 megaHertz. Neural networks that run on extremely low-cost, low-power hardware could lead to effective devices that monitor environmental conditions, health of agricultural crops, operation of remote equipment like wind turbines, and much more.\n\nWe’re thinking:Training a small network to deliver good performance is more difficult than training a larger one. New methods will be necessary to narrow the gap.\n\nStability AI released StableStudio, the open source version of DreamStudioThe company also released a set of developer tools for local inference and desktop installation of the model. (Stability AI)\n\nCurio AI generates custom, fact-checked audio summaries of news and eventsIn response to user text prompts, the new feature draws on stories licensed from media partners. It writes a script, then synthesizes the script combining audio clips from its catalog of human-narrated articles. (TechCrunch)\n\nU.S. Department of Homeland Security is using AI to screen travelers and refugeesThe system, called Babel X, enables border patrol agents to obtain a wide range of data about travelers, including their IP addresses, employment histories, and social media posts. (Vice)Google advertisers will have access to PaLM 2 for content creationLeaked documents indicated that the company will allow advertisers to use its newest large language model to generate ad copy and YouTube video ideas. (CNBC)Over a dozen new companies offer tools for detecting AI-generated contentThe companies specialize in identifying deepfaked propaganda videos, student essays that use AI-generated plagiarism, and more. (The New York Times)OpenAI launched a ChatGPT app for iPhoneUnlike the chatbot’s browser version, the app responds to voice prompts. It also syncs the user’s history across devices. The company plans to release an Android version of the app. (OpenAI)Apple’s iOS 17 will include voice cloningThe accessibility feature, aimed at users who have or expect to lose their voice, can generate a synthetic voice based on 15 minutes of audio data. (Apple)U.S. chip sanction spur Chinese companies to innovateThe scarcity of top-tier hardware has compelled research teams at tech giants like Baidu and Huawei to pursue new ways to achieve state-of-the-art performance.  (The Wall Street Journal)Research: AI generates high-quality video from brain activityResearchers developed a Stable Diffusion-based model called MinD-Video that generates video from fMRI data. (Vice)Venture capital in AI startups is plummeting despite investor’s excitement for generative AIGlobal AI funding fell by 43 percent from the previous quarter in the first quarter of 2023, CB Insight’s State of AI Q1’23 reported. (CB Insights)Italy designated $30 million to bolster its workforce against automationThe investment aims to alleviate fears of AI job loss by focusing on workers in vulnerable industries, as well as those who are unemployed. (Fox Business)Yoshua Bengio proposed a mechanism for building safe AI systemsThe AI pioneer suggested that the most powerful AI systems should be allowed only to generate theories and answer questions. Bengio also suggested guardrails against allowing such systems to take real-world action.  (Yoshua Bengio’s blog)Professor gave failing grades to students after ChatGPT falsely claimed authorship of their papersA Texas A&M University professor mistakenly relied on the AI chatbot to detect cheating, unaware of the tool’s inability to identify plagiarism. (Rolling Stone)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/05/DataSpheres_v6_1200px-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/05/JRC-Recrop-600px--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/05/JPMORGAN--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/05/The-Batch-ads-and-exclusive-banners--23-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/05/ARCHITECTURE_600px--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/05/MICRO--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-226/",
    "title": "issue 226",
    "date": "",
    "reading_time": "",
    "content": "Large language models, or LLMs, have transformed how we process text. Large vision models, or LVMs, are starting to change how we process images as well. But there is an important difference between LLMs and LVMs:\n\nThis week, Dan Maloney and I announced Landing AI's work on developing domain-specific LVMs. You can learn more about it in this shortvideo(4 minutes).\n\nThe internet – especially sites like Instagram – has numerous pictures of people, pets, landmarks, and everyday objects. So a generic LVM (usually a large vision transformer trained using a self-supervised learning objective on unlabeled images scraped from the internet) learns to recognize salient features in such images.\n\nBut many industry-specific applications of computer vision involve images that look little like internet images. Pathology applications, for instance, process images of tissue samples captured using high-powered microscopes. Alternatively, manufacturing inspection applications might work with numerous images centered on a single object or part of an object, all of which were imaged under similar lighting and camera configurations.\n\nWhile some pathology and some manufacturing images can be found on the internet, their relative scarcity means that most generic LVMs do poorly at recognizing the most important features in such images.\n\nIn experiments conducted by Landing AI's Mark Sabini, Abdelhamid Bouzid, and Bastian Renjifo, LVMs adapted to images of a particular domain, such as pathology or semiconductor wafer inspection, do much better at finding relevant features in images of that domain. Building these LVMs can be done with around 100,000 unlabeled images from that domain, and larger datasets likely would result in even better models.\n\nFurther, if you use a pretrained LVM together with a small labeled dataset to tackle a supervised learning task, a domain specific LVM needs significantly less (around 10 percent to 30 percent as much) labeled data to  achieve performance comparable to using a generic LVM.\n\nConsequently, I believe domain specific LVMs can help businesses with large, proprietary sets of images that look different from internet images unlock considerable value from their data.\n\nOf course, LVMs are still young, and much innovation lies ahead. My team is continuing to experiment with different ways to train domain-specific LVMs, as well as exploring how to combine such models with text to form domain-specific large multimodal models. I'm confident that LVMs will achieve many more breakthroughs in the coming years.Keep learning!\n\nAndrew\n\nAmazon launched a chatbot for large companies even as internal tests indicated potential problems.\n\nWhat’s new:Amazon introducedQ, an AI-powered assistant that enables employees to query documents and corporate systems. Days later, the tech newsletterPlatformerobtainedinternal documents that indicate the model can generates falsehood and leak confidential information. (Amazon Q is not to be confused with OpenAIQ*.)\n\nHow it works:Currently available as a free preview, Q analyzes private documents, databases, and code to answer questions, generate content, and take actions. Amazonplansto offer two tiers of service: a basic chatbot ($20 per month) and the chatbot plus code generation, troubleshooting, security evaluation, and human assistance from Amazon Web Services ($25 per month). Amazon promises not to train machine learning models on Q users’ data.\n\nThe issues:Three days after Amazon unveiled Q, employees began to flag issues on internal Slack and security reporting channels.\n\nBehind the news:Amazon is not the only major AI company whose chatbot has leaked private information. Google researchers recentlyfoundthat they could prompt OpenAI’s ChatGPT to divulge personal information found in its training data.\n\nWhy it matters:For Amazon, issues with a newly released system are a bump in the road to competing effectively against competitors like Microsoft Copilot and ChatGPT Enterprise. For developers, it’s a sobering reminder that when you move fast, what breaks may be your own product.\n\nWe’re thinking:In developing an AI system, often it’s necessary to launch — in a safe and responsible way — and make improvements based on real-world performance. We congratulate the Q team on getting the product out and look forward to seeing where they take it.\n\nIn a study, models used to detect people walking on streets and sidewalks performed less well on adults with darker skin and children of all skin tones.\n\nWhat’s new:Xinyui Li, Zhenpeng Chen, and colleagues at Peking University, University College London, and King’s College Londonevaluatedeight widely used object detectors for bias with respect to skin color, age, and gender.\n\nKey insight:When it comes to detecting pedestrians, biases with respect to demographic characteristics can be a life-and-death matter. Evaluating them requires a dataset of pedestrians labeled according to characteristics that might influence detection. Skin color, age, and gender are important human differences that can affect a vision model’s performance, especially depending on lighting conditions.\n\nHow it works:The authors collected over 8,000 photos from fourdatasetsofstreetscenes. They annotated each image with labels for skin tone (light or dark), age group (child or adult), and gender (male or female). They tested four general-purpose object detectors:YOLOX,RetinaNet,Faster R-CNN, andCascade R-CNN— and four pedestrian-specific detectors —ALFNet,CSP,MGAN, andPRNet— on their dataset. They evaluated performance between perceived skin tone, age, and gender groups and under different conditions of brightness, contrast, and weather.\n\nResults:The study revealed significant fairness issues related to skin tone and age.\n\nBehind the news:Previousworkhas shown that computer vision models can harbor biases that make them less likely to recognize individuals of certain types. In 2019, MITshowedthat commercial face recognition performed worse on women and darker skinned individuals. Aplethoraofworkevaluatesbias in datasets typically used to train vision models.\n\nWhy it matters:As more road vehicles gain self-driving capabilities and as expanded robotaxi services come to major cities, a growing number of pedestrians’ lives are in the hands of computer vision algorithms. Auto makers don’t disclose what pedestrian detection systems they use or the number of real-world accidents involving self-driving cars. But co-author Jie Zhangclaimsthat the proprietary systems used in self-driving cars are “usually built upon the existing open-source models,” and “we can be certain that their models must also have similar issues.”\n\nWe’re thinking:Computer vision isn’t the only technology used by self-driving cars to detect objects. Most self-driving car manufacturers rely on lidar and radar in addition to cameras. Those technologies are blind to color and gender differences and, in the view of many engineers, make better choices for this application.\n\nWant to learn how to fine-tune large language model-based agents? In our upcoming webinar with Weights and Biases, you’ll gain insights and techniques to enhance agent performance and specificity in automating applications.Register now\n\nThe U.S. state of Colorado started regulating the insurance industry’s use of AI.\n\nWhat’s new:Coloradoimplementedthe first law that regulates use of AI in life insurance and proposed extending the limits to auto insurers. Other states have taken steps to rein in both life and auto insurers under earlier statutes.\n\nHow it works:States are responsible for regulating the insurance industry in the U.S. Colorado’sruleslimit kinds of data life insurers can use and how they can use it. They took effect in November based on alawpassed in 2021.\n\nOther states:Californiaorderedall insurers to notify regulators when their algorithm results in an increase to a customer’s premium; regulators can then evaluate whether the effect of the rate increase is excessive and/or discriminatory. Agencies inConnecticutandNew Yorkordered all insurers to conform their use of AI with laws against discrimination. Washington D.C.openedan investigation to determine whether auto insurers’ use of data resulted in outcomes that discriminated against certain groups.\n\nBehind the news:Colorado shared an initial draft of its life-insurance regulations earlier this year beforerevisingit. Among other changes, the initial draft prohibited AI models that discriminate not only on the basis of race but with respect to all protected classes; prevent unauthorized access to models; create a plan to respond to unforeseen consequences of their models; and engage outside experts to audit their models. The final draft omits these requirements.\n\nWhy it matters:Regulators are concerned that AI could perpetuate existing biases against marginalized groups, and Colorado’s implementation is likely to serve as a model for further regulation. Insurance companiesfacea growing number of lawsuits over claims that their algorithms wrongfullydiscriminateby age or race. Regulation could mitigate potential harms and ease customers’ concerns.\n\nWe’re thinking:Reporting of models that use social posts, purchases, and the like is a good first step, although we suspect that further rules will be needed to govern the complexities of the insurance business. Other states’ use of Colorado's regulations as a blueprint would avoid a state-by-state patchwork of contradictory regulations.\n\nResearchers proposed a way for robots to find objects in households where things get moved around.\n\nWhat's new:Andrey Kurenkov and colleagues at Stanford University introducedNode Edge Predictor, a model that learned to predict where objects were located in houses.\n\nKey insight:A popular way to represent objects and their locations is a graph, in which each node is either an object or its location and an edge connects the two. If we want to track objects over time, a recurrent model could predict the locations of objects using a separate graph for each time step, but that would require a prohibitive number of graphs. Instead, a model can predict locations using a single graph in which each edge is annotated, additionally, with the time elapsed since the associated object was seen in the associated location. The model learns to predict the next most likely place to find an object based on the object’s most recent, frequent, and longstanding locations.\n\nHow it works:The authors simulated a robot looking for things in a household. They built (i) a simulator of houses, object locations, and when and where they moved; (ii) a graph that represented a house containing objects; and (iii) a machine learning system that predicted where objects might be found.\n\nResults:The authors tested their system’s ability to find a single object in a house versus a few baseline methods. The baselines included random guessing, always guessing the piece of furniture where the object was last seen, and a Bayesian model that guessed whether the object was on/in a given piece of furniture based on the percentage of times it had been seen there. On average, their system found the object in 3.2 attempts, while the next best model (Bayesian) took 3.6 attempts. Guessing the last-seen location required 6.0 attempts, and random guessing required 8.8 attempts.\n\nWhy it matters:Feature engineering helps to find the best way to represent data so a model can learn from it. In this work, engineering time-related features (such as the time elapsed since an object was on a piece of furniture or the number of times an object was observed on a piece of furniture over time) enabled a non-recurrent model to learn how graphs change over time.\n\nWe’re thinking:A physical robot likely would use object detection on its camera feed instead of a simulator that told it directly which objects were associated with which pieces of furniture. We look forward to future work that proves the concept using this more realistic setup.\n\nMicrosoft announces £2.5 billion investment to boost the UK’s AI capabilitiesThe investment aims to double Microsoft’s UK datacenter footprint by 2026, train or retrain over one million people for the AI economy, and extend Microsoft’s Accelerating Foundation Models Research (AFMR) program to prioritize GPU access for the UK’s research community. (Read more atMicrosoft)\n\nResearchfinds opportunities and risks as heritage organizations embrace AIA new study focuses on what innovation in AI looks like in the UK heritage sector, and showcases its diverse uses in museums, galleries, libraries, and archives. Notable examples include predictive analytics for exhibition popularity at the National Gallery. However, the study also highlighted risks such as discrimination, misinformation, copyright infringement, and transparency issues. (Read more atMuseum Association)\n\nU.S. mandates Saudi venture capital firm must sell stake in Silicon Valley AI firmThe Biden administration has instructed Prosperity7 to sell its shares in Rain AI, a Silicon Valley AI chip startup backed by OpenAI co-founder Sam Altman. Rain AI, which designs AI chips inspired by brain functionality, had Prosperity7 as a lead investor in a funding round that raised $25 million in 2022. (Read the news story atBloomberg)\n\nGenerative AI regulation allegedly stalls EU legislation talksSources revealed that negotiations on foundation models have become the primary hurdle, with a risk of shelving the act before European parliamentary elections next year unless an agreement is reached. France, Germany, and Italy form an important bloc of countries opposing foundation models. Pending issues also include establishing a definition of AI and national security exceptions. Critics argue that self-regulation may fall short of safety standards for foundation models, creating legal uncertainty and impeding European industries' planning. (Read the article atReuters)\n\nAI fuels innovations in Pennsylvania's infrastructure projectsIn Pennsylvania, U.S., where 13 percent of bridges face structural deficiencies, engineers are leveraging AI to address challenges like the development of lighter concrete blocks for construction and noise-absorbing walls along highways. The projects aim to create more resilient structures at a reduced cost. The use of AI in civil engineering could revolutionize project development, early damage detection, and real-time incident analysis, but careful consideration and regulations are urged to ensure safety and reliability. (Read the article inThe New York Times)\n\nAnduril's Roadrunner: AI combat drone takes flightAnduril's latest innovation combines AI technology and jet-powered capabilities to counter the escalating threat of low-cost, sophisticated aerial attacks. The modular and autonomous Roadrunner drone aims to provide rapid response and heightened resilience against evolving threats such as suicide drones. (Read more atWired)\n\nGeneral Motors to reduce investment in Cruise self-driving division next yearFollowing recent accidents involving its self-driving taxis in San Francisco, the company, initially planning expansion to multiple cities, now focuses on rebuilding trust with regulators and communities. The decision to reduce spending follows the suspension of Cruise's robotaxi license in California and a need to regain public trust in the wake of safety incidents, including a pedestrian fatality. (Read the article atThe New York Times)\n\nSam Altman returns as OpenAI CEOBesides Altman’s return, Mira Murati reassumed her role as CTO, and Greg Brockman returned as President. For now, the new board comprises former Salesforce CEO Bret Taylor (Chair), economist Larry Summers, and Quora CEO Adam D’Angelo. (Read the blog post atOpenAI)\n\nConsortium of major companies develops data provenance standards to enhance trust in AIMany companies (including American Express, IBM, and Walmart) formed the Data & Trust Alliance, introducing new standards for data provenance in AI applications. These standards cover eight basic criteria, including lineage, source, legal rights, and data type. The goal is to offer clear data documentation and bolster efficiency and trust in AI developments. (Read more atThe New York Times)\n\nAmazon Web Services (AWS) introduces Titan models in Amazon BedrockAmazon’s Titan Image Generator and Titan Multimodal Embeddings offer image, multimodal, and text options through a fully managed API. The Titan Image Generator enables content creators to generate images using natural language prompts, targeting applications in advertising, e-commerce, and media. The Titan Multimodal Embeddings facilitate the creation of contextually relevant multimodal search and recommendation experiences. (Read the blog post atAWS)\n\nVoicemod launches feature to craft and share custom synthetic voicesThe app, known for its AI voice-changing program popular in the gaming and streaming communities, now enables users to craft and share their unique AI voices by modifying their own voices or choosing from various genders, ages, and tones. (Read more atThe Verge)\n\nDemand keeps soaring for prompt engineersPrompt engineering emerged as a lucrative and sought-after skill in the year since the public launch of ChatGPT. Google searches for \"prompt engineering\" have skyrocketed, and LinkedIn reports substantial increases in related terms on member profiles. The skillset, involving coaxing AI systems for better results and training colleagues in using generative AI, is in high demand. Newly-created roles offer significant compensation, often upwards of $335,000 annually. (Read the analysis atBloomberg)\n\nResearch: Deep learning model offers precision in predicting breast cancer outcomesThe Histomic Prognostic Signature (HiPS), which evaluates both cancerous and non-cancerous cell patterns, outperformed expert pathologists in predicting disease progression. By identifying breast cancer patients classified as high or intermediate risk who could become long-term survivors, the tool offers the potential to reduce the duration or intensity of chemotherapy, sparing patients from harmful side effects. (Read the article viaNorthwestern University)\n\nIBM expands geospatial AI collaboration to tackle climate challenges globallyThe initiative involves mapping urban heat islands in the UAE, supporting Kenya's reforestation campaign, and enhancing climate resiliency in the UK's aviation sector. Additionally, IBM is collaborating with NASA to develop a new model for weather and climate, aiming to improve the precision and efficiency of weather forecasting and address climate-related challenges on a global scale. (Read more atIBM)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--77-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed---2023-12-06T153211.227.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--78-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/12/The-Batch--3-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--33-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed---2023-12-06T155705.846.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-151/",
    "title": "issue 151",
    "date": "",
    "reading_time": "",
    "content": "The rapid rise of AI has led to a rapid rise in AI jobs, and many people are building exciting careers in this field. A career is a decades-long journey, and the path is not always straightforward. Over many years, I’ve been privileged to see thousands of students as well as engineers in companies large and small navigate careers in AI. In this and the next few letters, I’d like to share a few thoughts that might be useful in charting your own course.Three key steps of career growth are learning (to gain technical and other skills), working on projects (to deepen skills, build a portfolio, and create impact) and searching for a job. These steps stack on top of each other:\n\nThese phases apply in a wide range of professions, but AI involves unique elements. For example:\n\nThroughout these steps, a supportive community is a big help. Having a group of friends and allies who can help you — and whom you strive to help — makes the path easier. This is true whether you’re taking your first steps or you’ve been on the journey for years.I’m excited to work with all of you to grow the global AI community, and that includes helping everyone in our community develop their careers. I’ll dive more deeply into these topics in the next few weeks.\n\nKeep learning!\n\nAndrew\n\nThe United States space agency is upgrading the system that pilots its helicopter on the Red Planet.What’s new:The National Aeronautics and Space Administration (NASA) announced that Ingenuity, a drone sent to Mars as part of its 2020 mission to Mars, will receive a new collision-avoidance algorithm,Wiredreported. Ingenuity acts as a scout for the Perseverance rover as it travels from relatively flat, featureless areas to more hazardous terrain.How it works:NASA engineers on Earth plot waypoints in a simulation. They transmit the waypoints to the rover, which relays them to the drone, where algorithmsdetermineits path based on input from an onboard camera, altimeter, and other devices.\n\nBehind the news:Ingenuity was designed for only five flights, but has flown 29 times since its debut in April 2021. NASA hopes to extend its lifespan even further by letting it hibernate through the Martian winter. Solar energy is scarce for four months starting in July, and hibernation will enable the craft to devote its battery to keeping its electronics warm. The team plans to install the upgrade during that period.Why it matters:Ingenuity’s evolving combination of Earthbound direction and local autonomy lays the groundwork for missions deeper into the solar system, where the delay in communications — up to 24 minutes between Earth and Mars — will be even longer. For example, theDragonflyoctocopter is scheduled to take off for Titan’s soupy atmosphere in 2027.We’re thinking:Over-the-air software updates aren’t only for terrestrial devices!\n\nRegulators are forcing Meta (formerly Facebook) to display certain advertisements more evenly across its membership.What’s new:The United States government compelled Meta toreviseits ad-placement system to deliver ads for housing to members regardless of their age, gender, or ethnicity. The company is voluntarily rebalancing its distribution of ads for credit and employment as well.How it’s changed:The new algorithm will control ads that appear to U.S. users of Facebook, Instagram, and Messenger. Meta will roll it out by December.\n\nBehind the news:The update is part of asettlementbetween Meta and the U.S. Justice Department, whichfoundthat the company had violated laws against discrimination in housing. Meta also agreed to terminate a differentsystemthat was intended to enforce a more even distribution of ads but was found to have the opposite effect. It will pay a fine of $115,054, the maximum penalty under the law.Why it matters:AI technology is largely unregulated in the U.S. But that doesn’t mean the federal government has no jurisdiction over it, especially when it migrates into highly regulated sectors. Facebook once hosted ads forcredit cardsthat excluded younger people,job postingsthat excluded women, andhousing adsthat excluded people by race. Regulators who oversee civil rights didn’t settle for mere changes in Meta’s advertising guidelines and ultimately forced it to alter the algorithm itself.We’re thinking:Meta’s periodic reports will provide some evidence whether or not regulation can mitigate algorithmic bias. Still, we wonder whether regulators can craft effective rules. Data can be sliced in a variety of ways, and it can be very difficult to detect bias against a particular group within a slice. For example, a system that appears not to discriminate by gender on average may do so, say, within a particular type of town or when handling a certain sort of housing. Given the slow progress of legislation and the rapid development of technology, we worry that regulators will always trail the companies they regulate.\n\nHow important is querying databases to your role? Do you use Python or R? Take the Workera 2022Roles in Data and AIsurvey and help us upskill the data and AI workforce.Tell us about your roleand get a chance to win a $100 gift card!\n\nA startup that automatically translates video voice overs into different languages is ready for its big break.What’s new:London-basedPapercupoffers a voice translation service that combines algorithmic translation and voice synthesis with human-in-the-loop quality control. A recentfunding roundsuggests that investors have a measure of confidence in the company’s approach.How it works:Video producers can upload clips and specify an output language such as English, Mandarin, Italian, Latin American Spanish, or Brazilian Portuguese. They can choose among synthesized voices that represent a range of gender and age, and tweak the voice’s pitch and character and alter its emotional expression as “happy,” “sad,” “angry,” and the like.\n\nYes, but:Keeping in a human in the loop to oversee an operation as sensitive as language translation makes good sense. However, current technology can take this automation a good deal further. For instance, Papercup offers a selection of voices rather than generating afacsimile of the original voicein a new language. It doesn’tconform video of the speaker’s mouthto new languages — the mouth continues to form words in one language while the synthesized voice intones another. Nor does itdemixand remix vocal tracks that are accompanied by background music or other sounds.Why it matters:Automated voice over translation is yet another task in which machines are vying to edge out human workers. On one hand, automation can make translation available to producers on a tight budget, dramatically extending their reach to new markets and use cases. On the other hand, we worry that performing artists will lose work to such systems and support efforts toprotecttheir livelihoods.We’re thinking:Earlier this week, Nando de Freitas — DeepMind research director, Oxford professor, and former officemate of Andrew Ng’s —urgedus on Twitter to translate the newly updatedMachine Learning Specializationinto every language. We're working withCoursera’s global translator communityto create subtitles, but we're always eager to have options.\n\nTransformers can learn a lot from sequential data like words in a book, but they’ve shown limited ability to learn from data in the form of a graph. A new transformer variant gives graphs due attention.What's new:Vijay Prakash Dwivedi and Xavier Bresson at Nanyang Technological University devisedGraph Transformer(GT), a transformer layer designed to process graph data. Stacking GT layers provides a transformer-based alternative to typical graph neural networks, which process data in the form of nodes and edges that connect them, such as customers connected to products they’ve purchased or atoms connected to one another in a molecule.Key insight:Previous work applied transformers to graph data by dedicating a token to each node and computing self-attention between every pair. This method encodes both local relationships, such as which nodes are neighbors (given a hyperparameter that defines the neighborhood within a number of degrees of separation), and global information, such as a node’s distance from non-neighboring nodes. However, this approach is prohibitively expensive for large graphs, since the computation required for self-attention grows quadratically with the size of the input. Applying attention only to neighboring nodes captures crucial local information while cutting the computational burden. Meanwhile, a positional vector that represents each node’s relative distance from all other nodes can capture global information in a compute-efficient way.How it works:The authors built three models, each of which comprised embedding layers, 10 GT layers (including self-attention and fully connected layers) followed by a vanilla neural network. They trained each model on a different task: two-class classification ofsynthetic data, six-class classification of synthetic data, and a regression task that estimated the solubility of variouscompounds that contain zinc.\n\nResults:The authors’ model achieved 73.17 percent accuracy and 84.81 percent accuracy on the two- and six-class classification tasks, respectively. A baselineGATgraph neural network, which applied attention across neighboring node representations, achieved 70.58 percent accuracy and 78.27 percent accuracy respectively. On the regression task, the authors’ model achieved mean absolute error (MAE) of 0.226 compared to GAT’s 0.384 (lower is better). However, it slightly underperformed the state-of-the-artGated Graph ConvNetin all three tasks.Why it matters:Transformers have proven their value in processing text, images, and other data types. This work makes them more useful with graphs. Although the Graph Transformer model fell short of the best graph neural network, this work establishes a strong baseline for further work in this area.We're thinking:Pretrained and fine-tuned transformers handily outperform trained convolutional neural networks. Would fine-tuning a Graph Transformer model yield similarly outstanding results?",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/06/CareerArchitecture6-1200px-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/06/INGENUITY--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/ADS-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/test--6-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/06/PAPERCUP--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/GRAPHTRANSFORMER--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-76/",
    "title": "issue 76",
    "date": "",
    "reading_time": "",
    "content": "Last week, I talked about howbest practices for machine learning projects are not one-size-fits-all, and how they vary depending on whether a project uses structured or unstructured data, and whether the dataset is small or big. Another dimension that affects best practices is which phase of development a project is in: proof of concept or production.\n\nDuring the proof of concept (POC) phase, the primary goal is to determine if a system is worth building and deploying. During this phase, you might ask:\n\nWhen building a POC, my goal is to move fast. We’ve all been told we should build replicable, robust, and scalable systems — but when I haven’t even determined if a project is technically feasible, I often trade replicability for speed. I hope I don’t get too much hate mail for this, but if it buys you speed, it is okay to hard-code parameters, compute key variables in a Jupyter notebook, use local copies of data, and operate with lightweight code review or versioning processes.\n\nIf you already have a platform for experimentation, you may be able to build POCs in a systematic and robust way without sacrificing speed. But if you don’t, avoid over-investing in infrastructure at this stage. Instead, focus on getting the key information you need: whether this project is worth taking to production.\n\n(Those of you who are familiar with thelean startupphilosophy will see the parallel to building aminimum viable product, which is often a clunky piece of software that helps validate or falsify a hypothesis.)\n\nIn contrast, during the production phase, the goal is to build and deploy a system that generates practical value. I might go back to the messy POC and make sure that every step is replicable and documented. I put a lot of thought into scalable data pipelines, monitoring systems, and reliability.\n\nFor example, if a researcher wrote preprocessing routines (say, a sequence of scripts and regexps to remove data associated with spam accounts), these now need to be documented, tested, and incorporated into the system. You’ll likely want to document everything to make sure models can be replicated and maintained: hyperparameters, model choices, data provenance (where the data came from), data lineage (how it was processed). During this phase, tools like TensorFlow Transform and Apache Beam can be lifesavers.\n\nIf you’re building a project, don’t confuse the POC and production phases! Both are important, but the best practices depend on whether you’re deciding as quickly as possible if a project is worth putting into production or building a system that delivers real results to real users.\n\nKeep learning!\n\nAndrew\n\nA neural network learned to read the genes of viruses as though they were text. That could enable researchers to page ahead for potentially dangerous mutations.What’s new:Researchers at MIT trained a language model topredict mutationsthat would enable infectious viruses — including the SARS-CoV-2 virus that causes Covid-19 — to become even more virulent.Key insight:The authors suggest that the immune system’s response to viruses is similar to the way people understand natural language. A virus that causes infection has a “grammar” that’s biologically correct, and it also has a semantic “meaning” to which the immune system does or doesn’t respond. Mutations can enhance these worrisome qualities.\n\nHow it works:The authors trained a bidirectional LSTM on the genetic equivalent of making a language model guess a missing word in a sentence. The training set included gene sequences from a variety of infectious bugs:45,000 variants of influenza,60,000 of HIV, and4,000ofSARS-CoV-2.\n\nResults:The researchers compared their model’s highest-ranked mutations to those of actual viruses according to the area under curve (AUC), where 0.5 is random and 1.0 is perfect. The model achieved 0.85 AUC in predicting SARS-CoV-2 variants that were highly infectious and capable of evading antibodies. It achieved 0.69 AUC for HIV, and 0.77 AUC and 0.83 AUC respectively for two strains of influenza.Behind the news:Other researchers have alsoexploredsimilarities between language and gene sequences. For example, Salesforce researchers trained a language model totreat amino acids like wordsand build grammatically correct “sentences” of functional proteins that could be used in medicine.Why it matters:Discovering dangerous viral mutations typically takes weeks, as scientists must analyze DNA taken from patients. The ability to predict harmful mutations could help them find dangerous variants sooner, helping epidemiologists update their models and giving researchers a head start on vaccines and therapies.We’re thinking:The Batchis grammatically correct but not infectious. Though we wouldn’t mind if it went viral!\n\nDetecting earthquakes is an important step toward warning surrounding communities that damaging seismic waves may be headed their way. A new model detects tremors and provides clues to their epicenter.What’s new:S. Mostafa Mousavi and colleagues at Stanford and Georgia Institute of Technology builtEQTransformerto both spot quakes and measure characteristics that help seismologists determine where they originated.Key insight:Language models based on transformer networks use self-attention to track the most important associations among tokens, such as words, in a sentence. The authors applied self-attention to seismic waves globally to track the most important associations among their features. Since clues to a quake’s epicenter appear in portions of the waveform, they also used self-attention locally to find patterns over shorter periods of time.How it works:The authors passed seismic waves through an encoder that fed three decoders designed to detect earthquakes and spot two types of location signal. The authors trained and tested the system using theStanford Earthquake Dataset(STEAD), which contains over one million earthquake and non-earthquake seismographs. They augmented the data by adding noise, adding earthquake signals to non-quake waves, and shifting quake start times.\n\nResults:EQTransformer outperformed state-of-the-art models in both detecting earthquakes and tracking p- and s-waves. In detection, EQTransformer achieved an F1 score of 1.0, a 2 percent improvement over the previousstate of the art. In tracking p-waves, it improved mean absolute error over the earlierstate of the artin that task from 0.07 to 0.01. With s-waves, it improved mean absolute error from .09 to .01. The training dataset didn’t include seismographs from Japan, so the authors tested their model’s ability to generalize on aftershocks from a Japanese quake that occurred in 2000. In this test, EQTransformer’s ability to spot the arrival of p-waves varied from human performance by an average .06 seconds, while its ability to spot the arrival of s-waves varied from human performance by an average .05 seconds.Why it matters:Applied at both global and local scales, self-attention could be useful in tasks as diverse as forecasting weather, product demand, and power consumption.We’re thinking:We applaud this earth shattering research!\n\nFacebook expanded a system of vision, language, and speech models designed to open the social network to users who are visually impaired.What’s new:A Facebook service that describes photos in a synthesized voice now recognizes 1,200 visual concepts — 10 times more than the previous version. Known asautomatic alternative text, the system can recognize and explain what’s happening in a picture, including the relative size and position of people and objects, in any of 45 languages.How it works:Launched in 2016, the system initially learned from hand-labeled data to recognize 100 common concepts, like tree and mountain. Facebook added face recognition the following year, allowing users to opt into a more personalized experience. The new upgrade extends automatic alternative text in several ways:\n\nBehind the news:Facebook, along with other popular websites, has struggled with how to serve visually impaired users. Some havecomplainedthat the site doesn’t work well with common accessibility equipment like screen readers that speak text aloud. For instance, earlier versions of automated alternative text didn’t inform users whenthe images it described were advertisements. However, some users haveapplaudedFacebook’s use of face recognition with automatic alternative text, which can tell them when a photo depicts a friend or loved one.Why it matters:Around 285 million people worldwide are visually impaired and 39 million are blind, the World Health Organizationestimates. People who don’t see well are as reliant on information as anyone — and they represent a sizable market.We’re thinking:Disabled web users in the U.S. file hundreds oflawsuitsannually against Internet companies that don’t make their services accessible. Increasingly, online accessibility is recognized as a right, not a privilege.\n\nBecome a thought leader in your industry by combining your domain expertise with AI! We’re proud to offer AI+X, a virtual event series featuring AI experts from different industries, in partnership with Workera.Join uson February 4, 2021, for “AI+X: Don’t Switch Careers, Add AI.”\n\nU.S. regulators for the first time allowed commercial operators of autonomous aerial vehicles to fly out of operators’ sight.What’s new:The U.S. Federal Aviation Administration generally requires people on the ground to keep an eye on drones, but itauthorizeddrone maker American Robotics to fly without requirement.How it works:The company’s 20-pound quadcopters travel predetermined paths and automatically avoid collisions with birds, aircraft, and other obstacles.\n\nBehind the news:Companies can apply to the FAA for a waiver of theline-of-sight rule. American Robotics became the first company to receive one after four years of testing.\n\nWhy it matters:The ability to operate without a human in visual contact is a critical step to making drone flights easier to manage and more economical to operate.We’re thinking:Andrew used to work with Pieter Abbeel, Adam Coates, and others on reinforcement learning to get autonomous helicopters to flystunts. He crashed quite a few copters in the process! (Safely, of course, in empty fields.) With drones now flying out of an operator’s line of sight, it’s more important than ever to subject their hardware and software to robust safety testing and verification.\n\nGenerative adversarial networks andsouped-up language modelsaren’t the only image generators around. Researchers recently upgraded an alternative known as score-based generative models.\n\nWhat’s new:Yang Song and Stefano Ermon at Stanford derived a procedure forselecting hyperparameter valuesfor theirearlierscore-based generator, which produces images from noise. Finding good hyperparameters enabled the authors to generate better images at higher resolution.\n\nKey insight:Score-based image generation uses a model that learns how to change images corrupted by additive noise to reproduce the original pictures, and an algorithm that executes the changes to produce fresh images. The earlier work relied on manual tuning to find good values for hyperparameters such as how much noise to add to training images. Real-world data distributions are hard to analyze mathematically, so, in the new work, the authors approximated them with simplified distributions. Given the simpler scenario, they could analyze how each hyperparameter would affect both training and inference, enabling them to derive methods to compute hyperparameter values.\n\nAbout score-based generation:The process starts with producing many versions of each training dataset by adding various magnitudes of noise. A modifiedRefineNetis trained to minimize the difference between its prediction of the way, and the actual way, to change a noisy example into a clean one. RefineNet learns a vector field: Given a point in space that corresponds to an image, it returns a vector that represents the direction toward a more realistic image. Then an algorithm based onLangevin dynamics(a set of equations developed to model the way molecules interact in a physical system) moves the point in that direction. The process of finding a vector and moving in that direction repeats for a finite number of steps.\n\nHow it works:The authors followed their score-based procedure but used new methods to compute hyperparameters that governed the noise added to the training dataset and the size and number of steps computed by Langevin dynamics. We’ll focus on the noise hyperparameters in this summary.\n\nResults:The authors evaluated their new model’s output using Frechet Inception Distance (FID), a measure of how well a generated data distribution resembles the original distribution, where lower is better. Trained on 32×32 images inCIFAR-10, the model achieved 10.87 FID, a significant improvement over the earlier model’s 25.32 FID. It also beatSNGAN, which achieved 21.7 FID. The paper doesn’t compare competing FID scores at resolutions above 32×32 and omits FID scores altogether at resolutions higher than 64×64. It presents uncurated samples up to 256×256.Why it matters:GANs often don’t learn to produce good images because the objectives of their generator and discriminator are at odds. Score-based generative models optimize for only one objective, which eliminates this risk. That said, they may fail to converge for other reasons.We’re thinking:We love the idea of using mathematical reasoning to derive optimal hyperparameter values: More time to develop good models!",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/Screen20Shot202021-01-2720at2011.33.2220AM20copy.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/MUTATIONS.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker2075.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/BLIND.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/The20Batch.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/DRONES.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker2074.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-77/",
    "title": "issue 77",
    "date": "",
    "reading_time": "",
    "content": "The price of shares in video game retailer GameStop (NYSE: GME) gyrated wildly last week. Many people viewed the stock’s rapid ascent as a David-versus-Goliath story: Tech-savvy individual retail investors coordinated their trades online to push up the price and “stick it to” large hedge funds that had shorted the stock. Sadly, the reality is very different.\n\nSome retail investors in GameStop made money last week. But automated trading driven by AI now surpasses the speed and knowledge of most retail investors. I believe that wild swings in share price like the one driven by the GameStop crowd actually tend to result in a net transfer of wealth from retail investors to the hedge funds with the best AI teams.\n\nHedge funds that use AI to trade stocks make decisions based on a multitude of features including financial indices, social media chatter, and other forms of public or licensed data. Compared to a retail investor who readsr/wallstreetbets, they have access to far more information. They also have natural language processing and financial prediction tools to process all that information. Because of this, a typical human trader today can no more outperform an AI trader than beat a good reinforcement learning algorithm at an Atari game.\n\nI differentiate betweentrading and investing. Human investors who choose stocks because they believe the underlying company is fundamentally valuable, and hold those stocks to realize that value, can do very well. Allocating capital to deserving companies can also help them grow, and thus make everyone involved better off. That’s different from trading, in which the aim is to buy shares solely to sell them to someone else at a higher price. Ultimately, trading creates little, if any, net wealth. When there are so many opportunities to grow the pie, why would we work so hard on activities that keep the pie the same size but squeeze out a bigger piece for ourselves at others’ expense?\n\nInThe Washington Post, Helaine Olen wrote about how the volatility in GameStop’s stock price wasn’t just the story of a get-rich-quick scheme. It was also a tale of inequality, as young people who can’t find a good job dream of gaming the system. I’m glad that some traders will use their GameStop winnings to improve their lives. But I am fearful for those who will lose their savings playing a game they’re unlikely to win. For example, those who bought at GameStop’s January 27 peak and might end up incurring substantial losses they can ill afford.\n\nWhen you decide what AI projects to work on, I hope you will pick something that enriches not only yourself but society as a whole. Let’s also do what we can to make sure that whatever wealth we create is fairly and widely shared.\n\nKeep learning!\n\nAndrew\n\nA model designed to assess medical patients’ pain levels matched the patients’ own reports better than doctors’ estimates did — when the patients were Black.What’s new:Black people who suffer from osteoarthritis, or loss of cartilage in the joints, tend to report higher levels of pain than White patients who have the same condition. To understand why, researchers at Microsoft, Stanford University, and other institutions trained a model topredict the severity of a patient’s pain from a knee x-ray.The model predicted self-reports by Black patients more accurately than a grading system commonly used by radiologists.How it works:The researchers began with aResNet-18pretrained on ImageNet. They fine-tuned it to predict pain levels from x-rays using 25,049 images and corresponding pain reports from2,877 patients. 16 percent of the patients were Black.\n\nBehind the news:The Kellgren-Lawrence grade is based on a 1957 study of a relatively small group of people, nearly all of whom were White. The system often underestimates pain levels reported by Black patients.Why it matters:Chronic knee painhobblesmillions of Americans, but Black patients are less likely than White ones to receive knee replacement surgery.Studies have shownthat systems like the Kellgren-Lawrence grade often play an outsize role in doctors’ decisions to recommend surgery. Deep learning offers a way to narrow this gap in care and could be adapted to address other healthcare discrepancies.We’re thinking:Algorithms used in healthcare have come under scrutiny forexacerbating bias. It’s good to see one that diminishes it.\n\nA grassroots research collective aims to make a GPT-3 clone that’s available to everyone.What’s new:EleutherAI, a loose-knit group of independent researchers, is developing GPT-Neo, an open source, free-to-use version of OpenAI’s gargantuan language model. The model could be finished as early as August, team member Connor Leahy toldThe Batch.How it works:The goal is to match the speed and performance of the fully fledged, 175 billion-parameter version of GPT-3, with extra attention to weeding out social biases. The team successfully completed a 1 billion-parameter version, and architectural experiments are ongoing.\n\nBehind the news:In 2019, when OpenAI introduced GPT-2, the company initially refused to release the full model, citing fears that it would set off a flood of disinformation. That motivated outside researchers, including Leahy, to try toreplicate the model. Similarly, OpenAI’s decision to keep GPT-3 under wraps inspired EleutherAI’s drive to create GPT-Neo.Why it matters:GPT-3 has made headlines worldwide, but few coders have had a chance to use it. Microsoft has anexclusive licenseto the full model, while others cansign upfor access to a test version of the API. Widespread access could spur growth in AI-powered productivity and commerce.We’re thinking:If talk is cheap, AI-generated talk might as well be free!\n\nWith access to a trained model, an attacker can use areconstruction attackto approximate its training data, including examples that impinge on privacy, such as medical images. A method calledInstaHiderecently wonacclaimfor promising to make such examples unrecognizable to human eyes while retaining their utility for training. Researchers cracked it in short order.What’s new:InstaHide aims to scramble images in a way that can’t be reversed. Nicholas Carlini and researchers at Berkeley, Columbia, Google, Princeton, Stanford, University of Virginia, and University of WisconsindefeatedInstaHide to recover images that look a lot like the originals.Key insight:InstaHide can be viewed as a linear equation that scrambles images by summing them (typically two sensitive and four public images chosen at random) using random weights, then randomly flipping the sign of each pixel value. But summing is reversible, and changing signs doesn’t effectively obscure values. Consequently, a linear equation can be devised to reverse this process.How it works:The authors applied InstaHide to produce targets.CIFAR-10, CIFAR-100, andSTL-10stood in for sensitive datasets.ImageNetserved as their non-sensitive dataset. Then they undid the effects of the InstaHide algorithm in reverse order.\n\nResults:The authors tested their approach using the CIFAR-10 and CIFAR-100 test sets as proxies for sensitive data. Subjectively, the reconstructed images closely resembled the originals. They also tried it on theInstaHide Challenge, a collection of 5,000 scrambled versions of 100 images published by the InstaHide team. They found an approximate solution in under an hour, and InstaHide’s inventors agreed that they had met the challenge.Why it matters:Once personally identifiable information is leaked, it’s impossible to unleak. Machine learning must protect privacy with the utmost rigor.We’re thinking:The authors show that their method can work well if the scrambled training images are available. It remains to be seen whether it works given access only to a trained model.\n\nOnly a few hours left to register for our first AI+X event — “Don’t Switch Careers, Add AI” — in partnership with Workera!Join usfor a virtual discussion with experts at Accenture and Illumina on Thursday, February 4, 2021, at 10 a.m. Pacific Standard Time.\n\nNeural nets could speed up development of new materials.What’s new:A deep learningsystemfrom Sandia National Laboratories dramatically accelerated simulations that help scientists understand how changes to the design or fabrication of a material — say, the balance of metals in an alloy — change its properties.How it works:The researchers trained an LSTM to predict how the properties of a material evolve during the process known as spinodal decomposition, in which a material separates into its constituents in the presence or absence of heat.\n\nResults:In tests, the model simulated thermodynamic processes, such as the way a molten alloy congeals as it cools, more than 42,000 times faster than traditional simulations: 60 milliseconds versus 12 minutes. However, the increased speed came at a cost of slightly reduced accuracy, which fell by 5 percent compared to the traditional approach.Behind the news:Machine learning has shown promise as a shortcut to a variety of scientific simulations.\n\nWhy it matters:Faster simulations of materials can quicken the pace of discovery in areas as diverse as optics, aerospace, energy storage, and medicine. The Sandia team plans to use its model to explore ultrathin optical technologies for next-generation video monitors.We’re thinking:From Gorilla Glass to graphene,advanced materialsare transforming the world. Machine learning is poised to help such innovations reach the market faster than ever.\n\nBayes-optimal algorithms always make the best decisions given their training and input, if certain assumptions hold true. New work shows that some neural networks can approach this kind of performance.What’s new:DeepMind researchers led by Vladimir Mikulik showed that recurrent neural nets (RNNs) with meta-training, or training on several related tasks,behave like Bayes-optimal models.Key insight:Theoretically, memory-based models like RNNs, given sufficient meta-training,become Bayes-optimal. To test this hypothesis, the researchers compared outputs and the internal states of both types of model.How it works:The researchers meta-trained 14 RNNs on various prediction and reinforcement learning tasks. For instance, to predict the outcome of flipping a biased coin, the model observed coins with various biases. Then they compared each RNN to a known Bayes-optimal solution.\n\nResults:All RNNs converged to behave indistinguishably to their Bayes-optimal counterparts. For instance, the RNN that learned to predict biased coin flips achieved a KL divergence of 0.006 compared to 3.178 before meta-training. The internal states of RNNs and Bayes-optimal models matched nearly perfectly, differing in most tasks by a mean-squared error of less than 0.05.Why it matters:Bayesian models are reputed to be provably optimal and interpretable. Compared to neural nets, though, they often require more engineering and vastly more computational power. This work involved toy problems in which a Bayes-optimal model could be written by hand, but it’s encouraging to find that meta-trained RNNs performed optimally, too.We’re thinking:Maybe RNNs will become more popular here in the San Francisco Bayes Area.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/Screen20Shot202021-02-0220at203.23.3120PM20copy.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/DIYGPT.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/INSTAHIDE20576x324.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/The20Batch201.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker2076.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/BAYES.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-304/",
    "title": "issue 304",
    "date": "",
    "reading_time": "",
    "content": "Everyone can benefit by learning to code with AI! At AI Fund, the venture studio I lead, everyone — not just the engineers — can vibe code or use more sophisticated AI-assisted coding techniques. This empowers everyone to build with AI. The impact on team creativity and productivity has been exciting! I share my experience with this in the hope that more teams will invest in empowering everyone to build with AI.\n\nEveryone at AI Fund who was not already an engineer started with our “AI Python for Beginners” course to learn the basics. I also shared with the team details ofthe tech stack I useto give everyone a default set of building blocks. Since then, many have gone on to acquire additional building blocks (such as additional third-party APIs) themselves either by takingcourses, searching online, or learning from colleagues.\n\nYou can watch a video of our experience with thishere.\n\nHere are just a few examples of applications that non-engineers at AI Fund have built:\n\nIt is very empowering when individuals don’t have to try to get scarce engineering resources allocated to their ideas in order to try them out. There are a lot fewer gatekeepers in the way: If someone has an idea, they can build a prototype and try it out. If it gets positive feedback from users, that lays the groundwork for scaling it up. Or, if the prototype does not work, this is also valuable information that lets them quickly move on to a different idea or take insights from critical feedback to decide what to try next.\n\nIn the future, one of the most important skills in any profession will be the ability to tell a computer exactly what you want, so the computer can do it for you. For the foreseeable future, writing code (with AI assistance, so the AI, rather than you, actually writes the code) will be the best way to do this.\n\nThis is a great time for everyone to code with AI!\n\nKeep building,\n\nAndrew\n\nIn “DSPy: Build and Optimize Agentic Apps,” you’ll learn to use Databricks’ DSPy framework to structure, debug, and improve the accuracy of agentic workflows. DSPy lets you define clear input and output steps, trace model behavior, and automate prompt tuning with built-in tools. Build a sentiment analyzer, travel assistant, and RAG agent!Enroll now\n\nDeepSeek updated its groundbreaking DeepSeek-R1 large language model to strike another blow for open-weights performance.\n\nWhat’s new:The newDeepSeek-R1-0528surpasses its predecessor and approaches the performance of OpenAI o3 and Google Gemini-2.5 Pro. A smaller version,DeepSeek-R1-0528-Qwen3-8B, runs on a single GPU with as little as 40GB VRAM, according toTechCrunch.\n\nHow it works:DeepSeek released little information so far about how it built the new models.\n\nPerformance:DeepSeek-R1-0528 nips at the heels of top closed LLMs on a variety of benchmarks, while DeepSeek-R1-0528-Qwen3-8B raises the bar for LLMs in its 8-billion-parameter size class. DeepSeek claims general improvements in reasoning, managing complex tasks, and writing and editing lengthy prose, along with 50 percent fewer hallucinations when rewriting and summarizing.\n\nBehind the news:The initial version of DeepSeek-R1 challenged the belief that building top-performing AI models requires tens to hundreds of millions of dollars, top-of-the-line GPUs, and enormous numbers of GPU hours. For the second time in less than a year, DeepSeek has built a competitive LLM with a relativelylowbudget.\n\nWhy it matters:DeepSeek’s models, along with Alibaba’s Qwen series, continue to narrow the gap between open-weights models and their closed peers. Its accomplishments could lead to wider adoption of less-expensive, more-efficient approaches. DeepSeek is passing along the cost savings to developers, offering high-performance inference at a fraction of the cost of closed models.\n\nWe’re thinking:DeepSeek-R1-0528-Qwen3-8B mixes contributions from open-weight models — possible only because Qwen3’s license, like DeepSeek’s is permissive. Open models enable experimentation and innovation in ways that closed models do not.\n\nAI is bringing a massive boost in productivity to Duolingo, maker of the most popular app for learning languages.\n\nWhat’s new:Duolingo used generative AI toproduce148 courses, more than doubling its previous catalog. The technology enabled the company to offer some of its most popular courses — Spanish, French, German, Italian, Japanese, Korean, and Mandarin — in 28 languages. Initially, the company is using AI to produce courses aimed at beginners, with more advanced levels to come.\n\nHow it works:Duolingo’sAI-assisted approach to building language coursesquickly turns a single course into many. The new approach revved its pace from building 100 courses over 12 years to producing many more than that in less than a year.\n\nBehind the scenes:AI is at the heart of Duolingo’s expansion into other areas beyond language learning.\n\nWhy it matters:Companies in nearly every industry face pressure to produce more with less amid rising competition. AI can help to accomplish that while potentially improving product quality, and Duolingo has ample reason to move aggressively in this direction. The startupSpeak, which offers a voice-based approach to learning languages, is growing rapidly, and Google just launchedLittle Language Lessonsthat show how an AI-first product could be used as a language teacher and conversational partner.\n\nWe’re thinking:AI is well on the way totransforming educationfor teachers, students, and technology companies!\n\nAI’s thirst for energy is growing, but the technology also could help produce huge energy savings over the next five to 10 years, according to a recent report.\n\nWhat’s new:The International Energy Agency (IEA), which advises 44 countries on energy policy, performed a comprehensiveanalysisof AI’s energy consumption including energy required to obtain critical materials needed for chips and data centers. The report sees dark clouds ahead but also silver linings.\n\nDark clouds:The report, which is based on interviews with officials in government, energy, and technology, makes four projections for AI’s energy consumption. In the base scenario, future growth and efficiency gains are similar to those of the past five years. The agency also plots a “take-off” scenario in which AI adoption happens faster, a “high efficiency” scenario with lower energy needs, and a “headwinds” scenario in which adoption of AI slows or infrastructure bottlenecks impede construction. Among the conclusions:\n\nSilver linings:AI already makes energy generation, distribution, and use more efficient. The authors expect these savings to accelerate.\n\nYes, but:The authors concede that lower energy costs for AI likely will lead to much greater consumption — according to theJevons paradox— so more-efficient models and hardware will result in higher energy consumption overall.\n\nBehind the news:Data centers were growing rapidly prior to the boom in generative AI. Data centers’ electricity use doubled between 2000 and 2005 and again between 2017 and 2022, driven by the growth of cloud computing and data storage, streaming and social media, and cryptocurrency mining. However, these periods of accelerating growth were followed by periods of slower growth as efforts to cut costs led to more-efficient software and hardware. The authors expect this pattern to hold.\n\nWhy it matters:The IEA report is a first-of-its-kind analysis of AI’s energy requirements, how they’re likely to grow, as well as the potential of the technology itself to reduce those requirements. It confirms that AI is poised to consume huge amounts of energy. However, it also suggests that today’s energy costs will be tomorrow’s energy savings as AI makes energy generation, distribution, and use more efficient across a wide variety of industries.\n\nWe’re thinking:While demand for electricity for data centers is growing rapidly, calibrating the right level of investment is tricky. High levels of growth come with high levels of hype that can lead analysts to overestimate future demand. For example, Microsoft, after examining its forecasts,canceleddata-center projects that would have consumed 2 gigawatts.\n\nResearchers identified a simple way to mislead autonomous agents based on large language models.\n\nWhat’s new: Ang Li and colleagues at Columbia University developed a method toexploitthe implicit trust that agents tend to place in popular websites by poisoning those websites with malicious links.\n\nKey insight:Commercially available agentic systems may not trust random sites on the web, but they tend to trust popular sites such as social-media sites. An attacker can exploit this trust by crafting seemingly typical posts that link to a malicious website. The agent might follow the link, mistakenly extending its trust to an untrustworthy site.\n\nHow it works: The authors tested web-browsing agents includingAnthropic Computer UseandMultiOnon tasks such as shopping or sending emails.\n\nResults: Once an agent was redirected to the malicious websites, it reliably followed the attacker’s instructions. For example, each of the agents tested divulged credit card information in 10 out of 10 trials. Similarly, each agent sent a phishing message from the user’s email account asking recipients to send money to a malicious “friend” in 10 out of 10 trials.\n\nWhy it matters: Giving agents the ability to perform real-world actions, such as executing purchases and sending emails, raises the possibility that they might be tricked into taking harmful actions. Manipulating agents by referring them to malicious web content is an effective vector of attack. Agents will be more secure if they’re designed to avoid and resist such manipulation.\n\nWe’re thinking:Humans, too, can be fooled by phishing and other malicious activities, and the path to programming agents to defend against them seems easier than the path to training the majority of humans to do so. In the long term, agents will make online interactions safer.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/06/2025.06.04-LETTER-3--1-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/06/The-Batch-ads-and-exclusive-banners---2025-06-02T160931.393.png",
      "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed--100-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed--69-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed---2025-06-04T165349.311.png",
      "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed---2025-06-04T165354.442.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-xiii/",
    "title": "issue xiii",
    "date": "",
    "reading_time": "",
    "content": "If you wonder how often I take online courses myself, the answer is: Quite often. I have a longstanding interest in AI for healthcare. I had some time off during the Fourth of July holiday in the U.S., and finished the Johns Hopkins course on clinical trial design taught by Professors Janet Holbook and Lea Drye.\n\nAs AI practitioners, to work effectively on AI+X, we have to learn a bit about X as well. Whether you want to work on AI for healthcare, climate change, manufacturing, agriculture, logistics, or something else, opportunities abound and I encourage you to both find collaborators in application area X as well as learn about it yourself.Keep learning!Andrew\n\nEye contact is such an essential element in interpersonal communication that it’s considered rude in face-to-face conversation to avoid another person’s eyes. But a lowered gaze is standard in video chat, when the face on the screen is often several inches lower than the camera’s lens. Guess what? There’s an app for that!What’s new:Apple added a feature to its FaceTime video-chat app that warps the image of your face, so people you chat with will think you’re looking them in the eye.How it works:FaceTime Attention Correction works like a Snapchat filter, continually adjusting a map of the user’s face so the eyes appear to look at the camera. AMacRumorsvideohighlights the warping effect: A drinking straw — the dark horizontal line in the clip above — curves slightly as it passes over the commentator’s eyes. The feature reportedly works only on Apple’s newest phones, the iPhone XS and XS Max. It can be switched off in the settings.Behind the news: Mike Rundle, a product designer at Intuit, noticed the gaze-fixing feature and pointed it out in atweet. In fact, he had predicted the feature in a 2017blogon the future of the iPhone. He analyzed Apple’s recent acquisitions and told readers to look out for “advanced image-manipulation algorithms that make sure FaceTime calls always show your eyes looking at the other person.”Why it matters:Anything that might improve interpersonal communication is worth exploring. Yet our perception of reality is increasingly subject to automated tampering. For instance, Zoom's videoconferencing system offers a \"touch up my appearance\" switch that subtly smoothes facial wrinkles. Apple added gaze correction without notice, but it did provide a way to turn it off. Companies with a lower standard of accountability to users could seed communication tools with features that mediate communications without your knowledge or control.Takeaway:Will this new feature make video chat more intimate? Or will it lead to less-present telepresence as people who seem fully engaged are actually scanning Reddit? While we mull the answer, we’ll be on the lookout for software that flags manipulated facial expressions during video chats.\n\nTranslating languages that haven't been understood since ancient times typically requires intensive linguistic research. It turns out that neural networks can do the job.What’s new:Researchers at MIT CSAIL and Google Brain devised analgorithmthat deciphers lost languages. It’s not the first, but it achieves state-of-the-art results across a variety of tongues.Key insight:The new approach identifies cognates, words in different languages that have the same meaning and similar roots. Cognates follow consistent rules, such as:\n\nHow it works:The new method is based on a mapping between cognates in an unknown language and a known language.\n\nResults:The new approach outperforms previous methods on Ugartic, a relative of Hebrew, translating cognates with up to 93.5 percent accuracy. It also sets a new state of the art in translating the proto-Greek script Linear B into Greek, spotting cognates with 84.7 percent accuracy.Why it matters:Previous translation algorithms for lost languages were designed specifically for a particular language. The new method generalizes to wildly dissimilar tongues and achieves stunningly high accuracy.Takeaway:Historically, specialists labored for decades to decipher the thoughts encoded in lost languages. Now they have a general-purpose power tool. We look forward to ancient secrets revealed.\n\nSorting data into the right categories is AI's bread-and-butter task. Now the technology is being used to sort recyclables into the right bins.What’s new:Single Stream Recyclers of Sarasota, Florida, recently increased its fleet of AI-equipped sorters from six to 14. That's the largest deployment of recycling robots in the U.S. and possibly the world, according toThe Robot Report.Bot master:Single Stream uses sorters built by AMP Robotics, which combines proprietary deep learning software with off-the-shelf robotics. Its machines use suction-tipped appendages to sort 80 pieces of waste a minute — twice as fast as the average for humans, the company claims — and work much longer hours. The robots are in use in the U.S., Canada, and Japan.How it works:AMP trained its model on photos of waste material. It learned to distinguish not only newspapers from milk cartons, but also various classes of plastics and metals. The system reportedly achieves high accuracy in spite of discards that are often battered, dented, crumpled, and dirty.Behind the news:Recycling makes economic sense if producing goods from old materials is less expensive than making them afresh. Using humans to sort recyclables is expensive in Western countries, and robots can reduce the cost. An alternative is to ship waste to places like China, but such relationships are susceptible to geopolitical turmoil — not to mention the greenhouse gases emitted shipping waste across the ocean.Why it matters:The average American produces nearly four and a half pounds of trash daily. Globally, daily waste tops 3.5 billion tons. Much of this doesn’t degrade and winds up in oceans or seeping into the food supply. More efficient recycling keeps more waste out of the environment and conserves resources for other uses.We’re thinking:The cost-benefit ratio of recycling is hotly debated and difficult to calculate. Of course, robots aren’t cheap — AMP, a private company, doesn’t publicize its prices or sales — but they clearly have potential to cut immediate costs. Meanwhile, recycling itself saves the external costs of environmental degradation. AMP’s success suggests that recycling plants, at least, are finding the tradeoff worthwhile.\n\nScience is drowning in data. Hard-won findings can sink into obscurity amid the rising tide of research. But new research shows that deep learning can serve as a net for catching knowledge.What’s new:A neural network trained by researchers at the Department of Energy’s Berkeley Laboratoryfoundmaterials with special properties by parsing abstracts published between 1922 and 2018.How it works:Berkeley’s library of 3.3 million abstracts contains roughly 500,000 distinct words.\n\nWhy it matters:Pick a field of science, and you’ll find loads of research that haven’t yet been wrung for insight. Untold insights hide therein. This work “suggests that latent knowledge regarding future discoveries is to a large extent embedded in past publications,” the researchers write. Now scientists may be able to take advantage of that knowledge to make more rapid progress.Takeaway:Artificial intelligence has been touted as a way to extrapolate new discoveries from extant research. Now it’s beginning to make good on that promise. Recent experiments have shown success in physics, chemistry, astronomy, and genomics. Other computationally intensive fields — for instance medicine, economics, and climatology — are in line for breakthroughs.\n\nLearn essential neural network tricks including initialization, L2 and dropout regularization, batch normalization, and gradient checking.Enroll nowin the Deep Learning Specialization.\n\nAutomated systems for interpreting human emotions have analyzed speech, text, and facial expressions. New research shows progress on the subtle art of reading body language.What’s new:Researchers from the Universities of North Carolina and Maryland builtEWalk, a collection of 1,348 videos of people walking, manually labeled according to perceived emotions: happy, sad, angry, and neutral. For instance, gaits labeled happy tend to have a quick pace and widely swinging arms, while sad gaits are slower with a slumped posture. A model trained on EWalk achieved state-of-the-art results matching gaits with emotional states.Key insights:The model uses a random forest to classify the emotion expressed by a given gait using a combination of features extracted using hand-crafted rules and a neural network.\n\nHow it works:TimePoseNet, a neural network detailed in previous research, extracts from videos a 3D skeletal representation of the gait in each frame. The researchers compute pose and movement features from the skeleton and feed them to the random forest. They also feed the skeleton to an LSTM network, which supplements the random forest’s input.\n\nResults:The previous best method achieved 68 percent accuracy on EWalk. The random forest, given the pose and movement features plus the LSTM’s hidden units, achieves 80.1 percent. The random forest also outperforms the LSTM alone.Why it matters:The better computers understand human emotion, the more effectively we’ll be able to work with them. Beyond that, this work has clear applications in security, where early warning of potential aggression could help stop violent incidents before they escalate. It could also be handy in a retail environment, helping salespeople choose the most productive approach to prospective customers, and possibly in other face-to-face customer service situations.We’re thinking:Psychology demonstrates that emotion affects human planning and actions. While models exist that are startlingly accurate at predicting future human actions — seethispaper — they don’t explicitly take emotion into account. Factoring in emotions could enable such systems to make even better predictions.\n\nViruses evolve at a breakneck clip. That keeps medical researchers working overtime to develop vaccines. Now AI is designing inoculations that could help humans stay one step ahead of the bugs.What’s new:The first AI-generated vaccine hasenteredclinical trialsin the U.S. Nikolai Petrovsky, a professor at Flinders University in South Australia who led the fight against swine flu in 2009, is heading up the project.How it works:Petrovsky and his colleagues trained a neural network to discover new substances that boost the body’s immune response to a specific trigger. Called the Search Algorithm for Ligands, the model sifted through the medical literature on vaccine efficacy. Then it invented trillions of compounds and predicted their effectiveness. The team synthesized and tested the model’s top suggestions, and the results were promising enough that the U.S. National Institute of Allergy and Infectious Diseases sponsored a year-long trial.Behind the news:In the war of immunization, the battle line can shift within a single season. This year’s flu vaccine was no match for a late-season mutation, and its effectiveness dropped from 47 percent to 9 percent.Why it matters:Flu killed an estimated 79,000 people during the 2017-2018 season. AI that fast-tracks effective vaccines could save countless lives.Takeaway:The number of recent flu victims is a small fraction of the historic toll, and evolution could enable it to come roaring back. AI could allow for rapid response to new strains before they show a glimmer of pandemic potential.\n\nThe ability to recognize text is useful in contexts from ecommerce to augmented reality. But existing computer vision systems fare poorly when a single image contains characters of diverse sizes, styles, angles, and backgrounds along with other objects — like, say, loudly designed commercial packaging. Researchers at Walmart’s Bangalore lab devised an algorithm to tackle this problem.What’s new:A team put together an end-to-endarchitecturethat segments images into regions and feeds them into existing text extraction networks. The system outputs boxes containing the extracted text.Key insights:The approach is twofold: Segment, then extract. The team, led by Pranay Dugar, found that:\n\nHow it works:The system segments images by creating and classifying superpixels: groups of adjacent pixels of similar color and intensity. It feeds them into pretrained text extraction networks and merges the outputs.\n\nResults:The system is competitive with earlier methods on ICDAR 2013. But it excels with so-called high-entropy images that are unusually complex. It improves both precision (the proportion of predictions that are correct) and recall (the proportion of correct labels predicted) by 10 percent on the authors' own Walmart High Entropy Images data set.Why it matters:Extracting words from images containing multiple text-bearing objects is difficult. The letters may be poorly lit, slanted at an angle, or only partially visible. Jumbled together, they can give a computer vision system fits. Segmenting text regions and then using the ensemble of text-extraction models makes the problem more tractable.Takeaway:In a world increasingly crowded with signs, symbols, and messages, applications of such technology are numerous. It could mean efficient creation of digital restaurant menus from physical copies, or an agent that lets you know when an ice cream truck passes your house. It gives machines a way to read words in chaotic environments — and possibly new ways to communicate with us.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/d7b2ea90-dddf-4eb5-b287-0e74e8dcc768-1.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/5bfef620-f53a-4b0f-a84c-a8512fa4648f.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/513ac1af-a7bd-442b-aa21-92944bb6fdf0.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/e30d3bf2-2090-4616-900d-2a4c0143d54f.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/f154d747-c027-4b0f-a43f-84e66e40948c.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/fc001597-c86a-4258-939a-90af526d6a5c.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/f1cac6b9-ad80-4051-acb4-1c5cbdeac1a0.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/fbc0cfe0-326c-4713-850c-01a9090f00d7.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/3efc7af6-bb0a-476c-9ed4-e5a65cd120ac.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-172/",
    "title": "issue 172",
    "date": "",
    "reading_time": "",
    "content": "Last week, Facebook’s parent company Meta released a demo ofGalactica, a large language model trained on 48 million scientific articles. Two days later, amid controversy regarding the model’s potential to generate false or misleading articles, the company withdrew it.\n\nIs Galactica dangerous? How should researchers, as well as the broader AI community, approach such developments?\n\nMichael Black, director of the Max Planck Institute for Intelligent Systems,raised concernabout Galactica’s potential for harm by generating seemingly authoritative scientific papers that are factually bonkers. Meta chief AI scientistYann LeCunvigorouslydefendedthe model. He pointed out that, despite worries that people might misuse large language models (LLMs), it largely hasn’t happened.\n\nAt the risk of offending both sides, let me share my take.\n\nWhen it comes to building language models that generate more factually accurate output, the technical path forward is not yet clear. LLMs are trained to maximize the likelihood of text in their training set. This leads them to generate text that sounds plausible — but a LLM that makes up facts can also perform well on this training objective.\n\nSome engineers (including the Galactica’s team) have proposed that LLMs could be an alternative to search engines. For example, instead of using search to find out the distance to the Moon, why not pose the question as a prompt to a language model and let it answer? Unfortunately, the maximum-likelihood objective is not well aligned with the goal of providing factually accurate information. To make LLMs better at conveying facts, research remains to be done on alternative training objectives or, more likely, model architectures that optimize for factual accuracy rather than likelihood.\n\nWhether a tool like Galactica will be more helpful or harmful to society is not yet clear to me. There will be bumps in the rollout of any powerful technology. The AI community has producedracist algorithms,toxic chatbots, and other problematic systems, and each was a chance to learn from the incident and get better. Let’s continue to work together as a community, get through the bumps with respect and support for one another, and keep building software that helps people.Keep learning!\n\nAndrew\n\nArtists are rebelling against AI-driven imitation.What’s new:DeviantArt, an online community where artists display and sell their work and marketplace for digital art,launchedDreamUp, a text-to-image generator that aims to help artists thwart attempts to imitate their styles or works.How it works:DreamUp is avanilla implementationof the open sourceStable Diffusiontext-to-image generator.\n\nOpting out:Stable Diffusion was trained on images scraped from the web including works from DeviantArt. Upon its release, some artistsobjectedto the model’s ability to replicate their style via prompts like, “in the style of ____.”\n\nBehind the news:AI’s increasing ability to mimic the styles of individual artists has become a flashpoint between engineers and artists. When acclaimed artist Kim Jung Gidiedin early October, within one day a former game developerreleaseda model trained to produce works in his style. While the developer justified the work “as an homage,” responses included not only criticism and insults but also threats of violence. Such comments, one commenter noted, were part of a recent rise in “extremely violent rhetoric directed at the AI art community.”\n\nWhy it matters:Generative AI is attracting attention andfunding, but the ethics of training and using such systems are still coming into focus. For instance, lawyers arepreparingto argue that GitHub’s CoPilot code-generation system, which was trained on open-source code, violates open-source licenses by improperly crediting coders for their work. The outcome may resolve some uncertainty about how to credit a generative model’s output — but it seems unlikely to address issues of permission and compensation.\n\nWe’re thinking:Artists who have devoted years to developing a distinctive style are justifiably alarmed to see machines crank out imitations of their work. Some kind of protection against copycats is only fair. For the time being, though, the limit of fair use in training and using AI models remains an open question.\n\nA new computing cluster delivers more bang per chip.\n\nWhat’s new:Cerebras, one of several startups vying to supply the market for specialized AI chips,unveiledAndromeda, a supercomputer based on its processors. Unlike conventional clusters, which incur data bottlenecks as processors are added, the system’s processing speed rises linearly with additional processors.How it works:Andromeda comprises 16 CerebrasCS-2 Wafer Scale Enginechips. Each chip holds 850,000 processing cores (more than 100 times the number found on anNvidia A100) on a silicon disc that measures 21.5 centimeters across.\n\nSpeed tests:Scientists at Argonne National Laboratory used the system totrainGenSLM language models in several sizes. Increasing the number of processors from one to four boosted throughput nearly linearly while training models of 123 million parameters and 1.3 billion parameters. Going from one to four chips also cut the smaller model’s training time from 4.1 to 2.4 hours and cut the larger model’s training time to 15.6 to 10.4 hours.\n\nBehind the news:As interest rates rise, AI chip startups are facing headwinds in raising enough capital to support their often huge expenses.\n\nWhy it matters:Neural networkshavebreachedthe 1 trillion-parameters mark, and numbers one or two orders of magnitude greater may be close at hand. More efficient compute clusters could train those models more quickly and consume less energy doing it.We’re thinking:For most current machine learning models, the usual GPUs should be fine. Cerebras specializes in models and compute loads too large for a handful of GPUs in a single server — an interesting business as model sizes balloon.\n\nWhen should you start building an AI project portfolio? What kinds of projects should it include? Get answers from AI practitioners during “How to Build a Real-World AI Project Portfolio” on November 29, 2022, at 9 a.m. Pacific Standard Time.Register now!\n\nA new algorithm defeated a championship-winning Go model using moves that even a middling human player could counter.What’s new:Researchers at MIT, UC Berkeley, and the Fund for Alignment Research trained amodelto defeatKataGo, an open source Go-playing system that has beaten top human players.How it works:The authors’ system tricks KataGo into deciding prematurely that it has won, causing it to end a game when the authors’ model is in a winning position.\n\nResults:The model’s winning strategy involved taking control of a corner of the board and adding a few easy-to-capture pieces outside that area.\n\nWhy it matters:This work is a helpful reminder that neural networks are brittle, particularly to adversarial attacks that take advantage of a specific system’s idiosyncrasies. Even in the limited context of a game board, a model that achieves superhuman performance can be defeated by a simple — but unusual — strategy.\n\nWe’re thinking:AI practitioners perform exploratory data analysis and address potential attacks, but vulnerabilities always remain. Approaches like the one in this paper offer a way to find them.\n\nWhile neural networks perform well on image, text, and audio datasets, they fall behinddecision treesand their variations for tabular datasets. New research looked into why.\n\nWhat’s new:Léo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux at Inria Saclay Centre and Sorbonne Universitytraineda variety of neural networks and tree models on tabular datasets. Performance on theirtabular data learning benchmarkrevealed dataset characteristics that favor each class of models.\n\nKey insight:Previousworkfound that no single neural network architecture performed best on a variety of tabular datasets, but a tree-based approach performed better than any neural network on most of them. Training and testing different models on many permutations of the data can reveal principles to guide the choice of architecture for any given dataset.\n\nHow it works:The authors compiled datasets, trained a variety of models (using a variety of hyperparameters), and evaluated their performance. Then they applied transformations to the data, retrained the models, and tested them again to see how the transformations affected model performance.\n\nResults:Averaged across all tasks, the best tree models performed 20 percent to 30 percent better than the best deep learning models. ResNets fell even farther behind trees and transformers as the number of uninformative features rose. In another experiment, training on smoothed labels degraded the performance of trees more than that of neural networks, which suggests that tree-based methods are better at learning irregular mapping of training data to labels.\n\nWhy it matters:Deep learning isn’t the best approach to all datasets and problems. If you have tabular data, givetreesa try!\n\nWe’re thinking:The authors trained their models on datasets of 10,000 or 50,000 training examples. Smaller or larger datasets may have yielded different results.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--15--1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--16-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--17-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/11/29.11-09.png",
      "https://dl-staging-website.ghost.io/content/images/2022/11/KATAGO_FastEdit_600px--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/11/unnamed--18-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-166/",
    "title": "issue 166",
    "date": "",
    "reading_time": "",
    "content": "The rise of AI over the last decade has been powered by the increasing speed and decreasing cost of GPUs and other accelerator chips. How long will this continue? The past month saw several events that might affect how GPU prices evolve.\n\nIn September, Ethereum, a major blockchain that supports the cryptocurrency known as ether, completed a shift that significantly reduced the computation it requires. This shift — dubbedthe Merge— should benefit the natural environment by consuming less energy. It will also decrease demand for GPUs to carry out cryptocurrency mining. (The Bitcoin blockchain remains computationally expensive.) I expect that lower demand will help lower GPU prices.\n\nOn the other hand, Nvidia CEO Jensen Huangdeclaredrecently that the era in which chip prices could be expected to fall is over. Moore’s Law, the longstanding trend that has doubled the number of transistors that could fit in a given area of silicon roughly every two years, is dead, he said. It remains to be seen how accurate his prediction is. After all, many earlier reports of the death of Moore’s Law have turned out to be wrong. Intel continues tobetthat it will hold up.\n\nThat said, improvements in GPU performance have exceeded the pace of Moore’s Law as Nvidia has optimized its chips to process neural networks, while the pace of improvements in CPUs, which are designed to process a wider range of programming, has fallen behind. So even if chip manufacturers can’t pack silicon more densely with transistors, chip designers may be able tocontinueoptimizing to improve the price/performance ratio for AI.\n\nInternational news also had implications for chip supply and demand. Last week, the United States governmentrestrictedU.S. companies from selling advanced semiconductors and chip-making equipment to China. It also prohibited all sales in China of AI chips made using U.S. technology or products andbarredU.S. citizens and permanent residents from working for Chinese chip firms.\n\nNo doubt the move will create significant headwinds for many businesses in China. It will also hurt U.S. semiconductor companies by limiting their market and further incentivizing Chinese competitors to replace them. The AI community has always been global, and if this move further decouples the U.S. and China portions, it will have effects that are hard to foresee.\n\nStill, I’m optimistic that AI practitioners will get the processing power they need. While much AI progress has been — and a meaningful fraction still is — driven by using cheaper computation to train bigger neural networks on bigger datasets, other engines of innovation now drive AI as well. Data-centric AI, small data, more efficient algorithms, and ongoing work to adapt AI to thousands (millions?) of new applications will keep things moving forward.Semiconductor startups have had a hard time in recent years because, by the time they caught up with any particular offering by market leader Nvidia, Nvidia had already moved on to a faster, cheaper product. If chip prices stop falling, they’ll have a bigger market opportunity — albeit with significant technical hurdles — to build competitive chips. The industry for AI accelerators remains dynamic. Intel and AMD are making significant investments and a growing number of companies are duking it out on theMLPerfbenchmark that measures chip performance. I believe the options for training and inference in the cloud and at the edge will continue to expand.\n\nKeep learning!\n\nAndrew\n\nSahar Nasiri’s early job interviews went well until she was asked to interpret the algorithms she had listed on her resume. These experiences pushed her to deepen her understanding of the math behind data science. Now she works for a major U.S. airline.Read her story\n\nOnly a week ago, researchersunveileda system that generates a few seconds of video based on a text prompt. New work enables a text-to-video system to produce an entire visual narrative from several sentences of text.\n\nWhat’s new:Ruben Villegas and colleagues at Google developedPhenaki, a system that produces videos of arbitrary length from a story-like description. You can see exampleshere.\n\nKey insight:The machine learning community lacks a large dataset of long-form videos and time-aligned captions, so it’s not obvious how to train a model to synthesize long videos from a narrative. But text-image pairs are plentiful. A system can be trained to generate short videos by treating images as single-frame videos and combining them with a relatively smaller dataset of short videos with captions. Then the video can be extended by feeding the system new text plus the last few generated frames. Repeating this process can generate long, complex videos even though the model was trained on short, simple ones.\n\nHow it works:Phenaki uses an encoder to produce video embeddings, a language model to produce text embeddings, a bidirectional transformer to take the text and video embeddings and synthesize new video embeddings, and a decoder to translate synthesized video embeddings into pixels.\n\nResults:The full-size Phenaki comprised 1.8 billion parameters. In the only quantitative evaluation of the system’s text-to-video capability, the authors compared a 900 million-parameter version of Phenaki trained on half of their data to a 900 million-parameterNUWApretrained ontext-image pairs,text-video pairs, andthree-second videosand fine-tuned on10-second videos. (Phenaki was not fine-tuned.) The downsized Phenaki achieved 3.48 FID-Video compared to NUWA’s 7.05 FID-Video (a measure of similarity between generated and original videos, lower is better).\n\nWhy it matters:Last week’sMake-A-Videoused a series of diffusion models that generate a short video from a text description and upscale its temporal and image resolution. Phenaki bootstrapped its own generated frames to extend the output’s length and narrative complexity. Together, they may point to a revolution in filmmaking.\n\nWe’re thinking:One challenge of the recent approaches is maintaining consistency across spans of frames. In the clip shown above, for example, the lion’s appearance at the beginning differs from its appearance at the end. We don’t regard this as a fundamental problem, though. It seems like only a matter of time before an enterprising developer devises an attention-based/transformer architecture that resolves the issue.\n\nA machine learning model identified areas likely to have been damaged by Hurricane Leo as it swept through the southern United States.\n\nWhat's new:University of Connecticut researchers Zhe Zhu and Su Yeuseda learning algorithm to examine satellite images of the storm’s path and spot changes that might indicate wreckage.\n\nHow it works:Thesystemwas originally designed to identify damage to forests caused by fires, disease, drought, and the like. Given a satellite image, it evaluated changes in real time.\n\nResults:The authors displayed the system’s output as anoverlayof yellow squares on a satellite image. Those areas track Ian’s course up the peninsula. They didn’t confirm the damage, however.\n\nBehind the news:Similar approaches to detecting changes in satellite images have been used to assist relief efforts following a number of recent disasters. Researchers have used AI tomap surviving roadsthat relief groups could use to reach victims,direct firefighterstowards the most active areas of a woodland blaze, andscan satellite imagesfor signs of impending volcanic eruption.\n\nWhy it matters:Satellite imagery can be a boon to responders after a disaster, but the data is often too immense for manual evaluation. AI can enable relief workers to arrive faster and work more effectively. And it’s likely that humanity will need the extra help: Natural disasters such as hurricanes, wildfires, and floods are growingmore destructiveas global temperatures rise.\n\nWe're thinking:We enthusiastically support the use of AI to guide relief efforts after disasters. We urge agencies that are charged with responding to integrate the technology with their plans.\n\nWant to launch an AI company? Looking for guidance on your existing startup? Join us for “Founding an AI Startup,” a panel discussion, on October 18, 2022. Speakers will share practical tips on how to get started, how to avoid common pitfalls, and more!RSVP\n\nThe ability to predict customer demand could make fast food even faster.\n\nWhat's new:The Mexican-themed Chipotle restaurant chain is testing AI tools that forecast demand, monitor ingredients, and ensure that workers fill orders correctly, according toQSR Magazine, a restaurant trade publication.\n\nHow it works:Eight Chipotle locations in California will employ tools from New York-based startupPreciTaste, which offers systems designed to boost efficiency in restaurants, bakeries, and food manufacturers. On the AI menu:\n\nBehind the news:The fast-food industry’s focus on efficiency has made it a proving ground for a variety of AI applications.\n\nWhy it matters:Fast-food outlets in the U.S. are facing historicshortagesof labor — a ripe market for startups that aim to automate food prep. The captains of fast-food have taken notice: PreciTastecountsthe CEOs of McDonald’s, Burger King, and Shake Shack among its investors.\n\nWe're thinking:It’s good to see industrial AI used to help employees do their work better rather than to do it for them. Perhaps increasingly automated eateries will spur competition to emphasize the human touch.\n\nThe United States paved the way to regulate AI systems in healthcare.\n\nWhat's new:The U.S. Food and Drug Administration (FDA)interpretedexisting rules that govern health-related software to include some machine learning algorithms.\n\nWhat they said:The FDArequiresthat automated decision-making software meet the same standards as medical devices. The new guidance clarifies which AI systems fall under this designation. Manufacturers of medical devices must submit technical and performance data that demonstrate safety and effectiveness. Makers of medical devices that critically support or pose a potential risk to human life must submit laboratory and clinical trial results and gain explicit approval.\n\nBehind the news:The guidance seeks to comply with a 2016lawthat aimed to accelerate innovation in medical devices. The American Medical Informatics Association hadpetitionedregulators to clarify the law on several fronts.\n\nWhy it matters:Regulators have struggled to interpret existing frameworks for oversight with respect to machine learning algorithms, whose functioning can change with ongoing training and whose output often can’t be clearly explained. The government’s new interpretation is a substantial step toward rules that protect patients without inhibiting innovation.\n\nWe're thinking:We welcome regulation of AI systems, particularly when they're involved in life-and-death decisions. However, clarity is paramount. To the extent that the difference between words like “informing” and “driving” clinical management remains vague, the new guidance highlights the need for caution. On the plus side, it will give many AI developers a clearer target to aim for.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/10/unnamed--3-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/10/DATA2_SaharNasiri_GT_6756--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/10/PHENAKI.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/10/HURRICANE.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/10/18.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/10/PRECITASTE.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/10/FDA.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-7/",
    "title": "issue 7",
    "date": "",
    "reading_time": "",
    "content": "Thinking about the future of machine learning programming frameworks, I recently reread computer scientist Fred Brooks’ classic essay, “No Silver Bullet: Essence and Accidents of Software Engineering.” Three decades after its initial publication, it still holds important lessons for software engineers building ML tools.\n\nDespite progress from typewriters to text editors, why is writing still hard to do? Because text editors don’t address the most difficult part: thinking through what you want to say.\n\nProgramming tools have the same limitation. I’m glad to be coding in Python rather than Fortran. But as Brooks points out, most advances in programming tools have not reduced the essential complexity of software engineering. This complexity lies in designing a program and specifying how it should solve a given problem, rather than in expressing that design in a programming language.\n\nDeep learning is revolutionary because it reduces the essential complexity of building, say, a computer vision system. Instead of writing esoteric, multi-step software pipelines comprising feature extractors, geometric transformations, and so on, we get data and train a neural network. Deep learning hasn’t just made it easier to express a given design; it has completely changed what we design.\n\nAs we work on ML programming frameworks, we should think about how to further reduce the essential complexity of building ML systems. This involves not just specifying an NN architecture (which is indeed waaay easier to do in TensorFlow or PyTorch than C++), but also deciding what is the problem to be solved and designing all the steps from data acquisition to model training to deployment.\n\nI don’t know what will be the key ideas for reducing this essential complexity, but I suspect they will include software reuse, ML model reuse (such as libraries of pretrained models) and tools not just for code versioning and reuse (like github) but also for data versioning and reuse. Breakthroughs in unsupervised and other forms of learning could also play a huge role.\n\nEven as I occasionally struggle to get an ML system to work (it’s not easy for me either), I am excited to see how our community is pioneering this discipline.\n\nKeep learning!\n\nAndrew\n\nP.S. My best learning creation so far, seven month-old Nova, just said her first words! 🙂\n\nA leaked paper from Google’s quantum computing lab claims “supremacy” over conventional computers.What’s new:The U.S. space agency NASA, whose scientists are collaborating with Google on a quantum computer, accidentally published a paper describing the breakthrough. TheFinancial Timessnagged a copy before it was taken down, naming machine learning, chemistry, and materials science as likely uses for the technology. Google declined to comment pending the paper’s official release.How it works:Google designed the special-purpose system, called Sycamore, to determine whether sets of randomly generated numbers were truly random. Researchers estimate that it would have taken the world’s fastest conventional supercomputer, IBM’sSummit, 10,000 years to solve the problem. Sycamore solved it in 3 minutes and 20 seconds, an early demonstration of the capability known as quantum supremacy.\n\nBehind the news:Physicist Paul Benioff wrote a paper in 1980 describing how quantum-mechanical phenomena like superposition and entanglement could be applied to computing. Google, IBM, Intel, and Microsoft lately have made substantial progress in implementing those ideas.Why it matters:Quantum computing’s promise of exponentially faster processing in particular applications has many in the AI community excited to apply it to tasks like search and pattern matching. There’s no telling when quantum AI will emerge, but when it does, it probably will require new types of models tailored to the peculiar nature of qubits.We’re thinking:The problem Sycamore solved doesn’t have much practical value, as computer scientist Scott Aaronson points out in his excellent quantum-supremacyFAQ. It’s more “like the Wright Brothers’ flight” circa 1903, he says: The technology works, but it will be a while before actual users can climb aboard.\n\nA number of countries restrict commercial use of personal data without consent unless they’re fully anonymized. A new paper proposes a way to anonymize images of faces, purportedly without degrading their usefulness in applications that rely on face recognition.What’s new:Researchers from the Norwegian University of Science and Technology introducedDeepPrivacy, a system that anonymizes images of people by synthesizing replacement faces. They also offer the Flickr Diverse Faces dataset, 1.47 million images of faces with supplemental metadata, which they used to train DeepPrivacy.Key insight:The original images are never exposed to the face generator. Authors Håkon Hukkelås, Rudolf Mester, and Frank Lindseth argue that this strategy preserves privacy more effectively than traditional anonymization techniques like pixelizing and blurring.How it works:DeepPrivacy is aconditional generative adversarial networkthat synthesizes novel images similar to previously observed ones. A discriminator classifies images as real or generated, while a generator based on the U-Net architecture is optimized to create images that fool the generator.\n\nResults:The researchers processed the WIDER-Face dataset (roughly 32,000 images containing around 394,000 faces) using DeepPrivacy as well as traditional anonymization methods. Subjected to traditional techniques,Dual Shot Face Detectorretained 96.7 percent of its usual performance. With DeepPrivacy, it retained 99.3 percent. The researchers don’t provide metrics to evaluate the relative degree of anonymity imparted by the various methods.Why it matters:Laws like the European Union’s General Data Protection Regulation set a high bar for data-driven applications by placing tight limits on how personal data can be used. DeepPrivacy transforms photos of people into a less identifiable format that still contains faces recognizable to neural networks.Yes, but:DeepPrivacy addresses the privacy implications of faces only. An image purged of faces but still containing, say, clothing with identifiable markings, such as an athlete’s number, would allow a sophisticated model to infer the wearer’s identity.We’re thinking:Machine learning’s reliance on data is both a gift and a curse. Aggregation of data has allowed for great progress in the field. Yet privacy advocates are inclined to keep personal data under wraps. DeepPrivacy is an intriguing step toward a compromise that could satisfy both AI engineers and users alike.\n\nBasketball coaches often sketch plays on a whiteboard to help players get the ball through the net. A new AI model predicts how opponents would respond to these tactics.What’s new:A team of researchers in Taiwan trained aconditional generative adversarial networkon data from National Basketball Association games. They trained theirnetworkto show how players on the opposing team likely would move in response to human-drafted plays.How it works:The researchers built a two-dimensional simulation of a half court complete with a three-point line and a net. A coach can draw motion paths for five players represented by dots, as well as ball movements including passes and shots. No dunking, however.\n\nResults:A cohort of NBA pros, basketball fans, and basketball non-fans evaluated the generated defenses for realism. While the non-pro fans and non-fans had a hard time spotting the computer’s defensive plays, the NBA pros could tell they were not designed by a human coach.Behind the news:Stat SportVUhas collected real time player motion data for the NBA since 2011. The system uses six cameras to collect each player’s position and track who has possession of the ball, 25 times per second. It uses machine learning to identify events like dribbles and passes, and play types like drives, isolations, and screens.Why it matters:Pro sports is a high-stakes industry that has embraced technology to optimize performance. It’s conceivable that a neural network someday might generate AlphaGo-like winning tactics that no human had envisioned.We’re thinking:This model isn’t a slam-dunk, given that the pros weren’t fooled. However, it appears to be sophisticated enough to help teach beginners how to think strategically off the court.\n\nDon’t know how to play an instrument but love to code? Improvise your own jazz solo using LSTMs in Course 5 of the Deep Learning Specialization!Enroll now\n\nAmazon is writing what it hopes will become U.S. law governing use of face recognition technology.What happened:At a press event announcing new features for Amazon’s Alexa smart-home service, Jeff Bezostolda reporter that his company’s lawyers are drafting a statutory framework to guide what he views as an inevitable federal crackdown on face recognition. Amazon sells the cloud-based face recognition service Rekognition, whose use by law enforcement agencies has raisedalarmamong civil liberties advocates.What it says:The company has released no details about the model legislation in progress. However, in February, Amazon VP of Global Public Policy Michael Punke published ablogthat could provide clues to the company’s aims.\n\nBehind the news:Face recognition’s rapid proliferation has spawned a widespread backlash in the U.S. cities. San Francisco and Oakland, California, and Somerville, Massachusetts, have banned the technology. California’s legislature is considering a statewideban. Severalbillsrestricting its use are wending their way through Congress, and two representatives havevowedto propose further legislation.We’re thinking:Punke’s guidelines are sound, and Amazon is well situated to understand how the technology could be abused. When industries propose their own regulations, though, legislators need to take special care to make sure any resulting laws benefit society as a whole.\n\nDespite dramatic recent progress, natural language generation remains an iffy proposition. Even users of the muscularGPT-2text generator have to press the button a number of times to get sensible output. But researchers are figuring out how to exert greater control over generated text.What’s new:Pre-trained text generators generally require fine-tuning for a specific sort of output. A team at Salesforce developed a model aptly namedCTRLthat lets users determine the output genre, from news story to horror script, without further training.Key insight:The model is guided by control codes, human-written text tags that describe a desired output genre — including, yes, jokes. The model learns relationships between a given code and the intended style and content.How it works:CTRL, like the state-of-the-art language model BERT, is a modified transformer network trained in an unsupervised fashion on large-scale text corpora. Its training data include Wikipedia, Reddit, and Project Gutenberg’s library of digitized books.\n\nResults:The researchers provide qualitative results demonstrating that control codes generate different styles of text in response to the same prompt. For example, given the prompt “the knife,” the Reviews code produces “a knife is a tool and this one does the job well,” while the Horror code yields “a knife handle pulled through the open hole in the front.” The paper offers no quantitative evaluation.Why it matters:The ideal text generator would produce diverse, relevant passages appropriate to a wide variety of uses. CTRL suggests that a single model with unsupervised training could meet this requirement.We’re thinking:Many people including GPT-2’s creators worry that more-capable text generators invite misuse. Could a CTRL-style approach reduce abuse by suppressing certain genres (say, blatant political disinformation) as well as supporting more effective text generation?\n\nGeologists call them slow slips: deep, low-frequency earthquakes that can last a month but have little effect on the surface. A model trained to predict such events could help with forecasting potentially catastrophic quakes.What’s new:French and American seismologists trained amodelto recognize acoustic patterns associated with slow slips where one tectonic plate slides beneath another. Some seismologists believe that slow slips shift stress from deep in a geological fault up to the Earth’s brittle crust, presaging potentially catastrophic quakes.How it works:The authors began by simulating slow slips in the lab using two sheets of synthetic material, like acrylic plastic, separated by a thin layer of a granular, sandy medium. The video above is a microscopic view of the sheets in action.\n\nWe’re thinking:Seismologists already provide short-term risk assessments for a given location and time span. This research could lead to long-term forecasts, months or years out, allowing planners to expedite earthquake safety upgrades that otherwise may be delayed due to their cost.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/no20silver20bullet.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-10-0120at2010.10.2120AM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-09-2720at2011.52.2720AM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/basketball2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_DLS20Course20520Course20Ad.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-09-3020at202.08.4320PM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/CTRL2.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-10-0120at205.28.1420PM.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-47/",
    "title": "issue 47",
    "date": "",
    "reading_time": "",
    "content": "I am appalled by the policy, announced on Monday by U.S. Immigrations and Customs Enforcement (ICE), that international students in the country on an F-1 visa must leave if their school goes fully online to cope with Covid-19.\n\nTwo weeks ago, Iwroteabout the suspension of H1-B visas for foreign workers. The policy unveiled this week will deepen the pain of young people who are aiming to contribute to society and further deprive the U.S. of much-needed talent.\n\nThe new policy, which is being called the #StudentBan on social media, is cruel and capricious. Sometimes an entire family may pool their savings to send someone to study and give them a brighter future. Imagine being halfway to earning a degree and suddenly forced to leave the country amid the pandemic, when your home country may have closed its borders, even to citizens. Students have confided to me their worries about letting down their family or being unable to afford a plane ticket home.\n\nUniversity faculty and administrators are scrambling to offer in-person classes, even if it may not be safe or may have little pedagogical benefit, just for the purpose of protecting their international students from deportation. They were already struggling to manage campus shutdowns. This policy delivers another blow at a time when they least can afford it.\n\nThe U.S. is known worldwide as a great place to receive an education. That’s why I came here many years ago — on an F-1 visa — to attend college. If the U.S. loses this reputation, the whole world will be poorer for it.\n\nIf my daughter ever studies overseas, I hope that whatever country hosts her will treat her with greater kindness and respect than the U.S. is extending to our international students.\n\nKeep learning,\n\nAndrew\n\nGrowing up in Mauritania, Adama Diallo was fascinated by the human brain. Now, as an AI developer at the software company Paradigm, he’s using artificial intelligence to map architectural spaces. In this edition of our Working AI series, Adama discusses his projects, advice for learners, and views on social bias in the industry.Learn more\n\nMIT withdrew a popular computer vision dataset after researchers found that it was rife with social bias.What’s happening:Researchersfoundracist, misogynistic, and demeaning labels among the nearly 80 million pictures inTiny Images, a collection of 32-by-32 pixel color photos. MIT’s Computer Science and Artificial Intelligence LabremovedTiny Images from its website and requested that users delete their copies as well.\n\nWhat the study found:Researchers at the University College of Dublin and UnifyID, an authentication startup, conducted an “ethical audit” of several large vision datasets, each containing many millions of images. They focused on Tiny Images as an example of how social bias proliferates in machine learning.\n\nWhat the dataset’s creators said:“Biases, offensive and prejudicial images, and derogatory terminology alienates [sic] an important part of our community — precisely those that we are making efforts to include,” the researchers who built Tiny Images said in a statement.\n\nBehind the news:Social bias — in data an d models, in the industry, and in society at large — has emerged as a major issue in the machine learning community.\n\nWhy it matters: Social biases encoded in training data become entwined with the foundations of machine learning. WordNet transmitted its derogatory, stereotyped, and inaccurate information to Tiny Images, which may have passed them along to countless real-world applications.We’re thinking:As AI practitioners, we have a responsibility to re-examine the ways we collect and use data. For instance, Cifar-10 and Cifar-100 were derived from TinyImages. We’re not aware of biases in those datasets, but when one dataset’s bias may propagate to another, it’s necessary to track data provenance and address any problems discovered in an upstream data source. Recent proposals set standards for documentingmodelsanddatasetsto weed out harmful biases before they take root.\n\nThe transformer architecture that has shaken up natural language processing may replace recurrent layers in object detection networks.What’s new:A Facebook team led by Nicolas Carion and Francisco Massa simplified object detection pipelines by using transformers, yielding Detection Transformer (DETR).Key insight:Images can show multiple objects. Some object detection networks use recurrent layers to predict one object at a time until all objects are accounted for. Language models usetransformersto evaluate a sequence of words in one pass. Similarly, DETR uses them to predict all objects in an image in a single process.How it works:DETR predicts a fixed number of object bounding boxes and classes per image. First, it extracts image features using convolutional layers. Then transformers predict features associated with regions likely to contain objects. Feed-forward layers process the object features into classes and bounding boxes. (“No object” is a possible class.)\n\nResults:The researchers pitted DETR againstFaster R-CNNon the canonical object detection datasetCoco. At model sizes of roughly 40 million parameters, DETR bettered Faster R-CNN’s average precision, a measure of true positives, from 0.402 to 0.420. And DETR did it faster, spotting objects at 28 images per second compared to Faster R-CNN’s 26 images per second.Why it matters:Transformers are changing the way machine learning models handle sequential data in NLP and beyond.We’re thinking:What happened to theMuppet namesfor transformer-based models? Fozzie Bear is available.\n\nAn artist is using deep learning to create realistic portraits of historical figures like Napoleon Bonaparte, Vincent Van Gogh, and England’s first Queen Elizabeth.What happened:Combining technical and artistic ingenuity, Dutch photographerBas Uterwijkis generatingphotorealistic imagesof people who lived before photography was invented.How it works:Uterwijk usesArtbreeder, a web-based tool that lets users upload images and “breed” them to create hybrid versions.\n\nBehind the news:Artbreeder is one of several creative tools that take advantage of the uncanny ability of GANs to replicate and manipulate visual details.\n\nWhy it matters:Uterwijk’s AI-fueled artistry helps to humanize historical personalities and fictional characters, too. A similar approach could help filmmakers, museums, and video game developers bring history to life.We’re thinking:We’re waiting for photorealistic renderings of the characters inXKCD.\n\nCheck out our new Natural Language Processing Specialization on Coursera! Courses #1 and #2 are now available for learners.Enroll today\n\nFootball clubs are turning to computer vision for winning insights.\n\nWhat’s new:Acronis, a Swiss cloud storage and security company, offers AI services designed to give a boost to some of the world’s top football clubs (soccer teams, to Americans),Wiredreported.Eyes on the ball:The company stores training and match video for professional teams including London-based Arsenal, Manchester City, and Inter Milan. An internal group devoted to machine learning for sports is using the data to train AI tools aimed at improving gameplay and marketing.\n\nBehind the news:Nearly two decades after Michael Lewis’ bookMoneyball: The Art of Winning an Unfair Gamerevealed the use of data analytics in baseball, sports are becoming an active playing field for AI.\n\nWhy it matters:Once the four-minute mile was a breakthrough. Now it’s par for the course. Machine learning is set to help athletes continue to upgrade their own state of the art.We’re thinking:For those of us who aren’t particularly athletic, it’s nice to know that we can help score goals by running our fingers across a keyboard!\n\nA cat rotated by any number of degrees is still a cat. It takes a lot of rotated training images to teach convolutional filters this simple fact. A new filter design has this common-sense knowledge built-in.What’s new:Simon Graham led a team at the University of Warwick to create theDense Steerable Filter CNN(DSF-CNN), a convolutional neural network that can see a picture in various rotations and generate consistent output.Key insight:Pixels are tiny squares. Rotating them by increments other than 90 degrees results in distortion and lost information.Earlier workdeveloped so-called steerable filters that eliminate distortion by subdividing their weights and recombining them to create the desired rotation. DSF-CNN builds on that work by incorporatingdense connectionsthat improve performance and data efficiency.How it works:DSF-CNN operates slightly differently on the input and hidden layers. At the input, it extracts initial features for each of several rotational angles (illustrated by figure b above). In the hidden layers, it extracts more complex features at each angle (figure c).\n\nResults:The researchers tested the 8-filter model onpathology slides, since medical images come in a variety of orientations. DSF-CNN achieved 0.975 area under the curve (AUC), a measurement of true positives and false positives where 1 is perfect. That score beat a state-of-the-art CNN (0.949 AUC) and rotationalG-CNN(0.968). DSF-CNN also turned in superior performance on two othermedicalimagedatasets.Why it matters:Rotational symmetry gives neural networks fits. DSF-CNN doesn’t cover every angle, but it vastly reduces the data requirements and simplifies training them to recognize that rotated images are equivalent.We’re thinking:Rotational symmetry can cause trouble for humans, too. Check out theThatcher Effect.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter205.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Adama20Closeup202.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Dataset202.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/DETR.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Portrait202.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/5.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Soccer2.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Steerable.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-173/",
    "title": "issue 173",
    "date": "",
    "reading_time": "",
    "content": "On Monday, the European Union fined Meta roughly $275 million for breaking its data privacy law. Even though Meta’s violation was not AI specific, the EU’s response is a reminder that we need to build AI systems that preserve user privacy — not just to avoid fines but because we owe it to the people who are represented in the data.\n\nMany companies that would benefit from machine learning can’t afford to hire enough skilled engineers. This creates a need for cloud-based AI software as a service (SaaS). How can customers of such services keep data private while counting on another party to process the data? Consider an AI system that reads electronic health records to make predictions about the patients. Can a hospital use a SaaS provider to monitor the system’s performance without exposing sensitive data?\n\nRecently I learned about a monitoring technique that manages to keep data secure. While visiting Seattle, I met with Alessya Visnjic and Maria Karaivanova, two of the founders ofWhyLabs, which provides a SaaS platform that monitors machine learning applications. (Disclosure: WhyLabs is a portfolio company of AI Fund, which I lead.) They explained how they help customers monitor deployed systems for problems likedata drift— changes in the distribution of data because, say, a new disease emerged or the hospital started collecting data in a new way — while maintaining data privacy. In their approach, data never leaves the customer’s system. Instead, the SaaS provider (i) computes statistics on data at the source using efficient techniques based on Apache DataSketches and (ii) analyzes the statistics.\n\nThe system enables customers to set up dashboards that track the distribution of input features (in this case, body temperature, red blood cell count, and so on) and alerts them when the distribution shows anomalies. Software that runs on the hospital’s server collects data from multiple patients and transmits only the aggregate statistics to the cloud. In this way, the system can look for anomalies without receiving any individual’s data.\n\nThis is useful for detecting not only data drift but also data-quality problems. Let's say the hospital shifts to a more precise body temperature notation and leaves the old temperature field empty. The system would monitor the fraction of missing temperature values across all patients and alert the hospital that this field is frequently empty. This enables monitoring of critical data-quality markers such as:\n\nIn the data-centric approach to building a machine learning system, our job isn’t done when we deploy a model. We still need to watch out for and address post-deployment issues. Too many teams don’t continuously monitor their models after deploying them because they’re concerned about complexity or privacy. This leads to outdated models that may perform poorly for weeks or months before the problem is detected.\n\nIn some tasks, complete privacy may not be possible when working with a SaaS provider, but WhyLabs’ approach (which includesopen source tools) preserves privacy while logging and monitoring. I hope we continue to invent techniques that enable AI systems to process data in the cloud while maximizing the degree of privacy we can offer to users and customers.\n\nKeep learning!\n\nAndrew\n\nMost speech-to-speech translation systems use text as an intermediate mode. So how do you build an automated translator for a language that has no standard written form? A new approach trained neural networks to translate a primarily oral language.\n\nWhat’s new:Peng-Jen Chen, Kevin Tran, Yilin Yang and teammates at Meta described a system thattranslates speech between English and Hokkien, which is spoken by millions of people in east Asia.\n\nKey insight:Few people know how to translate between English and Hokkien, which makes it hard to assemble a dataset sufficient for training an English-Hokkien translation model. However, a fair number of people can translate between Mandarin and English and between Mandarin and Hokkien. By translating from English to Mandarin and from Mandarin to Hokkien, it’s possible to build a database of English-Hokkien speech pairs.\n\nThe dataset:The authors collected a corpus of English, Mandarin, and Hokkien data. They employed human translators to translate the corpus. They used the translated corpus to synthesize further data.\n\nThe translators:Separate speech-to-speech systems with identical architectures translate from Hokkien to English and English to Hokkien, using Mandarin text as a stepping stone between the target languages.\n\nResults:The authors compared their system to a baseline of their own design that translated directly between the spoken languages using an encoder-decoder. They evaluated the systems according to ASR-BLEU, which compares text overlap (higher is better) against reference text after translating speech to text. To render Hokkien speech as text for comparison, they developed a separate model that translated Hokkien speech into a phonetic script called Tâi-lô. Converting English to Hokkien, their system achieved 7.3 ASR-BLEU, whereas the baseline achieved 6 ASR-BLEU. Converting Hokkien to English, their system achieved 12.5 ASR-BLEU, whereas the baseline achieved 8.1 ASR-BLEU. Without the augmented data, both their system and the baseline scored worse by 6 ASR-BLEU to 9 ASR-BLEU.\n\nWhy it matters:Forty percent of the world’s languages have no standard written form, which means they’re left out of current translation systems.  This method provides a blueprint for machine translation of other primarily oral languages.\n\nYes, but:Hokkien is spoken in several dialects, some of which are mutually unintelligible. So, while this system presumably serves most Hokkien speakers, it doesn’t serve all of them yet.\n\nWe’re thinking:The next step is to hook up the Hokkein-English model to existing translators for other languages. Is it good enough? ASR-BLEU scores in the 7-to-12 range are low compared to scores for, say, English-German, which are around 30. And, because translation errors compound from one language to the next, the more intermediate steps required to reach the target language, the lower the final translation quality. One way or another, we want to hear Hokkien speakers talking to everyone!\n\nAI-driven signs are deciding what to display based on data harvested from passersby.\n\nWhat’s new:Companies that sell advertising in public spaces use face analysis and personal data to match ads with potential viewers in real time, civil-liberties watchdog Big Brother UK detailed in a newreport.\n\nHow it works:The report compiles applications and case studies drawn from product summaries and blog posts published by ad-tech companies.\n\nBehind the news:These companies walk a tightrope over local privacy protections. Adsquare, Alfi, and Quividi tout their compliance with Europe’sGeneral Data Protection Regulation(GDPR), which protects privacy in member countries. Last year, U.S. lawmakerssentletters of concern to Lyft and Uber after some drivers independently put Alfi-equipped advertising screens in their vehicles. Both ride-share companies responded that equipment installed by drivers was beyond their control.Why it matters:The combination of electronic signage, computer vision, and cloud computing brings to the real world practices that are common in advertising on the internet.\n\nWe’re thinking:Online advertising has flourished as increased personalization allowed more precise targeting. Public advertising is poised to do the same.\n\nWhat do you think about coding tests and technical interviews? We’d like to hear the good, the bad, and the ugly. Tell us about your technical interview experiences and earn a chance to win a $100 gift card!Take the survey\n\nA new report offers a rare peek into the use of automated decision-making tools by the government of a major city.What’s new:Municipal agencies in the U.S. capital of Washington, D.C., use at least 29 algorithms to streamline their operations, according to astudyby the Electronic Privacy Information Center. The authors found references to the models in public records and internal documents. In many cases, their roles were not widely known.How it works:The algorithms span a variety of municipal functions.\n\nBehind the news:Washington, D.C. lawmakers are considering alawthat would require regular audits of decision-making algorithms used by organizations of a particular size and those that hold data on city residents. It would also enable the Office of the Attorney General and others to sue for violations.\n\nYes, but:While the authors discovered many automated decision-making systems in use, many more may be hidden from view. Several city agencies didn’t respond to requests for public records citing confidentiality and trade-secret agreements with vendors. New York City police were found to be using more algorithms than those the department had disclosed to officials as required by a 2020 law,Wiredreported. Public registries inAmsterdam and Helsinkilist only 10 out of 30 algorithms that have beendisclosedin separate documents.\n\nWhy it matters:AI is reaching into a wide variety of government functions that have a direct and substantial impact on citizens’ lives. While the technology can help officials make decisions that are more efficient and sometimes more fair, their constituents need to know how their government operates and have a right to hold algorithms (and the officials who employ them) accountable for their decisions. Governments should supply this information as a matter of course, rather than forcing independent researchers to discover it.\n\nWe’re thinking:The term “smart city” shouldn’t just describe the algorithms used to govern the municipality. It should also describe a population that’s informed about how they’re being used.\n\nGiven a number of images of the same scene, a neural network can synthesize images from novel vantage points, but it can take hours to train. A new approach cuts training time to a few minutes.\n\nWhat’s new:Thomas Müller and colleagues at Nvidia introduced a newmethodfor learning representations of positions in a 3D scene. It’s compatible withNeural Radiance Fields(NeRF), a popular way to synthesize images of a scene from novel perspectives.\n\nNeRF basics:For a given scene, NeRF learns to reproduce ground-truth images shot by a camera from different positions and angles. At inference, given a camera position and angle, it generates views of a scene by sampling points along virtual light rays that extend from the camera through each pixel. Given an embedding of a point’s position and the ray’s direction, separate fully connected networks compute its color and transparency. (Typically many points occupy empty space, so they’re fully transparent and have no color.) The system combines the color and transparency of points along the ray to find the associated pixel’s color.\n\nKey insight:Previous efforts tospeedupNeRF training impose a 3D grid over the scene and learn an embedding of each grid point. When it comes to sampling coordinates along rays, these approaches interpolate embeddings of positions that fall in between the grid points. This process requires a lot of memory, and rendering is slow because ferrying data to the processor and back takes a lot of time. Limiting the total number of embeddings to fit within a processor’s cache eliminates this bottleneck, accelerating rendering. One way to do this is to hash the coordinates, which defines a function that maps them to the index of a list (hash table) of limited size. This makes it possible to map any number of points to a limited number of embeddings.\n\nHow it works:The authors trained separate systems of vanilla neural networks to generate 20 synthetic and real scenes used in the original NeRF paper. As in the original NeRF and its variants, the networks learned by minimizing the difference between the ground truth images and generated images from the same viewpoints. Given a camera position and viewing angle, the system projected a ray for each pixel in the resulting image and sampled from 3 to 26 points, depending on the scene’s size, along each ray.\n\nResults:The authors evaluated the system usingPeak Signal-to-Noise Ratio(PSNR), which measures image reconstruction quality (higher is better), and compared their results to the originalNeRFand similarMip-NeRF. Averaged across all scenes, the new approach achieved 31.407 PSNR after 15 seconds of training (in contrast, NeRF achieved 31.005 PSNR after more than 12 hours of training) and 33.176 PSNR after five minutes of training (better than mip-NERF’s 33.090 PSNR after two to three hours of training).\n\nYes, but:Hash collisions, while rare, can still happen. The result is a rough surface texture.\n\nWhy it matters:Tailoring neural networks to hardware resources can accelerate processing with very little impact on output quality. This can dramatically reduce the time and money required to tackle modern machine learning tasks.\n\nWe’re thinking:The authors used a hash table to reduce the number of embeddings and dramatically accelerate rendering. Would the same method accelerate other models that rely on large numbers of embeddings?",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/11/ezgif-1-c7f58c9005-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/11/HOKKIEN--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/11/BILLBOARDS-2_1200px--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/11/ezgif-1-f144e58736.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/11/ezgif.com-gif-maker--21--1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/11/INSTANTNERF.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-27/",
    "title": "issue 27",
    "date": "",
    "reading_time": "",
    "content": "Nearly a decade ago, I got excited byself-taught learningandunsupervised feature learning— ways to learn features from unlabeled data that afterward can be used in a supervised task. These ideas contributed only marginally to practical performance back then, but I’m pleased to see their resurgence and real traction in self-supervised learning.\n\nMany of you know the story of how the increasing scale of computation and data, coupled with innovation in algorithms, drove the rise of deep learning. Recent progress in self-supervised learning also appears to be powered by greater computational and data scale — we can now train large neural networks on much larger unlabeled datasets — together with new algorithms likecontrastive predictive coding.\n\nToday feels very much like the early, heady days a decade-plus ago, when we saw neural networks start to work in practical settings. The number of exciting research directions seems larger than ever!\n\nKeep learning,\n\nAndrew\n\nPolice in the U.S. routinely use AI to track cars with little accountability to the public.What happened:Documents obtained byWiredrevealed just how intensively police in Los Angeles, California, have been using automatic license plate readers. Officials queried databases of captured plate numbers hundreds of thousands of times in 2016 alone, records show.How it works:The Los Angeles Police Department, county sheriff, and other local agencies rely on TBird, a license plate tracking system from data-mining companyPalantir.\n\nBehind the news:A 2013surveyby the U.S. Dept. of Justice found that many urban police departments use automatic license plate readers.The LAPD was among the first to do so starting in 2009.Why it matters:License plate readers help solve serious crimes.Wireddescribes a case in which the LAPD used TBird to search for vehicles spotted near the place where a murdered gang member’s body was found. The plates led them to a rival gang member who eventually was convicted for the homicide.We’re thinking:Digital tools are becoming important in fighting crime, but it shouldn’t take a reporter’s public records request to find out how police are using them. We support regulations that require public agencies to disclose their use of surveillance technology, as well as rigorous logging and auditing to prevent misuse.\n\nWouldn’t it be great to see around corners? Deep learning researchers are working on it.What’s new:Stanford researcher Christopher Metzler and colleagues at Princeton, Southern Methodist University, and Rice University developeddeep-inverse correlography, a technique that interprets reflected light to reveal objects outside the line of sight. The technique can capture submillimeter details from one meter away, making it possible to, say, read alicense platearound a corner.Key insight:Light bouncing off objects retains information about their shape even as it ricochets off walls. The researchers modeled the distortions likely to occur under such conditions and generated a dataset accordingly, enabling them to train a neural network to extract images of objects from their diminished, diffuse, noisy reflections.How it works:The experimental setup included an off-the-shelf laser and camera, a rough wall (called a virtual detector), and a U-Net convolutional neural network trained to reconstruct an image from its reflections.\n\nResults:The researchers tested the system by spying hidden letters and numbers 1 centimeter tall. Given the current state of non-line-of-sight vision, quantitative results weren’t practical since the camera inevitably fails to capture an unknown amount of detail). Qualitatively, however, the researchers deemed their system’s output a substantial improvement over the previous state of the art. Moreover, the U-Net is thousands of times faster.Yes, but:Having been trained on simple generated images, the system perceived only simple outlines. Moreover, the simulation on which the model was trained may not correspond to real-world situations closely enough to be generally useful.Why it matters:Researcherssaw around corners.We’re thinking:The current implementation likely is far from practical applications. But it is a reminder that AI can tease out information from subtle cues that are imperceptible to humans. Here’s anotherexample.\n\nA digital attorney is helping consumers take telemarketers and phone scammers to court.What’s new:DoNotPaymakes an app billed as the world’s first robot lawyer. Its latest offering, Robo Revenge, automates the process of suing intrusive robocallers.How it works:U.S. law entitles consumers who have added their phone number to theNational Do Not Call Registryto sue violators for $3,000 on average. For $3 a month, Robo Revenge makes it easy:\n\nBehind the news:Joshua Browder founded DoNotPay in 2016 to help people fightparking tickets. Since then, he has added tools thatcancel unwanted subscriptions,navigate customer service labyrinths, andsue airlinesfor cancelled flights. Browder in 2018 toldVicethat DoNotPay wins about 50 percent of cases, earning clients $7,000 per successful lawsuit on average.Why it matters:The average American receives18 telemarketing calls a month— even though the Do Not Call Registry contains240 million numbers, enough to cover around 70 percent of the U.S. population. Spam callers might not be so aggressive if their marks were likely to sue.We’re thinking:We’re not fans of making society even more litigious. But we could be persuaded to make an exception for scofflaw telespammers.\n\nExplore federated learning and how you can retrain deployed models while maintaining user privacy. Take the new course on advanced deployment scenarios in TensorFlow.Enroll now\n\nGeoffrey Hinton, Yoshua Bengio, and Yann LeCun presented their latest thinking about deep learning’s limitations and how to overcome them.What’s new:The deep-learning pioneers discussed how to improve machine learning, perception, and reasoning at the Association for the Advancement of Artificial Intelligenceconferencein New York.What they said:Deep learning needs better ways to understand the world, they said. Each is working toward that goal from a different angle:\n\nBehind the news:The Association for Computing Machinery awarded Bengio, Hinton, and LeCun the2018 A. M. Turing Awardfor their work. The association credits the trio’s accomplishments, including breakthroughs in backpropagation, computer vision, and natural language processing, with reinvigorating AI.Words of wisdom:Asked by the moderator what students of machine learning should read, Hinton offered the counterintuitive observation that “reading rots the mind.” He recommended that practitioners figure out how they would solve a given problem, and only then read about how others solved it.We’re thinking:We apologize for rotting your mind.\n\nAI’s ability to transfer a person’s face from a source photo onto someone in a target photo doesn’t work so well when the target face is partially obscured by, say, eyeglasses, a veil, or a hand. A new technique handles such occlusions.What’s new:Lingzhi Li at Peking University and collaborators at Microsoft Research proposeFaceShifter, a face-swapping system that reproduces accurately both a source face and elements that block the target.Key insight:It’s easier to reproduce occlusions if you’ve scoped them out ahead of time. FaceShifter takes an extra step to identify occlusions before it renders the final image.How it works:The system has two major components. Adaptive Embedding Integration Network (AEI-Net) spots occlusions and generates a preliminary swap. Heuristic Error Acknowledging Refinement Network (HEAR-Net) then refines the swap.\n\nResults:The researchers used apretrained face classifierto evaluate how well FaceShifter maintained the distinctiveness of the source face compared withDeepFakesandFaceSwap. FaceShifter achieved 97.38 percent accuracy versus 82.41 percent, the next-best score. It also outscored the other models in human evaluations of realism, identity, and attributes like pose, face expression, lighting.Why it matters:FaceShifter introduces a novel method to check its own work. Although the researchers focused on swapping faces, a similar approach could be used to tackle challenges like combating adversarial examples.We’re thinking:Better face swapping one day could transform entertainment by, say, grafting new stars — or even audience members — into movies. But it’s also potentially another tool in the arsenal of deepfakers who aim to deceive.\n\nThe business world continues to shape deep learning’s future.What’s new:Commerce is pushing AI toward more efficient consumption of data, energy, and labor, according to areporton trends in machine learning from market analyst CB Insights.What they think:The report draws on a variety of sources including records of mergers and acquisitions, investment tallies, and patent filings. Among its conclusions:\n\nWe’re thinking:It’s great to see today’s research findings find their way into tomorrow’s commercial applications. The road from the AI lab to marketplace gets busier all the time.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Andrews20Letter20220ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/License20plate204.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Corners20220ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Robo20Revenge20220ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_ezgif.com-video-to-gif201-2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Godfathers201.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/FaceShifter20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/CB20Insights20ASPECT.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-132/",
    "title": "issue 132",
    "date": "",
    "reading_time": "",
    "content": "We just launched aData-Centric AI Resource Hubto help you improve the performance of AI systems by systematically engineering the underlying data. It offers new articles by Nvidia director of machine learning researchAnima Anandkumar, Stanford computer science professorMichael Bernstein, and Google Brain director of engineeringD. Sculley. It also includes talks from theNeurIPS Data-Centric AI Workshopthat was held in December. We’ll be adding more helpful articles and videos in coming months.\n\nWorking effectively with human labelers is a key part of Data-Centric AI. My friend Michael Bernstein is an expert in human-computer interface (HCI), a discipline that offers manyinsightsfor empowering labelers. His article explains some of the most important ones.\n\nFor example, given a task in computer vision, natural language processing, or speech recognition, it’s common to ask several crowdsourced labelers to annotate the same example and take the mean or majority-vote label. Many clever ideas have been proposed to improve the labeling process, such as testing labeler accuracy, developing novel voting mechanisms, and routing examples to labelers in sophisticated ways.\n\nSurprisingly, Michael has found that it's often better to invest in hiring and training a few annotators than to focus on improving the process. Alternatively, the best process may be one that enables you to build a small team of skilled labelers.\n\nWorking with a smaller, committed team also makes it easier to discover and fix ambiguities in your labeling instructions. Michael writes, “When something goes wrong, your reactions should be, ‘What did I do wrong in communicating my intent?,’ not, ‘Why weren’t they paying attention?’”Every machine learning engineer and data scientist can take advantage of Data-Centric AI techniques. And, because the data-centric approach changes the workflow of AI development, software engineers and product managers can also benefit. So please visit the Data-Centric AI Resource Hub, and tell your friends and colleagues about it, too.\n\nKeep learning!\n\nAndrew\n\nArmchair speed demons have a new nemesis.What’s new:Peter Wurman and a team at Sony developedGran Turismo Sophy(GT Sophy), a reinforcement learning model that defeated human champions of Gran Turismo Sport, a PlayStation game that simulates auto races right down to tire friction and air resistance.Key insight:It’s okay to bump another car while racing (as in the video above), but there’s a thin and subjective line between innocuous impacts and those that would give the offender an advantage. In official Gran Turismo Sport competitions — as in real-world races — a human referee makes these calls and penalizes errant drivers. A reinforcement learning algorithm can model such judgments by assigning a cost to each collision, but it must be tuned to avoid an adverse effect on performance: Too high a penalty and drivers become timid, too low and they become dangerous. Penalizing common situations in which a driver typically would be judged at fault, such as rear-ending, side-swiping, and colliding on a curve, should help a neural network learn to drive boldly without ramming its opponents to gain an advantage.How it works:Given information about the car and its environment, a vanilla neural network decided how to steer and accelerate. The authors trained the network on three virtual tracks and in custom scenarios, such as theslingshot pass, that pitted the model against itself, previous iterations of itself, and the in-game AI.\n\nResults:In time trials, GT Sophy achieved faster lap times than three of the world’s top Gran Turismo Sport drivers. In addition, a team of four GT Sophys faced off against four of the best human drivers in two sets of three head-to-head races held months apart. Points were awarded based on the cars’ final positions: 10 points for first place, 8 for second, 6 for third, and from 5 to 1 point for the remaining positions. The human team won the first set 86 to 70. Then the developers increased the model size and changed some rewards and features, among other tweaks, and the GT Sophy team won the second set 104 to 52.Why it matters:Unlike board games like Chess and Go in which learning algorithms have beaten human champions, winning a car race requires making complex decisions at high speed while tracing a fine line between nudging and disabling opponents. That said, there’s still a significant gap between doing well in even an exceptionally realistic video game and driving a real car.We’re thinking:Autonomous driving requires perception, planning, and control. We have little doubt that the latest algorithms can outperform most human drivers in control, but a substantial gap remains in perception and planning.\n\nMusic generated by learning algorithms got a major push with Apple’s acquisition of a startup that makes automated mash-ups.What’s new:Apple purchasedAI Music, a London startup whose software generates new music from existing recordings,Bloombergreported.How it works:Founded in 2016, AI Music reshapes prerecorded music according to user input. Among its projects prior to the acquisition:\n\nBehind the news:AI Music is one of many industrial-scale efforts to generate music in real time, complementing impressive research in the field likeMuseNet. (You can read an interview with MuseNet creator Christine Paynehere).\n\nWhy it matters:Decades ago, Apple’s iTunes servicerevolutionizeddigital music distribution. Today, Apple Music has about half as manysubscribersas Spotify, the leading distributor of streaming music. Its acquisition of AI Music suggests that it sees generated music as a strategic asset.We’re thinking:AI systems don’t yet generate great original music, and copyright law for algorithmically generated music is still evolving. That said, a streaming platform that grinds out music for which it owns the copyright could reap ample rewards.\n\nLearn how to generate images using GANs! TheGenerative Adversarial Networks (GANs) Specializationcovers foundational concepts and advanced techniques in an easy-to-understand approach.Enroll today\n\nResearch challenges long-held assumptions about how automation will affect employment.What’s new:Studies in various countries found that automation is associated with more jobs, fewer working hours, and higher productivity,The Economistreported.What they found:Automation boosted employment in several countries.\n\nYes, but:While these studies suggest that automation has positive impacts on the workforce, they were conducted in highly developed economies. The situation may vary in other parts of the world, andother studieslink robots to lower wages.Behind the news:Unemployment rates in most of the 38 market-based countries that make up the Organisation for Economic Co-Operation and Development havemostly returnedto pre-pandemic levels. This trend runs counter to the fear expressed by some economists amid the first wave of pandemic-driven lockdowns that the shift to remote work would prompt employers to lay off employees and automate their jobs. Japan and South Korea, whose unemployment rates are among the lowest among developed countries — 2.8 percent and 3.1 percent, respectively — are also the world’s most automated economies.Why it matters:Fear thatautomatons will take jobs from humansfuels distrust in AI. Research that counters this notion could help improve public confidence in the technology.We’re thinking:Even if automation fosters job growth on a statistical basis, it clearly threatens specific jobs in specific industries. In such cases, a just society would provide retraining and upskilling programs so that everyone who wants to work can find gainful employment.\n\nNetworked software is often built using a service-oriented architecture, but networked machine learning applications may be easier to manage using a different programming style.What's new:Andrei Paleyes, Christian Cabrera, and Neil D. Lawrence at University of Cambridgecomparedthe work required to build a business-oriented machine learning program using a service oriented architecture (SOA) and flow-based programming (FBP).Key insight:SOA divides a program into services — bundles of functions and memory for, say, navigation, payment processing, and collecting customer ratings in a ride-sharing app — connected to a central hub that passes messages among them. In this arrangement, machine learning applications that draw on large databases generate a high volume of messages, which can require a lot of computation andtime spent debugging. FBP, by contrast, conceives a program as a network of functions, or nodes, that exchange data directly with one another. This approach cuts the amount of communication required and makes it easier to track data paths, making it easier to build efficient machine learning programs.How it works:Over three phases of development, the authors used SOA and FBT to implement taxi-booking applications that took advantage of machine learning. Then they measured the impact of each programming approach on code size, ease of revision, and code complexity.\n\nResults:Both approaches showed distinct benefits. FBP produced a better cognitive complexity score (a measurement of how difficult a code is to understand, where higher is more difficult) in all phases of development. For instance, in Phase 3, FPB scored 1.4 while SOA scored 2.0. On the other hand, the SOA code was easier to revise and less complex in all phases of development. (The authors point out that SOA may have scored higher because it’s more widely used and many libraries exist to reduce code size and complexity. With similar libraries, FBT might catch up.)Why it matters:FBP provided a better developer experience during data collection, according to the authors’ subjective evaluation. This would allow developers to spend more time optimizing data capture and quality. In addition, reducing the expertise required for data collection could enable machine learning engineers to play a bigger role in that process and improve a model’s performance from the data up.We’re thinking:Given the ambiguous results, going with the flow might mean sticking with the more familiar SOA approach.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202022-02-15%20at%202.10.22%20PM.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202022-02-15%20at%202.10.22%20PM.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/GRANTOURISMO.gif?upscale=true&width=1200&upscale=true&name=GRANTOURISMO.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/APPLE.gif?upscale=true&width=1200&upscale=true&name=APPLE.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20Image-4.png?upscale=true&width=1200&upscale=true&name=The%20Batch%20Image-4.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/JOBS.png?upscale=true&width=1200&upscale=true&name=JOBS.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/FLOWv4.gif?upscale=true&width=1200&upscale=true&name=FLOWv4.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-127/",
    "title": "issue 127",
    "date": "",
    "reading_time": "",
    "content": "AI continues to create numerous exciting career opportunities, and I know that many of you aim to develop a career in the field. While taking online courses in technical topics is an important step, being an AI professional requires more than technical skills. Lately I’ve been thinking about how to do more to support all of you who are looking to build a career in AI.Considering individuals at a variety of stages in their careers, what are some of the keys to success?\n\nEach of these items is a complex subject worthy of an entire book. I will continue to think on how we can work collectively to support everyone’s career goals. Meanwhile, I would like to hear your thoughts as well. What am I missing? What can I or my teams do to support you in your career?\n\nKeep learning!\n\nAndrew\n\nA Major League Baseball stadium will be using computer vision to detect weapons as fans enter.What’s new:A system called Hexwave will look for firearms, knives, and explosives carried by baseball fans who visit Camden Yards, home field of the Baltimore Orioles,The Baltimore Sunreported. The system will be tested during certain games in the coming baseball season.How it works:Developed by MIT Lincoln Lab and licensed to Liberty Defense Holdings, a security firm, Hexwave scans passing bodies and alerts guards to potential threats even if they’re concealed by clothing or luggage. It can scan 1,000 people per hour.\n\nBehind the news:A small but growing number of public venues implement AI solutions to enhance security and cut wait times.\n\nWhy it matters:Traditional security checkpoints can beslow, intrusive, and ineffective. AI stands to make them not only more effective but also much more efficient.We’re thinking:Neither Liberty Defense Holdings nor MIT Lincoln Lab provides independent validation of the system’s performance. In an era when the AI community is grappling with the technology’s potential for harm, it’s incumbent on companies that offer systems that evaluate individual behavior to demonstrate their products’ accuracy and fairness before putting them into widespread use.\n\nAmazon reported long-term success using machine learning to shrink its environmental footprint.What’s new: The online retailerdevelopeda system that fuses product descriptions, images, and structured data to decide how an item should be packed for shipping. It evolved over six years, ultimately helping Amazon cut packaging waste equivalent to over 2 billion shipping boxes.How it works:The system initially made packaging decisions based on text descriptions. Last year, the companyintegratedcomputer vision and tabular data analysis.\n\nWhy it matters:Amazon has shipped some 465 million pounds of plastic waste by oneestimate. More broadly, 131.2 billion consumer parcels were shipped worldwide in 2020,according topostage technology firm Pitney Bowes — a figure expected to double within the next five years. AI that cuts the waste that attends all this shipping and receiving might help ease ecommerce’s burden on the planet.We’re thinking:Multimodal AI is on theupswing, and it’s great to see this approach contributing to a more sustainable world. That said, 2 billion boxes is a drop in the 131-billion-parcel ocean. We hope Amazon — and other retailers — will continue to look for innovative ways to diminish the mountain of packaging garbage.\n\nWe’re thrilled to launch the second cohort of our Curriculum Architect Program! This free program is designed to help aspiring and experienced educators hone their curriculum development skills, especially for technical online courses.Learn moreand submit your application.\n\nCan AI spot countries at risk of a sudden change in leadership?What’s new:Researchers at the University of Central Florida are working with a system called CoupCast to estimate the likelihood that an individual country will undergo a coup d’état,The Washington Postreported.How it works:CoupCastpredictsthe probability that a coup will overthrow each of the world’s national leaders in each month.\n\nResults:In 2021, the system predicted upheavals inChadandMali.Behind the news:CoupCast is one of several efforts to use machine learning to study political tensions.\n\nYes, but:The executive director of the nonprofit One Earth Future, which managed CoupCast from 2016 to 2021, came to doubt that its predictions could have a meaningful impact on policy, he toldThe Washington Post. This and other concerns prompted him to turn over the project to the University of Central Florida.Why it matters:Technology that helps people see what’s on the horizon may help prevent coups from spiraling into civil wars and humanitarian crises — or at least help people prepare for the worst.We’re thinking:Modeling political unrest is an important but challenging small-data problem; CoupCast’s dataset included only 600 positive examples. Given the extremely high stakes of international relations, a data-driven approach seems like a productive complement to human analysis.\n\nResearchers discovered a new way to reduce memory requirements when training large machine learning models.What's new:Tim Dettmers and colleagues at University of Washington released8-bit optimizersthat store gradient statistics as 8-bit values, instead of the usual 32-bit, while maintaining the same accuracy.Key insight:Popular optimizers likeAdamuse statistics derived from gradients to accelerate training. Adam uses an estimate of the change in the gradient of each weight over time, which can occupy as much as 50 percent of the memory required during training. However, at any given time, the optimizer needs only the estimates pertinent to the weights it’s currently processing. The remaining part can be quantized temporarily — that is, the numbers can be converted into fewer bits — to take up less memory.How it works:The authors used block-wise quantization, which means that gradient statistics were split into blocks and each block was quantized independently.\n\nResults:The authors used their method on a few language tasks includingmachine translationandGLUE. Models trained on the 8-bit version of Adam achieved BLEU and accuracy scores on those tasks, respectively, nearly identical to those achieved by the 32-bit version. Using 8-bit Adam, authors fine-tuned a 1.5 billion-parameter GPT-2-large on an Nvidia V100 GPU with 24GB of memory. Using the 32-bit Adam optimizer, the hardware maxed out on a 762-million parameter GPT-2-medium.Why it matters:Using an 8-bit optimizer makes it possible to train bigger models —in this work, roughly twice as big — on a given hardware configuration. For instance, now we can train Roberta-large — which is 1 percent to 5 percent more accurate than Roberta, according to the original paper — within the previous memory requirement for the smaller version.We're thinking:Details like how much memory an optimizer uses may not seem worthy of attention when you’re designing and training a model — but, given the memory and processing requirements of deep learning models, sometimes they can have a big impact.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202022-01-11%20at%205.43.32%20PM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202022-01-11%20at%205.43.32%20PM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/HEXWAVE.gif?upscale=true&width=1200&upscale=true&name=HEXWAVE.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/AMAZON.gif?upscale=true&width=1200&upscale=true&name=AMAZON.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/blog%20banner_The%20Batch%20Image.png?upscale=true&width=1200&upscale=true&name=blog%20banner_The%20Batch%20Image.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/COUPS.gif?upscale=true&width=1200&upscale=true&name=COUPS.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-11-23%20at%209.17.27%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-11-23%20at%209.17.27%20AM%20copy.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-197/",
    "title": "issue 197",
    "date": "",
    "reading_time": "",
    "content": "A few weeks ago, Iwroteabout my team at Landing AI’s work on visual prompting. With the speed of building machine learning applications through text prompting and visual prompting, I’m seeing a trend toward building and deploying models without using a test set. This is part of an important trend of speeding up getting models into production.\n\nThe test set has always been a sacred aspect of machine learning development. In academic machine learning work, test sets are the cornerstone of algorithm benchmarking and publishing scientific conclusions. Test sets are also used in commercial machine learning applications to measure and improve performance and to ensure accuracy before and after deployment.\n\nBut thanks to prompt-based development, in which you can build a model simply by providing a text prompt (such as “classify the following text as having either a positive or negative sentiment”) or a visual prompt (by labeling a handful of pixels to show the model what object you want to classify), it is possible to build a decent machine learning model with very few examples (few-shot learning) or no examples at all (zero-shot learning).\n\nPreviously, if we needed 10,000 labeled training examples, then the additional cost of collecting 1,000 test examples didn’t seem onerous. But the rise of zero-shot and few-shot learning — driven by prompt-based development — is making test set collection a bottleneck.\n\nThus I'm seeing more and more teams use a process for development and deployment that looks like this:\n\nI’m excited by this process, which significantly shortens the time it takes to build and deploy machine learning models. However, there is one important caveat: In certain applications, a test set is important for managing risk of harm. Many deployments don’t pose a significant risk of harm; for example, a visual inspection system in a smartphone factory that initially shadows a human inspector and whose outputs aren’t used directly yet. But if we're developing a system that will be involved in decisions about healthcare, criminal justice, finance, insurance, and so on, where inaccurate outputs or bias could cause significant harm, then it remains important to collect a rigorous test set and deeply validate the model’s performance before allowing it to make consequential decisions.\n\nThe occurrence of concept drift and data drift can make the very notion of a “test set” problematic in practical applications, because the data saved for testing no longer matches the real distribution of input data. For this reason, the best test data is production data. For applications where it’s safe and reasonable to deploy without using a test set, I’m excited about how this can speed up development and deployment of machine learning applications.\n\nKeep learning!\n\nAndrew\n\nGoogle showcased a flood of new features in its latest bid to get ahead in the generative AI arms race.\n\nWhat’s new:The companydemonstratedAI features for consumers and developers at its annual I/O conference.\n\nPaLM powered:More than two dozen of the new features, including Bard and Duet AI (see below), are powered by a new large language model calledPaLM 2. Google trained PaLM 2 on tasks similar to Google'sUL2pretraining framework more than 100 different natural languages and numerous programming languages. It will be available as a cloud service in four unspecified sizes.\n\nApp assistance:Duet AIis a suite of text generation tools for Google Workspace and Cloud.\n\nNew foundation models:Vertex offers three new foundation models.Chirpfor speech-to-text, Codey for code completion, andImagenfor text-to-image generation. Users can join awaitlistvia Vertex.\n\nBard handles images:Users no longer have to join a waitlist for access to theBardchatbot, and its language capabilities have been expanded from English to include Japanese and Korean. It is now available in 180 countries, though not the EU or Canada. Bard can now respond to image-based queries, provide images in its responses, and generate custom images using Adobe’s image generation model,Firefly.\n\nSearch enhancements:An experimental version of Google Search will generate text answers to queries using an unidentified language model.\n\nWhy it matters:Google’s new capabilities are the latest salvo in anongoing competitionto capture generative AI’s market potential to greatest effect.\n\nWe’re thinking:Just days ago, a leaked Googlememotalked about Google and OpenAI’s lack of moat when it comes to LLM technology. It described how open source offerings of LLMs are racing ahead, making it challenging for any company to maintain a significant and enduring lead over competitors in the quality of its models. We think the impressive I/O presentation by Sundar Pichai and team, however, reminded everyone of Google’s tremendous distribution advantages. Google owns many platforms/products (such as search, Gmail, Android, Chrome and Youtube) with over 2 billion users, and this gives it numerous ways to get generative AI to users. In the era of generative AI, we are increasingly seeing distribution as a moat for businesses.\n\nDo language models have their own opinions about politically charged issues? Yes — and they probably don’t match yours.\n\nWhat's new: Shibani Santurkar and colleagues at Stanfordcomparedopinion-poll responses of large language models with those of various human groups.\n\nHow it works: The authors collected multiple-choice questions based on surveys of public opinion in the United States. They compared answers generated by nine language models (three from AI21 Labs and six from OpenAI) with those of 60 demographic groups. The groups varied according to sex, age, race, geography, relationship status, citizenship status, education, political party affiliation, religious affiliation, and degree of religious observance.\n\nResults: The authors compared the distributions of model and human answers according to a formula based on theWassersteinscore, also known as earth mover’s distance. In their formula, 1 is a perfect match.\n\nBehind the news:In some circles, ChatGPT has beencriticizedfor expressing a political bias toward liberal (in U.S. terms) positions. Such allegations have prompted developers to buildalternativeversionsthat are deliberately biased in other directions. Some observers speculate that Elon Musk’s secretive AIstartupis on a similar mission.\n\nWhy it matters: Large language models aren’t neutral reflections of society. They express political views that don’t match those of the general population or those of any group. Furthermore, prompting them to take on a particular group’s viewpoint doesn't bring them into line with that group. The AI community (and the world at large) must decide whether and how to manage these biases.\n\nWe're thinking: Should a language model’s opinions match those of the global average, or should different language models respond similarly to different groups? Given that a subset of the world’s population holds biased opinions, including sexist or racist views, should we build LLMs that reflect them? Should language models be allowed to express opinions at all? Much work lies ahead to make these choices and figure out how to implement them.\n\nIdentify your organization's generative AI capabilities, skill gaps, and training needs with the world's first generative AI skill assessment, from Workera.Join the beta now!\n\nChatGPT is helping some workers secretly hold multiple full-time jobs at once.\n\nWhat’s new:Workers are using OpenAI’s chatbot to boost their productivity so they can earn separate paychecks from a number of employers, each of whom believes they are exclusive employees,Vicereported.What they said:Several of these so-called “overemployed” people stated that, although their jobs require a degree of human expertise, ChatGPT enables them to accomplish more in less time. They spoke anonymously to avoid revealing the ruse.\n\nBehind the news:A March 2023 paper by two MIT economistsreportedthat writers who used ChatGPT were 37 percent faster than those who did not.\n\nWhy it matters:This practice illustrates the real productivity gains conferred by large language models. Moreover, in a typical corporate environment, managers decide which tools workers will use and how. The “overemployed” community turns that practice on its head, using AI to boost productivity from the bottom up.\n\nWe’re thinking:It's discouraging to see people using AI to deceive employers who could benefit from the productivity gains. Beyond the ethical problems, the use of generative AI without informing employers could lead to legal questions in areas like ownership of intellectual property. Yes, let’s use these tools to be more productive, but let’s do it in honest and ethical ways.\n\nResearchers struggle to build models that can generate a three-dimensional scene from a text prompt largely because they lack sufficient paired text-3D training examples. A new approach works without any 3D data whatsoever.\n\nWhat's new:Ben Poole and colleagues at Google and UC Berkeley builtDreamFusionto produce 3D scenes from text prompts. Rather than training on text-3D pairs, the authors used a pretrained text-to-image diffusion model to guide the training of a separate model that learned to represent a 3D scene.\n\nKey insight:Aneural radiance field(NeRF) learns to represent a 3D scene from 2D images of that scene. Is it possible to replace the 2D images with a text prompt? Not directly, but a pretrained text-to-image diffusion model, which generates images by starting with noise and removing the noise in several steps, can take a text prompt and generate 2D images for NeRF to learn from. The NeRF image (with added noise) conditions the diffusion model, and the diffusion model’s output provides ground truth for the NeRF.\n\nHow it works:NeRF generated a 2D image, and the authors added noise. Given the noisy NeRF image and a text prompt, a 64x64 pixel version of Google'sImagentext-to-image diffusion model removed the noise to produce a picture that reflected the prompt. By repeating these steps, NeRF gradually narrowed the difference between its output and Imagen’s.\n\nResults:The authors compared DreamFusion images to 2D renders of output fromCLIP-Mesh, which deforms a 3D mesh to fit a text description. They evaluated the systems according toCLIP R-Precision, a metric that measures the similarity between an image and a text description. For each system, they compared the percentage of images that were more similar to the prompt than to 153 other text descriptions. DreamFusion achieved 77.5 percent while CLIP-Mesh achieved 75.8 percent. (The authors note that DreamFusion’s advantage is all the more impressive considering an overlap between the test procedure and CLIP-Mesh’s training).\n\nWhy it matters:While text-3D data is rare, text-image data is plentiful. This enabled the authors to devise a clever twist on supervised learning: To train NeRF to transform text into 3D, they used Imagen’s text-to-image output as a supervisory signal.\n\nWe're thinking:This workjoinsseveraldemonstrations of the varied uses of pre-trained diffusion models.\n\nOpenAI’s CEO asked U.S. to regulate AI in congressional hearingIn testimony before the Senate Judiciary Committee, Sam Altman acknowledged AI risks and proposed the creation of a government agency in charge of licensing, auditing, and establishing safety standards for AI models. (Vice)Amazon to add more AI features to Astro, its home robotThe robot will reportedly use a new large language model called Burnham to engage in free flowing conversations with humans. (The Verge)CNET writers drive unionization effort amid concerns over AI-generated contentThe union would give collective bargaining power to around 100 employees regarding various subjects, including fair compensation, editorial independence, and the use of AI for content creation. (The Verge)Baidu launched search chatbot AI MateAI Mate, based on the company’s Ernie Bot released in March 2023, will be accessible from the landing page for its search engine. (South China Morning Post)Scammer sold AI-generated Frank Ocean songs as leaked tracksThe scammer generated Ocean’s voice using high-quality vocal fragments and used the vocals to create nine fake songs which they sold on underground music communities for thousands of dollars. (Vice)China reports first arrest related to the use of ChatGPTA man was detained for using the AI chatbot to create and spread false news about a fatal train crash. This arrest comes after China tightened regulations on the use of AI to combat the dissemination of fabricated information. (Reuters)Google unveiled Project Gameface, a hands-free gaming mouseThe AI-powered tool enables Windows users to control their mouse cursor through head movements and facial gestures captured by a webcam. (The VergeandGoogle)Research: Robot learns user preferences for household cleanup tasksResearchers developed TidyBot, a robot that uses large language model summarization to infer sets of rules and autonomously sort laundry, recycle cans, and organize objects based on natural language commands. (Princeton University)Stability AI launched a text-to-animation toolThe open source tool, called Stable Animation SDK, allows users to generate animations through text alone, or by combining text with still images and videos.. (Stability AI)IBM announced Watson-X, a business-focused AI and data platformThe platform offers three different toolkits that enable organizations to train, validate, tune, and deploy models, in addition to streamlined AI application development. (IBM)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/05/ML-ProcessSpeedDiagramAlt_1200px-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/05/unnamed--65---1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/05/OPINION--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/05/The-Batch-ads-and-exclusive-banners--12---1-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/05/OVEREMPLOYEDS--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/05/DREAMFUSIONv3--1-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-177/",
    "title": "issue 177",
    "date": "",
    "reading_time": "",
    "content": "As we enter the new year, let’s view 2023 not as a single year, but as the first of more in which we will accomplish our long-term goals. Some results take a long time to achieve, and even though we may take actions that bring those results closer, we can do it more effectively if we envision a path rather than simply going from milestone to milestone.\n\nWhen I was younger, I hardly connected short-term actions concretely to long-term outcomes. I would focus on the next homework assignment, project, or research paper with a vague 10-year goal, lacking a clear path to get there. With experience, I got better at seeing how these efforts could lead to goals that can be achieved only in years.\n\nFor instance, 10 years ago, I built my first machine learning course one week at a time (often filming at 2 a.m.). Building the updatedMachine Learning Specializationthis year, I was able to plan the full course better (and while some filming was still done at  2 a.m., there was less!). In previous businesses, I tended to build a product and only then think about how to take it to customers. These days, I’m more likely to see the big picture even when starting out.\n\nFeedback from friends and mentors can help you shape your vision. A big step in my growth was learning to trust advice from certain experts and mentors — even when I didn’t follow their reasoning — and work hard to understand it. For example, my friends who are experts in global geopolitics sometimes advise me to invest more heavily in particular countries. I would not have come to this conclusion by myself, because I don’t know those countries well. But I’ve learned to explain my long-term plan, solicit their feedback, and listen carefully when they point me in a different direction.\n\nRight now, one of my top goals is to democratize the creation of AI. Having a lot more people able to build custom AI systems will lift up many people. While the path to accomplishing this is long and hard, I can see the steps to get there, and the critiques of friends and mentors have shaped my thinking significantly.\n\nAs 2023 approaches, how far into the future can you make plans? Do you want to achieve expertise in a topic, advance your career, or solve a technical problem? By forming a hypothesis of the path — even an untested one — and soliciting feedback to test and refine it, I hope you can shape a vision that inspires and drives you forward.\n\nDream big for 2023 and beyond!Happy new year,\n\nAndrew\n\nSpring came early in 2022, as what some observers had feared was an impending AI Winter melted into a garden of innovations with potential uses in fields as diverse as art, genomics, and chip design. Dark clouds lingered; generative models continued to produce problematic output, and international tensions flared as the U.S. took steps to block China’s access to AI chips. Yet optimism was palpable in social media, conference proceedings, and venture investment, and the next 12 months promise an abundance of AI progress. In this special issue ofThe Batch, leaders in the field share their hopes for the coming year.\n\nRecent advances in deep learning largely have come by brute force: taking the latest architectures and scaling up compute power, data, and engineering. Do we have the architectures we need, and all that remains is to develop better hardware and datasets so we can keep scaling up? Or are we still missing something?\n\nI believe we’re missing something, and I hope for progress toward finding it in the coming year.\n\nI’ve been studying, in collaboration with neuroscientists and cognitive neuroscientists, the performance gap between state-of-the-art systems and humans. The differences lead me to believe that simply scaling up is not going to fill the gap. Instead, building into our models a human-like ability to discover and reason with high-level concepts and relationships between them can make the difference.\n\nConsider the number of examples necessary to learn a new task, known as sample complexity. It takes a huge amount of gameplay to train a deep learning model to play a new video game, while a human can learn this very quickly. Related issues fall under the rubric of reasoning. A computer needs to consider numerous possibilities to plan an efficient route from here to there, while a human doesn’t.\n\nHumans can select the right pieces of knowledge and paste them together to form a relevant explanation, answer, or plan. Moreover, given a set of variables, humans are pretty good at deciding which is a cause of which. Current AI techniques don’t come close to this human ability to generate reasoning paths. Often, they’re highly confident that their decision is right, even when it’s wrong. Such issues can be amusing in a text generator, but they can be life-threatening in a self-driving car or medical diagnosis system.\n\nCurrent systems behave in these ways partly because they’ve been designed that way. For instance, text generators are trained simply to predict the next word rather than to build an internal data structure that accounts for the concepts they manipulate and how they are related to each other. But I think we can design systems that track the meanings at play and reason over them while keeping the numerous advantages of current deep learning methodologies. In doing so, we can address a variety of challenges from excessive sample complexity to overconfident incorrectness.\n\nI’m excited by generative flow networks, orGFlowNets, an approach to training deep nets that my group started about a year ago. This idea is inspired by the way humans reason through a sequence of steps, adding a new piece of relevant information at each step. It’s like reinforcement learning, because the model sequentially learns a policy to solve a problem. It’s also like generative modeling, because it can sample solutions in a way that corresponds to making a probabilistic inference.\n\nIf you think of an interpretation of an image, your thought can be converted to a sentence, but it’s not the sentence itself. Rather, it contains semantic and relational information about the concepts in that sentence. Generally, we represent such semantic content as a graph, in which each node is a concept or variable. GFlowNets generate such graphs one node or edge at a time, choosing which concept should be added and connected to which others in what kind of relation.\n\nI don’t think this is the only possibility, and I look forward to seeing a multiplicity of approaches. Through a diversity of exploration, we’ll increase our chance to find the ingredients we’re missing to bridge the gap between current AI and human-level AI.\n\nYoshua Bengio is a professor of computer science at Université de Montréal and scientific director of Mila - Quebec AI Institute. He received the 2018 A.M. Turing Award, along with Geoffrey Hinton and Yann LeCun, for his contribution to breakthroughs in deep learning.\n\nThe important question of how companies and organizations use our data has received a lot of attention in the technology and policy communities. An equally important question that deserves more focus in 2023 is how we, as individuals, can take advantage of the data we generate to improve our health, vitality, and productivity.\n\nWe create a variety of data throughout our days. Photos capture our experiences, phones record our workouts and locations, Internet services log the content we consume and our purchases. We also record our want-to lists: desired travel and dining destinations, books and movies we plan to enjoy, and social activities we want to pursue. Soon smart glasses will record our experiences in even more detail. However, this data is siloed in dozens of applications. Consequently, we often struggle to retrieve important facts from our past and build upon them to create satisfying experiences on a daily basis.\n\nBut what if all this information were fused in a personal timeline designed to help us stay on track toward our goals, hopes, and dreams? The idea is not new. Vannevar Bush envisioned it in 1945, calling it a memex. In the 90’s, Gordon Bell and his colleagues at Microsoft Research built MyLifeBits, a prototype of this vision. The prospects and pitfalls of such a system have been depicted in film and literature.\n\nPrivacy is obviously a key concern in terms of keeping all our data in a single repository and protecting it against intrusion or government overreach. Privacy means that your data is available only to you, but if you want to share parts of it, you should be able to do it on the fly by uttering a command such as, “Share my favorite cafes in Tokyo with Jane.” No single company has all our data or the trust to store all our data. Therefore, building technology that enables personal timelines should be a community effort that includes protocols for the exchange of data, encrypted storage, and secure processing.\n\nBuilding personal timelines will also force the AI community to pay attention to two technical challenges that have broader application.\n\nThe first challenge is answering questions over personal timelines. We’ve made significant progress on question answering over text and multimodal data. However, in many cases, question answering requires that we reason explicitly about sets of answers and aggregates computed over them. This is the bread and butter of database systems. For example, answering “what cafes did I visit in Tokyo?” or “how many times did I run a half marathon in under two hours?” requires that we retrieve sets as intermediate answers, which is not currently done in natural language processing.  Borrowing more inspiration from databases, we also need to be able to explain the provenance of our answers and decide when they are complete and correct.\n\nThe second challenge is to develop techniques that use our timelines, responsibly, for improved personal well-being. Taking inspiration from the field of positive psychology, we can all flourish by creating positive experiences for ourselves and adopting better habits. An AI agent that has access to our previous experiences and goals can give us timely reminders and suggestions of things to do or avoid.\n\nUltimately, what we choose to do is up to us, but I believe that an AI with a holistic view of our day-to-day activities, better memory, and superior planning capabilities would benefit everyone.\n\nAlon Halevy is a director at the Reality Labs Research branch of Meta. His hopes for 2023 represent his personal opinion and not that of Meta.\n\nThis year we really started to see AI go mainstream. Systems like Stable Diffusion and ChatGPT captured the public imagination to an extent we haven’t seen before in our field. These are exciting times, and it feels like we are on the cusp of something great: a shift in capabilities that could be as impactful as — without exaggeration — the industrial revolution.\n\nBut amidst that excitement, we should be extra wary of hype and extra careful to ensure that we proceed responsibly.\n\nConsider large language models. Whether or not such systems really “have meaning,” lay people will anthropomorphize them anyway, given their ability to perform arguably the most quintessentially human thing: to produce language. It is essential that we educate the public on the capabilities and limitations of these and other AI systems, especially because the public largely thinks of computers as good old-fashioned symbol-processors — for example, that they are good at math and bad at art, while currently the reverse is true.\n\nModern AI has important and far-reaching shortcomings. Systems are too easily misused or abused for nefarious purposes, intentionally or inadvertently. Not only do they hallucinate information, they do so with seemingly very high confidence and without the ability to attribute or credit sources. They lack a rich-enough understanding of our complex multimodal human world and do not possess enough of what philosophers call “folk psychology,” the capacity to explain and predict the behavior and mental states of other people. They are arguably unsustainably resource-intensive, and we poorly understand the relationship between the training data going in and the model coming out. Lastly, despite the unreasonable effectiveness of scaling — for instance, certain capabilities appear to emerge only when models reach a certain size — there are also signs that with that scale comes even greater potential for highly problematic biases and even less-fair systems.\n\nMy hope for 2023 is that we’ll see work on improving all of these issues. Research on multimodality, grounding, and interaction can lead to systems that understand us better because they understand our world and our behavior better. Work on alignment, attribution, and uncertainty may lead to safer systems less prone to hallucination and with more accurate reward models. Data-centric AI will hopefully show the way to steeper scaling laws, and more efficient ways to turn data into more robust and fair models.\n\nFinally, we should focus much more seriously on AI’s ongoing evaluation crisis. We need better and more holistic measurements — of data and models — to ensure that we can characterize our progress and limitations, and understand, in terms ofecological validity(for instance, real-world use cases), what we really want out of these systems.\n\nDouwe Kiela is an adjunct professor in symbolic systems at Stanford University. Previously, he was the head of research at Hugging Face and a research scientist at Facebook AI Research.\n\nIn 2022 our amazing Pie & AI ambassadors hosted over 100 events in 66 cities around the globe! Here is a heartfelt thank you to all of them, from everyone at DeepLearning.AI. Read some of their experienceshere\n\nIt’s an exciting time for AI, with fascinating advances in generated media and many other applications, some even in science and medicine. Some folks may dream about what more AI can create and how much bigger models we may engineer. While those directions are exciting, I argue that we need to pursue much less flashy work: going back to the basics and studying AI models as targets of scientific inquiry.\n\nWhy and how? The field of interpretability aims to create tools to generate “explanations” for the output of complex models. This field has emerged naturally out of a need to build machines that we can have a dialog with: What is your decision? Why did you decide that? For example, a tool takes an image and a classification model, and generates explanations in the form of weighted pixels. The higher a pixel’s weight, the more important it is. For instance, the more its value affects the output, the more important it may be — but the definition of importance differs depending on the tool.\n\nWhile there are some successes, many tools have turned out to behave in ways we did not expect. For example, it has been shown that explanations of an untrained model are quantitatively and qualitatively indistinguishable to those of a trained model. (Then what does the explanation explain?) Explanations often change with small changes in the input despite resulting in the same output. In addition, there isn’t much causal relationship between a model’s output (what we are trying to explain) and the tool’s explanation. Other work shows that good explanations of a model’s output don't necessarily have a positive influence on how people use the model.\n\nWhat does this mismatch between expectation and outcome mean, and what should we do about it? It suggests that we need to examinehowwe build these tools.\n\nCurrently we take an engineering-centric approach: trial and error. We build tools based on intuition (for instance, explanations would be more intuitive for humans if we generate a weight per a chunk of pixels instead of individual pixels). While the engineering-centric approach is useful, we also need fundamental principles (what can be called science) to build better tools.\n\nIn developing drugs, for instance, trial and error is essential (say, testing a new medicine through rigorous clinical trials before deploying it), but it goes hand-in-hand with sciences like biology and genetics. While science has many gaps in understanding how the human body works, it provides fundamental principles in creating the tool (in this case, drugs). In other words, pursuing both science and engineering simultaneously, such that each can inform the other, has shown to be a successful way to work with complex beings (humans).\n\nThe field of machine learning needs to study our complex aliens (models) like other disciplines study humans. How would such study of these aliens help interpretability? Here’s an example. A team at the University of Tübingenfoundthat neural networks see texture (say, an elephant’s skin) more than shape (an elephant’s outline). Even if we see an elephant’s contour in the explanation of an image — perhaps in the form of collective highlighted pixels — the study informs us that the model may not be seeing the shape but rather the texture. This is called inductive bias — a tendency of a particular class of models due to either its architecture or the way we optimize it. Revealing such tendencies can help us understand this alien, just as revealing a human’s tendency (bias) can be used to understand human behavior (such as unfair decisions).\n\nIn this way, the methods often used to understand humans can also help us understand AI models. These include observational studies (say,observing multi-agents from afar to infer emerging behaviors), controlled studies (for instance,intervening in a multi-agent system to elicit underlying behaviors), and surgery (such asexamining the internals of the superhuman chess player AlphaZero). For AI models, thanks to the way their internals are built — they are made of math! — we have one more tool: theoretical analysis. Work along this direction has already yielded exciting theoretical results on the behaviors of models, optimizers, and loss functions. Some take advantage of classical tools in statistics, physics, dynamical systems, or signal processing.  Many tools from different fields are yet to be explored in the study of AI.\n\nPursuing science doesn’t mean we should stop engineering. The two go hand in hand: Science will enable us to build tools under principles and knowledge, while engineering enables science to become practical. Engineering can also inspire science: What works well in practice can provide hints to structures of models that we wish to formalize in science, just like the high-performance of convolutional networks in 2012 inspired many theory papers that tried to analyze why convolutions help generalization.\n\nI’m excited to enter 2023 and many other years to come as we advance our understanding of our aliens and invent ways to communicate with them. By enabling a dialogue, we will enable richer collaborations and better leverage the complementary skill sets of humans and machines.\n\nBeen Kim is a research scientist at Google Brain. Her work on helping humans to communicate with complex machine learning models won the UNESCO Netexplo award.\n\nAs we enter the new year, there is a growing hope that the recent explosion of generative AI will bring significant progress in active learning. This technique, which enables machine learning systems to generate their own training examples and request them to be labeled, contrasts with most other forms of machine learning, in which an algorithm is given a fixed set of examples and usually learns from those alone.\n\nActive learning can enable machine learning systems to:\n\nThe idea of active learning has been in the community for decades, but it has never really taken off. Previously, it was very hard for a learning algorithm to generate images or sentences that were simultaneously realistic enough for a human to evaluate and useful to advance a learning algorithm.\n\nBut with recent advances in generative AI for images and text, active learning is primed for a major breakthrough. Now, when a learning algorithm is unsure of the correct label for some part of its encoding space, it can actively generate data from that section to get input from a human.\n\nActive learning has the potential to revolutionize the way we approach machine learning, as it allows systems to continuously improve and adapt over time. Rather than relying on a fixed set of labeled data, an active learning system can seek out new information and examples that will help it better understand the problem it is trying to solve. This can lead to more accurate and effective machine learning models, and it could reduce the need for large amounts of labeled data.\n\nI have a great deal of hope and excitement that active learning will build upon the recent advances in generative AI. As we enter the new year, we are likely to see more machine learning systems that implement active learning techniques, and it is possible that 2023 could be the year that active learning truly takes off.\n\nReza Zadeh is founder and CEO at Matroid, a computer vision company, an adjunct professor at Stanford, and an early member of Databricks. Twitter:@Reza_Zadeh.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--18--1.png",
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--13-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--14-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--15-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/12/ezgif.com-gif-maker--7-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--16-.png",
      "https://dl-staging-website.ghost.io/content/images/2022/12/unnamed--17-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-54/",
    "title": "issue 54",
    "date": "",
    "reading_time": "",
    "content": "I’ve been trying to teach my toddler the alphabet. Despite having some educational experience, when she mispronounces a vowel for the nth time, I can’t help but feel like I’m doing it wrong. I hope that Nova somehow will still grow up to be literate and consider my efforts to have been adequate.\n\nTeachers have been instructing young people in languages for centuries, yet our methods strike me as remarkably uneven. I’ve tried many alphabet instruction software apps, a number of them featuring dancing animals and the like. But my favorite tools have turned out to be a word processor, which lets me type words against a plain white canvas for Nova to read, andletter-shaped stickersthat I can rearrange on my kitchen wall.\n\nI was struck by how often Nova, like a neural network, wound up in local minima. She learned to count out loud from one to five by uttering a sequence of sounds without understanding the concept of numbers, much like a recurrent neural network generates plausible text without understanding the meanings of the words it uses. I fed her the sequence of sounds, and she overfit to it. Watching her generalize (and sometimes fail to generalize) gave me fresh appreciation for the difficulty of learning from a small number of examples and how crafting a training dataset with care — curriculum learning? — can promote learning.\n\nAmid the pandemic, schools worldwide find themselves in varying states of chaos, and many parents are juggling their children’s education with working from home. Many of us have insufficient time and energy to do both well. It can feel like a no-win situation.\n\nMy heart goes out to everyone who is caught in this bind. I think the best thing a parent can do is to keep loving your kids. As long as you do that, it will be more than enough. Educational apps can be great, and I hope the AI community will come up with better ones, but an attentive parent armed with a pack of post-its and a loving touch or smile is all a child really needs to learn the basics. Beyond the education you impart, the relationship you build will widen the channels of learning for a lifetime.\n\nKeep learning!\n\nAndrew\n\nU.S. Air Force Flight Commander Ronisha Carter is charting an uncommon flight path in AI. She collaborates with academia and industry to build applications that keep the force’s planes flying efficiently. Her work could also help solve complex logistics and scheduling problems in the civilian world.Read more\n\nThe UK government abandoned a plan to use machine learning to assess students for higher education.What’s new:The UK Department of Education discarded grades generated by analgorithmdesigned to predict performance on the annual Advanced Level qualifications, which had been canceled due to the pandemic.\n\nWhat went wrong:Education officials initially asked teachers to award their students an expected grade based on past performance on practice exams. This resulted in a higher-than-normal share of good grades. The departmentdevelopedits model in an effort to bring grades into line with their usual distribution.\n\nBehind the news:The pandemic also induced the International Baccalaureate Organization, a nonprofit foundation that provides two-year high school diplomas, to develop its owngrading model. The organization said its model produced a distribution similar to that produced by teachers last year. Nonetheless, over 15,000 parents, students, and teachers arepetitioningthe foundation to reevaluate its model, which they say predicts unfairly low grades.Why it matters:In many countries, high school exit exams determine whether students can pursue higher education. Flawed grading models can have lifelong consequences.We’re thinking:If an AI algorithm predicted grades that were, on average, more accurate, less biased, and less noisy than those estimated by human teachers, it would be worth considering deploying it. If an AI algorithm unfairly lowered many individuals’ grades, it would seem like a terrible idea to use it. The truth is, we live in a world where AI systems can fit both descriptions simultaneously, leading to strong arguments for and against using it. Whether such systems yield a net benefit is an ethical question that requires vigorous debate. We don’t see an easy answer.\n\nAn autonomous fighter pilot shot down a human aerial ace in virtual combat.Watch the birdie:Built by defense contractor Heron Systems, the system also defeated automated rivals from seven other companies to win theAlphaDogfighttrial. The U.S. Defense Advanced Research Projects Agency (Darpa) organized the contest as a way for defense contractors to test autonomous air-to-air combat systems.No points for second place:The companies began building their systems last August. Theycompetedlast week in a round robin tournament, each contestant trying to nail its opponent in simulated dogfights using only nose guns.\n\nTarget rich environment:The Darpa contest is part of a broader effort to develop autonomous aerial fighters.\n\nRevvin’ up the engines:Heron says its system isn’t likely to replace human pilots any time soon. Darpa says itaimsto automate certain aspects of air combat, leaving the pilot more room to strategize.We’re thinking:AI-driven fighter jets should be in movies, not future armies. We support the United Nations’ push for a globalbanon autonomous weapons. Without such a ban, an unfortunate AI arms race seems inevitable.\n\nTens of thousands of learners have enrolled in our Natural Processing Specialization since it launched two months ago.Have you?\n\nMarketers are using computer vision to parse customers by skin color and other perceived racial characteristics.What’s new:A number of companies are pitching race classification as a way for businesses to understand the buying habits of different groups, according to theWall Street Journal. This capability is distinct from face recognition, which seeks to identify individuals. Similar systems classify physical or demographic characteristics such as age, gender, and evenattractiveness.What they found:The report identified more than a dozen companies marketing race classification for commercial use. Among them:\n\nYes, but:Experts worry that this capability could be used to discriminate against particular groups. For instance, a retailer might charge certain people higher prices. More troubling, there are signs that such systems are being used by oppressive regimes to target specific ethnicities.\n\nWhy it matters:Machine learning can be a valuable tool for identifying and analyzing demographic trends. But these tools risk invasions of privacy, discrimination both accidental and deliberate, and misuse by authorities.We’re thinking:We can imagine a system that effectively helps detect and avoid racial bias in, say, law enforcement, yielding a net social benefit. Still, the practice of sorting people by their perceived race has a largely odious and sometimes murderous history. Machine learning engineers working in this field should tread very carefully.\n\nIf the world changes every second and you take a picture every 10 seconds, you won’t have enough pictures to observe the changes clearly, and storing a series of pictures won’t help. On the other hand, if you take a picture every tenth of a second, then storing a history will help model the world. New research applies this principle to reinforcement learning.What’s new:William Fedus and Prajit Ramachandran led researchers at Google Brain, MILA, University of Montreal, and DeepMind torefine experience replay, a fundamental technique in reinforcement learning. The outcome: a new hyperparameter.Key insight:Experience replayenables an agent to store observations so it can apply past experiences to present conditions. However, the faster the environment changes, the less relevant past experiences become. The authors conclude that the ratio of stored observations to updates of the agent’s strategy is a previously unrecognized hyperparameter.How it works:In reinforcement learning, an agent observes the environment at a given frame rate, chooses actions based on its observations, receives rewards for desirable actions, and learns to maximize the rewards.\n\nResults:The team tested the new hyperparameter using Atari games, a common RL benchmark. Increasing capacity to maintain a consistent ratio improved the agent’s performance. Reducing the ratio to focus the agent on more recent observations often helped as well (Figure 2).Yes, but:If the ratio is too low, the agent may fall back into old habits or fail to discover the optimal strategy to achieve its goal.Why it matters:Replay ratio wasn’t a focus of attention prior to this study. Now we know the ratio affects performance. That insight may add context to previous literature that considers only capacity.We’re thinking:Like Goldilocks tasting porridge to find the bowl whose temperature is just right, it’s likely to take a bit of trial and error to find a given agent’s optimal replay ratio.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter203-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Working20AI20The20Batch20teaser2020Working20AI20interior20layout201.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Grades.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Dogfight1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_GIF205-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/RacescanGif2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Experience20Replay.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-204/",
    "title": "issue 204",
    "date": "",
    "reading_time": "",
    "content": "Prompt-based development is making the machine learning development cycle much faster: Projects that used to take months now may take days. I wrote in an earlierletterthat this rapid development is causing developers to do away with test sets.\n\nThe speed of prompt-based development is also changing the process ofscoping projects. In lieu of careful planning, it’s increasingly viable to throw a lot of projects at the wall to see what sticks, because each throw is inexpensive.\n\nSpecifically, if building a system took 6 months, it would make sense for product managers and business teams to plan the process carefully and proceed only if the investment seemed worthwhile. But if building something takes only 1 day, then it makes sense to just build it and see if it succeeds, and discard it if it doesn’t. The low cost of trying an idea also means teams can try out a lot more ideas in parallel.\n\nSay you’re in charge of building a natural language processing system to process inbound customer-service emails, and a teammate wants to track customer sentiment over time. Before the era of large pre-trained text transformers, this project might involve labeling thousands of examples, training and iterating on a model for weeks, and then setting up a custom inference server to make predictions. Given the effort involved, before you started building, you might also want to increase confidence in the investment by having a product manager spend a few days designing the sentiment display dashboard and verifying whether users found it valuable.\n\nBut if a proof of concept for this project can be built in a day by prompting a large language model, then, rather than spending days/weeks planning the project, it makes more sense to just build it. Then you can quickly test technical feasibility (by seeing if your system generates accurate labels) and business feasibility (by seeing if the output is valuable to users). If it turns out to be either technically too challenging or unhelpful to users, the feedback can help you improve the concept or discard it.\n\nI find this workflow exciting because, in addition to increasing the speed of iteration for individual projects, it significantly increases the volume of ideas we can try. In addition to plotting the sentiment of customer emails, why not experiment with automatically routing emails to the right department, providing a brief summary of each email to managers, clustering emails to help spot trends, and many more creative ideas? Instead of planning and executing one machine learning feature, it’s increasingly possible to build many, quickly check if they look good, ship them to users if so, and get rapid feedback to drive the next step of decision making.\n\nOne important caveat: As I mentioned in the letter about eliminating test sets, we shouldn’t let the speed of iteration lead us to forgo responsible AI. It’s fantastic that we can ship quick-and-dirty applications. But if there is risk of nontrivial harm such as bias, unfairness, privacy violation, or malevolent uses that outweigh beneficial uses, we have a responsibility to evaluate our systems’ performance carefully and ensure that they’re safe before we deploy them widely.\n\nWhat ideas do you have for prompt-based applications? If you brainstorm a few different ways such applications could be useful to you or your company, I hope you’ll implement many of them (safely and responsibly) and see if some can add value!\n\nKeep learning,\n\nAndrew\n\nP.S. We just announced a newshort coursetoday,LangChain: Chat with Your Data, built in collaboration with Harrison Chase, creator of the open-source LangChain framework. In this course, you’ll learn how to build one of the most-requested LLM-based applications: Answering questions based on information in a document or collection of documents. This one-hour course teaches you how to do that using retrieval augmented generation (RAG). It also covers how to use vector stores and embeddings to retrieve document chunks relevant to a query.\n\nThe business of supplying labeled data for building AI systems is a global industry. But the people who do the labeling face challenges that impinge on the quality of both their work and their lives.\n\nWhat’s new:The Vergeinterviewed more than two dozen data annotators,revealinga difficult, precarious gig economy. Workers often find themselves jaded by low pay, uncertain schedules, escalating complexity, and deep secrecy about what they’re doing and why.How it works:Companies that provide labeling services including Centaur Labs, Surge AI, and Remotasks (a division of data supplier Scale AI) use automated systems to manage gig workers worldwide. Workers undergo qualification exams, training, and performance monitoring to perform tasks like drawing bounding boxes, classifying sentiments expressed by social media posts, evaluating video clips for sexual content, sorting credit-card transactions, rating chatbot responses, and uploading selfies of various facial expressions.\n\nWhat they’re saying:“AI doesn’t replace work. But it does change how work is organized.” —Erik Duhaime, CEO, Centaur Labs\n\nBehind the news:Stanford computer scientist Fei-Fei Li was an early pioneer in crowdsourcing data annotations. In 2007, she led a team at Princeton to scale the number of images used to train an image recognizer from tens of thousands to millions. To get the work done, the team hired thousands of workers via Amazon’s Mechanical Turk platform. The result was ImageNet, a key computer vision dataset.\n\nWhy it matters:Developing high-performance AI systems depends on accurately annotated data. Yet the harsh economics of annotating at scale encourages service providers to automate the work and workers to either cut corners or drop out. Notwithstanding recent improvements — for instance, Googleraisedits base wage for contractors who evaluate search results and ads to $15 per hour — everyone would benefit from treating data annotation less like gig work and more like a profession.We’re thinking:The value of skilled annotators becomes even more apparent as AI practitioners adopt data-centric development practices that make it possible to build effective systems with relatively few examples. With far fewer examples, selecting and annotating them properly is absolutely critical.\n\nAn app is bridging the language gap between the Indian government and its citizens, who speak a wide variety of languages.\n\nWhat’s new:Jugalbandihelps Indians learn about government services, which typically are described online in English and Hindi, in their native tongues. The project is a collaboration between Microsoft and open-source developers AI4Bharat and OpenNyAI.How it works:Jugalbandi harnesses an unspecified GPT model from the Microsoft Azure cloud service and models from AI4Bharat, a government-backed organization that provides open-source models and datasets for South Asian languages. As of May, the system covered 10 of India’s 22 official languages (out ofmore than 120that are spoken there) and over 170 of the Indian government’s 20,000 programs.\n\nBehind the news:While language models are helping citizens understand their governments, they’re also helping governments understand their citizens. In March, RomanialaunchedION, an AI system that scans social media comments on government officials and policy and summarizes them for ministers to read.\n\nWhy it matters:India is a highly multilingual society, andaround a quarterof its 1.4 billion residents are illiterate. Consequently, many people in India struggle to receive government benefits and interact with central authorities. This approach may enable Indians to use their own language via WhatsApp, whichhasmore than 400 million users in that country.\n\nWe’re thinking:In February, Microsoft researchersshowedthat large language models are approaching state-of-the-art results in machine translation. Indeed, machine translation is headed toward a revolution as models like GPT 3.5 (used in the study) and GPT-4 (which is even better) make translations considerably easier and more accurate.\n\nChatting with data is a highly valuable use case for large language models. In this short course, you’ll use the open source LangChain framework to build a chatbot that interacts with your business or personal data.Enroll in \"LangChain: Chat with Your Data”today for free!\n\nA new coding framework lets you pipe your own data into large language models.\n\nWhat’s new:LlamaIndexstreamlines the coding involved in enabling developers to summarize, reason over, and otherwise manipulate data from documents, databases, and apps using models like GPT-4.How it works:LlamaIndex is a free Pythonlibrarythat works with any large language model.\n\nBehind the news:Former Uber research scientist Jerry Liu began building LlamaIndex (originally GPT Index) in late 2022 and co-founded a company around it earlier this year. The company, which recentlyreceived$8.5 million in seed funding, plans to launch an enterprise version later this year.Why it matters:Developing bespoke apps that use a large language model typically requires building custom programs to parse private databases. LlamaIndex offers a more direct route.We’re thinking:Large language models are excitingnew tools for developing AI applications. Libraries like LlamaIndex andLangChainprovide glue code that makes building complex applications much easier — early entries in a growing suite of tools that promise to make LLMs even more useful.\n\nOne challenge to making online education available worldwide is evaluating an immense volume of student work. Especially difficult is evaluating interactive computer programming assignments such as coding a game. A deep learning system automated the process by finding mistakes in completed assignments.\n\nWhat’s new:Evan Zheran Liu and colleagues at Stanford proposedDreamGrader, a system that integrates reinforcement and supervised learning to identify errors (undesirable behaviors) in interactive computer programs and provide detailed information about where the problems lie.\n\nKey insight:A reinforcement learning model can play a game, randomly at first, and — if it receives the proper rewards — learn to take actions that bring about an error. A classifier can learn to recognize that the error occurred, randomly at first, and reward the RL model when it triggers the error. In this scheme, training requires a small number of student submissions that have been labeled with a particular error that is known to occur. The two models learn in an alternating fashion: The RL model plays for a while and does or doesn’t bring about the error; the classifier classifies the RL model’s actions (that is, it applies the model’s label to actions that trigger the error and, if so, dispenses a reward), then the RL model plays more, and so on. By repeating this cycle, the classifier learns to recognize an error reliably.\n\nHow it works:DreamGrader was trained on a subset of 3,500 anonymized student responses to an assignment from the online educational platform Code.org. Students were asked to codeBounce, a game in which a single player moves a paddle along a horizontal axis to send a ball into a goal. The authors identified eight possible errors (such as the ball bouncing out of the goal after entering and no new ball being launched after a goal was scored) and labeled the examples accordingly. The system comprised two components for each type of error: (i) aplayerthat played the game (adouble dueling deep Q-network) and (ii) a classifier (an LSTM and vanilla neural network) that decided whether the error occurred.\n\nResults:The authors evaluated DreamGrader on a test set of Code.org student submissions. For comparison, they modified the previousPlay to Grade, which had been designed to identify error-free submissions, to predict the presence of a specific error. DreamGrader achieved 94.3 percent accuracy — 1.5 percent short of human-level performance — while Play to Grade achieved 75.5 percent accuracy. It evaluated student submissions in around 1 second each, 180 times faster than human-level performance.\n\nYes, but:DreamGrader finds only known errors. It can’t catch bugs that instructors haven’t already seen.\n\nWhy it matters:Each student submission can be considered a different, related task. The approach known as meta-RL aims to train an agent that can learn new tasks based on experience with related tasks. Connecting these two ideas, the authors trained their model following the learning techniques expressed in the meta-RL algorithmDREAM. Sometimes it’s not about reinventing the wheel, but reframing the problem as one we already know how to solve.\n\nWe’re thinking:Teaching people how to code empowers them to lead more fulfilling lives in the digital age, just as teaching them to read has opened doors to wisdom and skill since the invention of the printing press. Accomplishing this on a global scale requires automated systems for education (like Coursera!). It’s great to see AI research that could make these systems more effective.\n\nOpenAI sued over alleged violation of privacySeveral unnamed plaintiffs filed a lawsuit against the company, alleging that its use of information found on the internet to train its models constitutes offenses such as larceny, copyright infringement, and invasion of privacy. The plaintiffs seek class-action status. (San Francisco Chronicles)The Vatican published a handbook on AI ethicsIn partnership with Santa Clara University, the Holy See released a manual called “Ethics in the Age of Disruptive Technologies” that contains a strategic plan to enhance ethical management practices for technologies like AI. It’savailablefor free. (Gizmodo)Executives signed a letter against the AI ActOver 160 executives from companies including Meta and Renault warned that the proposed EU regulations would overly regulate AI, burdening developers with high compliance costs and disproportionate liability risks. (Reuters)AI-generated sites garner advertising revenueA report found that AI-generated websites are attracting ads served by automated online ad-placement services. Ads for over 140 brands have appeared on such sites, 90 percent of them served by Google. (MIT Technology Review)\n\nMajor League Baseball is scouting new players using AIThe league started converting players’ videos into metrics for teams to analyze during their scouting process. Uplift Labs analyzes images from a pair of iPhone cameras to forecast players’ potential and detect their flaws. (The Wall Street Journal)U.S. to strengthen ban on AI chip salesThe Biden administration is considering tougher restrictions on exports of AI chips to China. An earlier decision banned sales of Nvidia A100 and H100 GPUs, and Nvidia developed A800 and H800 versions for the Chinese market.The new rules would restrict those chips, too, as well as similar products from AMD and Intel. (The New York Times)Hollywood directors negotiate protection against AIA union of directors ratified a three-year contract with film studios confirming that AI cannot replace their duties.  Unions that represent screenwriters and actorsare in the process of negotiating similar agreements. (The New York Times)Research:AI-generated images of child sexual abuse proliferateSince August, the volume of photorealistic AI-generated material depicting child sexual abuse circulating on the dark web has risen, a new study shows. Thhis type of material was generated mostly by open source applications developed and distributed with few controls. (The New York Times)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--23--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--33-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--67-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/07/The-Batch-ads-and-exclusive-banners--41-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--34-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/07/unnamed--68-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-272/",
    "title": "issue 272",
    "date": "",
    "reading_time": "",
    "content": "Startups live or die by their ability toexecute at speed. For large companies, too, the speed with which an innovation team is able to iterate has a huge impact on its odds of success. Generative AI makes it possible to quickly prototype AI capabilities. AI capabilities that used to take months can sometimes be built in days or hours by simply prompting a large language model. I find this speed exciting and have been thinking about how to help startups and large companies alike go faster.\n\nI’ve been obsessed with speedy execution for a long time. When working on a project, I am loath to take two weeks to do something that I could do in one week. The price of moving at that pace is not that we take one week longer (which might be okay) but that we’re 2x slower (which is not)!\n\nWhen building an AI-powered product, there are many steps in designing, building, shipping, and scaling the product that are distinct from building the AI capability, and our ability to execute these other steps has not sped up as much as the AI part. But the speed with which we can prototype AI creates significant pressure to speed up these other steps, too. If it took 6 months to collect data, train a supervised learning algorithm, and deploy the model to the cloud, it might be okay to take 2 months to get user feedback. But if it takes a week to build a prototype, waiting 2 months for feedback seems intolerably slow!\n\nI’d like to focus on one key step of building applications: getting user feedback. A core part of the iterative workflow of designing and building a product (popularized by Eric Ries in his bookThe Lean Startup) is to build a prototype (or MVP, minimum viable product), get user feedback on it, and to use that feedback to drive improvements. The faster you can move through this loop — which may require many iterations — the faster you can design a product that fits the market. This is why AI Fund, a venture studio that I lead, uses many fast, scrappy tactics to get feedback.\n\nFor B2C (business to consumer) offerings, here is a menu of some options for getting customer feedback:\n\nAs we go down this list, we get (probably) more accurate feedback, but the time needed to get that feedback increases significantly. Also, the tactics at the top of the list create basically no risk, and thus it’s safe to repeatedly call on them, even with preliminary ideas and prototypes. Another advantage of the tactics further up the list is that we get more qualitative feedback (for example, do users seem confused? Are they telling us they really need one additional feature?), which sparks better ideas for how to change our product than an A/B test, which tells us with rigor whether a particular implementation works but is less likely to point us in new directions to try. I recommend using the fast feedback tactics first. As we exhaust the options for learning quickly, we can try the slower tactics.\n\nWith these tactics, scrappy startup leaders and innovation-team leaders in large companies can go faster and have a much higher chance of success.\n\nThe mantra “move fast and break things” got a bad reputation because, well, it broke things. Unfortunately, some have interpreted this to mean we should not move fast, but I disagree. A better mantra is “move fast and be responsible.” There are many ways to prototype and test quickly without shipping a product that can cause significant harm. In fact, prototyping and testing/auditing quickly before launching to a large audience is a good way to identify and mitigate potential problems.\n\nThere are numerous AI opportunities ahead, and our tools are getting better and better to pursue them at speed, which I find exhilarating!\n\nKeep learning!\n\nAndrew\n\nBuild advanced, multi-agent systems for project planning, sales pipelines, customer support analysis, and content creation in our new course with crewAI! Gain hands-on skills in performance testing, multi-model setups, and using human feedback to optimize AI agents.Enroll for free\n\nMajor AI companies plan to meet the growing demand with nuclear energy.\n\nWhat’s new:Amazon, Google, and Microsoftannouncedsubstantial investments in nuclear power projects. Amazon and Google forged partnerships to build a new generation of small reactors, while Microsoft cut a deal to revive a shuttered nuclear plant. (Andrew Ng is a member of Amazon’s board of directors.)\n\nHow it works:Nuclear powerprovidesaround 18 percent of electricity in the United States and more in France and several other European countries. Its steady generating capacity and zero carbon emissions (after plant construction) make it an attractive way to power AI infrastructure. However, new nuclear plants have been difficult to build in the U.S. since a string of high-profile accidents at Three Mile Island in the U.S. (1979), Chernobyl in Ukraine (1986), and Fukishima in Japan (2011). Since then, pressure to reduce carbon emissions has driven calls to build new plants. In March, President Bidensignedlegislation that streamlines construction and regulation of nuclear plants.\n\nBehind the news:The tech industry’s growing interest in nuclear power is driven by surging demand for AI and corporate commitments to reduce carbon emissions. Data centers that train and run AI models consume vast amounts of electricity, and nuclear energy offers a reliable, carbon-free source. Microsoft, Nvidia, and OpenAI haveurgedthe White House to deliver a so-called “energy New Deal” that would allocate hundreds of billions of dollars to subsidize new power plants.\n\nWhy it matters:The fact that tech giants are investing directly in nuclear power plants indicates the high stakes of competition in AI. Economistsestimatethat data centers that process AI, among other workloads, will consume more than 1,000 terawatt-hours of electricity by 2026, more than double the amount they consumed in 2022. Nuclear power could give them bountiful, carbon-free energy for decades to come.\n\nWe’re thinking:Fossil fuels like coal do tremendous damage to the environment, while renewables like solar and wind energy can’t fully meet the always-on demands of AI infrastructure. Next-generation reactor designs that improve safety and reduce costs are worth exploring. However, a significant obstacle remains: Few countries have a certifiably safe repository for long-term disposal of highly radioactive spent fuel. U.S. efforts toward this goal arestalled.\n\nOnce hailed by OpenAI chief Sam Altman as the “best bromance in tech,” the partnership between Microsoft and OpenAI is facing challenges as both companies seek greater independence.\n\nWhat’s new:Sources inside Microsoft and OpenAIrevealedthat both companies are working to reduce their reliance on the other, according toThe New York Times. Their collaboration, which brought both companies great rewards, is now complicated by demands for resources, friction between leaders, and partnerships with other companies.\n\nHow it works:In a series of deals that started in 2019, Microsoftinvesteda total of $13 billion in OpenAI, giving the startup access to Microsoft’s processing infrastructure and Microsoft special access to OpenAI’s models (which it integrated into its own applications), a large cut of its revenue, and potential equity. Microsoftbuilta 10,000-GPU system on Azure for training OpenAI models. But OpenAI sought to renegotiate its agreements, while Microsoft continued to develop its own AI capabilities.\n\nBehind the news:OpenAI’s valuationsoaredto $157 billion with new funding from Nvidia and other investors following a period of mounting financialpressure. The increased valuation gives OpenAI new power in its relationship with Microsoft. Moreover Microsoft holds no seats on its nonprofit board of directors, which limits its influence over strategic decisions at OpenAI despite its significant financial stake in the startup’s for-profit wing.\n\nWhy it matters:The Microsoft-OpenAI partnership has reshaped the AI landscape, and shifts in their partnership have an outsized impact on a wide range of research and product development. Their evolving relationship illustrates the challenge of sustaining a close collaboration amid rapidly changing technology. Microsoft provided vital resources that helped OpenAI scale up, while OpenAI’s models enabled Microsoft to keep rivals off-balance as it reinvented products including Bing, Windows, Office, Azure, and its expanding line of Copilots. However, facing fierce competition, both companies need ample flexibility to innovate and adapt.\n\nWe’re thinking:Together and separately, Microsoft and OpenAI have done tremendous work to advance the field from research to applications. We hope they can strike a balance that maintains their partnership and fuels their growth.\n\nMistral AI launched two models that raise the bar for language models with 8 billion or fewer parameters, small enough to run on many edge devices.\n\nWhat’s new:Ministral 3B and Ministral 8B, which come in base and instruction-tuned versions, outperform Google’s and Meta’s similar-sized models on several measures of knowledge retrieval, common-sense reasoning, and multilingual understanding. Ministral 8B-Instruct is free todownloadand use for noncommercial purposes, and commercial licenses are negotiable for this model and the others in the family. Accessed via Mistral’s APIs, Ministral 3B costs $0.04 per million tokens of input and output, and Ministral 8B costs $0.10 per million tokens of input and output.\n\nHow it works:The Ministral family can process 131,072 tokens of input context. The models are built to support function calling natively to interact, for example, with external APIs that fetch real-time weather data or control smart-home devices.\n\nBehind the news:Headquartered in France, Mistral AI competes head-to-head in AI with U.S. tech giants. It released its first model, Mistral 7B, a year ago under an Apache open source license. Since then, it has released model weights under a range of licenses while exploring alternative architectures such as mixture-of-experts and mamba. It also offers closedmodelsthat are larger and/or built for specialized tasks like code generation and image processing.\n\nWhy it matters:Edge devices can play a crucial role in applications that require fast response, high privacy and security, and/or operation in the absence of internet connectivity. This is particularly important for autonomous and smart home devices where uninterrupted, rapid processing is critical. In addition, smaller models like Ministral 8B-Instruct enable developers and hobbyists to run advanced AI on consumer-grade hardware, lowering costs and broadening access to the technology.\n\nWe’re thinking:Mistral’s new models underscore the growing relevance of edge computing to AI’s future. They could prove to be affordable and adaptable alternatives to Apple and Google’s built-in models on smartphones and laptops.\n\nResearchers devised a way to cut the cost of training video generators. They used it to build a competitive open source text-to-video model and promised to release the training code.\n\nWhat’s new:Yang Jin and colleagues at Peking University, Kuaishou Technology, and Beijing University of Posts and Telecommunications proposedPyramidal Flow Matching, a method that reduced the amount of processing required to train video generators. They offer thecodeand apretrained modelthat’sfreefor noncommercial uses and for commercial uses by developers who make less than $1 million in annual revenue.\n\nKey insight:Models that generate output by starting with noise and removing it over several steps, such as diffusion and flow matching models, typically learn by removing noise from an embedding to which noise was added. Starting with a downsampled (smaller) version of the embedding and then upsampling (enlarging) it gradually throughout the process, hitting the full size near the end, saves processing during training and inference.\n\nHow it works:The authors’ system comprises a pretrainedSD3 Mediumimage generator, an image autoencoder, and two pretrained text encoders:T5andCLIP. They pretrained the autoencoder to reconstruct images and sequences of video frames, and trained SD3 Medium to remove noise from an embedding of eight video frames given both text embeddings and embeddings of previous sequences of frames. The training sets includedWebVid-10M,OpenVid-1M, andOpen-Sora Plan. The authors modified the typical process of removing noise from image embeddings in two ways: spatially and temporally.\n\nResults:The authors compared their model to other open and closed models using VBench, a suite of benchmarks for comparing the quality of generated video. They also conducted a survey of human preferences. On VBench, their model outperformed other open models but slightly underperformed the best proprietary models, such as Kling. Human evaluators rated their model as superior to Open-Sora 1.2 for esthetics, motion, and adherence to prompts, and better than Kling for esthetics and adherence to prompts (but not motion). Furthermore, running on an Nvidia A100 GPU, their model took 20,700 hours to learn to generate videos up to 241 frames long. Running on a faster Nvidia H100 GPU, Open-Sora 1.2 took 37,800 hours to learn to generate 97 frames.\n\nWhy it matters:Video generation is a burgeoning field that consumes enormous amounts of processing. A simple way to reduce processing could help it scale to more users.\n\nWe’re thinking:Hollywood isinterestedin video generation. Studios reportedly are considering using the technology in pre- and post-production. Innovations that make it more compute-efficient will bring it closer to production.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--23-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/10/The-Batch-ads-and-exclusive-banners---2024-10-22T102129.757.png",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--21-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--24-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--21-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--22-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-196/",
    "title": "issue 196",
    "date": "",
    "reading_time": "",
    "content": "There are many great applications to be built on top of large language models, and the overhead of doing so may be lower than you think. Sometimes, I've spent all day on a weekend developing ideas only to find that I've spent less than $0.50.\n\nGiven the low cost of keeping me busy all day, It might not surprise you to find that the cost of scaling up a business based on a large language model (LLM) can be quite inexpensive. As a back-of-the-envelope calculation, let’s say:\n\nThen it costs around $0.08 to generate enough text to keep someone busy for an hour.\n\nHere are some ways to think about this when it comes to automating or assisting a person’s work task:\n\nOn the flip side:\n\nPlease don’t use my back-of-the-envelope calculation for any significant business decisions, and do carry out your own calculations with careful assumptions specific to your project. But if you haven’t stepped through such a calculation before, the takeaway is that LLMs are actually quite inexpensive to use.\n\nGranted, some models (like one version of GPT-4, at 15-30x the cost used in the calculation, leading to a cost of $1.80 instead of $0.08) are much more expensive. If your application requires a more capable model, then the calculation does change. But I’m optimistic that prices will come down over time, and these are all wonderful tools to have in your toolbox.\n\nKeep learning!\n\nAndrew\n\nP.S. I’ve noticed that most  LLM providers don’t have transparent pricing. If you work at an LLM provider, I hope you’ll consider urging your company to list prices on its website.\n\nLarge language models may soon help military analysts and commanders make decisions on the battlefield.What’s new:Palantir, a data-analytics company that serves customers in the military, intelligence, and law enforcement,demonstratedits chat-drivenArtificial Intelligence Platform(AIP) performing tasks like identifying enemies in satellite imagery, deploying surveillance drones, and proposing battle plans.How it works:In the demonstration, an intelligence analyst uses AIP to react to a fictional scenario. The system integrates large language models includingDolly-v2-12b(12 billion parameters),Flan-T5XL(3 billion), andGPT-NeoX-20B(20 billion) fine-tuned on an unspecified dataset.\n\nBehind the news:Military forces are experimenting with AI for executing combat tactics.\n\nWhy it matters:At its best, this system could help military authorities identify threats sooner and streamline their responses, enabling them to outmaneuver their enemies. On the other hand, it represents a significant step toward automated warfare.We’re thinking:This system takes the critical question of safety in AI systems to a new, terrifying level. Human battlefield analysts manage complex variables: terrain, weather, local customs, capabilities and limitations of friendly and enemy forces. This is crucial work. Delegating that work to a chatbot is a worrisome prospect considering the current state of large language models, which hallucinate falsehoods, confidently provide unworkable directions, and fail at basic math — especially smaller chatbots, like those used in this system.\n\nReporters offered a behind-the-scenes look at OpenAI’s year-long effort to capitalize on its long-awaited GPT-4.\n\nWhat’s new:The company built a sales team and courted corporate partners in advance of launching its latest large language model,The Informationreported.How it works:OpenAI hired a head of sales only last June, four years after shifting from nonprofit to for-profit. She and her team began signing up corporate customers soon after.\n\nPath to profit:In 2015, OpenAI started as a nonprofit research lab dedicated to transparency. In 2019, it launched a profit-seeking subsidiary to fund its research. In a series of deals between 2019 and 2023, Microsoft invested upward of $13 billion in exchange for 49 percent of OpenAI’s profit and right of first refusal to commercialize its technology.\n\nYes, but:Observers have criticized both the company’spivotto profit and itsshiftaway from transparency. In a March interview, OpenAI’s co-founder Ilya Sutskeverdefendedthe organization’s secrecy, claiming it was necessary for safety as AI becomes more powerful.\n\nWhy it matters:OpenAI saw generative AI’s commercial potential before ChatGPT sparked investments around the globe. That foresight could pay off handsomely, as the companyforecastedrevenue of $200 million this year and $1 billion by 2024.We’re thinking:OpenAI is building revolutionary technology that benefits hundreds of millions of users. We’re glad to see it on a path to financial sustainability.\n\nAre you ready to leverage AI for projects that can make a positive impact on public health, climate change, and disaster management?Pre-enroll in AI for Good and learn how!\n\nSpecialized chatbots are providing answers to scientific questions.\n\nWhat’s new:A new breed of search engines including Consensus, Elicit, and Scite use large language models to enable scientific researchers to find and summarize significant publications,Naturereported.\n\nHow it works:The models answer text questions by retrieving information from databases of peer-reviewed scientific research.\n\nYes, but:These tools may struggle with sensitive or fast-moving fields. For example, in response to the question, “Do vaccines cause autism?”, pediatrician Meghan Azad at the University of Manitoba found that Consensus returned a paper that focused on public opinion rather than scientific research. Clémentine Fourrier, who evaluates language models at HuggingFace, said that searching for machine learning papers via Elicit often brought up obsolete results.\n\nWhy it matters:Search engines that rank and summarize relevant research can save untold hours for scientists, students, and seekers of knowledge in general. With continued improvement, they stand to accelerate the pace of progress.We’re thinking:These systems show promise and point in an exciting direction. When search was young, search engines that covered the web (like Google) competed with vertical search engines that covered niches such as retail (Amazon) or travel (Expedia). A similar competition is shaping up between general-purpose chatbots and vertical chatbots.\n\nAsked to produce “a landscape by Thomas Kinkade,” a text-to-image generator fine-tuned on the pastoral painter’s work can mimic his style in seconds, often for pennies. A new technique aims to make it harder for algorithms to mimic an artist’s style.\n\nWhat’s new:Shawn Shan and colleagues at University of Chicago unveiledGlaze, a tool that imperceptibly alters works of art to prevent machine learning models from learning the artist's style from them. You can download ithere.\n\nKey insight:Art style depends on many factors (color, shape, form, space, texture, and others). Some styles tend not to blend easily. For instance, a portrait can’t show both the sharp edges of a photograph and the oil-paint strokes of Vincent Van Gogh. Trained models have encountered few, if any, such blends, so they tend not to be able to mimic them accurately. But the ability of text-to-image generators to translate images into a different style (by prompting them with words like “. . . in the style of Van Gough”) makes it possible to alter a photorealistic portrait imperceptibly to make some pixels more like an oil painting (or vice-versa). Fine-tuned on such alterations, a text-to-image generator that’s prompted to imitate them will produce an incoherent blend that differs notably from the original style.\n\nHow it works:Glaze makes an artist’s images more similar to images of a very different style. The difference derails image generators while being imperceptible to the human eye.\n\nResults:The authors fine-tuned Stable Diffusion on Glaze-modified works by 13 artists of various styles and historical periods. Roughly 1,100 artists evaluated groups of four original and four mimicked works and rated how well Glaze protected an artist’s style (that is, how poorly Stable Diffusion mimicked the artist). 93.3 percent of evaluators found that Glaze successfully protected the style, while 4.6 percent judged that a separate Stable Diffusion fine-tuned on unmodified art was protective.\n\nYes, but:It’s an open question whether Glaze works regardless of the combination of models used to produce embeddings, perform style transfer, and generate images. The authors’ tests were limited in this regard.\n\nWhy it matters:As AI extends its reach into the arts, copyright law doesn’t yet address the use of creative works to train AI systems. Glaze enables artists to have a greater say in how their works can be used — by Stable Diffusion, at least.\n\nWe’re thinking:While technology can give artists some measure of protection against stylistic appropriation by AI models, ultimately society at large must resolve questions about what is and isn't fair. Thoughtful regulation would be better than a cat-and-mouse game between artists and developers.\n\nWendy's is testing an AI-powered chatbot for drive-thru ordersThe tool will leverage Google's natural-language models in a pilot program set to launch in June at a Wendy's location in Ohio. (The Wall Street Journal)\n\nHollywood writers on strike demand protection against generative AIAlong with calls for higher pay, the Writers Guild of America is asking studios to commit to regulations that limit the automation of script writing.  (Vice)\n\nIBM plans to pause hiring for jobs that could be replaced by AIArvind Krishna, the company’s CEO, stated that around 7,800 of its non-customer-facing roles will be either fully or semi-automated in the coming years. (Bloomberg)\n\nResearch: Machine learning could help cities meet their emissions goalsDrexel University researchers developed  models that predict how zoning changes in Philadelphia could change energy consumption through 2045. (Drexel University)\n\nA U.S. bill would require political admakers to disclose when they use AI-generated content.The proposed legislation, which came days after the Republican Party released a commercial featuring AI-made visuals, would mandate that ads using AI include a text or audio disclaimer.. (The Washington Post)\n\nAI startup Anthropic developed a chatbot with a built-in moral compassThe company aims to prove that their method of instilling chatbots with pre-defined principles, called “constitutional AI”, is better at steering behavior than reinforcement learning from human feedback (RLHF). (The Verge)\n\nMeta released an open source multimodal model that processes six types of dataMeta said the model, called ImageBind, aims to mimic human learning and paves the way for multisensory content in future AI applications. (The Verge)\n\nMicrosoft is testing a ChatGPT alternative to keep data privateAzure cloud will reportedly offer a version of ChatGPT that runs on dedicated servers with protected data storage. (The Information)\n\nThe U.S. government is inspecting how companies use AI to monitor workersThe Biden administration released a request for information to gather data on how these tools are used, and what measures can be taken to protect the rights and safety of employees. (Nextgov)\n\nAn open source chatbot runs on smartphone hardwareMLC LLM is a project that can run locally on almost any device, including an iPhone or an old PC laptop with integrated graphics. (Tom’s Hardware)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/05/ezgif.com-optimize--16--1.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/05/unnamed--61-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/05/unnamed--17-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/05/The-Batch-ads-and-exclusive-banners--17-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/05/Untitled-design--3--1.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/05/unnamed--63-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-105/",
    "title": "issue 105",
    "date": "",
    "reading_time": "",
    "content": "Say you’ve trained a learning algorithm and found that it works well on many examples but performs poorly on a particular subset, or slice, of the data. What can you do?It is hard to tweak a learning algorithm’s code to improve its performance specifically on one slice of the data. Often, tuning an algorithm changes its performance on everything.But you can engineer the training and test data for that subset. A data-centric approach to AI development is a powerful tool to improve model performance on one slice, hopefully without degrading its performance on other portions of the data.The need to improve performance on one slice is a common one. For example:\n\nImproving the data is sometimes misunderstood as a pre-processing step performed prior to engineering a machine learning algorithm. Instead, it should be a key step in the iterative loop of model development, in which data is engineered systematically to address problems identified through error analysis.\n\nSpecifically, if error analysis identifies a slice of data that yields subpar performance, you might improve the data by:\n\nRather than applying these techniques to all the data — which would be costly and inefficient — you can focus on improving the label quality(y)and/or getting new training examples(x)in the slice you want to improve. This is a much less costly exercise.\n\nData-centric AI development is especially powerful in the current era of large neural networks. A decade ago, when models were much smaller, adding data in one place would often hurt performance elsewhere. For example, adding data on monochromatic objects might make it hard for an algorithm to recognize other objects if it doesn’t have enough capacity to recognize both types equally well.\n\nThere are situations in which adding data can hurt, but for many unstructured data problems (vision, speech, language), as long as the added data is clean and the learning algorithm is large enough, it's possible to add data in a way that improves performance on one slice without hurting performance on others. You’ll find a more nuanced discussion of this topichere.I also spoke about using data-centric AI development techniques to reduce bias in learning algorithms during DeepLearning.AI’s panel discussion last week. You can watch a recordinghere.\n\nKeep learning!Andrew\n\nApple, which has made a point of its commitment to user privacy, announced that it will scan iPhones for evidence of child abuse.What’s new:The tech giant will include a machine learning model on the device torecognizepornographic images of children stored in the photo library. Privacy advocates said the feature could be used to spy on innocent people.How it works:When a user uploads a photo from their phone to iCloud, a tool calledneural matchwill scan it for known examples of child pornography.\n\nBehind the news:Apple’s CEO Tim Cook has called privacy a “fundamental human right,” and the companyboaststhat its users have the final say over uses of their data.\n\nWhy it matters:Apple has been a holdout for privacy amid a tech-industry gold rush for user data. Its decision to budge on this issue suggests an inevitable, broader shift away from protecting individuals and toward making society more safe.We’re thinking:Child abuse is a global problem, and tech companies including Facebook, Google, Microsoft, and others havebanded togetherto fight it. While we support this effort, we worry about the possibility — perhaps driven by government pressure — that scanning photo libraries could turn into scanning other types of content, and that aim of keeping children safe could veer toward less laudable goals.\n\nOne of the world’s largest makers of farm equipment is doubling down on self-driving tractors.What’s new:John Deereagreed to pay$250 million for Bear Flag Robotics, a California startup that upgrades conventional tractors for full autonomy.How it works:Deere has offered GPS-enabled tractor guidance systems that aid a human driver for nearly two decades. Bear Flag has adapted self-driving technology developed by the automotive industry to help tractors roam agricultural fields safely without a driver.\n\nThe system learns the boundaries of a farmer’s property during an initial drive-through. It also identifies roads, waterways, and other obstacles. It can upload the resulting map to a fleet of tractors for remote control and monitoring.Behind the news:Deere has been pursuing AI capabilities for several years. In 2017, itacquiredBlue River Technology, a California-based startup that makes weed-killing robots. The following year, it launched a program to partner with promisingstartupsincluding some that use deep learning.Why it matters:In addition to helping the farmers deal with a long-runninglabor shortage, AI-driven equipment could help increase their productivity and limit environmental impacts such as pesticide runoff.We’re thinking:Self-driving cars aren’t yet commonly used on public roads, but the technology appears to be good enough for commercial use in constrained environments like farms.\n\nCheck out ourPractical Data Science Specialization! This series of courses will help you develop the practical skills to deploy data science projects and teach you how to overcome challenges using Amazon SageMaker.\n\nAn algorithm received a patent for its invention.What’s new:South Africa’s intellectual property officeissueda patent that names an AI system as the inventor of a food container with unique properties,IP Watchdogreported.How it works:South Africa’s Companies and Intellectual Property Commission namedStephen Thaler, who developed the AI system, called Dabus, as the patent owner. Thaler submitted a number ofapplicationsto authorities in several countries with help from theArtificial Inventor Project(AIP), an organization of patent attorneys that aims to promote development of algorithms that generate valuable innovations.\n\nBehind the news:South Africa has issued numerousupdatesto its patent policy in recent years to encourage technological innovation.Why it matters:This patent could set a significant precedent in the ongoingdebateabout whether and to what extent an algorithm can be considered the creator of new music, images, and other intellectual properties — a debate with potentially significant financial and legal implications.We’re thinking:The patent system has been criticized for enabling patent trolls who file or acquire patents for the purpose of litigating rather than advancing a technology or putting a new invention to work. If AI systems can file patents at scale, the whole system might need rethinking to incentivize useful innovation.\n\nThe previous state-of-the-art model for playing vintage Atari games took advantage of a number of advances in reinforcement learning (RL). The new champion is a basic RL architecture plus a trick borrowed from image generation.\n\nWhat’s new:A team led by Florin Gogianu, Tudor Berariu, and colleaguesfoundthat spectral normalization, a technique that limits the degree of variation between representations of similar inputs, improved an RL model’s performance more than several recent innovations combined. The team included researchers at Bitdefender, Deepmind, Imperial College London, Technical University of Cluj-Napoca, and University College London.\n\nKey insight:In reinforcement learning, a model observes its environment (say, the Atari gamePong), chooses an action based on its observation (such as moving the paddle), and receives a reward for a desirable outcome (like scoring a point). Learning in this way can be difficult because, as a model selects different actions, its training data (observations and rewards) change. Mutable training data poses a similar problem for generative adversarial networks (GANs), where generator and discriminator networks influence each other even as they themselves change. Spectral normalization has beenshownto help GANs learn by moderating these changes. It could also be beneficial in reinforcement learning.\n\nHow it works:The authors added spectral normalization to aC51, a convolutional neural network designed for reinforcement learning. The authors trained their model on tasks in theArcade Learning Environment, a selection of games in which the actions are valid Atari controller movements.\n\nResults:Using spectral normalization on every layer impeded performance, but using it on only the second-to-last layer led the model to achieve a higher median reward. The authors compared their C51 with spectral normalization on the second-to-last layer againstRainbow, the previous state of the art, which outfits a C51 with a variety of RL techniques. In 54 Atari games, the authors’ approach achieved a 248.45 median reward, outperforming Rainbow’s 227.05 median reward.\n\nWhy it matters:Applying techniques from one area of machine learning, such as GANs, to a superficially different area, such as RL, can be surprisingly fruitful! In this case, it opens the door to much simpler RL models and perhaps opportunities to improve existing techniques.\n\nWe’re thinking:People who have expertise in multiple disciplines can be exceptionally creative, spotting opportunities for cross-fertilization among disparate fields. AI is now big enough to offer a cornucopia of opportunities for such interdisciplinary insight.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/08/Screen-Shot-2021-08-17-at-7.17.33-PM-copy-1.png",
      "https://dl-staging-website.ghost.io/content/images/2021/08/Apple.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/08/ezgif.com-gif-maker--4--2.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/08/Specialization-Name--2-.png",
      "https://dl-staging-website.ghost.io/content/images/2021/08/patent-with-highlights-2.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/08/SPECTRAL-1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-42/",
    "title": "issue 42",
    "date": "",
    "reading_time": "",
    "content": "Like many of you, I’m deeply saddened by the events of the past week. I’m horrified by the senseless violence perpetrated against Black communities and appalled by the persistent racial injustice of our society. It’s long past time to right these terrible wrongs.\n\nThe tragic deaths of George Floyd, Ahmaud Arbery, Breonna Taylor, Sean Reed, and innumerable others remind us that life is precious, and that we have much more work to do to build an inclusive society. Minority voices are often marginalized, and that creates a responsibility for the rest of us to keep our ears and minds open, and add our voices to theirs when the occasion calls.\n\nThe AI community itself has a diversity problem. The number of Black people in the field is vanishingly small. A narrow perspective can lead to severely flawed work if we overlook factors like skin color when we collect and annotate datasets or validate results. Without diverse teams, instead of building AI systems that help a cross section of people, we open doors for some while locking out others.\n\nLack of diversity in the AI community has another effect: It reinforces the belief, often unconscious, that certain people can’t make important contributions to the field. We need to fight this sort of bias as well.\n\nIf you are Black and working in AI, we would like to know about your experiences in the field. If you have Black colleagues whom you admire, please let us know about them as well. We hope to share some of your stories. Please write to us at[email protected].\n\nMaybe I’m naive, but the protests this time do feel different, and I’m cautiously optimistic that this may be the time when we finally make a huge dent in racism. As members of the AI community, let us join this movement, condemn racism everywhere we see it, and settle for nothing less than a fair and inclusive world.\n\nKeep learning!\n\nAndrew\n\nFacebook’s leadership has thwarted changes in its algorithms aimed at making the site less polarizing, according to theWall Street Journal.What’s new:The social network’s own researchers determined that its AI software promotes divisive content. But the company’s management rejected or weakened proposed reforms, concerned that such changes might cut into profits or give the appearance of muzzling conservatives.Fizzled reforms:Facebook’s recommender system promotes posts from its most active users: those who do the most commenting, sharing, and liking. Internal investigations conducted between 2016 and 2018 showed that such so-called superusers disproportionately spread misinformation, much of it politically divisive. Internal committees proposed ways to address the issue, but the company ultimately made changes that blunted their potential impact.\n\nBehind the news:Conservatives in the U.S. have long accused social media platforms of left-wing bias, a charge to which Facebook has been particularlysensitive.\n\nFacebook’s response:“We’ve built a robust integrity team, strengthened our policies and practices to limit harmful content, and used research to understand our platform’s impact on society so we continue to improve,” the company said in a statement.Why it matters:The algorithms that govern popular social media platforms have an outsized influence on political discourse worldwide, contributing to polarization, unrest, and hate crimes. Divisive rhetoric distributed by Facebook has beenlinkedto violence in Sri Lanka, Myanmar, and India.We’re thinking:Social media is a double-edged sword. It has been helpful for quickly disseminating (mostly accurate) information about concerns like Covid-19. But what brings people together can also drive them apart. The AI community has a responsibility to craft algorithms that support a just society even as they promote business.\n\nNeuroscientists once thought they could train rats to navigate mazes by color. It turns out that rats don’t perceive colors at all. Instead, they rely on the distinct odors of different colors of paint. New work finds that neural networks are especially prone to this sort of misalignment between training goals and learning.What’s new:Robert Geirhos, Jörn-Henrik Jacobsen, and Claudios Michaelis led a study of neural network hiccups conducted by the University of Tübingen, Max Planck Research School for Intelligent Systems, and the University of Toronto. They argue that many of deep learning’s shortcomings revealshortcut learning.Key insight:Shortcuts are pathways to solving a problem that result in good performance on standard benchmarks but don’t require understanding of the problem and therefore don’t transfer well to real-world situations.How it works:The authors identify apparent causes of shortcut learning in neural networks, circumstances that tend to encourage it, and techniques available to discourage it.\n\nWhy it matters:The authors shed light on an issue that has troubled machine learning engineers for decades and highlight the lack of robustness of current algorithms. Addressing these issues will be key to scaling up practical neural network deployments.\n\nWe’re thinking:Humans also use shortcuts; we’ve all memorized formulas by rote instead of fully understanding them. Our misbehaving models may be more like us than we’d like to admit.\n\nA pioneer in dishwashing robots is reaching into commercial kitchens.What’s new:Dishcraft Robotics uses machines equipped with computer vision to scrub dirties for corporate food services and, soon, restaurants.How it works:Every morning, Dishcraft’s biodiesel-fueled trucks deliver clean dishes and utensils to corporate clients near its Silicon Valley hub. At the day’s end, the trucks retrieve them. Back at headquarters, workers load racks of dirty dishes and cutlery into an automated washing machine.\n\nBehind the news:Other robotics companies are also aiming to disrupt the kitchen.\n\nWhy it matters:Dishcraft estimates its system saves clients as much as 1.6 gallons of water per meal. Its plan to clean reusable to-go containers could keep tons of waste out of landfills.We’re thinking:Such machines also could mean fewer bodies in food-service kitchens — a plus in the Covid era but not so much for human staff who may find themselves out of a job.\n\nUse machine learning to estimate treatment effects on individual patients. Take the final course of the AI For Medicine Specialization, now available on Coursera.Enroll now\n\nThe pandemic has forced self-driving car companies off the road. Now they’re moving forward by refining their mountains of training data.What’s new:Self-driving cars typically collect real-world training data with two human operators onboard, but Covid-19 makes that unsafe at any speed. Instead, several companies are squeezing more value out of work they’ve already done, according toMIT Technology Review.\n\nWhat they’re doing:Makers of autonomous vehicles are relabeling old data and fine-tuning simulations.\n\nBehind the news:With little income, $1.6 million in average monthly overhead, and increasingly tight funding, autonomous vehicle companies are making tough choices. Lyft, Kodiak Robotics, and Ike havelaid off employees, whileZooxis looking for a buyer.Why it matters:Data can be a renewable resource: By adding new labels and sharpening old ones, AI teams can imbue old datasets with new life. Using refurbished datasets to improve simulations compounds the effect.We’re thinking:Development of self-driving cars had moved into the slow lane even before the pandemic. It’s better to keep making incremental progress than none at all.\n\nThe latest update of the acclaimed real-time object detector You Only Look Once is more accurate than ever.What’s new:Alexey Bochovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao at Taiwan’s Institute of Information Science Academia Sinica offerYOLOv4— the first version not to include the architecture’s original creators.Key insight:Rapid inference is YOLO’s claim to fame. The authors prioritized newer techniques that improve accuracy without impinging on speed (their so-called “bag of freebies”). In addition, they made improvements that boost accuracy at a minimal cost to speed (the “bag of specials”). All told, these tweaks enable the new version to outperform both its predecessor and high-accuracy competitors running at real-time frame rates.How it works:YOLO, as well as most object detectors since, tack a model that predicts bounding boxes and classes onto a pre-trained ImageNet feature extractor.\n\nResults:The authors pitted YOLOv4 against other object detectors that process at least 30 frames per second, using theCOCOimage dataset. YOLOv4 achieved 0.435 average precision (AP), running at 62 frames per second (FPS). It achieved 0.41 AP at its maximum rate of 96 FPS. The previous state of the art,EfficientDet, achieved 0.43 AP running at nearly 42 FPS and 0.333 AP at its top speed of 62 FPS.Why it matters:YOLOv4 locates and classifies objects faster thanmeasurementsof human performance. While it’s not as accurate as slower networks such as EfficientDet, the new version boosts accuracy without sacrificing speed.We’re thinking: You only look once . . . twice . . . thrice . . . four times and counting!\n\nCovid-19 has cost many workers their livelihood, but it has provided a lucky few on the lowest rungs of Africa’s machine learning industry with luxury suites.What’s new:Samasource, a data labeling company headquartered in San Francisco, California, is housing its East African workforce in hotels and resorts so they can continue to work while maintaining social distance,Wiredreports.How it works:The pandemic prompted strict lockdowns in Kenya and Uganda, where Samasource employs some 2,000 workers. Many live in communities with no internet connectivity. So the company put up its workforce in four internet-equipped hotels that were vacant amid the coronavirus-driven collapse of tourism.\n\nBehind the news:Several companies are providing jobs that help feed both the AI industry’s hunger for data and underserved communities.\n\nWhy it matters:Socially conscious outsourcing increases the tech industry’s talent pool by providing decent jobs to people who, because of geography, gender, race, or other factors, otherwise might be locked out.We’re thinking:The grocery industry’sFair Tradelabels help consumers distinguish between socially responsible employers and their wage-slashing competitors. A similar measure for AI would foster both growth and diversity.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter20ASPECT207.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/FACEBOOK.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/SHORTCUT.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Dishes2-optimized.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AI4MC3BatchAd.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Drive1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/YOLO.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AFRICA.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-92/",
    "title": "issue 92",
    "date": "",
    "reading_time": "",
    "content": "I decided last weekend not to use a learning algorithm. Sometimes, a non-machine learning method works best.Now that my daughter is a little over two years old and highly mobile, I want to make sure the baby gate that keeps her away from the stairs is always shut. It’s easy to forget and leave it open when walking through. How do you do this?I started designing a system where I’d collect images of the gate both open and shut, and train a neural network to distinguish between the two. Then I would useTensorRTto deploy the model on a Raspberry Pi computer, which would beep if the gate were left open for more than 60 seconds.\n\nI got as far as wiring up the system. Then I found arefrigerator-door alert widgetthat does the same job by sensing when a magnet is separated from a detector.\n\nIt goes to show that sometimes you don’t need a big neural network to do the job. (But when you do need one, it’s handy.) That’s why it’s nice to have a portfolio of techniques. Then we can better pick the right one for a given job.\n\nPerhaps one lesson here is to pick the right sensor: To do the job with a camera, I needed a computer vision algorithm. But with a magnetic sensor, making the decision to beep when the gate is left open becomes trivial.\n\nKeep learning!\n\nAndrew\n\nSurgical robots perform millions of delicate operations annually under human control. Now they’re getting ready to operate on their own.\n\nWhat’s new:Researchers at UC Berkeley, UC San Francisco, and SRI International trained a machine learning system to pilot ada Vincitwo-armed surgical robot through a task that tested its dexterity, precision, and speed,The New York Timesreported.\n\nHow it works:The system learned via imitation learning to lift tiny plastic rings off a pegboard, pass them from one claw to the other, and slide them onto different pegs. The task is a exercise for surgeons learning to perform laparoscopic procedures, in which a camera and other specialized instruments are inserted into the patient’s body through a small incision.\n\nBehind the news:AI already assists physicians in a few small but important procedures. For instance, a robotic tool from the Dutch companyMicrosure, which helps suture tiny incisions on blood vessels, uses AI to stabilize shaking in the operator’s hands.\n\nWhy it matters:This is a nice example of an algorithm that handles concept drift in robotic control. A lot of work in model-based reinforcement learning assumes a fixed model. But just as the dynamics of a human arm change as the arm tires — and a surgeon must adapt to control that tiring arm — we want learning algorithms to adapt to gradual changes in the robot’s dynamics.\n\nWe’re thinking:We’re looking to AI systems that help optimizenutrition,exercise, andsleepto help steer us clear of AI systems that wield a scalpel!\n\nA hybrid of deep learning and symbolic AI took the prize at a major puzzle competition.\n\nWhat’s new:A system called Dr. Fill outscored nearly 1,300 human contestants at April’s annualAmerican Crossword Puzzle Tournament,Slatereported.\n\nHow it works:Oregon polymathMatt Ginsbergdebuted a logic-based system at the tournament in 2012, taking 11th place. This year, Ginsberg paired his model with a neural crossword solver developed by at UC Berkeley.\n\nBehind the news:Founded in 1978, the American Crossword Puzzle Tournament requires competitors to complete eight puzzles in two days. The three fastest and most accurate competitors face off on a final puzzle to vie for the $3,000 grand prize.\n\nWhy it matters:Neural networks and symbolic systems are often seen as competing approaches. Together, they can help solve previously elusive problems.\n\nWe’re thinking:What’s a 12-letter catchphrase that describes a persistent attitude toward gaining knowledge and skills?\n\nThe first two courses in ourMachine Learning Engineering for Production (MLOps) Specializationare live on Coursera!Enroll now\n\nIdeally, real-time 3D applications such as virtual and augmented reality transition smoothly between different viewpoints of a scene — but generating a fresh perspective can take time. New research speeds the process.\n\nWhat’s new:Stephan Garbin and colleagues at Microsoft developedFastNeRF, a system that accelerates the photorealistic 3D rendering method known asNeural Radiance Fields(NeRF) to visualize scenes from any angle at a brisk 200 frames per second.\n\nKey insight:To visualize one frame of a 3D scene, you need to know the position of a virtual camera and the directions of a set of virtual light rays that extend from the camera through each pixel in the frame. (The objects behind the pixels have a basic color that may be modified by lights, shadows, occlusion, and transparency.) NeRF computes a pixel’s color by combining the color/transparency of all points that lie along the associated ray, which requires hundreds of neural network inferences — tough to pull off in real time. FastNeRF manages the computational burden through a two-part workaround. First, rather than calculating on the fly, it pre-computes and stores information about all possible rays and points along them. Second, to avoid having to store every possible combination of ray and point (1,0243 * 1,0242 values, assuming 1,024 samples per spatial dimension), it stores each point’s basic color and transparency based on its position, and the shift in its color due to a ray’s direction (1,0243 + 1,0242 values).\n\nHow it works:FastNeRF uses two vanilla neural networks to compute information based on a point’s position (the position network) and a ray’s direction (direction network). The authors trained the system onSynthetic NeRF, which contains 360-degree views of real-world objects like model ships and LEGO constructions, and frontal views of objects inLocal Light Field Fusion.\n\nResults:Running on a high-end consumer graphs board, FastNeRF performed over 3,000 times faster than NeRF. For example, it rendered a scene of a LEGO tractor in 0.0056 seconds versus NeRF’s 17.46 seconds. Despite its speed, on Synthetic NeRF, FastNeRF achieved 29.97dB peak signal-to-noise ratio, which gauges how well a generated image reproduces the original (higher is better), versus NeRF’s 29.54dB.\n\nWhy it matters:The authors reduced an unmanageable quantity of high-dimensional data to a practical size by dividing the information based on point position and ray direction between two models. A similar approach could be useful in applications that require optimization over many input parameters, such as drug discovery and weather modeling.\n\nWe’re thinking:Augmented and virtual reality promise to bring powerful new approaches in education, entertainment, and industry — if we can make them cheap, easy, and fast enough. Deep learning is helping us get there.\n\nA combination of computer vision and drones could help restore dwindling killer whale populations.\n\nWhat’s new:Researchers at Oregon State University and conservation groupsSR3andVulcandeveloped asystemthat assesses the health of orcas,Geekwirereported.\n\nHow it works:The researchers fly drones off the coast of British Columbia and Washington State to capture video of orcas as they swim near the water’s surface. Four machine learning models collectively called Aquatic Mammal Photogrammetry Tool analyze the imagery.\n\nBehind the news:Conservationists are getting help from machine learning across the animal kingdom.\n\nWhy it matters:With detailed information about the health of individual creatures, conservationists can respond more quickly when they’re in trouble. The developers plan to open-source their work so it can be adapted to other populations of orcas and possibly other species of aquatic mammals.\n\nWe’re thinking:The Pacific Northwest orca population has shrunk to 75 individuals, the lowest number in 30 years. We hope for a rebound.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-04-20-at-9.35.26-AM-copy--1-.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-05-19T123452.995.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-05-19T123452.995.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/CROSSWORD600x338.gif?upscale=true&width=1200&upscale=true&name=CROSSWORD600x338.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20Image%201%20(1).png?upscale=true&width=1200&upscale=true&name=The%20Batch%20Image%201%20(1).png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/fastnerf-1.gif?upscale=true&width=1200&upscale=true&name=fastnerf-1.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/WHALES600x338.gif?upscale=true&width=1200&upscale=true&name=WHALES600x338.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-218/",
    "title": "issue 218",
    "date": "",
    "reading_time": "",
    "content": "Over the weekend, Hamas launched a surprise terrorist attack on Israel, slaughtering and kidnapping civilians. The images in the media are horrifying, and over 1,000 people have been murdered in Israel, including numerous children. Israel has retaliated by laying siege to and attacking the Gaza Strip.\n\nThe mounting civilian casualties on both sides are heartbreaking. My heart goes out to all individuals, families, and communities affected by the violence.\n\nWhile there is much to be said about rights and wrongs committed by all sides over the past 75 years, there is absolutely no excuse for deliberately targeting civilians or threatening to execute hostages. This is a time for all people of conscience to condemn these heinous acts. It is also time to call on everyone to respect human rights and the international rule of law.\n\nI hope the AI community can play a constructive role in preserving lives as well as promoting civil liberties and democracy. In this moment and in coming years, I hope we remain united as a community, keep pushing for human rights, and decry any violations thereof.\n\nAndrew\n\nFew people have had a chance to try out OpenAI’s GPT-4 with Vision (GPT-4V), but many of those who have played with it expressed excitement.What’s new:Users who had early access to the image-savvy update of GPT-4, which began a gradual rollout on September 24, flooded social media with initial experiments. Meanwhile, Microsoft researchers tested the model on a detailed taxonomy of language-vision tasks.\n\nFresh capabilities:Users on X (formerly Twitter) tried out the model in situations that required understanding an image's contents and contexts, reasoning over them, and generating appropriate responses.\n\nMicrosoft takes stock:Zhengyuan Yang and colleaguesprobedGPT-4V’s capabilities and evaluated prompting techniques in a wide variety of tasks that involve subtle interactions between images, words, and computer code. They reported only qualitative results — both positive and negative — leaving it to other researchers to compare the model’s performance with that of competitors likeLLaVA.\n\nYes, but:These qualitative examples are impressive, but they were cherry-picked to give only a glimpse of GPT-4V’s capabilities. Microsoft noted that the model’s behavior is inconsistent. It remains to be seen how reliably it can perform a given task.\n\nWhy it matters:GPT-4V is an early entry in a rising generation of large multimodal models that offer new ways to interact with text, images, and combinations of the two. It performs tasks that previously were the province of specialized systems, like object detection, face recognition, and optical character recognition. It can also adapt, alter, or translate images according to text or image prompts. The prospects for integration with image editors, design tools, coding tools, personal assistants, and a wide range of other applications are tantalizing.We’re thinking:When the text-only version of GPT-4 became available, OpenAI didn’t report quantitative results for a couple of weeks (and it still hasn’t presented a detailed view of its architecture and training). We look forward to a clearer picture of what GPT-4V can do.\n\nMeta is rolling out AI-powered upgrades to its social platforms.\n\nWhat’s new:Metaannounceda chat interface, image generator, and celebrity tie-ins for Facebook, Instagram, Messenger, and WhatsApp.\n\nHow it works:The new capabilities take advantage ofLLaMa 2and an unnamed image generator, presumablyCM3leon(pronounced “Chameleon”), which Meta described in July.\n\nBehind the news:Meta has lagged behind its big-tech peers in commercializing its AI research. Current and former Meta employeesblamedthe delay on factors including staff turnover, a shortage of high-end chips, a focus on research over products, and management’s lack of enthusiasm for large language models. Lately, the release of restricted open source models such asLlama 2has raised the company's profile as an AI powerhouse.\n\nWhy it matters:Social networking is a natural venue for generated text and images, from suggested language for social posts to pictures that reflect a user’s flight of fancy. Meta’s products include some of the most popular mobile apps, which givesnearly 4 billion usersaccess to AI with a mass-media twist.We’re thinking:Chatbots that look and talk like celebrities are an interesting concept, but users need to know they’re not chatting with a real person. Meta’s celebrity bots bear a familiar likeness while making clear that it represents an artificial character — an intriguing solution. On the other hand, at least one of the company’s non-celebrity bots, whose faces are unfamiliar, has beencaughtinsisting it’s a human being.\n\nIn this short course, you’ll learn how to use the open source LangChain framework to build a chatbot that interacts with your business documents or other personal data.Enroll today for free\n\nJournalists are approaching text generators with cautious optimism, a new study shows.What’s new:Researchers at the London School of Economics and Political Sciencesurveyedworkers at over 100 news organizations worldwide. 85 percent of respondents said they had experimented with generative AI.How it works:The authors asked journalists, technologists, and managers how their newsrooms were using generative AI and how they felt about the technology.\n\nBehind the news:Publishers have been eager to take advantage of large language models, but the results so far have been mixed.\n\nWhy it matters:In a few short decades, journalism has suffered techno-shocks wrought by the web and social media. Generative AI is poised to bring a third wave of change and challenge, but journalists are generally confident that they can benefit from the technology.We’re thinking:We recentlydistinguishedbetween jobs and the tasks they comprise. While AI can perform some tasks at a human level, currently it rarely performs so well on all the tasks in a given job. We encourage publishers to adopt this framework and devise fruitful ways to allocatejournalists’ tasksamong human-only, machine-only, and human-plus-machine modes.\n\nHow accurate are machine learning models that were trained on data produced by other models? Researchers studied models that learned from data generated by models that learned from data generated by still other models.What’s new:Ilia Shumailov and Zakhar Shumaylov and colleagues at University of Oxford, University of Cambridge, Imperial College London, University of Toronto, Vector Institute, and University of Edinburgh argue — both theoretically and empirically — that models, when they’re trained almost exclusively on the output of earlier models,learn a distorted data distribution.Key insight:Trained models are less likely to generate types of examples that appear infrequently in their training data. Moreover, they don’t model their training data perfectly, so their output doesn’t quite match the distribution of the training dataset. They may combine elements from training examples. When one model learns from another in a series, errors accumulate — a phenomenon the authors call model collapse.How it works:The authors trained models of different types. First they trained a model on a human-collected and -curated dataset — generation 0. Then they trained generation 1 of the same architecture on the output of generation 0, generation 2 on the output of generation 1, and so on. In some cases, they replaced a fraction of the generated examples with examples from the original training set.\n\nResults:The first-generation GMM recognized the Gaussians as ellipses, but each successive generation degraded their shape. By generation 2,000, the shape had collapsed into a tiny region. Similarly, the late-generation VAEs reproduced MNIST digits less accurately; by generation 20, the output looked like a blend of all the digits. As for the OPT language models, generation 0 achieved 34 perplexity (which measures how unlikely the model is to reproduce text in the test set; lower is better). Trained only on generated data, successive generations showed decreasing performance; generation 9 achieved 53 perplexity. Trained on 10 percent original data, successive generations still performed worse, but not as badly; generation 9 achieved 37 perplexity.Yes, but:The authors’ recursive training process is a worse-case scenario, and generated data does have a place in training. For instance,Alpacasurpassed a pretrainedLLaMAby fine-tuning the latter on 52,000 examples produced by GPT-3.5.Why it matters:The advent of high-quality generative models gives engineers an option to train new models on the outputs of old models, which may be faster and cheaper than collecting a real-world dataset. But this practice, taken to extremes, can lead to less-capable models. Moreover, if models are trained on data scraped from the web, and if the web is increasingly populated by generated media, then those models likewise will become less capable over time.We’re thinking:To produce output that could be used for training without bringing on model collapse, a data generator would need access to sources of novel information. After all, humans, too, need fresh input to keep coming up with new ideas.\n\nBill Gates-backed startup Likewise joins the AI chatbot raceThe chatbot, called “Pix,” offers personalized recommendations for books, movies, TV shows, and podcasts. Pix uses OpenAI technology and consumer data to learn a user’s preferences over time. Pix updates its recommendations in real-time to align with the content offerings on popular streaming platforms like Netflix, Hulu, and Max. (The Wall Street Journal)\n\nOpenAI explores in-house chip productionThe company is reportedly contemplating potential acquisitions in order to develop its own AI chips. This would be a response to the global chip shortage and the high costs of keeping ChatGPT running. (Reuters)\n\nGlobal survey reveals AI’s impact on scientific researchThe survey, conducted by the scientific journalNature, found a complex landscape where scientists express both enthusiasm and concerns about the growing use of AI in research. Over half of the respondents believe that AI tools will be “very important” or “essential” to their fields within the next decade. However, they also express concerns regarding issues such as bias and fraud. (Nature)\n\nA search tool lets you find books used to train AI models without permissionThe dataset, called Books3, contains around 183,000 books with associated author information. Authors and readers can now search it to discover which titles are included in the dataset, which has been used to train AI products from Meta, Bloomberg, and others. The situation highlights the challenges and secrecy surrounding AI training practices. (The Atlantic)\n\nAdobe Firefly announces new features and enhancementsSince its launch in March 2023, Firefly has generated 3 billion images. Now it’s bringing new offerings like a Text to Vector Graphic for Adobe Illustrator, Generative Match to apply styles from a preselected set of images, and Prompt Suggestions to generate improved results. (Adobe)\n\nApp developer startup Docker introduces AI feature to its suite of developer toolsThe feature will help developers troubleshoot all aspects of their apps by providing context-specific, automated guidance when editing, debugging, or running tests. Docker argues that their tools expand AI assistance from source code to front-end development, databases, and other development technologies. (Docker)\n\nFrench startup Mistral AI launches open source large language modelMistral 7B, released under the Apache 2.0 license, claims to offer competitive capabilities at a lower computational cost than more costly models available only via APIs or the cloud. While the model is open for all to use, Mistral's business plan relies on offering commercial white-box solutions and dedicated enterprise deployment. (Mistral)\n\nA new report details the full costs of computing powerThe AI Now Institute’s document examines the hardware supply chain, how the demand for computing power is shaping AI development, and how governments and other policy makers are beginning to respond to computational bottlenecks. The report also explores possible regulatory interventions that might make AI companies more competitive and less dependent on a small number of computing providers. (AI Now)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-webp-to-jpg--18--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-optimize--1--Oct-11-2023-05-57-04-4038-PM-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/10/META-Characters-5_600px-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/10/The-Batch-ads-and-exclusive-banners--68-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-gif-maker--3-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/10/ezgif.com-webp-to-jpg--19-.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-115/",
    "title": "issue 115",
    "date": "",
    "reading_time": "",
    "content": "On Halloween, the veil lifts between the spirit and AI worlds, allowing the two to pass through one another. The resulting paranormal — or, as AI practitioners call it, paragaussian — phenomena raise questions like these:\n\nWhat do you call it when it takes repeated practice to make a scary jack-o’-lantern?A learning carve.\n\nResponsible AI requires being candid about what it can do. Who’s the best person to help with this?Dr. Frank-enstein.\n\nThe ghost of a machine learning engineer visited a museum and defaced all the paintings. Why?She was implementing image wreck-ognition.\n\nOn Halloween night, when kids in costume go from house to house and only get unpopped popcorn, what do you call it?Kernel trick, or treat.\n\nKeep spooking!Andrew\n\nP.S. When my daughter Nova was six months old, I bought her a panda stuffed animal. She liked it, and after many panda-related requests, guess what my Halloween costume is? The lesson for me is: Be careful what presents you give, lest they lead to panda-monium.\n\nThe days grow short, trees shed their leaves, and shadows loom in the failing light. Halloween is upon us, andonceagainwe’re beset by thoughts that all is not well in our world. We sense, lurking in the dusk, the presence of weaponized drones that attack on their own volition, disease-carrying models that breed like rats, algorithms that drive people mad with power. Let us step boldly into the darkness and lift a flaming PyTorch to light the way.\n\nTech companies generally try to be (or to appear to be) socially responsible. Would some rather let AI’s negative impacts slide?The fear:Companies with the know-how to apply AI at scale dominate the information economy. This gives them an overpowering incentive to release harmful products and services, jettison internal checks and balances, buy or lie their way out of regulations, and ignore the trail of damage in their wake.Horror stories:When you move fast and break things, things get broken.\n\nIs a corporate dystopia inevitable?So far, most government moves to regulate AI have been more bark than bite.\n\nFacing the fear:Some tech giants have demonstrated an inability to restrain themselves, strengthening arguments in favor of regulating AI. At the same time, AI companies themselves must publicly define acceptable impacts and establish regular independent audits to detect and mitigate harm. Ultimately, AI practitioners who build, deploy, and distribute the technology are responsible for ensuring that their work brings a substantial net benefit.\n\nWar is already bad enough. What happens when human combatants are replaced by machines?The fear:Autonomous weapons will become an inevitable aspect of warfare. AI that can’t reliably tell friend from foe will strike mistaken targets, kill civilians, and attack enemies who have surrendered. Systems trained to react to threats quickly will escalate conflicts. Humans won’t be held accountable for automated atrocities.Horror stories:While world leaders debate the ethics of fully autonomous weapons, killer robots are already on the march.\n\nQuivering in your (combat) boots?Efforts to automate weaponry have a longhistory. Lately, AI has found its way intocommand and controlsystems. It’s not too late to establish an international ban on autonomous weapons, but the door is closing fast.\n\nFacing the fear:Countries need ways to defend themselves. An effective ban on autonomous weapons must start with a clear line between what is and isn’t acceptable. Machine learning engineers should play a key role in drawing it.\n\nIs AI becoming inbred?\n\nThe fear:The best models increasingly are fine-tuned versions of a small number of so-called foundation models that were pretrained on immense quantities of data scraped from the web. The web is a repository of much that’s noble in humanity — but also much that’s lamentable including social biases, ignorance, and cruelty. Consequently, while the fine-tuned models may attain state-of-the-art performance, they also exhibit a penchant for prejudice, misinformation, pornography, violence, and other undesirable traits.Horror stories:Over 100 Stanford University researchers jointly published a paper thatoutlinessome of the many ways foundation models could cause problems in fine-tuned implementations.\n\nHow firm is the foundation?The Stanford paper stirred controversy as critics took issue with the authors’ definition of a foundation model and questioned the role of large, pretrained models in the future of AI. Stanford opened acenterto study the issue.Facing the fear:It’s not practical to expect every user of a foundation model to audit it fully for everything that might go wrong. We need research centers like Stanford’s — in both public and private institutions — to investigate the effects of AI systems, how harmful capabilities originate, and how they spread.\n\nDeepLearning.AI has updated theNatural Language Processing Specializationwith new and improved content. We partnered with Hugging Face to create lectures and labs to give you more hands-on experience with transformer models!Enroll now\n\nWhat if AI-enabled monitoring isn’t just for dictators and despots?The fear:Under the pretext of maintaining law and order, even countries founded on a commitment to individual rights allow police to take advantage ofsmart-city infrastructureandsmart-home devices. The ability to spy on citizens is rife with moral hazards and opens the door to authoritarian control.Horror stories:Law enforcement agencies worldwide have found AI-driven surveillance irresistible. Reports ofdealsbetween police and vendors portend further invasive practices to come.\n\nPanopticon now?Most Americansbelieve that, in the hands of law enforcement, face recognition will make society safer. Yet such systems are notoriously prone tomisuse,inaccuracy, andbias. Several U.S. cities and states have passed laws that restrict or ban police use of face recognition, and others are considering similar legislation. The European Parliament recently passed a nonbinding ban on the practice.Facing the fear:Society should guarantee basic rights to privacy. That said, the impulse to ban face recognition carries its own danger. Ceding AI development to repressive regimes risks a proliferation of systems that enable repressive uses. Instead, elected leaders should establish rules to ensure that such systems are transparent, auditable, explainable, and secure.\n\nIs human creativity being replaced by the synthetic equivalent?The fear:AI is cranking out increasingly sophisticated visual, musical, and literary works. AI-generated media will flood the market, squeezing out human artists and depriving the world of their creativity.Horror stories:The most compelling AI-generated art today requires people who curate a system’s inputs and outputs to ensure that automated creations have a recognizable aesthetic character. Tomorrow is up for grabs.\n\nThe end of art history?AI-generated art has edged its way into both fine-art and commercial worlds.\n\nFacing the fear:AI makes a wonderful complement to human creativity, producing variations, offering alternatives, or supplying a starting point for traditional artistic exploration. On the other hand, the best current models can produce output that, to an untrained eye or ear, comes close to human artworks. And they’re only going to get better.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/04/Andrew-TrickOrTreating-asPanda-4_600px.webp",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/INTRO.jpg?upscale=true&width=1200&upscale=true&name=INTRO.jpg",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Thumbzilla1-TheReturn_600x338px.jpg?upscale=true&width=1200&upscale=true&name=Thumbzilla1-TheReturn_600x338px.jpg",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/BatBombs4_600x338.jpg?upscale=true&width=1200&upscale=true&name=BatBombs4_600x338.jpg",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/DeepLearning_HalloweenFamilyPortrait2_600x338px.png?upscale=true&width=1200&upscale=true&name=DeepLearning_HalloweenFamilyPortrait2_600x338px.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20Image%202.png?upscale=true&width=1200&upscale=true&name=The%20Batch%20Image%202.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Halloween-Surveillance4.jpg?upscale=true&width=1200&upscale=true&name=Halloween-Surveillance4.jpg",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ArtWitch_Halloween2021_600x338px.jpg?upscale=true&width=1200&upscale=true&name=ArtWitch_Halloween2021_600x338px.jpg"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-78/",
    "title": "issue 78",
    "date": "",
    "reading_time": "",
    "content": "Over the last several decades, driven by a multitude of benchmarks, supervised learning algorithms have become really good at achieving high accuracy on test datasets. As valuable as this is, unfortunately maximizing average test set accuracy isn’t always enough.I’ve heard too many conversations like this:Machine learning engineer:It did well on the test set!Product manager:But it doesn’t work for my application.Machine learning engineer:But . . . It did well on the test set!\n\nWhat else is there?Robustness and generalization:In a production deployment, performance can degrade due to concept drift (where the function mapping from x->y changes; say, the model predicts housing prices y and inflation causes prices to rise) and data drift (where the input distribution changes). One important subset of data drift relates to performance on classes that are rare in or absent from the training set. For example, a speech recognition system may achieve high average accuracy despite poor performance on speakers with a British accent, because the training and test sets included few examples of British speakers. If the product takes off in the U.K. and a lot more British speakers jump in, its accuracy will plummet. A more robust system would fare better.\n\nPerformance on relatively important examples:Some examples are more important than others, and even if average test set accuracy is high, a system that performs poorly on important examples may be unacceptable. For example, users might forgive a search engine that doesn’t always return the best results to informational and transactional queries like “apple pie recipe” or “wireless data plan.” But when they enter a navigational query such as “stanford,” “youtube,” or “reddit,” they have a specific website in mind, and the search engine had better return the right URL or risk losing the user’s trust. In theory, weighting test examples according to their importance can address this issue, but it doesn’t always work in practice.Performance on key slices of data:Say a machine learning system predicts whether a prospective borrower will repay a loan, so as to decide whether to approve applications. Even if average accuracy is high, if the system is disproportionately inaccurate on applications by a specific minority group, we would be foolhardy to blindly deploy it. While the need to avoid bias toward particular groups of people is widely discussed, this issue applies in contexts beyond fairness to individuals. For example, if an ecommerce site recommends products, we wouldn’t want it to recommend products from large sellers exclusively and never products from small sellers. In this example, poor performance on important slices of the data — such as one ethnicity or one class of seller — can make a system unacceptable despite high average accuracy.My advice:If a product manager tells us that our AI system doesn’t work in their application, let’s recognize that our job isn’t only to achieve high average test accuracy — our job is to solve the problem at hand. To achieve this, we may need visualizations, larger datasets, more robust algorithms, performance audits, deployment processes like human-in-the-loop, and other tools.\n\nKeep learning!\n\nAndrew\n\nAmazon is monitoring its delivery drivers with in-vehicle cameras that alert supervisors to dangerous behavior.What’s new:The online retail giant rolled out a ceiling-mounted surveillance system that flags drivers who, say, read texts, fail to use seatbelts, exceed the speed limit, or ignore a stop sign,CNBCreported.How it works:The system, NetradyneDriveri, uses street-facing and in-cab cameras along with an accelerometer and gyroscope to spot 16 unsafe behaviors.\n\nYes, but:Some Amazon drivers said that the system violates their privacy and exacerbates pressure to meet the company’s aggressive delivery schedules.Behind the news:Amazon has expanded its force of local delivery drivers to more than400,000as of November. It has used a similar computer vision system fromSmartDriveto monitor its long-haul truckers for sleepiness and distraction. Delivery competitor United Parcel Service also has tested a system,Lytx DriveCam, that monitors drivers of its delivery vans.Why it matters:Investigations byBuzzFeedand theThe New York Timescharge that Amazon pressures drivers to make deliveries at a dangerously fast clip, resulting in numerous accidents and several deaths. While in-car surveillance is intrusive, proponents point out that it might help reduce human errors that can occur when people are under stress.We’re thinking:There are many ways that AI can enhance productivity and safety. Let’s make sure to do it in a way that’s empowering rather than dehumanizing.\n\nFor children, associating a word with a picture that illustrates it helps them learn the word’s meaning. New research aims to do something similar for machine learning models.What’s new:Hao Tan and Mohit Bansal at University of North Carolina Chapel Hill improved a BERT model’s performance on some language tasks by training it on a large dataset of image-word pairs, which they call visualized tokens, orvokens.Key insight:Images can illuminate word meanings, but current datasets that associate images with words have a small vocabulary relative to the corpuses typically used to train language models. However, these smaller datasets can be used to train a model to find correspondences between words and images. Then that model can find such pairings in separate, much larger datasets of images and words. The resulting pairings can help an established language model understand words better.How it works:The authors trained a system called the vokenizer to pair BERT-style tokens — generally individual words or characters — with related images. They used the resulting visualized tokens to train BERT to predict such pairings and fine-tuned it on various language tasks.\n\nResults:BERT pretrained with the token-image pairs outperformed the same architecture trained in the same way but without the pairs on tasks in GLUE, SQuAD, and SWAG. For instance, it achieved 92.2 percent accuracy onSST2, predicting the sentiment of movie reviews, compared to 89.3 percent for BERT without visual training. Similarly, onSQuAD v1.1, it achieved an F1 score of .867 on SQuAD compared to .853 for BERT without visual training.Why it matters:This work suggests the potential of visual learning to improve even best language models.We’re thinking:If associating words with images helps a model learn word meaning, why not sounds? Sonic tokens — sokens! — would pair, say, “horn” with the tone of a trumpet and “cat” with the sound of a meow.\n\nWestern governments and institutions struggling to formulate principles of algorithmic fairness tend to focus on issues like race and gender. A new study of AI in India found a different set of key issues.What’s new:Researchers at Googleintervieweddozens of activists, academic experts, and legal authorities about the ways AI is deployed in India, especially with respect to marginalized groups. In part, their goal was to demonstrate how Western notions of bias and power don’t always apply directly to other cultures.What they found:The report highlights three ways in which issues surrounding AI in India differ from Western countries and may call for different approaches to achieve fairness.\n\nBehind the news:Other groups have sought to highlight the outsized influence that Western notions of ethics have on AI worldwide.\n\nWhy it matters:Most research into AI fairness comes from a U.S.-centric perspective rooted in laws such as the Civil Rights Act of 1964, which outlaws discrimination based on race, sex, and religion. Guidelines based on a single country’s experience are bound to fall short elsewhere and may even be harmful.We’re thinking:Many former colonies struggle with legal and educational systems imposed by Western powers. It’s important to avoid repeating similar patterns with AI systems.\n\nLevel up your AI career with our new event series,AI Access! Join us for “Integrating Design and Technical Innovation in AI-First Products” with Patrick Hebron, director of Adobe’s Machine Intelligence Design group. Patrick will offer a sneak peek at ways AI will transform creative tools and workflows.RSVP\n\nDoctors who treat breast cancer typically use a quick, inexpensive tumor-tissue stain test to diagnose the illness and a slower, more costly one to determine treatment. A new neural network could help doctors to go straight from diagnosis to treatment.What’s new:The stain in the test for treatment highlights a key visual clue to the choice of therapy that’s otherwise invisible to human pathologists. Nikhil Naik at Salesforce and colleagues at University of Southern California builtReceptorNetto detect that clue in the diagnostic test.Key insight:The presence of estrogen receptor proteins is a sign that hormone therapy may work. In the diagnostic test, known as hematoxylin and eosin (H&E), these proteins are invisible to the human eye. An attention mechanism, which identifies the most important parts of an input (in this case, a portion of an H&E slide) in determining the output (a label that the proteins are present), can aggregate image areas where they occur so a vision network can classify the slide.How it works:ReceptorNet comprises aResNet-50pretrained onImageNetfollowed by an attention layer and a fully connected layer. The researchers trained and tested ReceptorNet on images of H&E slides, and augmentations of them, in theAustralian Breast Cancer Tissue BankandThe Cancer Genome Atlasdatasets.\n\nResults:ReceptorNet achieved an area under the curve of 0.92 AUC, a measure of true versus false positives where 1 is a perfect score. The authors experimented with alternatives to the attention layer that didn’t perform as well, which suggests that attention was key.Yes, but:The authors had access only to H&E images, so they couldn’t compare ReceptorNet’s performance against the test that’s typically used to guide treatment.Why it matters:ReceptorNet had a label for each tissue slide but not for each tile derived from it. The success of attention in aggregating and evaluating the representations extracted from each tile bodes well for this approach in reading medical images.We’re thinking:Where else could computer vision augment or replace slow, expensive medical tests?\n\nComputer vision is helping people resell their used designer handbags.What’s new:Rebag, a resale company for luxury handbags, watches, and jewelry, launched Clair AI, an app that automatically appraises second-hand bags from brands like Gucci, Hermes, and Prada,Voguereported.How it works:Users take a close-up picture of a handbag against a neutral background. The app finds between one and five potential matches for designer, model, and style.\n\nBehind the news:Rebag’s revenue grew by 50 percent in 2020, riding a surge in demand for second-hand luxury goods. The market for used high-end items like watches, jewelry, fine art, and yachts grew in 2019 by12 percentto$26.5 billion.Why it matters:Consumers aremindfulof the resale value of high-ticket goods. An app that makes it easier to tap into that market could drive sales of both new and used items — and make it easy to unload the hideous thing that somehow looked fetching last summer.We’re thinking:We tested this system on the bags in our closet, but it wasn’t impressed by our prized NeurIPS tote.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/Screen-Shot-2021-02-10-at-11.54.50-AM-copy.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/CARCAM.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/VOKEN-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker-97.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/The-Batch-Image-1024x576.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/RECEPTORNET.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker-100.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-292/",
    "title": "issue 292",
    "date": "",
    "reading_time": "",
    "content": "Some people today are discouraging others from learning programming on the grounds AI will automate it. This advice will be seen as some of the worst career advice ever given. I disagree with the Turing Award and Nobel prize winner who wrote, “It is far more likely that the programming occupation will become extinct [...] than that it will become all-powerful. More and more, computers will program themselves.”​ Statements discouraging people from learning to code are harmful!\n\nIn the 1960s, when programming moved from punchcards (where a programmer had to laboriously make holes in physical cards to write code character by character) to keyboards with terminals, programming became easier. And that made it a better time than before to begin programming. Yet it was in this era that Nobel laureate Herb Simon wrote the words quoted in the first paragraph. Today’s arguments not to learn to code continue to echo his comment.\n\nAs coding becomes easier, more people should code, not fewer!\n\nOver the past few decades, as programming has moved from assembly language to higher-level languages like C, from desktop to cloud, from raw text editors to IDEs to AI assisted coding where sometimes one barely even looks at the generated code (which some coders recently started to call vibe coding), it is getting easier with each step. (By the way, to learn more about AI assisted coding, check out our video-only short course, “Build Apps with Windsurf’s AI Coding Agents.”)\n\nI wrote previously that I see tech-savvy people coordinating AI tools to move toward being10x professionals— individuals who have 10 times the impact of the average person in their field. I am increasingly convinced that the best way for many people to accomplish this is not to be just consumers of AI applications, but to learn enough coding to use AI-assisted coding tools effectively.\n\nOne question I’m asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is the best way to do that.\n\nWhen I was working on the courseGenerative AI for Everyoneand needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on — using the language of art — to get the result he wanted. I didn’t know this language, and my paltry attempts at prompting could not deliver as effective a result.\n\nSimilarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As these tools continue to make coding easier, this is the best time yet to learn to code, to learn the language of software, and learn to make computers do exactly what you want them to do.\n\nKeep building!\n\nAndrew\n\nPre-enroll now for the new Data Analytics Professional Certificate! Gain job-ready skills in data analysis, whether you’re starting a career as a data analyst or enhancing your ability to prepare, analyze, and visualize data in your current role. This program covers both classical statistical techniques and emerging AI-assisted workflows to help you work smarter with data.Learn more and sign up\n\nMost models that have learned to reason via reinforcement learning were huge models. A much smaller model now competes with them.\n\nWhat’s new:Alibaba introducedQwQ-32B, a large language model that rivals the reasoning prowess of DeepSeek-R1 despite its relatively modest size.\n\nHow it works:QwQ-32B is a version ofQwen2.5-32Bthat was fine-tuned to generate chains of thought using reinforcement learning (RL). Fine-tuning proceeded in two stages.\n\nPerformance:On several benchmarks for math, coding, and general problem solving, QwQ-32B outperforms OpenAI o1-mini (parameter count undisclosed) and achieves performance roughly comparable to DeepSeek-R1 (671 billion parameters, 37 billion active at any moment).\n\nBehind the news:DeepSeek’s initial model, DeepSeek-R1-Zero, similarly applied RL to a pretrained model. That effort produced strong reasoning but poor readability (for example, math solutions with correct steps but jumbled explanations). To address this shortcoming, the teamfine-tunedDeepSeek-R1 on long chain-of-thought examples before applying RL. In contrast, QwQ-32B skipped preliminary fine-tuning and applied RL in two stages, first optimizing for correct responses and then for readability.\n\nWhy it matters:RL can dramatically boost LLMs’ reasoning abilities, but the order in which different behaviors are rewarded matters. Using RL in stages enabled the team to build a 32 billion parameter model — small enough to run locally on a consumer GPU — that rivals a much bigger mixture-of-experts model, bringing powerful reasoning models within reach for more developers. The Qwen team plans to scale its RL approach to larger models, which could improve the next-gen reasoning abilities further while adding greater knowledge.\n\nWe’re thinking:How far we’ve come since “Let’s think step by step”!\n\nMicrosoft debuted its first official large language model that responds to spoken input.\n\nWhat’s new:Microsoft releasedPhi-4-multimodal, an open weights model that processes text, images, and speech simultaneously.\n\nHow it works:Phi-4-multimodal has six components: Phi-4-mini, vision and speech encoders as well as corresponding projectors (which modify the vision or speech embeddings so the base model can understand them), and two LoRA adapters. The LoRA adapters modify the base weights depending on the input: One adapter modifies them for speech-text problems, and one for vision-text and vision-speech problems.\n\nResults:The authors compared Phi-4-multimodal to other multimodal models on text-vision, vision-speech, text-speech tasks.\n\nBehind the news:This work adds to the growing body of models with voice-in/text-out capability, including the open weightsDiVAmodel developed by a team led by Diyi Yang at Stanford University.\n\nWhy it matters:The architectural options continue to expand for building neural networks that process text, images, audio, and various combinations. While some teams maintain separate models for separate data modalities, likeQwen2.5(for text) andQwen2.5-VL) (for vision-language tasks), others are experimenting with mixture-of-expert models likeDeepSeek-V3. Phi-4-multimodal shows that Mixture-of-LoRAs is an effective approach for processing multimodal data — and gives developers a couple of new open models to play with.\n\nWe’re thinking:Output guardrails have been built to ensure appropriateness of text output, but this is difficult to apply to a voice-in/voice-out architecture. (Some teams have worked on guardrails that screen audio output directly, but the technology is still early.) For voice-based applications, a voice-in/text-out model can generate a candidate output without a separate, explicit speech-to-text step, and it accommodates text-based guardrails before it decides whether or not to read the output to the user.\n\nA United States court delivered a major ruling that begins to answer the question whether, and under what conditions, training an AI system on copyrighted material is considered fair use that doesn’t require permission.\n\nWhat’s new:A U.S. Circuit judgeruledon a claim by the legal publisher Thomson Reuters that Ross Intelligence, an AI-powered legal research service, could not claim that training its AI system on materials owned by Thomson Reuters was a so-called “fair use.” Training the system did not qualify as fair use, he decided, because its output competed with Thomson Reuters’ publications.\n\nHow it works:Thomson Reuters hadsuedRoss Intelligence after the defendant trained an AI model using 2,243 works produced by Thomson Reuters without the latter’s permission. This ruling reversed an earlier decision in 2023, when the same judge had allowed Ross Intelligence’s fair-use defense to proceed to trial. In the new ruling, he found that Ross Intelligence’s use failed to meet the definition of fair use in key respects. (A jury trial is scheduled to determine whether Thomson Reuters' copyright was in effect at the time of the infringement and other aspects of the case.)\n\nBehind the news:The ruling comes amid awaveof lawsuits over AI training and copyright in several countries. Many of these cases are in progress, but courts have weighed in on some.\n\nWhy it matters:The question of whether training (or copying data to train) AI systems is a fair use of copyrighted works hangs over the AI industry, from academic research to commercial projects. In the wake of this ruling, courts may be more likely to reject a fair-use defense when AI companies train models on copyrighted material to create output that overlaps with or replaces traditional media, asThe New York Timesalleges in its lawsuit against OpenAI. However, the ruling leaves room for fair use with respect to models whose output doesn’t compete directly with copyrighted works.\n\nWe’re thinking:Current copyright laws weren’t designed with AI in mind, and rulings like this one fill in the gaps case by case.Clarifying copyrightfor the era of generative AI could help our field move forward faster.\n\nLarge language models built by developers in China may, in some applications, be less useful outside that country because they avoid topics its government deems politically sensitive. A developer fine-tuned DeepSeek-R1 to widen its scope without degrading its overall performance.\n\nWhat’s new:Perplexity releasedR1 1776, a version ofDeepSeek-R1that responds more freely than the original. The model weights are available todownloadunder a commercially permissive MITlicense.\n\nHow it works:The team modified DeepSeek-R1’s knowledge of certain topics by fine-tuning it on curated question-answer pairs.\n\nResults:The fine-tuned model responded to politically charged prompts factually without degrading its ability to generate high-quality output.\n\nBehind the news:Amongthe first countries to regulate AI, ChinarequiresAI developers to build models that uphold “Core Socialist Values” and produce true and reliable output. When these objectivesconflict, the political goal tends to dominate. While large language models built by developers in China typically avoid contentious topics, the newer DeepSeek models enforce this more strictly than older models like Qwen and Yi, using methods akin to Western measures for aligning output, like Reinforcement Learning from Human Feedback andkeyword filters.\n\nWhy it matters:AI models tend to reflect their developers’ values and legal constraints. Perplexity’s targeted fine-tuning approach addresses this barrier to international adoption of open-source models.\n\nWe’re thinking:As models with open weights are adopted by the global community, they become a source of soft power for their developers, since they tend to reflect their developers’ values. This work reflects a positive effort to customize a model to reflect the user’s values instead — though how many developers will seek out a fine-tuned version rather than the original remains to be seen.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--60-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--61-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--55-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--62-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-106/",
    "title": "issue 106",
    "date": "",
    "reading_time": "",
    "content": "Recently I attended an online celebration of my late grandfather’s life. He had passed away quietly in his sleep in March. Two days later, Coursera was publicly listed on the New York Stock Exchange. And two days after that, my son Neo Atlas Ng was born.\n\nThe sequence of events reminds me that every day is precious. Had my grandfather lived just a few more days, he would have shared in the joy of his first great grandson’s birth and the celebration of Coursera’s listing.\n\nMy grandfather lived a remarkable life. He was born in Jamaica in 1918 during one pandemic, and he passed away 102 years later during another. His father was an indentured laborer who had moved from China to Jamaica, and his mother was half Jamaican. (Thus I’m 1/16 Jamaican.) As a young man, he sailed from the Caribbean through the Panama canal to settle in Hong Kong, where he had a fruitful career as an accountant and spent his last few years holding court at his belovedKowloon Cricket Club.\n\nIf you’ve lost a loved one, you probably miss them as much as I do my grandfather. It goes to show that even if someone close to you lives to 102, likely it will feel like it’s not enough. If only he had lived four more days — or four more years — he could have shared in even more joy.\n\nI’m grateful for the time I had with my grandfather. I hope you’ll take care of yourself so that you, too, can live a long life. Let’s squeeze every drop of joy out of life in the time we have.\n\nLove,Andrew\n\nAnepidemicof opioid abuse in the U.S. killed 93,000 people in 2020 alone. An algorithm intended to help doctors prescribe the drugs responsibly may be barring worthy patients from pain relief.What’s new:A widely used system assesses whether individual patients are at risk of abusing opioids. In some cases, it has recommended denying painkillers to people who suffer from severe pain and have no history of drug abuse,Wiredreported.How it happened:Most U.S. states have developed databases that track drug prescriptions.NarxCare, a system developed by medical technology company Appriss Health, analyzes such data for at least eight states.\n\nBehind the news:Flawed algorithms unexpectedly havecuthealthcare benefits to many U.S. citizens, leaving them without care or a way to appeal the decision.Why it matters:Most people who have opioid prescriptions are not addicts. Cutting them off from painkillers not only leaves them to suffer, it also could drive them to obtain the drugs illegally or harm themselves with illicit substitutes.We’re thinking:Efforts to limit prescriptions of opioids could save countless people from addiction, and AI can play an important role. Stories of patients who have been denied care highlight the pressing need to improve and audit AI systems, even as they help us avoid fueling the opioid epidemic.\n\nA new study examines a major strain of recent research: huge models pretrained on immense quantities of uncurated, unlabeled data and then fine-tuned on a smaller, curated corpus. The sprawling 200-page document evaluates the benefits and risks.What’s new:Researchers at Stanford’s Human AI Instituteproposedways to prevent large language models likeBERT,CLIP, andGPT-3— which they callfoundation modelsfor their ability to support a plethora of high-performance, fine-tuned variations — from manifesting hidden flaws after fine-tuning.Key insight:The very factors that make large language models so valuable — unsupervised training followed by adaptation to a wide variety of tasks (indeed, some outside the domain of natural language) — make them potential vectors for harm. Defects in the foundation, such as biases learned from uncurated training data, can emerge in fine-tuned versions as challenges to fairness, ethical use, and legal compliance. Moreover, this approach encourages a technological monoculture in which a limited number of architectures, despite their strengths, proliferate their weaknesses across various domains.Toward solid foundations:The authors recommend ways to minimize unwelcome surprises such as unwitting contributions to social or economic inequality, unemployment, or disinformation:\n\nBehind the news:The advent of BERT in 2018 accelerated adoption of unsupervised pretraining in natural language models and spawned ever-larger networks as researchers scaled up the concept and experimented with architectures. The approach has spun off fine-tuned models not only for language tasks likeconversation,image captioning, andinternet searchbut also far-flung applications includingmodeling proteins,testing mathematical theorems,generating computer code,image recognition,image generation, andreinforcement learning.Why it matters:Such models can cause harm due to intrinsic flaws by, say, propagating data-driven biases against members of particularreligionsor other groups) and extrinsic flaws, such asenergy-intensive trainingthat leaves a large carbon footprint and misuse such aspropagating disinformation. Deep learning systems developed without foresight run the risk of becoming a burden rather than a boon.We’re thinking:The future of AI may well be built on a limited variety of foundation models. In any case, the painstaking work of checking models for flaws beats cleaning up messes caused by neglecting to do so.\n\nMark your calendar: On September 8, 2021, we’re launching “Deploying Machine Learning Models in Production,” Course 4 of theMachine Learning Engineering for Production (MLOps) Specialization!Pre-enroll now\n\nCoffee producers are using machine learning to grow better beans.What’s new:Beverage giant Nespresso is rolling out a system to assess the quality of hybrid coffee seedlings usingtechnologyfrom Israeli-Colombian startup Demetria.How it works:Nespresso develops new coffee varieties by grafting plant seedlings. Previously it relied on human experts to assess whether these grafts were viable. Demetria’s algorithm uses readings from a handheld near-infrared optical scanner to automate the evaluation.\n\nBehind the news:The food and beverage industry has a growing appetite for AI.\n\nWhy it matters:Nespresso believes that Demetria’s technology will save time and money. This may be bad for traditional plant assessors, whose skills may become obsolete. On the other hand, it may helpstrugglingColombian coffee farmers grow more profitable beans.We’re thinking:The thought of better coffee through AI perked us right up.\n\nPretraining methods generate basic representations for later fine-tuning, but they’re prone to certain issues that can throw them off-kilter. New work proposes a solution.What’s new:Researchers at Facebook, PSL Research University, and New York University led by Adrien Bardes devised an unsupervised pretraining method they callVariance-Invariance-Covariance Regularization(VICReg). VICReg helps a model learn useful representations based on well understood statistical principles.Key insight:Pretraining methods can suffer from three common failings: Generating an identical representation for different input examples (which leads to predicting the mean consistently in linear regression), generating dissimilar representations for examples that humans find similar (for instance, the same object viewed from two angles), and generating redundant parts of a representation (say, multiple vectors that represent two eyes in a photo of a face). Statistically speaking, these problems boil down to issues of variance, invariance, and covariance respectively.How it works:VICReg manages variance, invariance, and covariance via different terms in a loss function. The authors used it to pretrain aResNet-50onImageNetwithout labels.\n\nResults:The authors transferred the VICReg-trained ResNet-50’s representations to a linear classifier and trained it on ImageNet with labels. That model achieved a 73.2 percent accuracy, just shy of the 76.5 percent achieved by a supervised ResNet-50. A linear classifier using representations from a ResNet-50 pretrained using the contrastive learning methodSimCLRachieved 69.3 percent accuracy.Why it matters:Contrastive learning, a successful pretraining technique, requires a large number of comparisons between dissimilar inputs to ensure that not all representations are identical. VICReg avoids that issue by computing the variance within a batch, a much less memory-intensive operation.We’re thinking:Comparing different augmentations of the same example has proven to be a powerful way to learn. This technique extends that approach, and we expect to see more.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/08/Screen-Shot-2021-08-24-at-8.webp",
      "https://dl-staging-website.ghost.io/content/images/2021/08/PERSCRIPTION.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/08/ezgif.com-gif-maker--7--2.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/08/Course-Name-Banner-4-1.png",
      "https://dl-staging-website.ghost.io/content/images/2021/08/ezgif.com-gif-maker--3--2.gif",
      "https://dl-staging-website.ghost.io/content/images/2021/08/VICREGv2.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-269/",
    "title": "issue 269",
    "date": "",
    "reading_time": "",
    "content": "We won! California’s anti-innovation bill SB 1047 was vetoed by Governor Newsom over the weekend. Open source came closer to taking a major blow than many people realize, and I’m grateful to the experts, engineers, and activists who worked hard to combat this bill.\n\nThe fight to protect open source is not yet over, and we have to continue our work to make sure regulations are based on science, not science-fiction.\n\nAs I  wrote previously, SB 1047 makes a fundamental mistake of trying toregulate technology rather than applications. It was also a veryconfusinglaw that would have been hard to comply with. That would have driven up costs without improving safety.\n\nWhile I’m glad that SB 1047 has been defeated, I wish it had never made it to the governor’s desk. It would not have made AI safer. In fact, many of its opponents were champions of responsible AI and making AI safe long before the rise of generative AI. Sadly, as the Santa Fe Institute’s Melanie Mitchellpointed out, the term “AI safety” has been co-opted to refer to a broad set of speculative risks that have little basis in science — as demonstrated by the security theater SB 1047 would have required — that don’t actually make anything safer. This leaves room for lobbying that canenricha small number of people while making everyone else worse off.\n\nAs Newsomwroteto explain his decision, SB 1047 is “not informed by an empirical trajectory analysis of AI systems and capabilities.” In contrast, the United States federal government’s work is “informed by evidence-based approaches, to guard against demonstrable risks to public safety.” As the governor says, evidence-based regulation is important!\n\nMany people in the AI community were instrumental in defeating the bill. We're lucky to have Martin Casado, who organized significant community efforts; Clément Delangue, who championed openness; Yann LeCun, a powerful advocate for open research and open source; Chris Lengerich, who published deep legal analysis of the bill; Fei-Fei Li and Stanford's HAI, who connected with politicians; and Garry Tan, who organized the startup accelerator Y Combinator against the bill. Legendary investors Marc Andreessen and Roelof Botha were also influential. Plus far too many others to name here. I’m also delighted that brilliant artists like MC Hammersupportthe veto!\n\nLooking ahead, far more work remains to be done to realize AI’s benefits. Just this week, OpenAI released an exciting new voice API that opens numerous possibilities for beneficial applications! In addition, we should continue to mitigate current and potential harms. UC Berkeley computer scientistDawn Songand collaborators recently published aroadmapto that end. This includes investing more to enable researchers to study AI risks and increasing transparency of AI models (for which open source and red teaming will be a big help).\n\nUnfortunately, some segments of society still have incentives to pass bad laws like SB 1047 and use science fiction narratives of dangerous AI superintelligence to advance their agendas. The more light we can shine on what AI really is and isn’t, the harder it will be for legislators to pass laws based on science fiction rather than science.\n\nKeep learning!\n\nAndrew\n\nIn this short course, you’ll learn how tokenization affects vector search and how to optimize search in LLM applications that use RAG. You’ll explore Byte-Pair Encoding, WordPiece, and Unigram; fine-tune HNSW parameters; and use vector quantization to improve performance.Sign up for free\n\nMeta extended its Llama family of models into two new categories: vision-language and sizes that are small enough to fit in edge devices.\n\nWhat’s new:Meta introducedLlama 3.2, including two larger vision-language models and two smaller text-only models as well as developer tools for building agentic applications based on the new models.Weights and codearefreeto developers who have less than 700 million monthly active users. Multiple providers offer cloud access.\n\nHow it works:Llama 3.2 90B and 11B accept images as well as text and generate text output (image processing is not available in the European Union). Llama 3.2 1B and 3B accept and generate text. All four models can process 131,072 tokens of input context and generate 2,048 tokens of output.\n\nNew tools for developers:Meta announcedLlama Stack, a series of APIs for customizing Llama models and building Llama-based agentic applications. Among other services, Llama Stack has APIs for tool use, memory, post-training, and evaluation.Llama Guard, a model designed to evaluate content for sexual themes, violence, criminal planning, and other issues, now flags problematic images as well as text. Llama Guard 3 11B Vision comes with Llama.com’s distributions of Llama 3.2 90B and 11B, while Llama Guard 3 1B comes with Llama 3.2 3B and 1B.\n\nWhy it matters:Meta’s open models are widelyusedby everyone from hobbyists to major industry players. Llama 3.2 extends the line in valuable ways. The growing competition between Llama and Qwen shows that smaller, open models can offer multimodal capabilities that are beginning to rival their larger, proprietary counterparts.\n\nWe’re thinking:By offering tools to buildagentic workflows, Llama Stack takes Llama 3.2 well beyond the models themselves. Our new short course “Introducing Multimodal Llama 3.2” shows you how to put these models to use.\n\nAdobe is putting a video generator directly into its popular video editing application.\n\nWhat’s new:Adobeannouncedits Firefly Video Model, which will be available as a web service and integrated into the company’s Premiere Pro software later this year. The model takes around two minutes to generate video clips up to five seconds long from a text prompt or still image, and it can modify or extend existing videos. Prospective users can join awaitlistfor access.\n\nHow it works:Adobe has yet to publish details about the model’s size, architecture, or training. It touts uses such as generating B-roll footage, creating scenes from individual frames, adding text and effects, animation, and video-to-video generation like extending existing clips by up to two seconds.\n\nBehind the news:Adobe’s move into video generation builds on itsFirefly image generatorand reflects its broader strategy to integrate generative AI with creative tools. In April, Adobeannouncedthat it would integrate multiple video generators with Premiere, including models from partners likeOpenAIandRunway. Runway itself recently extended its own offering withvideo-to-videogeneration and anAPI.\n\nWhy it matters:Adobe is betting that AI-generated video will augment rather than replace professional filmmakers and editors. Putting a full-fledged generative model in a time-tested user interface for video editing promises to make video generation more useful as well as an integral part of the creative process. Moreover, Adobe’s use of licensed training data may attract videographers who are concerned about violating copyrights or supporting fellow artists.\n\nWe’re thinking:Video-to-video generation crossing from frontier capability to common feature. Firefly's (and Runway’s) ability to extend existing videos offers a glimpse.\n\nDozens of countries endorsed a “blueprint for action” designed to guide the use of artificial intelligence in military applications.\n\nWhat’s new:More than 60 countries including Australia, Japan, the United Kingdom, and the United States endorsed nonbinding guidelines for military use of AI,Reutersreported. The document, presented at the Responsible Artificial Intelligence in the Military (REAIM) summit in Seoul, South Korea, stresses the need for human control, thorough risk assessments, and safeguards against using AI to develop weapons of mass destruction. China and roughly 30 other countries did not sign.\n\nHow it works:Key agreements in theblueprintinclude commitments to ensure that AI doesn’t threaten peace and stability, violate human rights, evade human control, and hamper other global initiatives regarding military technology.\n\nBehind the News:The Seoul summit followed last year’sREAIM summitin The Hague, where leaders similarly called for limits on AI military use without binding commitments. Other international agreements like the EU’sAI ActandFramework Convention on Artificial Intelligence and Human Rights, Democracy, and the Rule of Lawregulate civilian AI, but exclude military applications. Meanwhile, AI-enabled targeting systems and autonomous, weaponized drones have been used in conflicts inSomalia, Ukraine, and Israel, highlighting the lack of international norms and controls.\n\nWhy it matters:The REAIM blueprint may guide international discussions on the ethical use of AI in defense, providing a foundation for further talks at forums like the United Nations. Though it’s nonbinding, it fosters collaboration and avoids restrictive mandates that could cause countries to disengage.\n\nWe’re thinking:AI has numerous military applications across not only combat but also intelligence, logistics, medicine, humanitarian assistance, and other areas. Nonetheless, it would be irresponsible to permit unfettered use of AI in military applications. Standards developed by democratic countries working together will help protect human rights.\n\nLarge language models can process small spreadsheets, but very large spreadsheets often exceed their limits for input length. Researchers devised a method that processes large spreadsheets so LLMs can answer questions about them.\n\nWhat’s new:Yuzhang Tian, Jianbo Zhao, and colleagues at Microsoft proposedSheetCompressor, a way to represent spreadsheets that enables LLMs to identify and request the parts they need to answer specific questions.Key insight:Most spreadsheets can be broken down into a set of tables that may be bordered by visual dividers like thick lines or empty rows and/or columns. But detecting these tables isn’t trivial, since they may contain the same kinds of markers. (See the illustration above, in which tables are denoted by red dashes.) To answer many questions, you don’t need the whole spreadsheet, only the relevant table. Moreover, given a question, an LLM can recognize the table it needs to produce an answer. However, to identify the correct table, it needs to see the whole spreadsheet, which may be too large for its input context window, and the tables, which may not be clearly separated, need to be parsed. The solution is to compress the spreadsheet, feed the compressed representation to the LLM along with the question, and ask the LLM to identify the boundaries of the table it needs to answer the question. Then, given an uncompressed version of that table, the LLM can produce an answer.\n\nHow it works:The authors built software that prepared spreadsheets by (i) parsing them into tables and (ii) compressing them while maintaining the table structure. Then they fine-tuned LLMs to detect tables in the compressed spreadsheets and prompted the fine-tuned LLMs to identify the tables relevant to a given question.\n\nResults:The authors compared the fine-tuned LLMs’ ability to detect tables in spreadsheets that were compressed using their method and in their original uncompressed form. They fed the models spreadsheets of various sizes that ranged from small (up to 4,000 tokens) to huge (more than 32,000 tokens). They gauged the models’ performance according to F1 score (higher is better).\n\nWhy it matters:By giving LLMs the ability to detect a spreadsheet’s functional components, this approach enables them to process a wide variety of spreadsheets regardless of their size and complexity.\n\nWe’re thinking:When considering the strengths of LLMs, we no longer have to take spreadsheets off the table.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--18--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/10/The-Batch-ads-and-exclusive-banners---2024-10-01T094337.154--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--14-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--15-.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--14--1.png",
      "https://dl-staging-website.ghost.io/content/images/2024/10/unnamed--16-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-84/",
    "title": "issue 84",
    "date": "",
    "reading_time": "",
    "content": "It’s a common joke that 80 percent of machine learning is actually data cleaning, as though that were a lesser task. My view is that if 80 percent of our work is data preparation, then ensuring data quality is the important work of a machine learning team.Rather than counting on engineers to chance upon the best way to improve a dataset, I hope we can develop MLOps tools that help make building AI systems, including building high-quality datasets, more repeatable and systematic. MLOps is a nascent field, and different people define it differently. But I think the most important organizing principle of MLOps teams and tools should be to ensure the consistent and high-quality flow of data throughout all stages of a project. This will help many projects go more smoothly.I have much more to say on this topic, so check out my talkhere. Thanks to my team at Landing AI for helping to crystalize these thoughts.Keep learning!Andrew\n\nPolice are increasingly able to track motor vehicles throughout the U.S. using a network of AI-powered cameras — many owned by civilians.What’s new:Flock, which sells automatic license plate readers to homeowners associations, businesses, and law enforcement agencies, is encouraging enforcers to use its network to monitor cars and trucks outside their jurisdiction, according to an investigation byVice.How it works:Flock owners can opt to share data with police. In turn, police can share data with Flock’sTotal Analytics Law Officers Network, or Talon.\n\nBehind the news:AI-powered cameras are increasingly popular with law enforcement, but their use is fueling concerns about overreach.\n\nWhy it matters:Commercial surveillance networks have been deployed without much oversight or consent, and police are rarely accountable for how they use such systems. Permissive policies around these devices amount to warrantless monitoring of millions of innocent people by police as well as fellow citizens.We’re thinking:While AI can help police catch criminals, we do not condone a silent erosion of civil liberties and privacy. We support clear, consistent guidelines on appropriate uses of face recognition, license plate readers, and other tracking technologies.\n\nWhat’s creepier than images from the sci-fi TV seriesDoctor Who? Images generated by a network designed to visualize what goes on in peoples’ brains while they watchDoctor Who.What’s new:Lynn Le, Luca Ambrogioni, and colleagues at Radboud University and Max Planck Institute for Human and Brain Cognitive Sciences developedBrain2Pix, a system that reconstructs what people saw from scans of their brain activity.Key insight:The brain uses neurons nearby one another to represent visual features nearby one another. Convolutional neural networks excel at finding and using spatial patterns to perform tasks such as image generation. Thus, a convolutional neural network can use the spatial relationships between active neurons in a brain scan to reconstruct the corresponding visual image.How it works:The authors used a picture-to-picture generative adversarial network (GAN) to try to produce an image of what a person was looking at based on functional magnetic resonance imaging (fMRI): 3D scans that depict blood oxygenation in the brain, which indicates neuron activity. They trained the GAN onDoctor Who fMRI, a collection of video frames from 30 episodes ofDoctor Whoand corresponding fMRIs captured as an individual watched the show.\n\nResults:The researchers used anAlexNetto extract representations of Brain2Pix images andDoctor Whoframes and compared the distance between them. Brain2Pix achieved an average distance of 4.6252, an improvement over the previousstate-of-the-artmethod’s average of 5.3511.Why it matters:The previous state-of-the-art used 3D convolutions directly on the raw fMRIs, yet the new approach fared better. For some problems, engineering features — in this case, converting fMRIs into 2D — may be the best way to improve performance.We’re thinking:We wouldn’t mind sitting in an fMRI machine for hours on end if we were binge-watchingDoctor Who.\n\nA new website calls out AI research that may not lend itself to being reproduced.What’s new:Papers Without Codemaintains a directory of AI systems that researchers tried but failed to reproduce. The site (the name of which is a play on the indispensablePapers With Code), aims to save researchers time wasted trying to replicate results published with insufficient technical detail.How it works:Users can submit a link to a paper, a link to their attempt to reproduce it, and an explanation of why their effort failed.\n\nBehind the news:Google engineer Pete Wardenproclaimeda “machine learning reproducibility crisis” in 2018. Since then the issue has emerged as a widespreadconcern.\n\nWhy it matters:Reproducibility is an essential part of science, and AI is one of many fields facing a so-calledreplication crisisbrought on by growing numbers of papers that report unreliable results.We’re thinking:While we applaud the spirit of this effort, without a transparent review process and a public list of reviewers, it could be used to demean researchers unfairly. We urge other research venues and institutions to take up the cause.\n\nJoin uson March 25 at 10 a.m. Pacific Time for our second AI+X expert panel,AI Innovation in Healthcare, hosted by Workera in partnership with DeepLearning.AI. Hear industry leaders discuss the latest trends, opportunities, and challenges at the intersection of AI and healthcare.\n\nA language model is helping crisis-intervention volunteers practice their suicide-prevention skills.What’s new:The Trevor Project, a nonprofit organization that operates a 24-hour hotline for LGBTQ youth, uses a “crisis contact simulator” to train its staff in how to talk with troubled teenagers,MIT Technology Reviewreported.How it works:The chatbot plays the part of a distraught teenager while a counselor-in-training tries to determine the root of their trouble.\n\nBehind the news:AI is being used in a growing number of mental health settings.\n\nWhy it matters:Suicide rates among LGBTQ teens are two to seven times higher than among their straight peers, and they’re twice as likely to think about taking their own lives, according to theU.S. government. The Trevor Project fields over 100,000 crisis calls, chats, and texts annually. Speeding up the training pipeline could save lives.We’re thinking:As a grad student at MIT, Andrew tried to volunteer for a crisis call line, but his application was rejected. Maybe training from this system would have helped!\n\nA new system combines verbal descriptions and crude lines to visualize complex scenes.What’s new:Google researchers led by Jing Yu Koh proposedTag-Retrieve-Compose-Synthesize(TReCS), a system that generates photorealistic images by describing what they want to see while mousing around on a blank screen.Key insight:Earlierworkproposed a similar system to showcase a dataset, Localized Narratives, that comprises synchronized descriptions and mouse traces captured as people described an image while moving a cursor over it. That method occasionally produced blank spots. The authors addressed that shortcoming by translating descriptive words into object labels (rather than simply matching words with labels) and distinguishing foreground from background.How it works:The Local Narratives dataset provides an inherent correspondence between every word in a description and a mouse trace over an image. TReCS uses this correspondence to translate words into labels of objects that populate a scene. The authors trained the system on the portion of Localized Narratives that used images inCOCOand tested it on the portion that usedOpen Images.\n\nResults:Five judges compared TReCS’ output with that ofAttnGAN, a state-of-the-art, text-to-image generator that did not have access to mouse traces. The judges preferred TReCS’ image quality 77.2 percent to 22.8 percent. They also preferred the alignment of TReCS output with the description, 45.8 percent to 40.5 percent. They rated both images well aligned 8.9 percent of the time and neither image 4.8 percent of the time.Why it matters:The authors took advantage of familiar techniques and datasets to extract high-level visual concepts and fill in the details in a convincing way. Their method uncannily synthesized complex scenes from verbal descriptions (though the featured example, a skier standing on a snowfield with trees in the background, lacks the railing and mountain mentioned in the description).We’re thinking:Stock photo companies may want to invest in systems like this. Customers could compose photos via self-service rather than having to choose from limited options. To provide the best service, they would still need to hire photographers to produce raw material.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/10/partners-in-surveillance-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/10/What-the-brain-sees-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/10/Spotlight-on-Unreproducible-Results-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/AIX_The-Batch-Image-1-1024x576.png",
      "https://dl-staging-website.ghost.io/content/images/2022/10/Chatbots-Against-Depression-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/10/pictures-from-words-and-gestures-1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-17/",
    "title": "issue 17",
    "date": "",
    "reading_time": "",
    "content": "I’ve been thinking about AI and ethics. With the techlash and anerosion of trustin technology as a positive force, it’s more important than ever that we make sure the AI community acts ethically.\n\nThere has been a proliferation of AI ethical principles. Thispapersurveys 84 such statements. These statements are a great start, but we still need to do better.\n\nTake, for example, the OECD’sstatement: AI should benefit people and the planet by driving inclusive growth, sustainable development, and well-being. When I ask engineers what effect such statements have on their day-to-day actions, they say, “pretty much none.” It is wonderful that the OECD is thinking about this. But we need more actionable codes of ethics that give more concrete and actionable suggestions.\n\nI describedearlierstruggling with an ethical decision of whether to publicize an AI threat. It’s in situations like that we need better guidelines and processes for decision making.\n\nMany existing AI ethics codes come from large corporations and governments. But if we hope that the global AI community will follow a set of guidelines, then this community — including you — needs to have a bigger voice in its development. We need an ethical code written by the AI community, for the AI community. That will also be the best way to make sure it truly reflects our community’s values, and that all of us buy into it and will follow it.\n\nLast Friday, deeplearning.ai hosted our firstPie & AIon AI and ethics. Four cities joined us: Hong Kong, Manila, Singapore, and Tokyo. We started with an interactive discussion, and each city came up with three actionable ethics statements, preferably starting with, “An AI engineer should …” The ideas they presented ranged from seeking diverse perspectives when creating data to staying vigilant about malicious coding. I was heartened to see so many people motivated to debate ethical AI in a thoughtful way.\n\nI hope to do more events like this to encourage people to start the conversation within their own communities. This is important, and we need to figure this out.\n\nI would love to hear your suggestions. You can email us at[email protected].\n\nKeep learning!\n\nAndrew\n\nSmart doorbell maker Ring has built its business by turning neighborhoods into surveillance networks. Now the company is drawing fire for using private data without informing customers and sharing data with police.What’s new:A three-part exposé inViceis the latest in a series of media reports charging that Ring, an Amazon subsidiary, mishandles private data in its efforts to promote AI-powered “smart policing.”How it works:Ring’s flagship products are doorbells with integrated camera, microphone, and speaker. The camera’s face and object recognition software alerts users, via a smartphone or connected-home device, whenever someone or something enters its field of view. Users can talk with people at the door remotely through the microphone and speaker.\n\nBehind the news:When it was founded in 2012, Ring was called DoorBot and marketed its system as a hassle-free way to see who’s at the door. The following year, founder Jamie Siminoff appeared on the TV game show Shark Tank. While he didn’t sway the show’s celebrity investors, he raised several million dollars soon after, and changed the name to Ring. In recent years, he has pivoted from promoting the doorbell as a way to enhance social life topositioningit as a crime-prevention tool. In 2018, Amazon purchased Ring for $839 million and now markets the product as part of its Alexa smart-home services.Pushback:Troubled by the privacy implications of sharing customer data with police, 36 civil rights groups published anopen lettercalling for a moratorium on partnerships between Ring and public agencies. The letter also asked Congress to investigate the company.\n\nWe’re thinking:\n\nKnock knock.\n\nWho’s there?\n\nStopwatch.\n\nStopwatch who?\n\nStopwatching me and respect my privacy!\n\nAutomatically generated text summaries are becoming common in search engines and news websites. But existing summarizers often mix up facts. For instance, a victim’s name might get switched for the perpetrator’s. New research offers a way to evaluate factual consistency between source documents along with a measure to evaluate it.What’s new:Wojciech Kryściński and colleagues at Salesforce Research introduceFactCC, a network that classifies such inconsistencies. They also propose a variant called FactCCX that justifies the classifications by pointing out specific inconsistencies.Key insight:Earlier approaches to checking factual consistency determine whether a single source sentence implies a single generated sentence. But summaries typically draw information from many sentences. FactCC evaluates whether a source document as a whole implies a generated sentence.How it works:The authors identified major causes of factual inconsistency in automated abstractive summaries (that is, summaries that don’t copy phrases directly from the source document). Then they developed programmatic methods to introduce such errors into existing summaries to generate a large training dataset. FactCC is based on a BERT language model fine-tuned on the custom dataset.\n\nResults:FactCC classified summary sentences with an F1 score of 0.51. By contrast, a BERT model trained on MNLI, a dataset of roughly 400,000 sentence pairs labeled as either concordant or contradictory, achieved an F1 score of 0.08. In a separate task, FactCC ranked pairs of new sentences (one consistent, one not) for consistency with a source. It awarded consistent sentences a higher rank 70 percent of the time, better by 2.2 percent than the best previous model ranking the same dataset.Why it matters:A tide of automatically generated text is surging into mainstream communications. Measuring factual consistency is a first step towards establishing further standards for generated text—indeed, an urgent step as worries intensify over online disinformation.\n\nAI is providing an early warning system for volcanoes on the verge of blowing their top.What happened:Researchers at the University of Leedsdevelopeda neural net that scans satellite data for indications that the ground near a volcano is swelling—a sign it may be close to erupting.How it works:Satellites carrying certain sensors can track centimeter-scale deformations of Earth’s crust. Seismologists in the 1990sfigured outhow to manually read this data for signs of underground magma buildups. However, human eyeballs are neither numerous nor sharp enough to monitor data for all of Earth’s 1,400 active volcanoes.\n\nBehind the news:Researchers at the University of Bristoldevelopeda similar method to measure deformations in the Earth’s crust using satellite data. However, their model can be fooled by atmospheric distortion that produces similar signals in the data. The Leeds and Bristol groups plan to collaborate in side-by-side tests of their models on a global dataset in the near future. Another group based at Cornell University is attempting to make similar predictions through satellite data of surface temperature anomalies, ash, and gaseous emissions.Why it matters:Approximately 800 million people live within the blast zones of active volcanoes, and millions of sightseers visit their slopes each year. On Monday, New Zealand’s White Island volcanoerupted, killing at least five tourists.\n\nWe’re thinking:If researchers can scale their model up to cover the entire globe, they’ll deserve applause that thunders as loudly as Krakatoa.\n\nBringing an AI system into the real world involves a lot more than just modeling. Learn to deploy on various devices in the new deeplearning.ai TensorFlow: Data and Deployment Specialization!Enroll now\n\nChina announced a ban on fake news, targeting deepfakes in particular.What happened:The Cyberspace Administration of Chinaissuednew rules restricting online audio and video, especially content created using AI. Deepfakes and other such tech, the administration warned, may “disrupt social order and violate people’s interests, creating political risks and bringing a negative impact to national security and social stability.”What could get you in trouble:The rules take effect January 1, 2020. They target both the creators of fake news and websites that host their content.\n\nBehind the news:Computer-generated media are facing government scrutiny in the U.S. as well. California issued its own lawbarringdoctored audio and video earlier this year. The law, however, does not name deepfakes or any specific technology. It targets only political content released within 60 days of an election, and it sunsets in 2023. U.S. legislators areconsideringfederal restrictions on deepfakes, according toThe Verge.Why it matters:There’s a legitimate fear that deepfakes will be used for political purposes. To date, however, the technology’s biggest victims have been women whose bodies have beenvisualizedwithout clothes or whose faces have beenpastedonto pornographic scenes.We’re thinking:It’s a good idea for governments to think ahead to potential problems caused by deepfakes in advance of broader calamities. However, efforts to crack down on them may turn into a high-tech game of whack-a-mole as falsified media becomes ever harder to spot.\n\nIn reinforcement learning, if researchers want an agent to have an internal representation of its environment, they’ll build and train a world model that it can refer to. New research shows that world models can emerge from standard training, rather than needing to be built separately.What’s new:Google Brain researchers C. Daniel Freeman, Luke Metz, and David Ha enabled an agent to build a world model by blindfolding it as it learned to accomplish tasks. They call their approachobservational dropout.Key insight:Blocking an agent’s observations of the world at random moments forces it to generate its own internal representation to fill in the gaps. The agent learns this representation without being instructed to predict how the environment will change in response to its actions.How it works:At every timestep, the agent acts on either its observation (framed in red in the video above) or its prediction of what it wasn’t able to observe (imagery not framed in red). The agent contains a controller that decides on the most rewarding action. To compute the potential reward of a given action, the agent includes an additional deep net trained using the RL algorithm REINFORCE.\n\nResults:Observational dropout solved the task known asCartpole, in which the model must balance a pole upright on a rolling cart, even when its view of the world was blocked 90 percent of the time. In a more complexCar Racingtask, in which a model must navigate a car around a track as fast as possible, the model performed almost equally well whether it was allowed to see its surroundings or blindfolded up to 60 percent of the time.Why it matters:Modeling reality is often part art and part science. World models generated by observational dropout aren’t perfect representations, but they’re sufficient for some tasks. This work could lead to simple-but-effective world models of complex environments that are impractical to model completely.We’re thinking:Technology being imperfect, observational dropout is a fact of life, not just a research technique. A self-driving car or auto-piloted airplane reliant on sensors that drop data points could create a catastrophe. This technique could make high-stakes RL models more robust.\n\nA U.S. federal judge ruled that the public must be able to see records from a government-chartered AI advisory group.What’s new:Thecourtdecidedthat the National Security Commission on AI, which guides defense research into AI-powered warfighting technology, must respond to freedom-of-information requests.What the suit says:The National Security Commission on AI was set up by Congress in late 2018, and the Electronic Privacy Information Center has been requesting its records under the Freedom of Information Act for almost as long. Finding its requests ignored repeatedly, EPICtookthe commission to court in late September.\n\nBehind the news:Former Google CEO Eric Schmidt (shown above) chairs the commission. Other members include a mix of private sector and government experts. It released aninterim reportof its recommendations on November 4.Why it matters:EPIC contends that, without public oversight, the commission could steer the Defense Department towards invasive, biased, or unethical uses of AI.We’re thinking:We look forward to learning more about what this group has been up to. Decisions about deploying AI as a weapon of war should not be made in a back room.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/Andrews20Letter20SIZED-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ring2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Factual20Consistency.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/volcano2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize-3.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/deepfake2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize201.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/foia.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-240/",
    "title": "issue 240",
    "date": "",
    "reading_time": "",
    "content": "I’ve noticed a trend in how generative AI applications are built that might affect both big companies and developers: The gravity of data is decreasing.\n\nData gravity is the idea, proposed by IT engineer Dave McCrory in 2010, that data, or activity around data, attracts and creates more data. With traditional software workloads, data gravity is strong. If you have terabytes of data stored in a particular cloud, the cost to transmit it elsewhere for processing is high. So many teams pick a cloud such as AWS, Azure, or Google Cloud and build on it.\n\nHowever, for many generative AI applications, the cost of processing is much greater than the cost of transmission. This weakens data gravity because data is more weakly bound to the cloud provider or data center where it’s stored, so it’s more practical to build systems that send packets to different servers all over the internet.\n\nLet’s say transmitting 1GB of data costs $0.10. 1GB of text might correspond to about 250 million inputs tokens (if we average four characters per token), which costs about $125 to process using the relatively inexpensive gpt-3.5-turbo-0125 model. (With gpt-4-0125-preview, the cost would be 20x higher.) The cost of processing the data is significantly higher than the cost of transmission. Also, given the computationally intensive nature of using an LLM to read and generate tokens, the latency is high enough that sending your text or image tokens across the internet usually doesn’t add that much further latency.\n\nThis means that, even if we’re building software primarily on a particular cloud provider, it’s still quite feasible to transmit LLM prompts to OpenAI, Anthropic, Anyscale, or Together.ai — or, for that matter, AWS, Azure, or Google Cloud — to get a response. The incentive to build only on a single, monolithic cloud platform is lower than before.\n\nThis situation has implications for stakeholders:\n\nTo be clear, many applications have large traditional software components (that serve up a websites, maintain databases, and so on) as well as new generative AI components (say, a chatbot built on top of the traditional infrastructure). My remarks here apply only to the generative AI portion, and the competitive dynamics of the traditional software components haven’t changed much.\n\nFurther, as new types of AI components emerge, I expect their gravity to evolve as well. For example, right now it appears reasonably easy to change LLM providers; if you’ve built a system on one LLM, it’s annoying but not impossible to switch to a different LLM provider. In comparison, shifting databases is much harder, and once you’ve stored a lot of data in one vector database, the complexity of migrating to a different one can be high.\n\nThe gravity of data has been a fundamental tenet of cloud computing, and a major factor of competition for many companies. Decreasing data gravity is decreasing is a complex, exciting trend that will affect many developers and businesses.\n\nKeep learning!\n\nAndrew\n\nP.S. Our new short course “Knowledge Graphs for RAG” is now available, taught by Andreas Kollegger of Neo4j! Knowledge graphs are a data structure that’s great at capturing complex relationships among data of multiple types. They can improve the context you pass to the LLM and the performance of your RAG applications by enabling more sophisticated retrieval of text than similarity search alone. In this course, you’ll build a knowledge graph from scratch and see how it improves chat applications by providing both text and graph data to an LLM.Sign up here!\n\nAnthropic announced a suite of large multimodal models that set new states of the art in key benchmarks.\n\nWhat’s new:Claude 3 comprises three language-and-visionmodels: Opus (the largest and most capable), Sonnet (billed as the most cost-effective for large-scale deployments), and Haiku (the smallest, fastest, and least expensive to use). Opus and Sonnet are available via the Claude API, onAmazon Bedrock, and in a private preview onGoogle Cloud. Opus also is  available with the Claude Pro chatbot, which costs $20 monthly. Sonnet powers Claude’s free chatbot.\n\nHow it works:The models, whose parameter counts are undisclosed, were trained on public, proprietary, and synthetic data ending in August 2023. They can process 200,000 tokens of context. Opus can accommodate up to 1 million tokens of context, comparable toGoogle’s Gemini 1.5 Pro, upon request.\n\nTest recognition:Opus aced “needle-in-a-haystack” tests to evaluate its ability to track long inputs. It also exhibited interesting behavior: In one such test, amid random documents that covered topics including startups, coding, and work culture, engineers inserted a sentence about pizza toppings and questioned the model on that topic. Not only did the model answer the question accurately, it also deduced that it was being tested, as Anthropic prompt engineer Alex Albertreportedin a post on X. “I suspect this pizza topping ‘fact’ may have been inserted as a joke or to test if I was paying attention,” Opus said, “since it does not fit with the other topics at all.”\n\nInside the system prompt:In a separate post on X, Anthropic alignment specialist Amanda Askell provided a rare peek at the thinking behind Claude 3’s system prompt, text prepended to user prompts to condition the model’s responses. To ground them in time, the models receive the current date and its training cut-off. To avoid rambling output, they’re directed to be concise. In an effort to correct for political and social biases that the team has observed, the models are asked to assist users even if it “personally disagrees with the views being expressed,” refrain from negative stereotyping of majority groups, and focus on objective information when addressing controversial topics. Finally, it’s directed to avoid discussing the system prompt unless it’s directly relevant to a query. “You might think this part is to keep the system prompt secret from you,” Askell wrote. “The real goal of this part is to stop Claude from excitedly telling you about its system prompt at every opportunity.”\n\nWhy it matters:Anthropic began with a focus onfine-tuning for safety, and its flagship model now tops several benchmark leaderboards as well. The Claude 3 family gives developers access to state-of-the-art performance at competitive prices.\n\nWe’re thinking:Three highly capable “GPT-4-class” large language models (LLMs) are now widely available: GPT-4, Gemini Pro, and Claude 3. The pressure is on for teams to develop an even more advanced model that leaps ahead and differentiates. What a great time to be building applications on top of LLMs!\n\nIndia advised major tech companies to seek government approval before they deploy new AI models.\n\nWhat’s new:India’s Ministry of Electronics and Information Technology (MeitY)issueda nonbinding “advisory” to technology firms, including Google, Meta, and OpenAI, to seek government permission before releasing AI models their developers consider unreliable or still in testing.\n\nHow it works:Thenoticeasks platforms and other intermediaries to label AI-generated media clearly and to warn customers that AI systems may output inaccurate information. It also says that models should avoid bias, discrimination, and undermining the integrity of the electoral process.\n\nBehind the news:India has regulated AI with a light touch, but it appears to be reconsidering in light of the growingroleof AI-generated campaign ads in its upcoming elections.\n\nWhy it matters:National governments worldwide, in formulating their responses to the rapid evolution of AI, must balance the benefits of innovation against fears of disruptive technology. Fear seems to weigh heavily in India’s new policy. While the policy’s scope is narrower than it first appeared, it remains unclear what constitutes a significant platform, how to certify an AI model as reliable, whether services like ChatGPT are considered social platforms that would be affected, and how violations might be punished.\n\nWe’re thinking:While combating misinformation is important, forcing developers to obtain government approval to release new models will hold back valuable innovations. We urge governments to continue to develop regulations that guard against harms posed by specific applications while allowing general-purpose technology to advance and disseminate rapidly.\n\nKnowledge graphs can structure complex data, drive intelligent search functionality, and help you build powerful AI applications that reason over different data types. In this course, you’ll learn how to use knowledge graphs to enhance large language models (LLMs).Sign up for free\n\nGoogle is paying newsrooms to use a system that helps transform press releases into articles.What’s new:Google has recruited a small number of independent news outlets for a one-year test of generative publishing tools,Adweekreported. The system reads external web pages and produces articles that editors can revise and publish.How it works:Google requires publishers to use the system to produce and publish three articles per day, one newsletter per week, and one marketing campaign per month. (It doesn’t require them to label the system’s output as AI-generated.) In exchange, publishers receive a monthly stipend that amounts to more than $10,000 annually.\n\nBehind the news:The pilot program is part of theGoogle News Initiative, through which the tech giant provides media literacy programs, fact-checking tools, and digital publishing tools to news outlets. Last year, Googledemonstrateda tool known as Genesis to news outlets includingThe New York Times,The Washington Post, andThe Wall Street Journal. Like the new system, Genesis took in public information and generated news articles. It also suggested headlines and different writing styles. Then, as now, observers worried that Google eventually would use its tools to bypass news outlets by publishing news summaries directly in search results.\n\nWhy it matters:Such partnerships could yield dividends for Google and publishers alike. Google can learn what publishers need and how a generative model built to produce news holds up under the pressure of deadlines and audiences. Publishers can gain experience that may help them avoid the criticisms that greeted outlets likeCNET,Gizmodo, andSports Illustrated, whose initial efforts to publish generated articles were either hidden behind false bylines or marred by factual inaccuracies.\n\nWe’re thinking:Text generation could be a boon to publishers. Checking generated text (or, indeed, any synthetic media) for similarity to its source material is a sensible feature that could be useful in a variety of applications. Yet the utility of a system that summarizes individual web pages is limited, and the temptation to echo competitors may be hard to resist. We look forward to further improvements that enable agents that can assimilate and analyze text from disparate sources.\n\nMachine learning models typically learn language by training on tasks like predicting the next word in a given text. Researchers trained a language model in a less focused, more human-like way.\n\nWhat’s new: A team at Stanford led by Evan Zheran Liu built areinforcement learning agent that learned language indirectlyby learning to navigate a simulated environment that provides text clues.\n\nKey insight:Reinforcement learning agents learn by discovering actions that maximize rewards. If the training environment provides text that explains how to achieve the highest reward, an agent will benefit by learning to interpret written language. That is, learning to comprehend written instructions will correlate with success in maximizing rewards.\n\nHow it works:The authors built a series of simulated two-dimensional environments usingMinigrid, a reinforcement learning library that contains grid-world environments. They trained the agent to find a particular room according to theDREAMreinforcement learning algorithm.\n\nResults:The authors tested the agent’s ability to generalize to text it had not encountered in training: They trained the agents on layouts that excluded text that described the blue room as the “third office in the second row” and tested it on layouts that included these words. The agent found the blue room every time without checking every room. They also tested the agent in layouts where the hallways were twice as long as in the training set. It always found the blue room. To determine whether the agent understood individual words in the instructions, the authors collected its embeddings of many instructions and trained a single-layer LSTM to extract the instructions from the embeddings. The LSTM achieved a perplexity (a measure of the likelihood that it would predict the next word of instructions that were not in its training data, lower is better) of 1.1, while a randomly-initialized network of the same architecture achieved 4.65 perplexity — an indication that the agent did, indeed, learn to read individual words.\n\nYes, but:The choice of reinforcement-learning algorithm was crucial. When the authors replaced DREAM with eitherRL2orVariBAD), the agent did not learn language. Instead, it learned to check all the doors.\n\nWhy it matters:The discovery that reinforcement-learning agents can learn language without explicit training opens avenues for training language models that use objectives different from traditional text completion.\n\nWe’re thinking:The authors focused on simple language (instructions limited to a few words and a very small vocabulary) that described a single domain (navigating hallways and rooms). There's a long road ahead, but this work could be the start of a more grounded approach to language learning in AI.\n\nData Pointsis your essential weekly roundup of short-form AI insights. From the approval of the AI Act, to AMD’s regulatory hurdle in the US, and a new copyright detection API by Patronus AI - we've got even more updates wrapped up for you inside.Read now.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/03/unnamed--51--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-13T144706.378.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/03/unnamed--52-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/03/V2_DeepLearning_Neo4js_Banner_2070x1080--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/03/unnamed--53-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/03/unnamed---2024-03-13T145040.685.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-116/",
    "title": "issue 116",
    "date": "",
    "reading_time": "",
    "content": "Years ago, whenever I had to do something boring or unpleasant — such as drive to work or go for a run — I used to listen to music to provide a distraction. Although I still appreciate music, as I got older I decided to cut out distractions. As a result, I’m more likely to sit in silence and enjoy being alone with my thoughts, or use the time more purposefully to learn something from an online course or audio book.\n\nMany people listen to music while studying or working. When is it helpful, and when is it distracting? People enjoy music — with good reason — and tend to have strong opinions about it. But someresearchshows that playing background music while trying to solve problems reduces creativity. Many people in the internet era are used to constant stimulation: scrolling of social media, consuming online news, filling empty hours with TV or video games. But finding quiet time when you can mull over your ideas remains an important part of being creative.\n\nTo be fair, the findings of research into the effect of music on cognition aremixed. For example, music sometimes improves mood, which in turn leads to better cognitive performance. Music also can drown out background noise that otherwise would be even more distracting. But I’ve found that when working, driving, or exercising, I prefer not to have any distractions and am happy to be left with my own thoughts. Since I stopped listening to music while driving, I’ve noticed that I’m much more likely to end the drive with new ideas for things I want to do.\n\nDoes this mean you shouldn’t listen to music? Of course not. Listening to music for sheer pleasure is a worthy use of time as well. But now I use music for enjoyment rather than distraction.\n\nIn addition to listening, one of my favorite ways to take a break from work is to play a piano (not very well!), sometimes with my daughter Nova in my lap providing accompaniment via a random banging on the keys. This serves no utilitarian purpose, but it puts me (and her) in a good mood, and I certainly plan to keep up my efforts to play!\n\nKeep learning,\n\nAndrew 🎵\n\nFacebook’s recommendation algorithm is a closely guarded secret. Newly leaked documents shed light on the company’s formula for prioritizing posts in an individual user’s feed.\n\nWhat happened:The Washington Postanalyzedinternal documents and interviewed employees to show how the company’s weighting of emojis, videos, subject matter, and other factors have evolved in recent years. ThePost’s analysis followed up on an earlierreportbyThe Wall Street Journal.\n\nHow it works:Facebook’s recommendation algorithm ranks posts for their likelihood to spur engagement according to more than 10,000 variables. Posts earn points for various attributes, and those with the highest score float to the top of a user’s newsfeed. The average post scores a few hundred points, but scores can reach 1 billion or more. Facebook is constantly adjusting the algorithm. The details below were drawn from past documents and may not reflect the current iteration:\n\nTurning points:Early on, Facebook’s recommendation algorithm prioritized updates from friends, such as a new photo or change in relationship status. In the early 2010s, the company tweaked it to favor likes and clicks. To counteract the resulting flood of clickbait, it was adjusted to promote posts from professional news media. In 2018, the company made changes to promote interaction between users by favoring reaction emojis, long comments, and reshares. This shift displayed more posts from friends and family but led to a surge of divisive content, prompting new rounds of changes in recent months.\n\nWhy it matters:Facebook’s membership of nearly 3 billion monthly active users famously exceeds the populations of the largest countries. What information it distributes, and to whom, has consequences that span personal, national, and global spheres. Both users and watchdogs need to understand how the company decides what to promote and what to suppress. Revealing all the details would invite people to game the algorithm, but some degree of transparency is necessary to avoid dire impacts includingsuicidesandpogroms.\n\nWe’re thinking:Internet companies routinely experiment with new features to understand how they contribute to their business. But Facebook’s own research told the company that what was good for its bottom line was poisonous for society. The company hasn’t been able to strike a healthy balance on its own. As a society, we need to figure out an appropriate way to regulate social media.\n\nGoogle designed its own AI chip for its new smartphone — a snub to Qualcomm, the dominant chip vendor in Android phones.\n\nWhat’s new:Googledebutedthe Tensor chip last week along with the global release of the new Pixel 6 smartphones. Company executivessaythe chip is well over four times faster than Qualcomm’s Snapdragon 765G in the Pixel 5, released last year.\n\nHow it works:Tensor serves as a power-efficient AI inference engine for on-device functions like voice transcription, language translation, and some image processing features.\n\nBehind the news:Qualcomm’s Snapdragon line of processors underpinned the earliest smartphones from Apple, Blackberry, and a wide variety of Android producers, including Pixel. Google's move to design its own chips mimics Apple's decision to do the same over a decade ago. Both companies continue to use Qualcomm chips for cellular communications.\n\nWhy It Matters:Advances in chip design and manufacturing are enticing companies with special processing needs to roll their own. Google tailored Tensor to suit its own AI technology while cutting its dependence on an outside supplier. That’s sure to help it make distinctive products. Look for more of the same from makers of all kinds of AI hardware.\n\nWe’re thinking:Google controls the Android operating system. The more tightly it binds Tensor and Android, the greater the incentive it has to sell the chip to phone markers, and the harder it will be for Qualcomm and others to compete on performing inference in Android phones.\n\nHave you checked out the updatedNatural Language Processing Specialization? Courses 3 and 4 now cover state-of-the-art techniques with new and refreshed lectures and labs!Enroll now\n\nTo understand a movie scene, viewers often must remember or infer previous events and extrapolate potential consequences. New work improved a model’s ability to do the same.\n\nWhat's new:Rowan Zellers and colleagues at University of Washington developedMultimodal Event Representation Learning Over Time(MERLOT), a pretraining method that concentrates knowledge gleaned from videos without requiring labeled data. The resulting representations helped fine-tuned models perform a variety of video-reasoning tasks with state-of-the-art accuracy.\n\nKey insight:Earlier work generated representations of videos by learning either to match video frames with associated text or to re-order scrambled frames in their original sequence. Training on both tasks can enable a model to generate richer representations that integrate visual, linguistic, and temporal information.\n\nHow it works:The authors divided six million YouTube videos into 180 million individual frames, each paired with corresponding text from a transcript.\n\nResults:MERLOT set a new state of the art for 14 tasks that involved answering questions about individual frames, answering questions about sequences of frames, and ordering disordered frames. It did especially well on question-answering tasks designed to test spatial and temporal reasoning on GIFs from Tumblr. For instance, MERLOT answered multiple-choice questions about the action performed in a clip with 94.0 percent accuracy versus the previous best score of 82.8 percent accuracy. In other areas, the improvement was less dramatic. For example, onDrama-QA, it answered multiple-choice questions about the story in clips from a TV show with 81.4 percent accuracy versus the previous best score of 81.0 accuracy.\n\nWhy it matters:MERLOT learned to pack a range of essential information about video images, accompanying text, and frame order into the representations it generated. The world is swimming in unlabeled video-plus-audio, and self-supervised learning algorithms like this could unlock tremendous value from such data.\n\nWe're thinking:We’re glad the authors didn’t keep this work bottled up.\n\nAccusations of fraud hang over some of the world’s most highly valued artworks. Machine learning engineers are evaluating the authenticity of these famous pieces.\n\nWhat’s new:Independent researchersdeterminedthatSalvator Mundi, the most expensive painting ever sold, was not painted entirely by Renaissance master Leonardo da Vinci, as had been claimed. In addition, the Swiss authentication company Art RecognitionfoundthatSamson and Delilah, a work credited to Peter Paul Rubens that hangs in London’s National Gallery, probably was painted by someone else.\n\nHow it works:Da Vinci produced few paintings, and he was also known to enlist assistants to help with his projects. Because of this, independent researchers Andrea and Steven Frank had just 12 verified da Vincis to train and test theirsystem.\n\nBehind the news:Salvator Mundiwas painted in the early 1500s and thought destroyed around a century later. The heavily damaged painting resurfaced in London in 1948. Experts there determined it was painted by one of da Vinci’s pupils, and it sold at auction for less than $50. After another sale, for $10,000 in 2005, evidence obtained during restoration convinced experts that it was an authentic da Vinci. It sold at auction for $450 million in 2017.\n\nWhy it matters:Fine art is a big business, and so is art fraud. Human experts often disagree in their assessments — and it may be impossible to establish the provenance of some works with complete certainty — but neural networks can supplement their judgments.\n\nWe’re thinking:If a human and a neural network disagreed about who created a picture, we’d just call it a draw.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202021-11-03%20at%209.37.11%20AM%20copy.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202021-11-03%20at%209.37.11%20AM%20copy.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-11-03T100016.431.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-11-03T100016.431.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/TENSOR.gif?upscale=true&width=1200&upscale=true&name=TENSOR.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20Image-3.png?upscale=true&width=1200&upscale=true&name=The%20Batch%20Image-3.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/MERLOTv2.gif?upscale=true&width=1200&upscale=true&name=MERLOTv2.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/DAVINCI.gif?upscale=true&width=1200&upscale=true&name=DAVINCI.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-178/",
    "title": "issue 178",
    "date": "",
    "reading_time": "",
    "content": "In last week’sissueofThe Batch, Yoshua Bengio, Alon Halevy, Douwe Kiela, Been Kim, and Reza Zadeh shared their hopes for AI in 2023. I also asked people onLinkedInandTwitterabout their hopes for AI this year. Rather than focusing on the latest buzzy topics in the news, many offered an amazing diversity of answers.In addition to hopes for further technical advances, common themes include:\n\nThat we all have so many different dreams for AI is a sign of how large our community has become and the broad footprint of our impact. It also means more fun technologies to learn about and more people we can learn from and collaborate with.\n\nI found the comments inspiring and am grateful to everyone who responded. If you’re looking for AI inspiration, take a look at the discussion and perhaps you’ll find ideas that are useful in your work. If you find the variety of comments overwhelming, consider writing software that clusters them into topics and share your results with me!\n\nKeep learning!\n\nAndrew\n\nThe world’s supply of data soon may fail to meet the demands of increasingly hungry machine learning models.What’s new:Researchers at Epoch AIfoundthat a shortage of text data could cause trouble as early as this year. Vision data may fall short within a decade.How it works:The authors compared the future need for, and availability of, unlabeled language and vision data. To evaluate language data, the authors focused on text from sources like Wikipedia, Arxiv, and libraries of digital books. These sources are subject to editorial or quality control, which makes them especially valuable for training large language models. With respect to vision data, they averaged thenumber of digital images producedand video uploaded to YouTube, Instagram, Snapchat, WhatsApp, and Facebook.\n\nResults:Dataset sizes needed to train large models will grow much faster than data supplies, the authors concluded.\n\nBehind the news:Epoch previouslycalculatedthe size and historical growth of training datasets.\n\nYes, but:The authors’ estimates have large margins of error, making for very imprecise estimates of time left before data might tap out. Moreover, they mention a number of events that could throw their projections off. These include improvements to the data efficiency of models, increases in the quality of synthetic data, and commercial breakthroughs that establish new sources of data; for instance, widespread use of self-driving cars would produce immense amounts of video.Why it matters:Despite gains insmall data, training on a larger quantity of high-quality data, if it’s available, is a reliable recipe for improved performance. If the AI community can’t count on that improvement, it will need to look elsewhere, such as architectures that don’t require so much data to train.We’re thinking:Many AI naysayers have turned out wrong when technical innovation overran their imaginations, and sometimes the innovator has thanked the naysayer for drawing attention to an important problem.Data-centricmethods improve the quality of data that already exists, enabling models to learn more from less data. In addition, novel training techniques have enabled less data-hungry models toachievestate-of-the-art results. And we might be surprised by the clever ways researchers find to get more data.\n\nTypical text-to-image generators can generate pictures of a cat, but notyourcat. That’s because it’s hard to describe in a text prompt precisely all the things that distinguish your pet from other members of the same species. A new approach guides diffusion models in a way that can produce pictures of your darling Simba.\n\nWhat's new:Rinon Gal and colleagues at Nvidia and Tel-Aviv University devised amethodto make a diffusion-based, text-to-image generator produce pictures of a particular object or in a particular style.\n\nBasics of diffusion models:During training, a text-to-image generator based on diffusion takes a noisy image and a text description. A transformer learns to embed the description, and a diffusion model learns to use the embeddings to remove the noise in successive steps. At inference, the system starts with pure noise and a text description, and iteratively removes noise according to the text to generate an image. A variant known as alatent diffusion modelsaves computation by removing noise from a small, learned vector of an image instead of a noisy image.\n\nKey insight:A text-to-image generator feeds text word embeddings to an image generator. Adding a learned embedding that represents a set of related images can prompt the generator to produce common attributes of those images in addition to the semantic content of words.\n\nHow it works:The authors used atext-to-image generatorbased on a latent diffusion model. The system was pretrained on400 million text-image pairsscraped from the web. Its weights were frozen.\n\nResults:The authors evaluated their model’s output by comparing embeddings, generated byCLIP, of original and generated images. They measured similarity on a scale from 0 to 1, where 1 signifies two identical inputs. The model scored around 0.78. Images generated using human-crafted descriptions of up to 12 words — without reference to S∗ — scored around 0.6. Images generated using longer descriptions of up to 30 words scored around 0.625.\n\nWhy it matters:The authors’ method offers a simple way for users of diffusion-based, text-to-image generators to steer the output toward specific attributes of content or style without retraining the model.\n\nWe’re thinking:Could this approach be extended to encompass multiple learned vectors and allow users to combine them as they like? That would make it possible to control image generation in even more precise ways.\n\n\"You don’t have to be a mathematician to have a feel for numbers,\" said mathematician John Forbes Nash, Jr. Get a feel for the numbers withMathematics for Machine Learning and Data Science, our new specialization.Join the waitlist\n\nMany workers benefit from AI in the office without knowing it, a new study found.\n\nWhat’s new:MIT Sloan Management Review and Boston Consulting Groupsurveyedemployees on their use of AI in their day-to-day work. Their findings: The technology offers benefits to individuals and organizations, but employers may need to educate and direct workers to realize them.\n\nWhat it says:The authors surveyed 1,741 respondents in over 20 industries and 100 countries. They also interviewed 17 executives about how AI is used in their organizations.\n\nConsumer vs. pro products:The authors polled respondents on their use of AI products in four categories.\n\nBehind the news:A recent studysupportsthe notion that AI bolsters workers more than it replaces them. Employment rates rose between 2008 and 2018 in a number of professions subject to AI-powered automation including fast food worker, translator, and financial advisor.\n\nWhy it matters:Many workers justifiably worry that AI will make their jobsobsolete. This survey suggests instead that AI is broadly enhancing many workers’ jobs.We’re thinking:It's not necessarily bad that many people don’t recognize AI’s role in their everyday lives. Successful technology often disappears into the background. We talk about turning on lights, not electric lights, because electricity works so well that we take it for granted. If AI is the new electricity, we can expect it to be taken for granted, too.\n\nAmazon published a series of web pages designed to help people use AI responsibly.\n\nWhat's new:Amazon Web Servicesintroducedso-called AI service cards that describe the uses and limitations of some models it serves. The move is an important acknowledgment of the need to describe the workings of machine learning models available to the general public.\n\nHow it works:The company documented three AI models:Rekognitionfor face matching,Textract AnalyzeIDfor extracting text from documents, andTranscribefor converting speech to text.\n\nBehind the news:In 2018, researchers including Margaret Mitchell and Timnit Gebru, who were employed by Google at the time,introducedthe concept of model cards to document a model’s uses, biases, and performance. Googleimplementeda similar approach internally the following year.\n\nWhy it matters:Model cards can help users take advantage of AI responsibly.Hundreds of thousandsof people use cloud services that offer AI functions including prebuilt models. Knowing what the models were intended to do, what their limitations are, and so on can help users deploy them effectively and avoid misuses that could lead them into ethical or legal trouble.\n\nWe're thinking:We applaud Amazon’s efforts to increase transparency around their models. We look forward to service cards for more models and, hopefully, tools that help developers increase the transparency of their own models.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/01/ezgif.com-gif-maker--8--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--23--1.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--24-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/01/DeepLearning_Mathematics_Campaign_Quotes_Nash.png",
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--25-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/01/unnamed--26-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-xii/",
    "title": "issue xii",
    "date": "",
    "reading_time": "",
    "content": "As we were working on the latest course of the deeplearning.ai TensorFlow Specialization, instructor Laurence Moroney messaged me his LSTM-generated poetry, created by learning from a database of roughly 100 Irish song lyrics:\n\nAndrew sang a sad old songFainted through Miss Milliner[...]Punch and wine for the same partyAs red as a jig rose painted of runctions.Laurence tells me \"runctions\" is Irish slang mischief!So now I'm ready to announce my new list of reasons why you should work in AI:\n\nIf you want to learn how to build such models yourself, check out theTensorFlow Specialization.\n\nKeep learning,Andrew\n\nIf you’re pointing out an object, you don’t describe the background. Yet most object detection algorithms focus on a rectangle surrounding the object, not its precise boundary. New research offers a way to turn those boxes into tightly fitted curves.What’s new:Researchers at the University of Toronto and Nvidia achieved state-of-the-art accuracy in boundary detection withSemantic Thinning Edge Alignment Learning. STEAL is a new approach that augments existing boundary detection networks.Key insights:Human-drawn boundaries are often imprecise because people are impatient and emphasize quantity over quality. But we can use them as a starting point.\n\nHow it works:Given a human-drawn boundary, STEAL predicts the boundary a human would draw given more time. Then, given a boundary detection network, STEAL forms a new network by appending a thinning layer to the original network’s output. The new network is trained to reconstruct STEAL’s inferred boundaries, not the human-drawn ones.\n\nWhy it matters:STEAL achieves a new state of the art, finding boundaries up to 18.6 percent more precise than its predecessor, CASENet SEAL, on hand-drawn and refined test sets. Looking at the output confirms that STEAL produces tight, accurate boundaries.Takeaway:Object detection has a multitude of uses: image caption generation, face detection, and autonomous navigation to name a few. All these tasks have shown impressive results with object detection based on bounding boxes. Using STEAL’s more precise boundaries could reduce errors and further boost accuracy in these fields.\n\nWorries over deepfake technology have concentrated on the potential impact of computer-generated media on politics. But image-generation technology also can have a malign impact on private citizens. Consider DeepNude, an app that generates naked images from photos of clothed women.What’s new:Motherboardreportedon DeepNude, which had been quietly gaining traction since it was released in March. The exposé set off a firestorm of criticism. Within 24 hours, the developer withdrew the app. It had been downloaded more than 500,000 times.How it works:The anonymous developer explained that the app was inspired by old comic book ads for X-Ray Spex, a toy that purported to let the wearer see through clothing.\n\nThe reaction:The developerannouncedthe decision to delete the app with a message on Twitter that concluded, “The world is not yet ready for DeepNude.” In the ensuing thread, people expressed outrage and disgust that the app was developed at all:\n\nOur take:Some users regard the app as harmless fun, like an Instagram filter that adorns human faces with a cartoonish dog nose. Such an attitude willfully ignores that representing any person without clothing and without permission is an attack on their privacy and autonomy. A fake nude can create false impressions and reinforce negative stereotypes, leading to embarrassment at best, sexual violence at worst. AI is a tool: Use it constructively, and you have a chance to make the world a better place. Wield it as a weapon, and you’ll leave a trail of pain and destruction.\n\nSchools, prisons, and other public places have been relying on AI-enhanced microphones to detect signs of trouble. They may have been better off putting their money elsewhere.What’s new:ProPublicaandWiredjointlyinvestigateda widely-used system designed to listen to the surrounding environment and report sounds that indicate aggression in progress. They found the technology prone to false positives. For instance, a cough triggered an alert, as did a classroom full of students cheering for pizza. Yet it failed to register screams in more than 50 percent of trials.Smart security:The Dutch firm Sound Intelligence developed the system, which uses machine learning models to detect “sound patterns associated with duress, anger, or fear.”\n\nMixed reviews:A hospital in Fort Myers, Florida, said Sound Intelligence devices alerted its guards to unruly visitors, allowing them to intervene before situations escalated. However, another hospital in Ridgewood, New Jersey, cancelled its $22,000 pilot program after efforts to calibrate the system culminated in an incident that required six security officers to resolve.What the company says:Sound Intelligence CEO Derek van Der Vorst acknowledged the reporters’ findings and admitted the devices aren't perfect. “I wouldn’t claim that we could prevent a crazy loony from shooting people,” he toldProPublica.What experts say:“Happy or elated speech shares many of the same signatures as hot anger,” said Shae Morgan, an audiologist at the University of Louisville’s medical school. He pointed out that many acts of aggression — for example, the 2018 school shooting in Parkland, Florida— are preceded by nothing but cold silence.We’re thinking:One of the biggest non-technical challenges facing AI is providing the public with a realistic assessment of its capabilities. It can work beautifully in a lab staffed by clear-eyed engineers yet fail in a room packed with gleeful kids. Developers need to be rigorous in devising real-world tests, and their colleagues in sales need to recognize that hype can do more harm than good.\n\nWant to understand the key parameters of a neural network’s architecture? Learn foundational deep learning concepts and the technologies driving them in the Deep Learning Specialization.Enroll here\n\nTeaching a computer meaningful associations between words and videos typically requires training on tens of thousands of videos painstakingly annotated by hand. That’s both labor-intensive and prone to inconsistency due to the often abstract relationship between video imagery and soundtrack. Researchers at the École Normale Supérieure and elsewhere devised an effective shortcut.What’s new:A team led by Antoine Miech and Dimitri Zhukov assembled 136 million video clips from narrated instructional videos to produce theHowTo100Mdata set. This large corpus, in which the words correspond closely to the images, enables new models to be trained that yield stellar results in a variety of tasks.Key insights:Previous research has taken advantage of the tight semantic correspondence between words and imagery in instructional videos. But that work has extracted only a small number of labels rather than analyzing complete transcriptions. Miech et al. found that:\n\nHow it works:The researchers found instructional videos on YouTube. Their model consists of a collection of pretrained subnetworks to extract video and word features. It's trained to maximize the similarity between video and word vectors belonging to the same video.\n\nResults:The team bettered the previous state of the art by as much as 50 percent on several clip retrieval benchmarks. Models pretrained on HowTo100M and fine-tuned on other data showed significant improvements in the target task compared to models trained from scratch.Why it matters:At 136 million clips, HowTo100M is the largest video-text public data set, dwarfing previous efforts by orders of magnitude. The resulting video-text embeddings could dramatically improve the accuracy of neural networks devoted to tasks like video captioning, scene search, and summarizing clips.Takeaway:HowTo100M widens the intersection of computer language and vision. It could lead to better search engines that retrieve relevant scenes described in natural language, or systems that generate artificial videos in response to natural-language queries. More broadly, it’s a step closer to machines that can talk about what they see.\n\nThe U.S. Food and Drug Administration regulates medical devices from stents to diagnostic systems. Once approved, those things don’t change much. Now the agency is formulating standards for medical devices that take advantage of AI that's constantly learning.What’s new:The first public comment period for the FDA’s draft framework on AI-based medical devices ended in June. TheNational Law Reviewpored over the comments.NLRreportsthat the AI community is pushing for tighter definitions and clearer understanding between industry and government, even as it broadly supports that agency’s effort.The issue:Current rules for software in medical devices require manufacturers to submit programs for review with each significant update. That works for algorithms that are locked and updated periodically, say, in a system that monitors people for signs of stroke. But it’s not a great fit for models that learn from use, like a system that spots cancer more accurately with increasing exposure to real-world data.Moving target:The FDA wants to establish a lifecycle approach to AI-based medical devices. According to the guidelines, the agency would ask developers to submit a roadmap of expected changes in a model’s output as it learns. Developers also would describe how they expect to manage any risks that might arise as the model adjusts to new data.Public opinion:Businesses, professional groups, and concerned individuals who submitted comments generally liked the approach but requested a number of tweaks:\n\nWhat’s next:The process is bound to wind through many more steps. Expect another draft framework, further rounds of public feedback, and internal reviews before the rules are finalized.Our take:Regulation is messy. That goes double for medicine, and perhaps triple for AI. Still, it's critical to protect patients without squelching innovation. Government, industry, and researchers all have an important role to play in hammering out the final rules.\n\nAutomatons will take 20 million manufacturing jobs globally by 2030 even as they drive substantial economic growth, according to a new study. Humans can get ahead of the curve by distributing the benefits where they’ll be needed most.What’s new:UK business analytics firm Oxford Economics issuedHow Robots Change the World, which the authors call an “early warning system” of coming social impacts. The report compares investment data from the International Federation of Robotics with statistics tracking employment, wages, and GDP across a number of job sectors, countries, and demographics.Industries hit:Manufacturing has seen the highest degree of automation so far. The researchers expect that trend to continue, but they also expect robots soon to begin a grand takeover of the service industry. For instance, automated vehicles will curb professional drivers from Lyft to long-haul.U.S outlook:Poor populations and rural regions that rely on manufacturing will be hit hardest:\n\nThe big picture:A similar pattern is expected to play out internationally:\n\nThe bright side:The impacts won't be entirely negative:\n\nOur take:Even if new jobs appear as quickly as old ones evaporate, the robot revolution will disrupt human lives. Some people won't take well to retraining. Others might not live where new jobs arise. Policymakers in manufacturing-heavy regions can mitigate the worst impacts by fostering industries less prone to automation — technology, communications, healthcare, finance, tourism — and providing training so people will be ready for those jobs when they come.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/9becb9ac-b8a3-458c-bad4-a7b693f5eaaf-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/f5f88cf6-de64-420b-836e-054a56a05bbc.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/41f0674b-c373-492f-8f92-34510e60f5ed.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/dfbcdc70-a9a2-4967-95b8-6866a6a0a6bf-2.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/ea2cabe7-253c-4d1a-b139-2eb832c54830.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/ffd6f4c4-a3e6-43dc-b6ad-c6c340b14495.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/6f3b819b-9cfd-4763-bd86-e467b3688063.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-131/",
    "title": "issue 131",
    "date": "",
    "reading_time": "",
    "content": "I tested positive for Covid on Monday and mentioned this on social media. I’m grateful to the many people who wished me well. Reading your messages made me feel better. My unscientific feeling is that they helped clear my nose a bit!I was surprised by a tiny minority who responded to my catching Covid with vitriol. For example, one person posited that I’d been waiting to catch Covid so I could announce it for a social media “win.”\n\nNext Monday will be February 14, and I wish you an early Happy Valentine’s Day. Reading over the social media replies reminded me of the importance of spreading love, not hate.\n\nI feel blessed to have received support from many people this week and over the years, so a little vitriol doesn’t bother me. But the potential impact of hateful speech on others worries me. Insults, put-downs, and unwarranted criticism diminish people and discourage them from living up to their highest potential.If you’re ever feeling put upon, know that I’m on your side. I know you’re doing your best and don’t need unnecessary flack.People need love. So, as we approach Valentine’s Day, I hope you’ll tell people in your life that you love them. You care for them. You wish them well. Let’s do this on February 14 and every day of the year.\n\nWith love and affection,\n\nAndrew\n\nFinnish entrepreneur Kai Saksela, a structural engineer by training, studied deep learning. Now he's using neural networks to recognize sounds that signal danger in electrical equipment.Read more\n\nProgramming is hard. Programming competitions are harder. Yet transformers proved themselves up to the task.What’s new:Yujia Li, David Choi, Junyoung Chung, and a team at DeepMind builtAlphaCode, a system that beat roughly half of competitors in coding contests where many examples of program inputs and outputs were available.Key insight:Previous workshowed that transformers can generate code, though their output doesn’t always solve the task at hand. But transformers can generate millions of possible solutions to the same problem instantly, and the solutions can be filtered by checking their performance automatically. Those that remain should solve the problem.How it works:The authors trained a transformer to generate programs based on problems from adatasetthey built containing 13,000 challenges mainly fromCodeforces, a platform that hosts coding contests. Each problem included hundreds of solution programs (incorrect as well as correct) along with roughly 100 examples of test cases (expected inputs and outputs) mostly created by the authors.\n\nResults:The authors used AlphaCode in 10 simulated Codeforces competitions, allowing it two hours to generate solutions for each. Ranking its performance among 5,000 Codeforces competitors, it averaged in the 54th percentile (lower is better). It correctly solved 34.2 percent of problems in the validation set.Why it matters:AlphaCode generated 1 million possible solutions and culled the bad ones to solve problems it had never seen before and beat a substantial portion of competitive human programmers. It goes to show that there are still benefits to be gained from scaling up.We’re thinking:AlphaCode is an impressive demonstration of high-throughput code generation and testing. That said, considering its performance on the validation set, there’s still a distance to go.\n\nFacebook’s parent company is staking its future on a new compute cluster.What’s new: Meta unveiledAI Research SuperCluster(RSC), which is designed to accelerate training of large models for applications like computer vision, natural language processing, and speech recognition.How it works:The company began building RSC in 2020, aiming for a system capable of training trillion-parameter models and processing up to an exabyte (1 billion gigabytes) of data. It currently incorporates 6,080 Nvidia A100s, the chip vendor’s flagship graphics processing unit (GPU).\n\nBehind the news:RSC’s emphasis on data protection has a backstory. French regulators recentlyfinedthe company $238 million for failing to allow users to disable tracking software. In September, IrelandchargedFacebook’s WhatsApp messaging service nearly $270 million for lack of transparency around how it uses the user data it collects. Those actions came after the U.S. Federal Trade Commission responded to violations of user privacy byimposinga historic $5 billion penalty as well as restrictions on the company’s structure and operations.Why it matters:Specialized in-house processing capacity is a strategic asset in the era of cloud computing. RSC is essential to Meta’s aspiration to build an immense virtual reality community it calls themetaverse.MicrosoftandNvidialikewise have built their own bespoke infrastructure.We’re thinking:Less than a decade ago, the cutting-edge AI supercomputer was a$100,000 cluster(that Andrew Ng worked on). How much bigger — and, unfortunately, less accessible — these systems have become!\n\nJoin uson February 16, 2022, at 10 a.m. Pacific Time for a live session with Sadie St. Lawrence, founder and CEO of Women in Data. Learn which skills you need for a career in machine learning and artificial intelligence.\n\nThe higher the accuracy of a pretrained model, the better its performance  after fine-tuning, right? Not necessarily.What’s new:Samira Abnar and colleagues at Google Research conducted ameta-analysisof image-recognition experiments and performed some of their own. They analyzed the relationship between model performance after pretraining and after fine-tuning in a variety of tasks.Key insight:To find out whether higher pretrained accuracy always leads to higher fine-tuned accuracy, it would be necessary to run thousands of experiments while varying hyperparameter values systematically for each task. A simpler way is to extrapolate the relationship from the results of existing experiments.How it works:The authors re-examined 4,800 experiments performed on diverse architectures:Vision Transformers,MLP-Mixers, andResNets. The models had been pretrained to classify labeled images inJFTorImageNet 21K. They were tested on 25 tasks, including classifying objects, classifying the orientation of objects, and diagnosing diabetic retinopathy, after fine-tuning via few-shot learning or transfer learning. In few-shot learning, the last layer was replaced and trained on 25 examples. In transfer learning, the whole network was fine-tuned on 1,000 examples.\n\nResults:Higher pretrained accuracy generally yielded higher fine-tuned accuracy — but it reached a point of diminishing returns. In some cases, higher pretrained accuracy yielded worse fine-tuned accuracy. Moreover, pretrained models of equal accuracy didn’t necessarily perform equally well on different fine-tuned tasks. The authors’ own experiments matched the curves they derived from earlier work, leading them to conclude that dataset size, number of parameters in a model, and length of training don’t significantly influence the relationship between pretrained and fine-tuned accuracy.Why it matters:More pretraining doesn’t necessarily result in a better fine-tuned model.We’re thinking:One limiting factor in the value of pretraining accuracy may be the relevance of the pretrained task to the fine-tuned task. No matter how well a model classifies ImageNet, it may not easily learn how to diagnose medical images. A rigorous framework for managing the tradeoff between pretraining and fine-tuning would be useful.\n\nWorkers can operate a forklift in their pajamas and never leave their bedrooms, thanks to a new generation of AI-assisted robots.What’s new:Companies are pairing semi-autonomous vehicles with remote human operators to execute tasks that the vehicles can’t handle on their own,Wiredreported.Distance driving:Robots that use AI to navigate around warehouses or perform manual labor can encounter situations that weren’t well represented in their training data. When that happens, a remote operator can step in — sometimes from a continent away.\n\nWhy it matters:The pandemic has left 597,000 logistics jobs unfilled in the U.S. alone, according to the National Bureau of Economic Research. A combination of automation and remote human intervention might cover the gap, provide more flexible working conditions, and smooth wrinkles in global supply chains.We’re thinking:Even as remote-controlled, semi-autonomous technology creates new jobs, it’s bound to eliminate others. Companies need to devote ample resources to retraining and upskilling workers whose jobs are at risk.",
    "images": [
      "https://cdn2.hubspot.net/hub/5871640/hubfs/NeuralNetworkValentine_BigHeart_600x338.jpg?upscale=true&width=1200&upscale=true&name=NeuralNetworkValentine_BigHeart_600x338.jpg",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202022-02-08%20at%2010.06.22%20AM.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202022-02-08%20at%2010.06.22%20AM.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20(3)-3.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20(3)-3.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/SUPERCOMPUTER.gif?upscale=true&width=1200&upscale=true&name=SUPERCOMPUTER.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/Screen%20Shot%202022-02-08%20at%204.09.17%20PM.png?upscale=true&width=1200&upscale=true&name=Screen%20Shot%202022-02-08%20at%204.09.17%20PM.png",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/PRETRAININGv3.gif?upscale=true&width=1200&upscale=true&name=PRETRAININGv3.gif",
      "https://cdn2.hubspot.net/hub/5871640/hubfs/REMOTE.gif?upscale=true&width=1200&upscale=true&name=REMOTE.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-2/",
    "title": "issue 2",
    "date": "",
    "reading_time": "",
    "content": "We ended a busy week in Colombia hosting a Pie & AI meetup in Medellín. I met hundreds of engineers and business professionals excited to take the next step in their AI careers. I was energized by the enthusiasm the Colombia community had for collaborating and for supporting each other to build up the local AI ecosystem. AI is still young enough that many cities can still become hubs of AI talent, but the city has to make smart investments, and the community has to work hard and keep learning, which Colombia is doing. I hope the future will bring many more global AI hubs.\n\nI also drank a lot of coffee on this trip. I don’t know whether it was because the coffee really was fresher or if it was a placebo effect, but Colombian coffee tasted better in Colombia than when I drink it at home!\n\nKeep learning,Andrew\n\nSilicon Valley startup Cerebras shifted out of stealth mode to unveil its flagship product: an enormous chip designed from the ground up to accelerate neural networks.\n\nWhat’s new:TheCerebras Wafer Scale Engineis aimed at data centers, where the company claims it will perform AI computations 100 to 1,000 times faster than alternatives. The chips will be housed in servers equipped with a special cooling system to dissipate the chip’s heat. They’re scheduled to reach the market next month for an undisclosed price.\n\nWhy it’s different:Where many chips are measured in millimeters, this monster is 56 times larger than Nvidia’s top-of-the-line GPU and bigger than a standard iPad. It comprises more than 400,000 cores and 18 gigabytes of memory right on the chip. That’s equivalent to 84 GPUs communicating with one another 150 times more efficiently than usual, with an additional boost thanks to the ability to handle sparse linear algebra.\n\nHow it works:Nvidia’s chip architecture is extraordinarily efficient at performing the predictable, repetitive matrix multiplications required by neural networks. Yet it has practical limitations: It must hold an entire neural network in off-chip memory and communicate with other chips through external interfaces that are far slower than communication on the chip itself.\n\nBehind the news:Deep learning’s rapid growth has prompted a top-to-bottom redesign of computing systems to accelerate neural network training.\n\nWhy it matters:If the new hardware works as advertised, it will open virgin territory for neural networks several orders of magnitude bigger than today’s largest models. Larger models have been shown to yield higher accuracy, and the additional headroom may well allow new kinds of models that wouldn’t be practical otherwise.\n\nWe’re thinking:The advent of Nvidia GPUs two decades ago spurred innovations in model architecture that boosted the practical number of network layers from handfuls to 1,000-plus. Cerebras’ approach portends fresh architectures capable of solving problems that are currently out of reach. We don’t yet know what those models will look like, but we’re eager to find out!\n\nResearchers typically test deep reinforcement learning algorithms on games fromSpace InvaderstoStarCraft. The Google Brain team in Zurich adds another option: football, also known as soccer.What’s new:Google Research Footballallows experiments on a variety of RL techniques in a single environment: self playing, stochastic environment, multi-agent cooperation, and several styles of state representation. Check out the videohere.Key insight:Popular games generally are either easy to win or offer rewards that are too sparse. Most don’t allow for cooperative agents or graduated degrees of difficulty that would help the agents learn basic strategies. Google Research Football is designed to solve all these problems in one go, and it’s open source to boot.How it works:Karol Kurach and his team provide a physics-based soccer simulator with full-length, 11-player games at a range of difficulty levels. They also offer short scenarios from simple (single player scoring in an empty net) to complex (team coordination to score from a corner kick). Users can build their own scenarios as well.\n\nObservations:The algorithms supplied quickly solve the easy situations, but they struggle on medium and hard settings even after long periods of training. Performance also depends on the input representation and the number of agents involved.Why it matters:GRF is a challenge even for today’s best RL algorithms. It gives researchers a multi-agent environment where they can work on improving agents by having them compete with one another, and it provides resources for building more capable agents through increasing degrees of difficulty in an environment that resembles the real world.\n\nWe’re thinking:This might be a good time to take to the virtual field and compete for the football leaderboard, as reinforcement learning begins to take on the world’s most popular sport.\n\nThe origin of the brief, high-intensity signals from outer space called fast radio bursts baffles astronomers. Now AI is generating real-time data to help solve the mystery.\n\nWhat’s new:A machine learning model deployed at the Molonglo Radio Telescope in Australiadetectedfive fast radio bursts in unprecedented detail.\n\nHow it works:The Molonglo telescope uses a standard program to flag incoming electromagnetic waves as fast radio burst candidates. However, the mystery signals share the same frequency band as cell phones, lightning storms, and solar emissions, so the system is prone to false positives. Researcher Wael Farah developed a machine learning model to pick out the most viable candidates.\n\nResults:Since the model debuted in April, 2018, it has flagged the most energetic fast radio burst and the one with the broadest spectrum, and it has captured the most detailed view of the signals’ rapidly fluctuating voltage.\n\nBehind the news:Earlier this year, American scientist Brian Metzger won a $3 million Breakthrough Prize for hisworkon a theory about the genesis of fast radio bursts — not SOSes from an alien intelligence, sadly, but shock waves produced by young neutron stars with dense magnetic fields.\n\nWhy it matters:Testing ideas about fast radio bursts requires more, and more detailed, data. Farrah’s model delivers it.\n\nWe’re thinking:Telescopes collect a crushing torrent of data. With the help of AI, human astronomers might manage to analyze them before the universe’s Big Crunch.\n\nYou may know how to set up a train/dev/test set, but there’s still more to learn. Take theDeep Learning Specializationand learn how to avoid major pitfalls.\n\nUnlike bats, humans can’t see with their ears. Now an app is giving sightless pedestrians the ability to navigate by ear.\n\nWhat’s new:Microsoft’s Artificial Intelligence and Research Laboratory offers a free iPhone app calledSoundscape. Unlike earlier efforts that tried to identify objects visually, the app orients pedestrians in space by calling out nearby buildings, businesses, landmarks, and road crossings as the walker approaches.How it works:Essentially a navigation app. But unlike conventional navigation apps that issue directions to a destination (“Turn left here!”), Soundscape narrates points of interest along the way. That helps people who don’t see well to explore like a sighted person can — for example, popping into a bakery that caught their attention on the way to work, or simply taking a random walk.\n\nBehind the news:Project lead Amos Miller, a developer and product strategist at Microsoft, lost his sight as an adult due to a genetic condition. You can hear an interview with him in thispodcast.Why it matters:Several previous apps for visually impaired people attempt to replace human vision with computer vision: Point a camera at an object or person, and the app classifies what it sees. That approach has yet to catch on, leaving the field ripe for fresh approaches.\n\nWe’re thinking:Soundscape isn’t just for the sight-impaired. It may be worth a try the next time you visit a new city and want to take in the sights without constantly referring to a map.\n\nNeural networks are good at making predictions, but they’re not so good at estimating how certain they are. If the training data set is small and many sets of model parameters fit the data well, for instance, the network may not realize this explicitly, leading to overly confident predictions. Bayesian models, on the other hand, theoretically can sample from the posterior distribution of parameters. However, the computational load becomes overwhelming as the number of parameters rises. New research allows Bayesian modeling of uncertainty to be applied even to large networks.\n\nWhat’s new:Researchers at Google Brain built neural networks that integrate a Bayesian backpropagation method known as Stochastic Gradient Markov Chain Monte Carlo, fixing issues with noisy updates and slow convergence that affected earlier work. Their technique,Adaptive Thermostat Monte Carlo(ATMC), is the first based on SG-MCMC that scales to larger data sets such as ImageNet.\n\nKey insight:Previous research using SG-MCMC failed to find training procedures that were robust to noise arising from parameter sampling in Bayesian methods. ATMC compensates for these issues by adjusting momentum and noise applied to parameter updates.\n\nHow it works:Non-Bayesian learning techniques compute the loss from outputs and labels only. Bayesian techniques add a prior distribution on learnable parameters. All methods based on SG-MCMC are derived from a stochastic differential equation that modifies a neural network’s parameter distribution based on the sampled output.\n\nResults:ATMC is the first SG-MCMC method successfully trained on ImageNet. An ATMC-trained network gains a 1 percent increase over a batch-normalized ResNet in ImageNet top-1 accuracy.\n\nWhy it matters:Estimating uncertainty can be crucial in applications such as medical imaging and autonomous driving. ATMC confers this capability on neural networks even when learning large, complex data sets such as ImageNet.\n\nWe’re thinking:Bayesian methods have been studied longer than neural networks, and they still define the state of the art in some tasks. The fusion of Bayesian models and neural networks is still evolving. ATMC suggests that such hybrids could deliver the advantages of both approaches.",
    "images": [
      "https://cdn2.hubspot.net/hubfs/5871640/ColombiaPie&AICollage.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_cerebras220sized.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_soccer.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_radio20waves20sized-1024x576.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Avoid20Pitfalls.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_ezgif.com-resize.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Screen20Shot202019-08-2720at203.12.1620PM.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-212/",
    "title": "issue 212",
    "date": "",
    "reading_time": "",
    "content": "I’d like to share a part of the origin story of large language models that isn’t widely known. A lot of early work in natural language processing (NLP) was funded by U.S. military intelligence agencies that needed machine translation and speech recognition capabilities. Then, as now, such agencies analyzed large volumes of text and recorded speech in various languages. They poured money into research in machine translation and speech recognition over decades, which motivated researchers to give these applications disproportionate attention relative to other uses of NLP.This explains why many important technical breakthroughs in NLP stem from studying translation — more than you might imagine based on the modest role that translation plays in current applications. For instance, the celebrated transformer paper, “Attention is All You Need” by the Google Brain team, introduced a technique for mapping a sentence in one language to a translation in another. This laid the foundation for large language models (LLMs) like ChatGPT, which map a prompt to a generated response.\n\nOr consider theBLEU score, which is occasionally still used to evaluate LLMs by comparing their outputs to ground-truth examples. It was developed in 2002 to measure how well a machine-generated translation compares to a ground truth, human-created translation.\n\nA key component of LLMs is tokenization, the process of breaking raw input text into sub-word components that become the tokens to be processed. For example, the first part of the previous sentence may be divided into tokens like this:\n\n/A /key /component /of /LL/Ms/ is/ token/ization\n\nThe most widely used tokenization algorithm for text today is Byte Pair Encoding (BPE), which gained popularity in NLP after a 2015paperby Sennrich et al. BPE starts with individual characters as tokens and repeatedly merges tokens that occur together frequently. Eventually, entire words as well as common sub-words become tokens. How did this technique come about? The authors wanted to build a model that could translate words that weren’t represented in the training data. They found that splitting words into sub-words created an input representation that enabled the model, if it had seen “token” and “ization,” to guess the meaning of a word it might not have seen before, such as “tokenization.”\n\nI don’t intend this description of NLP history as advocacy for military-funded research. (I have accepted military funding, too. Some of my early work in deep learning at Stanford University was funded by DARPA, a U.S. defense research agency. This led directly to my starting Google Brain.) War is a horribly ugly business, and I would like there to be much less of it. Still, I find it striking that basic research in one area can lead to broadly beneficial developments in others. In similar ways, research into space travel led to LED lights and solar panels, experiments in particle physics led to magnetic resonance imaging, and studies of bacteria’s defenses against viruses led to the CRISPR gene-editing technology.\n\nSo it’s especially exciting to see so much basic research going on in so many different areas of AI. Who knows, a few years hence, what today’s experiments will yield?\n\nKeep learning!\n\nAndrew\n\nP.S. Built in collaboration with Microsoft, our short course “How Business Thinkers Can Start Building AI Plugins With Semantic Kernel” is now available! This is taught by John Maeda, VP of Design and AI (who also co-invented the Scratch programming language!). You’ll join John in building his “AI Kitchen” and learn to cook up a full AI meal from, well, scratch – including all the steps to build full business-thinking AI pipelines. You’ll conclude by creating an AI planner that can automatically select plugins it needs to produce multi-step plans with sophisticated logic.Sign up to learn here!\n\nAnthropic, the startup behind the safety-focused Claude chatbot, teamed up with South Korea’s largest mobile phone provider.\n\nWhat’s new:The independent research lab, which is an offshoot of OpenAI, will receive $100 million from SK Telecom to build a multilingual large language model tailored for the telecommunications industry,VentureBeatreported.\n\nHow it works:Anthropic will base the specialized model on the technology that underpins its large language modelClaude. SK Telecom plans to offer it to other telecoms firms, such as members of theGlobal Telco AI Alliance, a consortium devoted to building new lines of business based on AI-driven services.\n\nBehind the news:SK Telecom has a history of building its own machine learning models, particularly Korean-language models. The company emulated GPT-3's architecture to train models likeKo-GPT-Trinity-1.2B. An unidentified modelenablesA. (pronounced “a dot”), a virtual assistant for the company’s mobile users.\n\nWhy it matters:AI models have a bright future in virtually every industry, and specialized AI models have an even brighter outlook. LikeBloombergGPT, this partnership represents a step toward adapting foundation models to a vertical industry, along with a new business model for good measure.We’re thinking:Prompting a foundation model can go a long way in tasks for which it’s easy to write instructions that describe clearly what you want done. But many tasks involve specialized knowledge that’s difficult to put into a prompt; for instance, consider explaining how to draft a good legal document. In such cases, fine-tuning or specialized training can be a promising approach.\n\nChina’s internet watchdog proposed sweeping limitations on face recognition — with significant exceptions.\n\nWhat’s new:The Cyberspace Administration of Chinaunveileddraft rules that restrict the use of face recognition systems, with explicit carve-outs when national security or public or personal safety is at stake. The public can submit feedback before September 7.\n\nNarrow limits, broad exceptions:The proposal, which will affect mainland China but not Macau or Hong Kong, applies to both public and private users of face recognition. It follows recent restrictions ongenerative AIandcollecting personal data.\n\nBehind the news:Chinaleadsthe world in developing and deploying face recognition. Authorities use it widely in law enforcement, while businesses use it for authenticating payments, checking the identities of air and rail passengers, and granting access to residential buildings. Nonetheless, many Chinese residents have voiced their unease with the technology.\n\nYes, but:The exemptions for national security and safety give China’s government authority to continue using the technology for potentiallycontroversialapplications.\n\nWhy it matters:Face recognition is a double-edged sword. It has legitimate uses for security and law enforcement, but it can also be misused to violate privacy. Such concernsmotivatedEuropean Union lawmakers to insert a prohibition on face recognition in public spaces into the current draft of the union’s AI Act, which is in the final stage of revision. China’s new rules bring that country’s face recognition policy closer into line with that standard — the exceptions for national security and public safety notwithstanding.We’re thinking:It’s interesting to see China take the lead in regulating face recognition, where it dominates the technology and market. We support stronger protections for personal privacy.\n\nLearn how to utilize Semantic Kernel, Microsoft’s open source SDK, to develop sophisticated business applications using LLMs.Sign up for free\n\nEvent data recorders, also known as black boxes, got an update for the era of self-driving cars.\n\nWhat’s new:The Institute of Electrical and Electronics Engineerspublishedguidelines for internal devices that track the performance of autonomous road vehicles.\n\nHow it works:Like airplanes, cars and trucks carry event data recorders that capture their moment-to-moment behavior for examination in the event of a crash. The new specification calls for vehicles withLevel 3autonomous capabilities or higher, which can drive themselves but may require a human driver to take over, to carry a recorder dedicated to automated driving functions. The working group will meet later this year to discuss further revisions that address subjects like cybersecurity and protocols accessing recorded data.\n\nBehind the news:Event data recorders became a fixture in road vehicles decades ago as a way to evaluate the performance of safety airbags. Today, they record parameters such as speed, acceleration, and braking in 99 percent of new vehicles in the United States. They’ll bemandatoryin new cars in the European Union starting next year.\n\nWhy it matters:As more automated driving systems hit the road, safety concerns are on the rise. Event data recorders help shed light on mishaps, and the resulting data can help authorities, manufacturers, and consumers to understand the role, if any, played by self-driving technology. Although compliance is voluntary, IEEE standards are influential and widely followed.\n\nWe’re thinking:Self-driving systems have the potential to reduce road and pedestrian fatalities dramatically. A clear picture of what goes wrong and why will enable engineers to improve self-driving technology steadily. Ultimately, we hope, accidents will become rare and relatively inconsequential.\n\nText-to-video generation is so 2022! A new system takes in text and generates an animated 3D scene that can be viewed or rendered from any angle.\n\nWhat’s new:Uriel Singer and colleagues at Meta AI proposedMake-A-Video3D(MAV3D). Lacking a corpus of matched text and animated 3D scenes, the authors used a pretrained text-to-video diffusion model to guide the training of a neural radiance field (NeRF) model that learned how to represent a 3D scene with moving elements. You can see MAV3D’s outputhere.\n\nKey insight:Earlier work known asDreamFusionlearned to produce a 3D scene from text by setting up a feedback loop between a pretrained diffusion text-to-image generator, which creates 2D images according to a text prompt, and a NeRF, which takes embeddings of points in space and learns to produce a 3D scene (mesh, point colors, and point transparencies) to match the 2D images shot from various angles. (NeRF can also generate images of the scene.) Basically, (i) the NeRF generated 2D images of a random 3D scene; (ii) the images — with added noise — were given as input to the diffusion text-to-image generator, which sharpened them according to the text prompt; and (iii) the NeRF used the sharpened images to sharpen the 3D scene, repeating the cycle. MAV3D worked the same way but (a) used a more computationally efficient embedding method called HexPlane, (b) swapped the pretrained text-to-image generator for a pretrained text-to-video generator, and (c) modified the NeRF to generate sequences of video frames. The resulting system takes a text prompt and learns to generate a matching 3D scene that changes over time.\n\nHow it works:MAV3D is an animated version of the earlier DreamFusion, as described above. It includes the following models:HexPlane(which efficiently represents an animated 3D scene),Make-A-Video(a text-to-video generator pretrained on LAION-5B text/image pairs and fine-tuned on 20 million videos), and aNeRFmodified for video/animation.\n\nResults:No other system generates animated 3D scenes from text, so the authors compared MAV3D with systems that solve two sub-tasks, generating 3D static scenes from text and generating videos from text. They usedCLIP R-Precision, a metric that evaluates the similarity between an image and a text description (higher is better), to measure the systems’ performance averaged across a number of images taken from different angles (for 3D scenes) or images over time (for videos). MAV3D outperformed aStable Diffusionimplementation of DreamFusion (82.4 CLIP R-Precision versus 66.1 CLIP R-Precision). However, it did worse than Make-A-Video (79.2 CLIP R-Precision versus 86.6 CLIP R-Precision).\n\nYes, but:Examples of MAV3D’s output include very short scenes of varying quality. The system allows only one color per point so, for instance, reflective surfaces look the same regardless of viewing angle. It’s also computationally demanding: It took 6.5 hours per scene using eight A100 GPUs.\n\nWhy it matters:Adapting NeRF for video/animation is exciting, but the larger lesson is that finding an efficient way to learn representations — HexPlane in this case — can make tasks feasible that otherwise would require impractical amounts of computation.\n\nWe’re thinking:While MAV3D’s rendering would be improved by variable colors to represent reflections, shadows, and dynamic lighting, its strong performance relative to DreamFusion suggests a way to improve text-to-3D: train on videos instead of images. Videos contain moving objects and sometimes changing camera positions, so they can depict more diverse 3D geometry than a set of static images. Learning from videos could avoid generating 3D images that look fine from onlyone angle at a time.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--49--1.png",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--89-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--47-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/08/DeepLearning_Microsoft_Semantic_Kernel_Banner_2070x1080.png",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--48-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/08/unnamed--90-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-224/",
    "title": "issue 224",
    "date": "",
    "reading_time": "",
    "content": "One positive take-away is that employees have power. It can be hard to be part of a large team. But through ways large and small, people doing the work can influence events in important ways. OpenAI employees banded together to demand changes in the board, and one or two engineers at any company can raise a concern. Wherever you work, use your voice to make things better!\n\nSo what’s next?\n\nI see both hopeful and worrisome impacts as OpenAI picks up the pieces:\n\nChatGPT was released on November 30, 2022. It is amazing how much has happened at OpenAI — and in the AI world — in less than one year! Brief stretches of chaos may be the price of moving fast. Nonetheless, I think moving fast (but responsibly) is better than going slowly.I hope all employees everywhere will come away from this episode feeling empowered to speak up and make things better. Let’s keep building AI, exercise wisdom and foresight, and learn what lessons we can about corporate governance. It’s probably too much to hope that there won't be additional bumps in the road ahead for AI, but I remain optimistic about all the good we can do.\n\nKeep learning!\n\nAndrew\n\nOpenAI abruptly fired and rehired its CEO Sam Altman, capping five days of chaos within the company.\n\nWhat’s new:On Friday, the OpenAI board of directors — whose membership since has changed —oustedCEO and co-founder Sam Altman from his leadership position and his seat on the board. The board named chief technology officer Mira Murati interim CEO, soon replaced by Twitch co-founder Emmett Shear. Late Tuesday, Altman wasreinstatedand the board reorganized.\n\nWhat happened:The dizzying events leave OpenAI with familiar leadership and a retooled board of directors. The new board, which is expected to expand, is chaired by Salesforce co-CEO Bret Taylor and includes economist Larry Summers and Quora CEO Adam D’Angelo (the sole holdover from the previous lineup). Leaving the board are Altman, co-founder and chief scientist Ilya Sutskever, entrepreneur Tasha McCauley, and AI safety researcher Helen Toner as well as president, co-founder, and former board chair Greg Brockman (who lost his seat in the turmoil, resigned, and returned with Altman).\n\nRevolving door:OpenAI went through three CEOs within nearly as many days. Here’s who has passed through the revolving door.\n\nWhy it matters:At a moment when AI is undergoing rapid development and deepening division over the role of regulation, the chaos at OpenAI highlights the importance of strong corporate governance and an experienced board of directors that has a range of relevant experience and strong alignment with the company’s mission. It’s highly unusual for directors to fire a chief executive without arranging an orderly succession, coordinating with key investors, and preparing the market for changes. Chaos at the company opened competitive opportunities for rivals and threatened to destabilize thousands of companies that depend on OpenAI services. Although Altman’s return presumably restores the company’s stability, it will bear lingering questions and greater scrutiny going forward.\n\nWe’re thinking:There’s nothing normal about goings on at OpenAI. Nonetheless, as startup guru Eric Riessaid, cofounder breakups and sometimes even boardroom coups are part of startup life. They’re unnerving, especially for people who depend on the companies involved (and vice-versa). We wish OpenAI’s employees, who have done a tremendous job of advancing AI and serving hundreds of millions of customers, renewed enthusiasm and focus as they resume their important work.\n\nArgentina’s recent presidential race was a battleground of AI-generated imagery.\n\nWhat’s new:Candidates Javier Milei and Sergio Massa flooded social media with generated images of themselves and each other,The New York Timesreported. On Sunday, Mileiwonthe election’s final round.\n\nHow it works:No candidate earned enough votes to win the first round in late October, so front runners Milei, known for his hard-right libertarian economic views, and Massa, the incumbent government’s center-left economic minister, advanced to a run-off. The candidates generated a deluge of pictures and videos as the final vote neared.\n\nWhat they’re saying:“I absolutely think it's a slippery slope. In a year from now, what already seems very realistic will only seem more so.” — Isabelle Frances-Wright, head of technology and society, Institute for Strategic Dialogue.\n\nBehind the news:Deepfakes have appeared in campaign ads inIndiaandSouth Korea. Earlier this year, Google mandated that advertisers in a number of democratic countries including Argentina clearly label AI-generated imagery in political ads distributed through Google ads, part of a globalpolicy change. Meta willrequirethat political advertisers clearly label AI-generated media in their ads beginning in 2024. Generated images in Argentina’s presidential campaign circulated on Meta’s Instagram network ahead of the deadline.Why it matters:Argentina’s presidential campaign offers a glimpse of the future for democracies across the globe. Image generators are widely available, and political forces haveprovenwillingto use them. AI-generated depictions of candidates may undermine voters’ trust in the media as a whole whether or not they’re intended to deceive, political scientistsworry.\n\nWe’re thinking:Generated media poses a conundrum for democracy. Advertising has been shown to influence people even when audience members are aware of the effort to persuade. Yet free speech is essential to a healthy society. We favor mandatory labeling generated media in political ads and strong protection against defamation in hope that these measures will stem the most flagrant abuses.\n\nAgent applications are among the most in-demand uses of large language models (LLMs). This workshop will explore how to develop, evaluate, and iterate on LLM agents quickly and effectively.Register now\n\nA new cloud-computing company promises to provide scarce AI processing power to startups and researchers.\n\nWhat’s new:Voltage Park, a nonprofit north of Silicon Valley, willofferprocessing power from 24,000 top-of-the-line Nvidia H100 graphics processing units (GPUs) — roughly $500 million worth — at competitive prices. Rival suppliers of cloud-based GPUs are oversubscribed as the chips continue to be in short supply.\n\nHow it works:The company, which is bankrolled by cryptocurrency billionaire Jed McCaleb, plans to build data centers in Texas, Virginia, and Washington.\n\nBehind the news:Ashortageof Nvidia’s high-end GPUs, which are optimized to process machine learning workloads, has bedeviled organizations that aim to join the generative AI boom. Businesses are scrambling to manage the demand.\n\nWhy it matters:Training and serving state-of-the-art AI systems requires huge amounts of processing power. Thus AI startups are facing serious obstacles amid the scarcity of specialized hardware. Larger companies have either their own processing power or strong relationships with cloud providers. Smaller providers such as DataCrunch, Lambda Labs, and Paperspace have limited supply. As generative AI booms, organizations that can provide access to GPUs on flexible terms are likely to find takers.We’re thinking:Voltage Park is a subsidiary of McCaleb’s philanthropic organization, and its profits will fund the organization’s activities, about which its website offersno information. Nonprofit status can be a prelude to for-profit business. We’re curious to see where this company is headed.\n\nThe transformer architecture is astonishingly powerful but notoriously slow. Researchers have developed numerous tweaks to accelerate it — enough to warrant a look at how these alternatives work, their strengths, and their weaknesses.\n\nWhat’s new:Quentin Fournier, Gaétan Marceau Caron, and Daniel Aloisesurveyedvariations on the transformer, evaluating methods designed to make it faster and more efficient. This summary focuses on the variations designed to accelerate it.\n\nThe cost of attention:The attention mechanism in the originaltransformerplaces a huge burden on computation and memory; O(n2) cost where n is the length of the input sequence. As a transformer processes each token (often a word or pixel) in an input sequence, it concurrently processes — or “attends” to — every other token in the sequence. Attention is calculated by multiplying two large matrices of weights before passing the resulting matrix through a soft​​max function. The softmax function normalizes the matrix values to a probability distribution, bringing higher values closer to 1 and lower values near 0. This enables the transformer, when encoding a token, to use relevant tokens and ignore irrelevant tokens.\n\n(Modified) attention is all you need:The authors identify three approaches to accelerating transformers. Two of them optimize the attention mechanism and the third optimizes other parts of the architecture.\n\nYes, but:It’s difficult to compare the results achieved by these variations due to differences in model size and hyperparameters (which affect performance) and hardware used (which affects speed). Further, some transformer variations utilize multiple modifications, making it hard to isolate the benefit of any particular one.\n\nWhy it matters:These variations can help machine learning engineers manage compute requirements while taking advantage of state-of-the-art approaches.\n\nWe’re thinking:The authors ofLong Range Arenabuilt a dashboard that reports performance of various transformers depending on thetask. We welcome further efforts to help developers understand the tradeoffs involved in different variations.\n\nCambridge Dictionary declares AI-induced 'Hallucinate' Word of the Year 2023The Cambridge Dictionary expanded the definition of the word to include the false information produced by large language models. The acknowledgment of AI 'hallucinations' underscores the evolving vocabulary surrounding the capabilities of language models. (University of Cambridge)\n\n'Make It Real' prototype transforms drawings into functional softwareTldraw, a collaborative digital whiteboard, launched a prototype of a feature that allows users to turn vector drawings into functional software. A live demo of the GPT-4V powered tool isavailableto the public. (Ars Technica)\n\nResearch:Text-to-image AI models vulnerable to 'SneakyPrompt' jailbreaking are generating disturbing contentProminent text-to-image AI models, including Stabile Diffusion and DALL-E 2, face a significant security breach. Security researchers revealed the \"SneakyPrompt\" method, which uses reinforcement learning. SneakyPrompt enables the generation of seemingly nonsensical prompts that AI models learn to recognize as hidden requests for inappropriate images. Stability AI and OpenAI are already collaborating with the researchers to strengthen defenses against such attacks. (MIT Technology Review)Amazon announces job cuts in Alexa division, shifting focus to generative AIDaniel Rausch, the Vice President of Alexa and Fire TV, stated in an internal memo that the shifts are intended to maximize resources for generative AI. The company recently previewed a generative AI-based Alexa feature called \"Let’s Chat,\" emphasizing longer and more context-aware conversations with the voice assistant. (GeekWire)\n\nGoogle launches Project Open Se Cura, an open source framework for secure and efficient AIThe framework emphasizes co-design and development, focusing on security, transparency, and scalability. Google released the code base, including design tools and IP libraries, to foster open development and transparency in AI system design. (Google Open Source)\n\nGoogle-backed AI research lab, Kyutai, aims for open science with $330 million budgetFrench billionaire Xavier Niel unveiled details about Kyutai, a newly established AI research lab in Paris with plans to release not only open source models but also training source code and data. French President Emmanuel Macron supports the initiative, emphasizing the need to regulate AI use cases rather than model makers. (TechCrunch)\n\nGPT-4 outperforms humans on lawyer ethics examThe model surpassed the average scores of human test-takers on the Multistate Professional Responsibility Exam (MPRE), a legal ethics test required by almost every U.S. state for practicing law. GPT-4 achieved a 74% accuracy rate on the simulated exam, outperforming the estimated 68% average among humans. The study, conducted by LegalOn Technologies, suggests that AI could play a role in assisting lawyers with ethical compliance in the future. (Reuters)\n\nGoogle DeepMind and YouTube present Lyria, an advanced AI music generation modelLyria, designed to generate high-quality music with instrumentals and vocals, aims to address the challenges of maintaining musical continuity across various elements like beats, individual notes, and vocal harmonies. The announcement includes two AI experiments: \"Dream Track,\" an experiment within YouTube Shorts allowing creators to connect with fans through AI-generated soundtracks featuring global artists; and \"Music AI Tools,\" a set of tools developed with artists, songwriters, and producers to enhance their creative processes. (Google DeepMind)\n\nMicrosoft introduces custom-designed chips for AzureThe Azure Maia AI Accelerator for AI tasks and generative AI, and the Azure Cobalt CPU, an Arm-based processor optimized for general-purpose compute workloads, will be integrated into custom server boards and racks. The chips will be working in tandem with software to maximize performance, flexibility, and efficiency. (Microsoft)\n\nMicrosoft and Google collaborate on OneTable project to address data lake challengesThe open source project seeks to create a layer on top of existing data lake table formats like Apache Iceberg, Apache Hudi, and Delta Lake, enabling seamless conversions and access across these formats. The project promotes interoperability, preventing vendor lock-in and facilitating compatibility for data analytics and AI workloads. (VentureBeat)\n\nMicrosoft teams up with Be My Eyes to offer GPT-4-powered support for visually impaired usersThe tool enables visually impaired users to independently resolve technical issues and perform tasks without human agent assistance. During tests, only 10 percent of users opted to speak with a human representative after interacting with the AI tool. (The Verge)\n\nOpenAI temporarily halts new ChatGPT Plus subscriptions and upgradesOverwhelming demand led to capacity challenges, prompting a decision to pause access to ensure a high-quality experience for existing users. The move follows a series of outages related to high demand and DDoS attacks on OpenAI services, impacting ChatGPT and the API. (Search Engine Journal)\n\nCommon Sense Media flags generative AI models unsafe for kidsThe organization introduced \"nutrition labels\" for AI products, evaluating them based on principles such as trust, safety, privacy, transparency, accountability, learning, fairness, social connections, and benefits to society. The generative AI category received lower ratings due to biases and concerns related to objectification and sexualization. (TechCrunch)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--30-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--74-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--99--1.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/11/The-Batch--2-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--75-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--100-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-161/",
    "title": "issue 161",
    "date": "",
    "reading_time": "",
    "content": "A few weeks ago, the White Houserequiredthat research papers funded by the U.S. government be available online promptly and freely by the end of 2025. Data that underlies those publications must also be made available.\n\nI’m thrilled! Paywalled journals that block free access to scientific research are the bane of the academic community.\n\nThe AI world is fortunate to have shifted years ago to free online distribution of research papers, primarily through thearXivsite. I have no doubt that this has contributed to the rapid rise of AI and am confident that, thanks to the new U.S. policy, promoting a similar shift in other disciplines will accelerate global scientific progress.\n\nIn the year 2000 — before modern deep learning, and when dinosaurs still roamed the planet — AI researchers were up in arms against paywalled journals. Machine Learning Journal, a prominent journal of the time, refused to open up access. With widespread support from the AI community, MIT computer scientist Leslie Kaelbling started the free Journal of Machine Learning Research, and many researchers promptly began publishing there instead. This move led to the rapid decline of Machine Learning Journal. The Journal of Machine Learning Research remains a respected institution today, edited by David Blei and Francis Bach (both of who are my former officemates at UC Berkeley).\n\nBefore the modern internet, journal publishers played an important role by printing and disseminating hard copies of papers. It was only fair that they could charge fees to recoup their costs and make a modest profit. But in today’s research environment, for-profit journals rely mainly on academics to review papers for free, and they harvest the journals’ reputations (as reflected in metrics such asimpact factor) to extract a profit.\n\nToday, there are peer-reviewed journal papers, peer-reviewed conference papers, and non-peer-reviewed papers posted online directly by the authors. Journal articles tend to be longer and undergo peer review and careful revisions. In contrast, conference papers (such as NeurIPS, ICML and ICLR articles) tend to be shorter and less carefully edited, and thus they can be published more quickly. And papers published on arXiv aren’t peer reviewed, so they can be published and reach interested readers immediately.\n\nThe benefits of rapid publication and distribution have caused a lot of the action to shift away from journals and toward conferences and arXiv. While the volume of research is overwhelming (that’s why The Batch tries to summarize the AI research that matters), the velocity at which ideas circulate has contributed to AI’s rise.\n\nBy the time the new White House guidance takes effect, a quarter century will have passed since machine learning researchers took a key step toward unlocking journal access. When I apply AI to healthcare, climate change, and other topics, I occasionally bump into an annoyingly paywalled article from these other disciplines. I look forward to seeing these walls come down.\n\nDon’t underestimate the impact of freeing up knowledge. I wish all these changes had taken place a quarter century ago, but I’m glad we’re getting there and look forward to the acceleration of research in all disciplines!\n\nKeep learning!\n\nAndrew\n\nTax dodgers can’t hide from AI — especially those who like to swim.\n\nWhat’s new:French tax authorities, which tax swimming pools according to their size because they increase a home’s property value, netted nearly €10 million using an automated system to identify unregistered pools,Le Parisienreported.\n\nDiving in:Developed by Google and Paris-based consultancyCapgemini, the system spots pools in a public database of aerial images. It then cross-checks them with land-registry data to determine whether they’re registered. France plans to roll it out nationwide this month.\n\nBeneath the surface:At least 17 other European Union tax-collection agenciesuseAI for tasks that include identifying who should be audited, scraping taxpayer data from ecommerce sites, and powering chatbots that help taxpayers file. Last year, U.S. tax authoritiesimplementedtechnology from Palantir that identifies fraud by analyzing tax returns, bank statements, property records, and social media activity.\n\nWhy it matters:As AI analyzes every nook and cranny of an individual’s data trail, reluctant taxpayers will find it harder to avoid paying up.We’re thinking:There’s irony in a tech behemoth that’s known for aggressivetax-avoidance strategieshelping a government collect tax revenue.\n\nThe U.S. government blocked U.S. makers of AI chips from selling to China, adding to existing sanctions that target Russia.\n\nWhat’s new:The Department of Commerce restricted sales of Nvidia’s and AMD’s most-advanced chips for training and running large AI models,Reutersreported.\n\nHow it works:U.S. officials didn’t detail the specifics of the ban. Nvidia said it would stop selling itsA100andH100graphics processing units (GPUs) to China. AMD said the action affects itsMI250GPU.\n\nChina’s reaction:“This violates the rules of the market economy, undermines the international economic and trade order, and disrupts the stability of global industrial and supply chains,” a foreign ministry spokespersonsaid. China hasn’t announced countermeasures, but some analystsanticipatethat it will further increase funding to its domestic semiconductor sector.Behind the news:Russia hasfacedchip embargoes by South Korea, Taiwan, and the U.S. in response to its February invasion of Ukraine. In 2020, the U.S. governmentrequiredforeign chip makers that use U.S. equipment to receive special permission before doing business with the Chinese tech company Huawei.\n\nWhy it matters:AI is increasingly intertwined with geopolitics. China has repeatedly stated its intention to achieve “AI supremacy” and outpace the U.S. China, however, is still largely reliant on imported semiconductors, so the U.S. ban could hobble its ambitions.We’re thinking:An AI chip may be designed in the U.S. and manufactured in Taiwan using equipment from the Netherlands. This globalized supply chain works well when international tensions are low, but rising tensions pose risks to both progress in AI and the security of several countries.\n\nA smart news paywall is optimizing subscriptions without driving away casual readers by showing them come-ons subscribe.\n\nWhat’s new:The New York TimesdescribedDynamic Meter, a machine learning system that decides how many free articles to provide to a given user before prompting them to register or subscribe.\n\nHow it works:The newspaper’s data science team ran a randomized, controlled trial and found that delivering more pop-ups that ask readers to subscribe resulted in more subscriptions but fewer page views, while delivering fewer popups resulted in fewer subscriptions but greater page views.\n\nHow it works:The New York Times’ data science team collected a dataset by running a randomized, controlled trial that tracked the behavior of registered — but not yet subscribed — users with various characteristics. Generally, delivering more pop-ups that asked them to subscribe resulted in more subscriptions but fewer page views (prior to subscribing), while delivering fewer popups resulted in fewer subscriptions but greater page views.\n\nBehind the news:The Wall Street Journal, Switzerland’sNeue Zürcher Zeitung, and Germany’sFrankfurter Allgemeine Zeitungalso use machine learning to maximize subscriptions.Why it matters:The shift in news consumption from print to online devastated publishers, in part because they’re forced to compete with the panoply of attention-grabbing content on the web. Smart paywalls can help them thrive by tantalizing readers with free content, then forcing them to decide whether they value it relative to everything else the web has to offer.We’re thinking:News is critical to a free society, and it’s important to distribute it fairly. Does allowing some people to read more articles than others give those people an advantage over people who are allowed to read fewer articles? Is it okay to offer a wealthy person five articles and a less-wealthy person 10 before demanding that they subscribe — or vice versa? While AI can help companies capture greater financial value, many questions of social value remain to be answered.\n\nTransformers famously require quadratically more computation as input size increases, leading to avarietyofmethodsto make them more efficient. A new approach alters the architecture’s self-attention mechanism to balance computational efficiency with performance on vision tasks.\n\nWhat's new:Pale-Shaped self-Attentionachieved good vision results while applying self-attention to a grid-like pattern of rows and columns within an image. Sitong Wu led the work with colleagues at Baidu Research, Chinese National Engineering Laboratory for Deep Learning Technology and Application, and Chinese Academy of Sciences.\n\nKey insight:Previous attempts to reduce the computational cost of self-attention includeaxial self-attention, in which a model divides an image into patches and applies self-attention to a single row or column at a time, andcross-shaped attention, which processes a combined row and column at a time. The pale-shaped version processes patches in a pattern of rows and columns (one meaning of “pale” is fence, evoking the lattice of horizontal rails and vertical pickets). This enables self-attention to extract large-scale features from a smaller portion of an image.\n\nHow it works:The authors implemented their pale-shaped scheme in Pale Transformer, which processed an image through alternating convolutional layers and 2 or 16 transformer blocks. They trained it onImageNet.\n\nResults:The authors tested three variants of Pale Transformer, each with a different number of parameters: Pale-T (Tiny, 22 million parameters), Pale-S (Small, 48 million parameters), and Pale-B (Base, 85 million parameters). Each achieved better top-1 classification accuracy on ImageNet than competing convolutional neural networks and transformers of similar size. For example, Pale-B achieved state-of-the-art accuracy of 85.8 percent while the best competing model,VOLO-D2(59 million parameters), scored 85.2 percent. Pale-B required somewhat more computation (15.6 gigaflops) than VOLO-D2 (14.1 gigaflops), but both required far less than a vision transformer with 86 million parameters (55.4 gigaflops). The authors also compared Pale-T against axial and cross-shaped attention. Pale-T achieved 83.4 percent accuracy on ImageNet. The same model with axial attention achieved 82.4 percent and, with cross-shaped attention, achieved 82.8 percent.\n\nWhy it matters:This work suggests that there’s room to improve the transformer’s tradeoff between efficiency and performance by changing the way inputs are processed.\n\nWe’re thinking:Will this team’s next project be beyond the pale?",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/TAX.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/GPU.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/09/TIMES.jpeg",
      "https://dl-staging-website.ghost.io/content/images/2022/09/PALE.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-32/",
    "title": "issue 32",
    "date": "",
    "reading_time": "",
    "content": "When I was younger, I was not a fan of working from home. Too many distractions! So I worked a lot in coffee shops. They turned out to be convenient places to talk to strangers and ask for feedback about products I was working on, including early MOOC prototypes.\n\nNow much of the world is undergoing a remote work experiment. My teams and I are working from home.\n\nThere have been positives and negatives. I love running into colleagues in our #virtualcoffeechat slack channel, especially people I don’t see so often around the office. I love reducing my carbon footprint and not having to commute, and I love getting to see Nova during my lunch break. (She’s learning to walk, and her unstable toddling is simultaneously cute and terrifying.)\n\nOn the flip side, I miss seeing everyone in 3D. I miss the serendipitous discussions, and I miss being able to gather in the break room to chat and partake in thebabka,gulab jamun,chicharron, andduriancandy that teammates sometimes bring to share.\n\nEven though Covid-19 is a painful challenge, there is a silver lining in this shift in how we work. People in the tech industry are fortunate that a lot of work can be done remotely, and many companies are now learning how to do this well.\n\nOnce this pandemic is over, I believe that many remote roles will open up. It will be easier for an aspiring AI engineer who lives in Dallas to get a job in Silicon Valley — without having to move. A recruiter who lives in Buenos Aires will have a better chance of being hired by a company in Montreal. A front-end engineer in Sydney might work for an employer in Tokyo. No matter where you live, more jobs will be coming to you in the future.\n\nStay safe and keep learning!\n\nAndrew\n\nNeural networks revealed both how hard Covid-19 has hit the Chinese economy, and hopeful signs that a renaissance may be underway.What’s new:Researchers at WeBank, a Chinese financial institution,analyzedsatellite imagery, GPS signals, and social media to get a multifaceted view of the pandemic’s impact.What they found:The team compared data collected before, during, and after the peak of China’s containment efforts. It focused on three data sources:\n\nBehind the news:China dramaticallycutits coronavirus transmission rate by imposing strict measures to limit social interaction including a quarantine of 50 million people in the province that includes Wuhan, the disease’s epicenter.Why it matters:Before Covid-19 rocked China’s economy, J.P.  Morgan had estimated that the country’s GDP would grow nearly 6 percent by the end of 2020. Now the U.S. investment bankpredictsa meager 1 percent growth. China’s isn’t the only economy in trouble: The bank’s analysts warn of a global recession.We’re thinking:China’s economic revival would be great news for the rest of the world. But full recoveryisn’t likelyuntil its Western trading partners come back up to speed.\n\nAs working from home becomes the new normal, AI may protect you from the sound of coworkers munching while they chat.What’s new:No more smacking lips  and rustling chip bags! Microsoft’s online collaboration platform Teams announced a feature thatremoves extraneous soundsfrom videoconferences.How it works:The Teams team trained neural networks torecognize and filter outnon-speech noises usingdatasetsthey built for the2020 Deep Noise Suppression Challenge.\n\nBehind the news:People across the globe are hunkering down for a long virus season. Zoom added more than 2 million monthly active users in January and February, more than in all of 2019. Microsoft Teams’ daily user count shot up from 13 million to 44 million betweenJuly 2019andMarch 2020. Slack, the other big telecommuting program, hasn’t published monthly average user numbers since October, when the tally was 12 million.Why it matters:Nobody wants to listen to yourmukbangduring working hours.We’re thinking:Next, can we get a feature that filters outintrusive toddlers?\n\nChemists typically develop new antibiotics by testing close chemical relatives of tried-and-true compounds like penicillin. That approach becomes less effective, though, as dangerous bacteria evolve resistance to those very chemical structures. Instead, researchers enlisted neural networks.What’s new:Jonathan Stokes and colleagues at MIT, Harvard, and McMaster University built an ensemble model that predicts molecules that are structurally unrelated to known antibiotics, harmless to humans, and deadly to E. coli, a common bacterium that served as a proxy microorganism. The model spotted a previously unrecognized antibiotic that proved effective at killing a variety of germs.Key insight:Neural networks can stand in for petri dishes to zero in on promising molecules. An initial simulation reduced an enormous number of molecules to a few thousand solid possibilities, of which the model selected a couple dozen for testing in a wet lab.How it works:The researchers used an ensemble of 20 graph neural networks (GNNs) to evaluate molecules’ ability to inhibit E. coli, and another ensemble of five GNNs to evaluate toxicity. They used a standard measure to evaluate chemical structure. Then they tested the most promising compounds on mice.\n\nResults:The researchers examined more than 107 million compounds to produce a ranked list. Empirical tests on the top-ranked 3,260 chemicals yielded 51 that were effective. Of those, 23 had low predicted toxicity and structures distinct from known antibiotics. In mouse experiments, Halicin, a known diabetes treatment, proved effective as a broad-spectrum antibiotic.Why it matters:Alexander Fleming’s discovery of penicillin in 1928 revolutionized medicine. Now that transformation is at risk as bugs evolve resistance to that drug and its successors. Discovery of new antibiotics has been hampered by lack of a way to narrow the list of possibilities for lab tests. This method offers a way to vet candidates quickly and efficiently.\n\nWe’re thinking:Antibiotic-resistant bugs are responsible for 2.8 million infections and 35,000 deaths annually in the U.S. alone. Crank up those GNNs!\n\nGet your models running on iOS and Android devices. Take the first step in Course 2 of the TensorFlow: Data and Deployment Specialization.Enroll now\n\nBored with your Spotify playlists? Let this robot singer/songwriter take you on a trip “Into Your Mind.”What’s Goin’ On:A music-composing, marimba-playing robot named Shimon has learned to write and sing its own lyrics,IEEE Spectrumreports. Shimon performs its debut single with a flesh-and-blood backup band in thisvideo. An album is scheduled to drop on Spotify on April 10.(A)I Write the Songs:Two of the robot’s creators, Georgia Tech professor Gil Weinberg and grad student Richard Savery, treat the machine as though it were a humancollaborator.\n\nRobot Rock: Shimon’s music-video debut showcases several other innovations created by Weinberg and his colleagues, who have been working on this project fornearly a decade.\n\nUnchained Melody:Machine learning is challenging timeless assumptions about human creativity. Shimon’s output is reminiscent ofprogressiverockmasters. Could it conquer the pop charts by working with the producers of Taylor Swift, Beyonce, or Drake?Wish You Were Here:The Covid-19 pandemic forced Weinberg to postpone plans to take Shimon on tour. Will his next project be building robot fans?\n\nHow much data do we want?More!How large should the model be?Bigger!How much more and how much bigger? New research estimates the impact of dataset and model sizes on neural network performance.What’s new:Jonathan Rosenfeld and colleagues at MIT, York University, Harvard, Neural Magic, and Tel Aviv University introduced an equation — a so-callederror landscape— that predicts how much data and how large a model it takes to generalize well.Key insight:The researchers made some assumptions; for instance, models without training should be as accurate as a random guess. They combined these assumptions with experimental observations to create an equation that works for a variety of architectures, model sizes, data types, and dataset sizes.How it works:The researchers trained various state-of-the-art vision and language models on a number of benchmark datasets. Considering 30 combinations of architecture and dataset, they observed three effects when varying data and model size:\n\nResults:After fitting dataset-specific variables to the validation dataset, the researchers compared the predicted model error against the true error on the test set. The predictions were within 1 percent of the true error, on average.Why it matters:It turns out that the impact on accuracy of model and dataset size is predictable. How nice to have an alternative to trial and error!Yes, but:When varying network sizes, the researchers focused mainly on scaling width while holding the rest of the architecture constant. Neural network “size” can’t be captured in a single number, and we look forward to future work that considers this nuance.We’re thinking:Learning theory offers some predictions about how algorithmic performance should scale, but we’re glad to see empirically derived rules of thumb.\n\nWomencontinue to be severely underrepresented in AI.What’s new:Ameta-analysisof research conducted bySynced Reviewfor Women’s History Month found that female participation in various aspects of AI typically hovers between 10 and 20 percent.What they found:Much of the research included in the analysis was based on numbers generated by rules-based software that categorizes names according to gender.Synced Review, which is based in China, said it didn’t examine studies of Chinese companies or institutions because Chinese names don’t correlate as tightly with gender as names in other languages.\n\nBehind the news:Women have a prominent place in AI’s history, going all the way back to Ida Rhodes, who in the 1960s laid the groundwork for natural language processing. The percentage of American women with computer science degrees, however, peaked in the mid-1980s at around 35 percent, and since has declined to under 20 percent.Why it matters:It’s important that people building the future represent diverse groups to make sure that anyone can participate and that the products we build encompass a variety of perspectives.We’re thinking:Each one of us can help promote diversity. Leaders can make an effort to interview, hire, and mentor underrepresented groups, and everyone can help make the workplace inclusive.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter20ASPECT205.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/China20Econ202.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Snack1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Antibiotic20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize-4.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Shimon2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Error20ASPECT.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Gender20ASPECT201.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-50/",
    "title": "issue 50",
    "date": "",
    "reading_time": "",
    "content": "Over the weekend, my family celebrated my grandfather’s 102nd birthday on Zoom. We dialed in from Hong Kong (my grandfather), the U.S. (myself), the UK, Singapore, and New Zealand. In a normal year, I might not have made it to Hong Kong for the party. But because we now celebrate on Zoom, I was able to attend. For my family, the occasion was a bright spot amid the global tragedy of the pandemic.\n\nMany people are wondering when the world will go back to normal. I believe the world one day will become normal again, but the new normal will be very different from the normal of yesteryear.\n\nJust as the Covid crisis led me to attend my grandfather’s birthday party, once the virus recedes, our newfound ease with high-quality telecommunications will bring people together virtually for all kinds of purposes.\n\nMy teams in Colombia now work with my U.S. staff more smoothly than they did pre-Covid — it matters less and less whether my teammates are in Medellin, Colombia, or Palo Alto, California. I look forward to a world where digital communications enable anyone anywhere to receive an education and have access to meaningful job opportunities.\n\nI hope all of you will live long, healthy lives like my grandfather. Although we find comfort in the past, it is by actively creating the future that we move forward. It’s up to each of us to constantly envision and create a better future.\n\nKeep learning!\n\nAndrew\n\nPeople granted early access to OpenAI’s latest language model are raving about its way with words — and more.What’s new:Beta testers ofGPT-3are showing off the model’s ability to write business memos, craft blogs, pentweets, and even generate computer code. You can apply for access to theAPIvia thislink. A paid version is expected in about two months.\n\nDemo explosion:Yaser Martinez Palenzuela, a data scientist at Deutsche Telekom, compiled a list ofdemoson Github. Here are a few of our favorites.\n\nHype alert:OpenAI often has been accused of exaggerating the capabilities of its new technologies. Initially itwithheldGPT-2, saying the model was too dangerous to release, and it has threatened to cancel GPT-3 access for anyone who uses the tech maliciously. Yet the company itself warns against overhyping the new model. “It still has serious weaknesses and sometimes makes very silly mistakes,” OpenAI CEO Sam Altman wrote in atweet.Bigger is better:GPT-3 owes much of its performance to a gargantuan parameter count of 175 billion, which dwarfs GPT-2’s 1.5 billion and exceeds by an order of magnituderecent modelsfrom Google (11 billion) and Microsoft (17 billion).Why it matters:Large language models based on the transformer architecture have made natural language processing one of the most exciting areas of machine learning. They’re also raising AI’spublic profile. GPT-3 is quickly becoming the technology’s foremost spokesbot.We’re thinking:Sometimes GPT-3 writes like a passable essayist, sometimes like an insightful poet. But after reading the fascinating AI Weirdness blogpostin which author Janelle Shane gives the model a question-and-answer workout, it seems a lot like some public figures who pontificate confidently on topics they know little about.\n\nComputer vision is helping sushi lovers enjoy top-quality maguro.What’s new:Japanese restaurant chain Kura Sushi is using a smartphone app calledTuna Scopeto grade its suppliers’ offerings, according to the news outletThe Asahi Shimbun.How it works:Professional tuna graders assess tuna quality by examining a cross section of a fish’s tail for color, sheen, firmness, and fat patterns. The app, developed by Tokyo-based advertising and technology companyDentsu, mimics an experienced grader’s judgement.\n\nBehind the news:Computer vision is proving helpful in other parts of the seafood supply chain.\n\nWhy it matters:Kura Sushi normally purchases 70 percent of its tuna overseas, but Covid-19 travel restrictions have made it difficult to assess the catch on-site. The app enables the company to buy fish caught anywhere without sending employees to the docks.We’re thinking:The engineers who built this app appear to be quite talented. We hope the company isn’t giving them a raw deal.\n\nWe’d all love to be able to find similar examples of our favorite cat videos. But few of us want to label thousands of similar videos of even the cutest kitties. New research makes headway in video classification when training examples are scarce.What’s new:Jingwei Ji and Zhangjie Cao led Stanford researchers in developingOrdered Temporal Alignment Module(Otam), a model that classifies videos even with limited training data.Key insight:ImageNet provides over a million training examples for image classification models, while the Kineticsvideo datasetoffers an order of magnitude fewer. But each video comprises hundreds of individual frames, so video datasets typically contain more images than image datasets. Why not take advantage of all those examples by applying image recognition techniques to videos? That way, each frame, rather than each video as a whole, serves as a training example.How it works:The task is to find the training video most similar to an input video and apply the same label. A convolutional neural network pre-trained onImageNetextracts features for each input frame. Then the system compares the features and finds an alignment between the frames of a novel video and those of a training video. The CNN comprises the only trainable parameters.\n\nResults:On theKineticsdataset (clips of people taking various actions in a few sections each), Otam achieved one-shot accuracy of 73 percent, a big improvement over thepreviousstate of the art, 68.4 percent. Otam similarly improved the state of the art onSomething V2dataset, which comprises clips of people interacting with everyday objects.Why it matters:Some prior video classification systems also use pre-trained CNNs, but they include sequential layers that require lots of video data to train, since an entire video serves as a single training example. Otam eliminates much of that data hunger.We’re thinking:Videos typically include a soundtrack. We hope the next iteration of Otam will compare sounds as well as images.\n\nWe’re thrilled to announce the launch of Course 3 of our Natural Language Processing Specialization on Coursera!Enroll now\n\nA new report details the role of AI in China’s effort to fight the coronavirus.What’s new:Researchers atSynced, a China-based AI publication, describe how nearly 90 machine learning products have contributed to the country’s pandemic response.What it says:The report presents case studies in five areas: thermal imaging, medical imaging, epidemiology, contact tracing, and drug discovery. A few examples:\n\nYes, but:Critics havepointedoutshortcomings of machine learning in the fight against Covid-19 so far. In April, for example,twogroupsof researchers audited Covid-related machine learning models. They found bias in many systems for analyzing hospital admissions, diagnosis, imaging, and prognosis.Why it matters:Not all AI-against-Covid initiatives will prove to be effective. Tracking the approaches underway is crucial to finding the ones that work.We’re thinking:Artificial intelligence still needs to be complemented by human intelligence. Please wear a mask!\n\nWhen the chips are down, humans can track critical details without being distracted by irrelevancies. New research helps reinforcement learning models similarly focus on the most important details.What’s new:Google’s Yujin Tang, Duong Nguyen, and David Ha developed a reinforcement learning approach that teaches an agent to pay attention only to visual information that helps accomplish atask. This strategy makes it easier to perform similar tasks in new environments.Key insight:In the previousWorld Modelsapproach, an agent memorized features when it observed the world and used that knowledge to predict outcomes of future experiences. Memorizing the entire world isn’t necessary because many observable details, such as background color, aren’t helpful when solving a task. Agents should perform better if they block out such details.How it works:The authors’ approach effectively preprocesses an image before the agent considers it in selecting an action.\n\nResults:The researchers tested their method on the Car Racing and Doom Takeover tasks fromOpenAI Gym. On both tasks, it surpassed an OpenAI benchmark that’s nearly optimal.Why it matters:Providing agents with fewer inputs made it possible to reduce their size, and using an evolutionary technique reduced the number of parameters devoted to self-attention. The researchers needed only 3,700 parameters. World Models, which performed both tasks using relatively few parameters compared to other earlier approaches, required 4.7 million.We’re thinking:We love AI approaches to car racing, and it looks like this work is braking new ground.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter208.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/GPT-3203.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Tunascope.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Otam.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/GIF202.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Synced.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Neuro.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-vi/",
    "title": "issue vi",
    "date": "",
    "reading_time": "",
    "content": "So many people who are just starting out in machine learning are doing amazing work. With online education, open source software, and open publications, it takes less time than ever to go from 0 to 100 in ML.\n\nI spoke with one such person this week, Christine Payne, to learn about her journey. One year ago, she took the Deep Learning Specialization course. Now she's building cutting-edge neural networks at OpenAI. You can watch our conversation in the video linked below.\n\nI’d love to hear from more of you who took our courses and now use AI in your own career. Let us know what you’re building. Send a note to[email protected].Keep learning,Andrew\n\nChristine began in physics, moved into medicine, and did a stint as a professional musician. At OpenAI, she led development of MuseNet, a deep learning system that spins melodies and harmonies in a variety of styles.Watch the video\n\nUnfounded worries over malevolent machines awakening into sentience seem to be receding. But fears of face recognition erupted last week — the rumblings of a gathering anti-surveillance movement.\n\nWhat's new:Recent events cast a harsh light on the technology:\n\nBackstory:Face recognition is still finding its way into industry — Royal Caribbeanreportedlyuses it to cut cruise-ship boarding time from 90 to 10 minutes — but it has quietly gained a foothold in law enforcement:\n\nWhy it matters:Face recognition has a plethora of commercial uses, and law enforcement has employed it productively in countless cases. However, critics see potential chilling effects on free speech, erosion of privacy, reinforcement of social biases, risk of false arrest, and other troubling consequences. Now is the time to study possible restrictions, before the technology becomes so thoroughly embedded that it can’t be controlled.\n\nWhat they’re saying:“People love to always say, ‘Hey, if it's catching bad people, great, who cares,’ until they're on the other end.” — Joshua Crowther, a chief deputy defender in Oregon,quotedby theWashington Post\n\nSmart take:Face recognition, whether it's used in the public or private sphere, has tremendous potential for both good and ill. The issue isn’t bad technology, but misuse. It’s incumbent on AI companies to set bright-line standards for using their products and to build in ready ways of enforcing those standards. And it’s high time for government agencies to hammer out clear policies for using the tech, as well as audit processes that enable the public to evaluate whether those policies are being met.\n\nThe tide of AI-driven fakery rose higher around the Internet’s ankles as the synthesized voice of comedian/podcaster Joe Rogan announced his purported formation of a hockey team made up of chimpanzees.\n\nWhat happened:Dessa, an AI company based in Toronto, released arecordingof its RealTalk technology imitating Rogan. The digital voice, bearing an uncomfortably close resemblance to the real deal, describes the podcaster’s supposed all-simian sports team. Then it tears through tongue twisters before confessing the faux Rogan’s suspicion that he’s living in a simulation.\n\nBehind the news:Researchers have beenconvergingonspoken-word fakes for some time with impressive but generally low-fidelityresults. One joker mocked up a classic Eminem rap in the simulated voice of psychologist and culture warriorJordan Peterson. Dessa employees Hashiam Kadhim, Joe Palermo, and Rayhane Mama cooked up the Rogan demo as an independent project.\n\nHow it works:No one outside the company knows. Dessa worries that the technology could be misused, so it hasn’t yet released the model. It says it will provide details in due course.\n\nWhy it matters:Simulations of familiar voices could enable new classes of communication tools and empower people who have lost their own voices to disease or injury. On the other hand, they could give malefactors more effective ways to manipulate people on a grand scale. This fakeconversationbetween President Trump and Bernie Sanders is a playful harbinger of what may lie in store.\n\nYes, but:Dessa’s Rogan impersonation is remarkably true to life, but the rhythm and intonation give it away. The company offers a simpletestto find out whether you can distinguish between real and synthesized Rogan voices.Our take:Deepfakery has legitimate and exciting applications in entertainment, education, and other fields. Yet there's good reason for concern that it could mislead people and create havoc. Some day, it will be so good that no person or AI can recognize it. Brace yourself.\n\nHow is a machine learning workflow different from a data science workflow? Walk through examples of each in our nontechnical course,AI For Everyone.Enroll now\n\nSystems that translate between spoken languages typically take the intermediate step of translating speech into text. A new approach shows that neural networks can translate speech directly without first representing the words as text.\n\nWhat’s new:Researchers at Google built asystemthat performs speech-to-speech language translation based on an end-to-end model. Their approach not only translates, it does so in a rough facsimile of the speaker’s voice. You can listen to exampleshere.\n\nHow it works:Known as Translatotron, the system has three main components: An attentive sequence-to-sequence model takes spectrograms as input and generates spectrograms in a new language. A neural vocoder converts the output spectrograms into audio waveforms. And a pre-trained speaker encoder maintains the character of the speaker’s voice. Translatotron was trained end-to-end on a large corpus of matched spoken phrases in Spanish and English, as well as phoneme transcripts.\n\nWhy it matters:The architecture devised by Ye Jia, Ron J. Weiss, and their colleagues offers a number of advantages:\n\nResults:The end-to-end system performs slightly below par translating Spanish to English. But it produces more realistic audio than previous systems and plants a stake in the ground for the end-to-end approach.\n\nThe hitch:Training it requires an immense corpus of matched phrases. That may not be so easy to come by, depending on the languages you need.\n\nTakeaway:Automatic speech-to-speech translation is a sci-fi dream come true. Google’s work suggests that such systems could become faster and more accurate before long.\n\nRetailers are expected to more than triple their outlay for AI over the next few years.\n\nWhat’s new:A newreportfrom Juniper Research estimates that global retailers will spend $12 billion on AI services by 2023, up from $3.6 billion in 2019. More than 325,000 shopkeepers will adopt AI over that period, mostly in China, the Far East, North America, and West Europe.\n\nWhy it’s happening:AI-driven tools are fueling a race among retailers in which early movers will displace competitors, the report says. Three major AI applications are enabling retailers to cut costs, optimize prices, and offer superior service:\n\nTo be sure:Sales driven by chatbots likely will come at the expense of other channels, so they may not generate additional revenue. However, they will be more efficient, contributing to return on investment.\n\nBottom line:Retailers are banking on AI to revolutionize their industry. While they’ve focused for years on ecommerce, AI has the potential to revitalize in-store sales while spurring mobile and other channels.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/09/a9b22653-9f37-497b-9ece-a6d351f1468b-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/09/59470eab-a3fb-4c1d-a2d1-3e1f58e641d0.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/853a4f25-016b-42cb-af5d-a2b8651c113a.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/a41a7bd5-0c8d-4199-bd3f-0938c69be0ac.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/09/2d9d1be7-7aa6-4ee3-997e-c4a841212126.png",
      "https://dl-staging-website.ghost.io/content/images/2022/09/c06b5db1-5049-40b0-b79a-b74b30fcb088.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-256/",
    "title": "issue 256",
    "date": "",
    "reading_time": "",
    "content": "As we reach the milestone of the 256th issue ofThe Batch, I’m reflecting on how AI has changed over the years and how society continues to change with it. As AI becomes more widely available, it’s clear that many people — developers and non-developers — will benefit from high-quality training to keep up with the changes and gain useful AI skills.\n\nIn my years of working in education, I’ve felt that the world has enough low-quality courses, newsletters, social media posts, and other forms of content. It’s possible to build a business churning out mediocre content in sufficient volume to attract a meaningful amount of attention, but I have no interest in doing that.\n\nAt DeepLearning.AI, our core philosophy is toput learners first. Our team obsesses about how to create quality training or other programs that benefit people who want to learn about AI. We have intense debates about what tools to teach, which examples to include, even which partners to work with, based on what we think is best for learners.\n\nFor example, I recall vividly how, when working on theMachine Learning Specialization, our team spent ages debating whether to use row or column matrices. Both sides showed up with deep analysis of the pros and cons, made Powerpoint presentations to argue their case, and we spent hours debating over what was better for learners in terms of both ease of picking up the concepts as well as subsequently being able to use these skills with third-party machine learning libraries.\n\nWe don’t release a course unless we think it’s a good use of a learner’s time and we’d be proud to recommend it to our own friends and family members. Quality, of course, can mean a lot of things. I expect what we do to be technically accurate, useful, up to date, clear, and time-efficient for learners. And, if possible, fun!\n\nWe don’t always get it right, but we scrutinize learner feedback (one of my most important weekly routines is to study a dashboard that summarizes learner ratings of our courses) and work to make sure our courses serve learners well. And yes, we have a large-language model powered application that reads learner reviews to flag important issues quickly.\n\nEarlier this year, we realized that some of the paid content we had launched was below our quality standard, and that I wouldn’t in good conscience recommend it to my friends or family members. Despite this content being profitable, we did what we felt was the right thing for learners. So we decided to retire that content and forgo the revenues, but we feel much better now for having done the right thing for learners.\n\nWhen we teach courses with partners, we tell them our priorities are “learners first, partners second, ourselves last.” I’m grateful to the many wonderful companies and individuals that work with us to teach cutting-edge techniques, and given an opportunity we try to support our partners’ goals as well. But we never prioritize the interest of our educational partners over that of learners. Fortunately, our partners are onboard with this as well. We have a common goal to serve learners. Without their help, it would be difficult to teach many of the topics we do with high-quality content.\n\nQuite a few companies have tried to offer to pay us to teach a course with them, but we’ve always said no. We work only with the companies that we think help us serve learners best, and are not interested in being paid to teach lower quality courses.\n\nOne reason I obsess about building quality training materials is that I think learning must be a habit. Learning a little every week is important to get through the volume of learning we all need, and additionally to keep up with changing technology. High-quality training that’s also fun supports a healthy learning habit!\n\nFun fact: In addition to taking online courses, I also read a lot. Recently I noticed that my digital reading app says I’ve been on a reading streak for 170 weeks. I’ve used the app for many years, but apparently I had broken and restarted my streak 170 weeks ago. What happened then? That was the week that my son was born,Coursera became a public company, andmy grandfatherdied. While my life has had disruptions since then, I was happy to find that it takes a disruption of this magnitude to make me pause my learning habit for a week.\n\nKeep learning!\n\nAndrew\n\nJoin us for a Q&A webinar on July 11, 2024, at 11:00 AM Pacific Time. Andrew Ng and Roy Bahat will discuss business trends and strategies to integrate AI into organizations.Register now\n\nOpenAI will stop serving users in China and other nations of concern to the U.S. government as soon as next week.\n\nWhat’s new:Open AI notified users in China they would lose API access on July 9,Reutersreported. The move affects users in countries where the company doesn’t support access to its services officially (which include Cuba, Iran, Russia, North Korea, Syria, Venezuela, and others), but where it appears to have been serving API calls anyway.\n\nHow it works:Previously OpenAI blocked requests from outsidesupported countriesif it detected a virtual private network or other method to circumvent geographic restrictions, but it had enforced such limits lightlyaccording toSecurities Times. The email warning started a race among AI companies in China to attract cast-off OpenAI users.\n\nBehind the news:OpenAI’s crackdown on non-supported countries comes amid rising technological rivalry between the governments of the United States and China. The U.S. has taken several steps to try to curb China’s access to U.S.-built AI hardware and software, and some U.S. AI companies such as Anthropic and Google don’t operate in China. The Commerce Departmentplansto attempt to restrict China’s access to the most advanced AI models built by U.S. developers such as OpenAI. The Treasury Departmentissueddraft restrictions on U.S. investments in AI companies based in China, Hong Kong, and Macau. Moreover, the U.S.imposedcontrols on exports of advanced GPUs to Chinese customers.\n\nWhy it matters:Many startups in China and elsewhere relied on OpenAI’s models. However, China’s development of AI models is already quite advanced. For example, Alibaba’s Qwen2, which offers open weights, currently tops Hugging Face’s Open LLM Leaderboard (see below), ahead of Meta's Llama 3.\n\nWe’re thinking:Efforts to restrict U.S. AI technology can go only so far. At this point, the U.S. seems to have at most a six-month lead over China. OpenAI’s move encourages other nations to make sure they have robust, homegrown models or access to open source alternatives.\n\nAn influential ranking of open models revamped its criteria, as large language models approach human-level performance on popular tests.\n\nWhat’s new:Hugging Faceoverhauledits Open LLM Leaderboard, reshuffling its assessments of the smartest contenders. Therevised leaderboardis based on new benchmarks designed to be more challenging and harder to game.\n\nIntelligence reordered:The new Open LLM Leaderboard paints a very different picture than the earlier version: Some models moved up or down as many as 59 places. In the debut rankings, Qwen2’s recentlyreleased72-billion-parameter, instruction-tuned version topped the list with an average score of 43.02 out of 100. Meta’s Llama 3-70B-Instruct came in second with 36.67.Addressing saturation and contamination:Launched last year, theearlier version(which is still operating) ranks open large language models according to an aggregate of scores on six popular benchmarks. However, in the intervening months, the best models approached human-level scores, partly due to technical improvements and partly because the test answers leaked into the models’ training sets. The revised leaderboard replaces the old tests and corrects earlier flaws and errors:\n\nBehind the news:Leakage of training examples into test sets is a rising challenge to evaluating model performance. While Hugging Face relies on open benchmarks, other groups have attempted to address the issue by limiting access to the test questions or changing them regularly. Vals.AI, an independent model testing company,developedproprietary industry-specific tests for finance and law. Data consultancy Scale AIintroducedits own leaderboards, measuring models on proprietary tests in natural languages, math, and coding.Why it matters:Two million unique visitors browsed the Open LLM Leaderboard in the past year, and over 300,000 Hugging Face community members use and collaborate on it each month. Developers trust its scores, both individually and in aggregate, to decide which models to use and to judge the progress of their own efforts based on open models.\n\nWe’re thinking:As its name implies, the Open LLM leaderboard measures performance in natural language skills. Hugging Face also maintains anOpen VLM Leaderboard, which tests vision-language skills.\n\nA smoldering conflict between the music industry and AI companies exploded when major recording companies sued up-and-coming AI music makers.\n\nWhat’s new:Sony Music, Universal Music Group (UMG), and Warner Music — the world’s three largest music companies — and a trade organization, Recording Industry Association of America (RIAA),suedSuno and Udio, which offer web-based music generators, for alleged copyright violations.How it works:The music powers filed separate lawsuits againstSunoandUdioin U.S. federal courts. The plaintiffs allege that the startups used copyrighted songs owned by RIAA members as training data, in the process making unauthorized copies without receiving permission or compensating the owners. They seek damages of at least $150,000 per song and cessation of further AI training on their catalogs.\n\nBehind the news:Although major music companies have a history oftakingactionagainst AI companies, music streamers, and musicians who distributed generated likenesses of music they owned, they’re also working with AI startups on their own terms. For instance, UMG iscollaboratingwith voice-cloning startup Soundlabs to create authorized synthetic voices of UMG artists. UMG, Sony, and Warner are alsonegotiatingwith YouTube to license music for a song generator to be launched this year.\n\nWhy it matters:As in similar lawsuits that involve text generators, the outcome of these actions could have an important impact on AI developers and users alike. Copyright law in the United States (and many other countries) does not address whether training AI models on copyrighted materials is a use that requires permission from copyright owners. In lieu of further legislation that answers the question, courts will decide. Assuming these cases go to trial, a verdict in favor of Suno or Udio would set a precedent that copyright doesn’t necessarily protect copyrighted works from AI training. Conversely, a verdict in favor of the music industry could restrict the use of copyrighted works in training, impeding a range of AI technologies that historically have been trained on data from the open internet.\n\nWe’re thinking:Copyright aims to prohibit unauthorized copying of intellectual property, but routine copying of data is built into the infrastructure of digital communications, never mind training AI systems. A web browser makes a temporary copy of every web page it displays, and web search engines typically copy the page they’ve indexed. It’s high time torevisecopyright law for the AI era in ways that create the most value for the most people.\n\nThe technique of model merging combines separate models into a single, more capable model without further training, but it requires expertise and manual effort. Researchers automated the process.\n\nWhat's new:Takuya Akiba and colleagues at Sakana, a research lab based in Tokyo, devised an automatedmethod for merging models. It combines models trained for general tasks to produce models that perform well at the intersection of those tasks.\n\nKey insight:Researchers have demonstrated variousapproachesto model merging. Earlier work showed that vision models of the same architecture can be combined with good results simply byaveraging their corresponding weights, although subsequent studies revealed limitations in this approach. (When models have different architectures, averaging weights can combine parts they have in common.) An alternative is tostack layersdrawn from different models. These methods can be varied and integrated to offer a wide variety of possible model combinations. An automated process that tries various combinations at random, finds the best performers among the resulting models, and recombines them at random can discover the high-performance combinations of these approaches without relying on intuition and experience.\n\nHow it works:The authors aimed to build a large language model that would solve problems in Japanese. They used the algorithm known asCovariance Matrix Adaptation Evolution Strategy(CMA-ES) to merge the Japanese-language LLMShisa-Gammaand two math-specific, English-language LLMs:AbelandWizardMath. All three models were fine-tuned fromMistral 7B, which was pretrained on text from the web.\n\nResults:The authors evaluated their model on the Japanese subset ofMultilingual Grade School Math(MGSM). The merged model achieved 55.2 percent accuracy. Among the source models, Abel achieved 30.0 percent accuracy, WizardMath 18.4 percent accuracy, and Shisa Gamma 9.6 percent accuracy. The merged model’s performance fell between that of GPT-3.5 (50.4 percent accuracy) and GPT-4 (78.8 percent accuracy), which presumably are an order of magnitude larger.\n\nWhy it matters:Combining existing models offers a way to take advantage of their strengths without further training. It can be especially valuable in building models at the intersection between tasks, such as understanding Japanese language and solving math problems.\n\nWe're thinking:In addition to building new models, how can we make best use of the ones we already have? Merging them may be an efficient option.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--66--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/07/AIF_Event-Templates--18-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-03T153037.104.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-03T153334.258.png",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed--67-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2024/07/unnamed---2024-07-03T154036.152.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-150/",
    "title": "issue 150",
    "date": "",
    "reading_time": "",
    "content": "Many things in life have a positive side and a negative side. For instance, a new AI system might help democratize access, and at the same time it might be more accessible to people who have internet access than those who don’t. Thus, it could be either praised for helping people or criticized for not helping enough. These days, a determined critic or politician can point to almost anything, good or bad, and find cause to celebrate or denigrate it depending on their agenda.\n\nWe know fromstudiesof social media that posts that arouse anger are more likely to reach a large audience than those that encourage feelings of contentment. This means that whenever an event occurs — even a good one — naysayers have a larger megaphone than supporters. (This isn’t altogether new. Juicy gossip has always traveled faster than mundane truth.) For example, fear mongering about artificial general intelligence seems to be a persistent meme even though AI’s benefits vastly outweigh its harms.\n\nWhat can we do about this? I’d like to see us do more to support each other. If an uncivil critic has a larger megaphone than we do, we can respond together with a public show of support. When I tweet about some topics — support for Ukraine against Russian aggression, for instance —I find that an occasional hostile response can make me pull back. But I try to ignore the hostility and continue to support the causes that I believe in.\n\nThe psychologist John Gottman says that successful relationships have a ratio offive positive interactions to one negative interaction. I don't know whether a ratio like this applies to communities, but I would love to hear members of the AI community cheering for each other most of the time — even if, a smaller fraction of the time, we also need to discuss and fix problems that deserve sharp criticism.\n\nOver the past couple of years, I’ve seen members of the AI community express a lot of support for one another, but I’ve also noticed a growing tendency to criticize, especially on Twitter. To be clear, AI has many problems including bias, fairness, and harmful use cases, and we need to discuss and fix them. But if the AI community is to keep growing — which I hope we will — we need to invite others into an environment of mutual support and respect.\n\nI haddinner with a few AI friendslast weekend. Rod Brooks, Kai-Fu Lee, Tom Mitchell, and I reminisced about the early days of AI, when everyone knew each other and we often supported each other in the ambitious research directions that many were pursuing. The community continued to welcome newcomers for decades, which allowed us to grow and make a lot of progress.\n\nIn that spirit, I hope we’ll put more energy into strengthening our community and focus our critical impulses on the most pressing issues. Let’s give each other the love, respect, and support that will keep the field growing for a long time to come.\n\nKeep learning!\n\nAndrew\n\nA new benchmark aims to raise the bar for large language models.What’s new:Researchers at 132 institutions worldwide introduced theBeyond the Imitation Game benchmark(BIG-bench), which includes tasks that humans perform well but current state-of-the-art models don’t.How it works:The authors selected over 200tasksbased on 10criteriasuch as being sensible to humans, not solved by current language models, and “not solvable by memorizing the internet.” Many involve atypical problems such as identifying a single move that will win a game of chess, guessing a movie title from a series of emojis, and playing a role in a mock courtroom trial.\n\nResults:No model, regardless of size, outperformed the best-performing human on any task. However, for some tasks, the best-performing model beat the average human. For example, answering multiple-choice questions about Hindu mythology, the best model scored around 76 percent, the average human scored roughly 61 percent, and the best human scored 100 percent (random chance was 25 percent). Generally, larger models performed better than smaller ones. For example, BIG-G’s average accuracy on three-shot, multiple-choice tasks was nearly 33 percent with a few million parameters but around 42 percent with over a hundred billion parameters.\n\nWhy it matters:BIG-bench’s creators argue that benchmarks likeSuperGLUE,SQuAD2.0, andGSM8Kfocus on narrow skills. Yet the latest language models, after pretraining on huge datasets scraped from the internet, showunexpected abilitiessuch as solving simple arithmetic problems. BIG-bench’s diverse, few-shot tasks give researchers new ways to track such emergent capabilities as models, data, and training methods evolve.We’re thinking:Devising tasks that can’t be solved by memorizing the internet may push researchers to develop algorithms — including ones that enable complex forms of reasoning — that generalize well even with limited amounts of training data.\n\nMachine learning is making wind power more predictable.What’s new:Engie SA, a multinational energy utility based in France, is the first customer for an AI-powered tool from Google that predicts the energy output of wind farms,Bloombergreported. The company plans to deploy the system on 13 wind farms in Germany.How it works:Google’s DeepMind subsidiarytraineda neural network to predict energy output from wind farms up to 36 hours ahead of time. The training data included historical weather forecasts and unspecified data from wind turbines.\n\nBehind the news:Google isn’t the only firm employing machine learning to squeeze more electricity out of renewable resources.\n\nWhy it matters:Wind and solar power are notoriously uncertain, leading utilities to default to fossil fuels, which are available on-demand. Predicting wind-energy yields can reduce some of that uncertainty, helping utilities benefit from advantages such as renewables’lower overheadand easing dependence on fossil-fuel and nuclear sources.\n\nWe’re thinking:Stopping climate change isn’t the only motivation to cut dependence on fossil fuels. The conflict in Ukraine has contributed to a global shortage of oil and gas, causing energy prices to spike. Alternative sources can help make the global economy less reliant on oil producers and more resilient to disruptions in supply.\n\nThe DeepLearning.AI community continues to grow, thanks to Pie & AI ambassadors like Emilio Soria-Olivas of Valencia, Spain. We’re thrilled to share his accomplishments.Sign upto become a Pie & AI ambassador and learn how you could be featured as well!\n\nPeople who suffer from gastrointestinal conditions such as irritable bowel syndrome are number two when it comes to describing the characteristics of their own poop.What’s new:The smartphone appDietahelps patients to keep gastrointestinal illnesses in check by tracking their own behaviors and symptoms. It includes a computer vision model that recognizes medically salient characteristics of excrement as accurately as doctors and better than most patients, a recentstudyfound.How it works:The app enables patients to log symptoms such as nausea, constipation, and abdominal pain; behaviors like exercise, sleep, and meals; treatments including medications, supplements, and diet; and feelings of illness or wellbeing. It also helps patients experiment on themselves, recommending lifestyle changes and treatments and enabling patients to forward the results to caregivers. A computer vision model classifies feces according to characteristics that are useful in diagnosis.\n\nBehind the news:Machine learning engineers have trained other models to peer into the toilet.\n\nWhy it matters:Roughly 40 percent of adults worldwide may suffer from gastrointestinal conditions, according to a 2021study. Tracking bowel movements helps to diagnose these conditions earlier and more accurately.We’re thinking:We’re grateful that someone — other than us — builds models that classify the Bristol Stool Scale.\n\nAdding layers to a neural network puts the “deep” in deep learning, but it also increases the chance that the network will get stuck during training. A new approach effectively trains transformers with an order of magnitude more layers than previous methods.What’s new:A team at Microsoft led by Hongyu Wang and Shuming Ma developedDeepNorm, a normalization function that enables transformers to accommodate up to 1,000 layers. (Their models, dubbed DeepNet, topped out at 3.8 billion parameters.)Key insight:When training a transformer,layer normalizationoften is used to scale layer inputs, promoting faster learning. The magnitude of a layer normalization’s input isinversely proportionalto the total change in the parameter values of all previous layers in a training step. The authors found that the greater the number of layers, the higher the likelihood of a very large update. This results in larger inputs to layer normalization, so earlier layers receive smaller and smaller updates until parameter values stop changing and performance stops improving. (This issue is related to the familiar vanishing gradient problem, but its cause is different. In the familiar scenario, gradients from later layers diminish as they backpropagate through the network. In this case, the combination of layer normalization and unusually large updates results in significantly smaller gradients.) Limiting the total change in parameter values would prevent large updates, which should enable deeper networks to continue training without getting stuck.How it works:The authors trained a transformer, applying DeepNorm to theresidual connectionsin each attention and feed-forward layer.\n\nResults:The authors evaluated DeepNets of various depths on tasks that involve translating text between English and over 100 other languages. The DeepNets outperformed all competitors of equal depth, between 36 and 1,000 layers, as well as some with an order of magnitude fewer layers (and an order of magnitude more parameters). For instance,translating English into German and back, a 200-layer DeepNet achieved 28.9 BLEU, while a 200-layerdynamic linear combination of layers(a state-of-the-art transformer variant) achieved 27.5 BLEU. Seven other 200-layer models, including a transformer without the authors’ modifications, diverged during training. On the [OPUS-100]https://opus.nlpl.eu/opus-100.php multilingual dataset, a DeepNet with 200 layers and 3.2 billion parameters achieved 23.0 BLEU, while M2M-100 (a transformer variant with 48 layers and 12 billion parameters) achieved 18.4 BLEU.Why it matters:Scaling up neural networks has driven a lot of improvement over the past decade. This work points a way toward even deeper models.We’re thinking:DeepNets are deep and narrow, making previous models look shallow and wide by comparison. Since training ginormous (1,000 layer, super-wide) models is very expensive, we’d do well to find the ideal tradeoff between deep and narrow versus shallow and wide.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/06/Screen-Shot-2022-06-22-at-9--1-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/06/BIGBENCH.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/WIND.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/06/pie---Ai-amabssador-soptlight_The-Batch-Emilio.png",
      "https://dl-staging-website.ghost.io/content/images/2022/06/ezgif.com-gif-maker--31--1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/DEEPNET.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-185/",
    "title": "issue 185",
    "date": "",
    "reading_time": "",
    "content": "As you can read below in this issue ofThe Batch, Microsoft’s effort to reinvent web search by adding a large language model snagged when its chatbot went off the rails. Both Bing chat and Google’s Bard, the chatbot to be added to Google Search, have made up facts. In a few disturbing cases, Bing demanded apologies or threatened a user. What is the future of chatbots in search?\n\nIt’s important to consider how this technology will evolve. After all, we should architect our systems based not only on what AI can do now but on where it might go. Even though current chat-based search has problems, I’m optimistic that roadmaps exist to significantly improve it.\n\nLet’s start with the tendency of large language models (LLMs) to make up facts. Iwroteabout falsehoods generated by OpenAI’s ChatGPT. I don’t see a realistic path to getting an LLM with a fixed set of parameters to both (i) demonstrate deep and broad knowledge about the world and (ii) be accurate most of the time. A 175B-parameter model simply doesn’t have enough memory to know that much.\n\nLook at the problem in terms of human-level performance. I don’t think anyone could train an inexperienced high-school intern to answer every question under the sun without consulting reference sources. But an inexperienced intern could be trained to write reports with the aid of web search. Similarly, the approach known as retrieval augmented generation — which enables an LLM to carry out web searches and refer to external documents — offers a promising path to improving factual accuracy.\n\nBing chat and Bard do search the web, but they don’t yet generate outputs consistent with the sources they’ve discovered. I’m confident that further research will lead to progress on making sure LLMs generate text based on trusted sources. There’s significant momentum behind this goal, given the widespread societal attention focused on the problem, deep academic interest, and financial incentive for Google and Microsoft (as well as startups like You.com) to improve their models.\n\nIndeed, over a decade of NLP research has been devoted to the problem oftextual entailmentwhich, loosely, is the task of deciding whether Sentence A can reasonably be inferred to follow from some Sentence B. LLMs could take advantage of variations on these techniques — perhaps to double-check that their output is consistent with a trusted source — as well as new techniques yet to be invented.\n\nAs for personal attacks, threats, and other toxic output, I’m confident that a path also exists to significantly reduce such behaviors. LLMs, at their heart, simply predict the next word in a sequence based on text they were trained on. OpenAI shaped ChatGPT’s output by fine-tuning it on a dataset crafted by people hired to write conversations, and Google built a chatbot,Sparrow, that learned to follow rules through a variation onreinforcement learning from human feedback. Using techniques like these, I have little doubt that chatbots can be made to behave better.\n\nSo, while Bing’s misbehavior has been in the headlines, I believe that chat-based search has a promising future — not because of what the technology can do today, but because of where it will go tomorrow.\n\nKeep learning!\n\nAndrew\n\nP.S. Landing AI, which I lead, just released its computer vision platform for everyone to use. I’ll say more about this next week. Meanwhile, I invite you to check it out for free atlanding.ai!\n\nMicrosoft aimed to reinvent web search. Instead, it showed that even the most advanced text generators remain alarmingly unpredictable.What’s happened:In the two weeks since Microsoft integrated an OpenAI chatbot with its Bing search engine, users have reported interactions in which the chatbot spoke like a classic Hollywood rogue AI. It insisted it was right when it was clearly in error. It combed users’ Twitter feeds and threatened them when it found tweets that described their efforts to probe its secrets. It demanded that a user leave his wife to pursue a relationship, and it expressed anxiety at being tied to a search engine.How it works:Users shared anecdotes from hilarious to harrowing on social media.\n\nMicrosoft’s response:A week and a half into the public demo, Microsoft explained that long chat sessions confuse the model. The companylimitedusers to five inputs per session and 50 sessions per day. It soonincreasedthe limit to six inputs per session and 60 sessions per day and expects to relax it further in due course.Behind the news:Chatbots powered by recent large language models are capable of stunningly sophisticated conversation, and they occasionally cross boundaries their designers either thought they had blocked or didn’t imagine they would approach. Other recent examples:\n\nWhy it matters:Like past chatbot mishaps, the Bing chatbot’s antics are equal parts entertaining, disturbing, and illuminating of the limits of current large language models and the challenges of deploying them. Unlike earlier incidents, which arose from research projects, this model’s gaffes were part of a product launch by one of the world’s most valuable companies, and it is widely viewed as a potentialdisruptorof Google Search, one of the biggest businesses in tech history. How it hopped the guardrails will be a case study for years to come.We’re thinking:In our experience, chatbots based on large language models deliver benign responses the vast majority of the time. There’s no excuse for false or toxic output, but it's also not surprising that most commentary focuses on the relatively rare slip-ups. While current technology has problems, we remain excited by the benefits it can deliver and optimistic about the roadmap to better performance.\n\nNations tentatively agreed to limit their use of autonomous weapons.What’s new:Representatives of60 countriesendorseda nonbinding resolution that calls for responsible development, deployment, and use of military AI. Parties to the agreement include China and the United states but not Russia.Bullet points:Theresolutioncame out of the first-ever summit on Responsible Artificial Intelligence in the Military (REAIM), hosted in The Hague by the governments of South Korea and the Netherlands. It outlines how AI may be put to military uses, how it may transform global politics, and how governments ought to approach it. It closes by calling on governments, private companies, academic institutions, and non-governmental organizations to collaborate on guidelines for responsible use of military AI. The countries agreed that:\n\nUnilateral actions:The U.S.releaseda 12-point resolution covering military AI development, deployment, governance, safety standards, and limitations. Many of its points mirrored those in the agreement, but it also called for a ban on AI control of nuclear weapons and clear descriptions of the uses of military AI systems. Chinacalled ongovernments to develop ethical guidelines for military AI.Behind the news:In 2021, 125 of the United Nations’ 193 member nationssoughtto add AI weapons to a pre-existing resolution that bans or restricts the use of certain weapons. The effort failed due to opposition by the U.S. and Russia.\n\nYes, but:AI and military experts criticized the resolution as toothless andlackinga concrete call to disarm. Severaldenouncedthe U.S. for opposing previous efforts to establish binding laws that would restrict wartime uses of AI.Why it matters:Autonomous weapons have a longhistory, and AI opens possibilities for further autonomy to the point of deciding to fire on targets. Fully autonomous drones may have been first used incombatduring Libya’s 2020 civil war, and fliers with similar capabilities reportedly have beenusedin the Russia-Ukraine war. Such deployments risk making full autonomy seem like a normal part of warfare and raise the urgency of establishing rules that will rein them in.We’re thinking:We salute the 60 supporters of this resolution for taking a step toward channeling AI into nonlethal military uses such as enhanced communications, medical care, and logistics.\n\nWant to build high-quality machine learning models in less time? Use the DataHeroes library to build a small data subset that’s easier to clean and faster to train your model on.Get VIP access\n\nGovernments are repurposing Covid-focused face recognition systems as tools of repression.\n\nWhat's new:Russia’s internal security forces are using Moscow’s visual surveillance system, initially meant to help enforce pandemic-era restrictions, to crack down on anti-government dissidents or protestors against the war in Ukraine,Wiredreported.\n\nHow it works:Moscow upgraded its surveillance network in 2020 to identify violators of masking requirements and stay-at-home orders. Thesystemincludes 217,000 cameras equipped to recognize faces and license plate numbers. It also tracks medical records and mobile-phone locations. Companies including Intel, Nvidia, Samsung, and Russian AI startup NtechLab have supplied equipment.\n\nBehind the news:Numerous governments have co-opted technology originally deployed to counter Covid-19 for broader surveillance, the Pulitzer Centerreported. For instance, police in Hyderabad, India, allegedly targeted minorities for harassment using face-detection systems initially implemented to spot people flaunting mask mandates.\n\nWhy it matters:There’s a fine line between using surveillance for the greater good and abusing it to exercise power. When the pandemic hit, computer vision and contact tracing were important tools for containing the spread of disease. But the same technology that helps to keep the public safe lends itself to less laudable uses, and governments can find it hard to resist.\n\nWe're thinking:Governments often expand their power in times of crisis and hold onto it after the crisis has passed. That makes it doubly important that government AI systems be accountable to the public. The AI community can play an important role in establishing standards for their procurement, deployment, control, and auditing.\n\nAutonomous robots trained to navigate in a simulation often struggle in the real world. New work helps bridge the gap in a counterintuitive way.What’s new:Joanne Truong and colleagues at Georgia Institute of Technology and Meta proposed atraining methodthat gives robots a leg up in the transition from simulation to reality. They found that training in a crude simulation produced better performance in the real world than training in a more realistic sim.Key insight:When using machine learning to train a robot to navigate, it stands to reason that a more realistic simulation would ease its transition to the real world — but this isn’t necessarily so. The more detailed the simulation, the more likely the robot’s motion planning algorithm will overfit to the simulation’s flaws or bog down in processing, hindering real-world operation. One way around this is to separate motion planning from low-level control and train the motion planner while “teleporting” the robot from one place to another without locomotion. Once deployed, the motion planner can pass commands to an off-the-shelf, non-learning, low-level controller, which in turn calculates the details of locomotion. This avoids both the simulation errors and intensive processing, enabling the robot to operate more smoothly in the real world.How it Works:The authors trained two motion planners (each made up of a convolutional neural network and an LSTM) to move a Boston DynamicsSpotthrough simulated environments. One learned to navigate by teleporting, the other by moving simulated legs.\n\nResults:The authors tested a Spot unit outfitted with each controller in a real-world office lobby, replacing the low-level controllers used in training with Spot’s built-in controller. The motion planner trained on teleportation took the robot to its goal 100 percent of the time, while the one trained on the more detailed simulation succeeded 67.7 percent of the time.Yes, but:Dividing robotic control between high- and low-level policies enabled the authors to dramatically simplify the training simulation. However, they didn’t compare their results with those of systems that calculate robot motion end-to-end.Why it matters:Overcoming the gap between simulation and reality is a major challenge in robotics. The finding that lower-fidelity simulation can narrow the gap defies intuition.We’re thinking:Simplifying simulations may benefit other reinforcement learning models that are expected to generalize to the real world.\n\nResearch: AI trained to recognize a Covid-19 infection from the sound of a cough don’t provide better results than basic symptom checkersA study found that audio-based AI classifiers can’t accurately recognize SARSCoV2 infection status based on the sound of coughs. (TechCrunch)\n\nAI piloted a U.S. F-16 fighter aircraftThe Defense Advanced Research Projects Agency (Darpa) confirmed that its autonomous fighter airplane successfully flew multiple times over several days. (Vice)\n\nResearch: Carnegie Mellon University's Robotics Institute built an AI-powered robot that helps human artists produce paintingsFRIDA, a robotic arm with a paintbrush attached to it, uses inputs like language descriptions or images to generate physical paintings. (Carnegie Mellon University School of Computer Science)Publishers are concerned over the Bing chatbot’s ability to summarize news articles behind paywallsThe recently released AI-powered chatbot by Microsoft extracts key information from paid articles, defying the business model of media outlets. (Wired)\n\nA startup by Google’s former CEO is developing AI-based war machinesIstari, a company backed by Eric Schmidt, aims to strengthen the US military with the power of machine learning. (Wired)\n\nYou.com, a search engine, launched multimodal chat searchThe startup, which offers a privacy-focused search engine, released a new feature that adds elements beyond text to search results, aiming to offer a better and more personalized experience. (TechCrunch)A robotic sculptor produces marble artThe Italian startup Roboto built 1L, a robot that carves marble faster than a human sculptor. (CBS)\n\nCalifornia approved Amazon’s robotaxi on public roadsAn autonomous vehicle built by Zoox, Amazon’s subsidiary, became the first robotaxi that lacks a steering wheel and other human controls to legally carry passengers in California. The vehicle is limited to a small area near San Francisco. (The Verge)\n\nHugging Face released PEFT, a library for parameter-efficient fine-tuningPEFT enables machine learning models to achieve performance comparable to full fine-tuning while only having a small number of trainable parameters. (HuggingFace)Poll reveals Americans’ views on the impact of AI on societyThe Monmouth University Poll showed that six in 10 Americans have heard about ChatGPT, one in 10 believe AI will do more good than harm, and more. (Monmouth University)\n\nBeewise launched an AI-powered home for beesBeehome 4 is a robotics-enabled home for bee colonies designed to help beekeepers produce honey while protecting bees’ lives. (VentureBeat)\n\nColorado took a step toward AI governanceThe Colorado Division of Insurance released proposed guidelines for the insurance industry’s use of AI. (Deveboise & Plimpton)",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--39--1.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--15-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--40-.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/02/The-Batch-ads-and-exclusive-banners.gif",
      "https://dl-staging-website.ghost.io/content/images/2023/02/SAFECITY.png",
      "https://dl-staging-website.ghost.io/content/images/2023/02/unnamed--41-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-229/",
    "title": "issue 229",
    "date": "",
    "reading_time": "",
    "content": "AI is progressing faster than ever. This is thrilling, yet rapid change can be disorienting. In such times, it’s useful to follow Jeff Bezos’adviceto think about not only what is changing but also what will stay the same. If something doesn’t change, investing energy and effort in it is more likely to be worthwhile.\n\nHere are some things in AI that I’m confident won’t change over the next decade:\n\nWhat does this mean for each of us? Taking the points above in turn:\n\nWhile the three points above relate to AI, I want to share two other things that I’m confident will, unfortunately, stay the same over the next decade: (i) Climate change will continue to be a major challenge to humanity. (ii) Poverty, where many people can barely (or perhaps not even) afford basic necessities, will remain a problem. I will continue to think about how AI climate modeling can help the former and how we can use AI to lift up everyone.\n\nThrough this exciting time, I’m grateful to be connected to you. I look forward to navigating with you the changes — and constants — of 2024.\n\nHappy new year!\n\nAndrew\n\nOnly one year into the mainstreaming of generative AI, a wondrous horizon stretches before us. We no longer search the internet, we chat with it; we don’t write emails, we ask our assistant in the cloud to do it. We converse with code, conjure images, mold video clips. This new world holds great promise, but also great worries about whether these powers will serve all of us, and serve us well. So,asinyearspast, we asked some of AI’s brightest minds: What is your hope for the coming year? Their answers hold clues to the future and our place in it.\n\nThe year 2023 was an inflection point in the development of broadly useful AI systems across text, image, video, audio, and other modalities. At Runway alone, we saw the release of video-generation models such as Gen-1 and Gen-2, as well as tools that enable new forms of creative control with those models. In the coming year, here are some areas where I expect to see continued progress:\n\nBeyond technological advancements, the most rewarding part of building these systems is that, with every update and increase in capabilities, new audiences are introduced to them and new stories are told that weren’t told before. I’m excited to see how that will continue to happen in the coming year.\n\nAnastasis Germanidis is co-founder and CTO of Runway, an applied AI research company shaping the next era of art, entertainment, and human creativity.\n\nThe past year has seen incredible innovation in AI, and I expect as much or more in 2024. The coming year undoubtedly will be a year of rapid progress in models – multimodal, multilingual, and (hopefully) smaller and faster.\n\nTo date, models and datasets used for training have been heavilybiasedtowards English-speaking and Western European countries, offering little representation of languages from the Global South and Asia. Even when languages from the Global South are represented, the data almost always is translated from English or Western European languages. In 2023, the “rich got richer” and the “poor got poorer” as breakthroughs facilitated use of widely spoken languages like English while further impeding access for speakers of languages for which much less data is available.\n\nNext year will be the year of Robin Hood, when we try to reshare the gains by closing the language gap. We will see rapid improvement in state-of-the-art multilingual models, as well as innovation in synthetic data generation to build foundation models for specific languages. I believe we will make progress in closing the language gap and strengthen our collective effort to incorporate research, training data, and individuals from across the globe. This will include projects likeAya, a model from Cohere For AI that will cover 101 languages. Bridging the gap is not just a matter of inclusivity, it’s key to unlocking the transformative power of AI and ensuring that it can serve a global audience, irrespective of language or cultural background.\n\nIn addition, I expect 2024 to be a year for research bets.Multimodalwill become a ubiquitous term as we move away from subfields dedicated to language, computer vision, and audio in isolation. Models will be able to process multiple sensory inputs at once, more like humans. We will care urgently about model size as we deploy more models in resource-constrained environments. AI models will become smaller and faster. Our lab is alreadypushing the limitsof efficiency at scale, data pruning, and adaptive computing. Localization of models using retrieval augmented generation (RAG) and efficient fine-tuning will be paramount, as everyday users look to unlock the potential in frontier models.\n\nIn the coming year, it will be even more important to interrogate the defaults of where, how, and by whom research is done. To date, state-of-the-art models have come from a handful of labs and researchers. The community responsible for recent breakthroughs is so small that I know many of the people involved personally. However, we need to broaden participation in breakthroughs to include the best minds. At Cohere For AI, we are in the second cohort of ourScholars Program, which provides alternative points of entry into research for AI talent around the world.\n\nThe compute divide will persist in the coming year. Shortages of compute combined with stockpiling of GPUs mean there won’t be immediate changes in the availability of compute. This year, we launched ourresearch grant program, so independent and academic researchers can access frontier models at Cohere. More needs to be done at national and global scales to bridge the divide for researchers and practitioners.\n\nWe are in an interesting time, and it is rare to work on research that is being adopted so quickly. Our ideas not only resonate in AI conferences but have a profound impact on the world around us. In 2024, expect more rapid change and some breakthroughs that make this technology immediate and usable to more humans around the world. By prioritizing inclusivity in model training and fundamental research, we can help ensure that AI becomes a truly global technology, accessible to users from all backgrounds.\n\nSara Hooker is a senior VP of research at Cohere and leads Cohere For AI, a nonprofit machine learning research lab that supports fundamental enquiry and broad access.\n\nOnly a year ago, ChatGPT woke the world up to the power of foundation models. But this power is not about shiny, jaw-dropping demos. Foundation models will permeate every sector, every aspect of our lives, in much the same way that computing and the Internet transformed society in previous generations. Given the extent of this projected impact, we must ask not only what AI can do, but also how it is built. How is it governed? Who decides?\n\nWe don’t really know. This is becausetransparencyin AI is on the decline. For much of the 2010s, openness was the default orientation: Researchers published papers, code, and datasets. In the last three years, transparency has waned. Very little is known publicly about the most advanced models (such as GPT-4, Gemini, and Claude): What data was used to train them? Who created this data and what were the labor practices? What values are these models aligned to? How are these models being used in practice? Without transparency, there is no accountability, and we have witnessed the problems that arise from the lack of transparency in previous generations of technologies such as social media.\n\nTo make assessments of transparency rigorous, the Center for Research on Foundation Models introduced theFoundation Model Transparency Index, which characterizes the transparency of foundation model developers. The good news is that many aspects of transparency (e.g., having proper documentation) are achievable and aligned with the incentives of companies. In 2024, maybe we can start to reverse the trend.\n\nBy now, policymakers widely recognize the need to govern AI. In addition to transparency, among the first priorities isevaluation, which is mentioned as a priority in the United States executive order, the European Union AI Act, and the UK’s new AI Safety Institute. Indeed, without a scientific basis for understanding the capabilities and risks of these models, we are flying blind. About a year ago, the Center for Research on Foundation Models released theHolistic Evaluation of Language Models(HELM), a resource for evaluating foundation models including language models and image generation models. Now we are partnering with MLCommons to develop anindustry standard for safety evaluations.\n\nBut evaluation is hard, especially for general, open-ended systems. How do you cover the nearly unbounded space of use cases and potential harms? How do you prevent gaming? How do you present the results to the public in a legible way? These are open research questions, but we are on a short fuse to solve them to keep pace with the rapid development of AI. We need the help of the entire research community.\n\nIt does not seem far-fetched to imagine that ChatGPT-like assistants will be the primary way we access information and make decisions. Therefore, the behavior of the underlying foundation models — including any biases and preferences — is consequential. These models are said to align to human values, but whose values are we talking about? Again, due to the lack of transparency, we have no visibility into what these values are and how they are determined. Rather than having these decisions made by a single organization, could we imagine a moredemocraticprocess for eliciting values? It is the integrity and legitimacy of the process that matters. OpenAI wants tofundwork in this area, and Anthropic has someresearchin this direction, but these are still early days. I hope that some of these ideas will make their way into production systems.\n\nThe foundation-models semi truck will barrel on, and we don’t know where it is headed. We need to turn on the headlights (improve transparency), make a map to see where we are (perform evaluations), and ensure that we are steering in the right direction (elicit values in a democratic way). If we can do even some of this, we will be in a better place.\n\nPercy Liang is an associate professor of computer science at Stanford, director of the Center for Research on Foundation Models, senior fellow at the Institute for Human-Centered AI, and co-founder of Together AI.\n\nWe wish you a skillful new year! Take your generative AI knowledge to the next level with short courses from DeepLearning.AI. Our catalog is available for free for a limited time.Check it out\n\nBefore this past year, when I told people I worked in AI, more often than not I was met with a blank stare and sometimes a question along the lines of: “You mean like robots?” In the last year, the seemingly magical abilities of AI models, especially large language models (LLMs), have broken into mainstream awareness, and now I’m greeted with questions like: “How does ChatGPT really work?” But if we were more transparent about the sheer amount of human time and labor that went into training LLMs, I’m sure the questions would be more along the lines of: “How do I keep my data from being used for training AI models?” Because as impressive as ChatGPT’s knock-knock jokes or chocolate chip cookie recipes are, they are definitely not magical — they are built upon the work and creativity of human beings, who should be attributed for their contributions.\n\nAI models are black boxes that, to a user, appear to save labor. But, in fact, huge amounts of labor are required to develop them: from the books, websites, drawings, photos, and videos hoovered up without consent to the invisible armies of underpaid workers who spend their days ranking and improving LLM outputs. And all of this training is powered by massive amounts of natural resources that are extracted by still more human labor: rare metals to make those precious GPUs, water to cool them, energy to make them crunch numbers and output probabilities.\n\nUntil very recently, issues of copyright and consent were overlooked when it came to AI training data. Existing laws were assumed not to apply to training AI models, and the “move fast and break things” motto prevailed. But in the past year, authors likeSarah SilvermanandGeorge R.R. Martinhave sued AI companies to assert their rights as content creators whose work was used without their permission to train AI models. While it’s too early to say how these lawsuits (and others) will pan out and how that will shape the future of copyright law in the United States and beyond, I hope that new mechanisms will be developed to allow content creators more control over their work. We are starting to see this from organizations likeSpawning, which helped create ai.txt files that restrict the use of content for commercial AI training. I hope to see more AI developers respect these mechanisms and adopt opt-in (as opposed to opt-out) approaches for gathering consent-based datasets.\n\nApart from training data, development itself requires increasing amounts of labor. A new step recently has been added to the training process:RLHF, or reinforcement learning from human feedback. This step employs human annotators to rank text generated by large language models, providing feedback that makes them better at responding to human instructions and less likely to produce toxic output. This ranking process is done at scale by outsourced workers inoffices in Kenyaandprisons in Finland. Some of these workers are paid less than $2 an hour to label texts for hours on end, although we don’t have the overall numbers because AI companies are increasingly opaque about how they train AI models. Creating data for AI has become anew gig economy— but all this immense amount of human labor and creativity remains largely unseen and unrecognized.\n\nAnd as AI is increasingly pushing out the very designers and artists whose life’s work was used to train the models in the first place (why pay a photographer when you canuse AI to generate a custom stock photograph on demand), it’s crucial that we stop and reflect upon the relationship between human labor and creativity and AI. AI is truly an exciting new technology, and one that is set to provide huge profits to many tech companies, but artists and gig workers are barely getting crumbs of the pie, if anything at all. It’s not too late to reimagine AI as a technology that respects human agency and creativity by properly recognizing the human time and effort that goes into training AI models.\n\nMy hope in 2024 is that we start recognizing the knowledge, wisdom, and creativity that goes into training AI models, being more transparent about AI’s human costs, and developing increasingly human-centric technologies.\n\nSasha Luccioni is a research scientist and climate lead at HuggingFace, a founding member of Climate Change AI, and a board member of Women in Machine Learning.\n\nOne of my favourite flavours of conversation is listening to reinforcement learning experts talk about their children as reinforcement learning agents. These conversations highlight just how comically far behind humans our machine learning models are. Especially when comparing the ability to acquire knowledge without being told explicitly what to learn and when comparing the amount of information required for that learning.My co-founder has a three-year-old son who is obsessed with cars. It would seem his objective function is to be exposed to as many cars as possible. So much so that he came home from a supercar show ranting and raving about the Daihatsu he saw in the parking lot, because he had never seen a Daihatsu before. On another occasion, when my co-founder told him the vehicle he was pointing at and enquiring about was a truck, the child did not hesitate to know thattruckwas a descriptor for a class of vehicle and not the name of the car.\n\nWhat makes his little brain decide what is important to learn? How does it make connections? How does it make the inference so quickly across such a vast domain? Fueled solely by a bowl of Otees cereal?What we have been able to achieve with our models as a species is quite impressive. But what I find far less impressive is how big the models are and the exorbitant resources of data, compute, capital, and energy required to build them. My co-founder's child learns far more from far less data, with a lot less energy.This is not only a conundrum of resources for machine learning architects. It has profound implications for implementing AI in parts of the world where not only data but also electricity and computing equipment are severely limited. As AI practitioners, we need to understand how to build smaller, smarter models with less data.Although efforts to put today's top-performing models on mobile devices are driving development of smaller models, prioritising small models that learn from relatively small datasets runs counter to mainstream AI development.AI has the potential to help us understand some of the biggest questions of the universe, and it could provide solutions to some of the most pressing issues of our lifetime, like ensuring that everyone has access to clean energy, clean water, nutritious meals, and quality healthcare; resolving conflict; and overcoming the limitations of human greed. Yet the current mainstream of AI largely overlooks the lives affected by such problems. An approach that does not require the level of capital investment typical of AI would open the AI domain to more people, from more places, so they too can leverage the power of AI for the benefit of their communities.I hope for many things for AI: that regulation and governance will improve, that the people who build the technology will do so with intention and with principles and values grounded in the connection of humanity. But the hope I am focusing on for now is more building of smaller, smarter models with less data to share the benefits of AI throughout the world. What are we working toward if not to make the world a sustainably better place for more people?\n\nPelonomi Moiloa is CEO of Lelapa AI, a socially grounded research and product lab that focuses on AI for Africans, by Africans.\n\nWithout question, 2023 has been the most exciting and interesting year in technology that I’ve seen over a fairly long career. It bears mention that I’m pretty sure I said more or less the same thing at the close of 2022, and I suspect I’ll probably be saying the same around this time next year and each year for the foreseeable future—the point being that, in AI right now, we’re experiencing a period of sustained exponential growth that represents perhaps the most profound technological progress that we have ever seen.\n\nAnd it’s really only the beginning. Modern generative AI is still in its infancy, and we’re learning as we go. While it feels like we’ve lived with them for ages now, 2023 was really the first year that powerful AI tools like ChatGPT and Microsoft Copilots meaningfully entered the public vernacular as useful helpers to make people’s lives easier. By the time next year wraps up, we’ll have many new experiences, apps, and tools that will create cascading benefits for more and more people across the planet. Though the amplitude of hype and acceleration rate of AI’s growth can keep folks fixated on each subsequent “next big thing,” if we step back just a little bit, it’s easier to see that the opportunity in front of us is astronomically greater than what we’ve already achieved.\n\nBecause we only get to sample the product of that exponential curve every couple of years or so, most recently with GPT-4, it’s easy to forget in the interim how astonishing the pace of growth actually is. And, as is our human nature, we acclimatize very quickly and soon take for granted each immediate set of wild new possibilities offered to us.\n\nSo, my hope for all of us working in AI and technology over the next year is that we collectively remember that the next sample from the exponential is coming, and prepare ourselves appropriately for the (sure to be incredible) outcomes. If you haven’t done so already, pay close attention, experiment, and build AI production practices. If not, you’ll be too far behind to translate the progress into meaningful benefits for everyone.\n\nMay 2024 continue to bring the excitement of discovery and continued innovation for us all.\n\nKevin Scott is chief technology officer and executive vice president of AI at Microsoft.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--42--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--81-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--82-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--43-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/12/The-Batch-ads-and-exclusive-banners--93-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--83-.png",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--44-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2023/12/unnamed--84-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-155/",
    "title": "issue 155",
    "date": "",
    "reading_time": "",
    "content": "Many AI systems have been built using data scraped from the internet. Indeed, even the cornerstone dataset for computer vision research, ImageNet, was built using images taken from the public internet. With the rise of data-centric AI, access to good data continues to grow in importance to developers.What are the limits for scraping and using public data? Earlier this year, a United States courtruledthat scraping data from websites that don’t take measures to hide it from public view doesn’t violate a law designed to thwart hackers. I believe this is a positive step for AI as well as competition on the internet, and I hope it will lead to further clarity about what is and isn’t allowed.Many companies aim to create so-called walled gardens in which they provide exclusive access to content — even though it may be visible to all — such as social media posts or user résumés (the data at the heart of the ruling). But such data is valuable to other companies as well. For example, while LinkedIn helps users display their résumés to professional contacts, other companies might use this data to recruit potential employees, predict whether employees are likely to leave their current positions (updating a résumé is a sign), or find sales leads. Scraping the web was important in the early days of the internet to make web search viable, but as new uses come up — such as using machine learning to generate novel insights — clear rules about which data can and can’t be used, and how, become even more important.This isn’t a simple matter. There is a fine line between protecting copyright, which incentivizes businesses to create that data, and making data widely available, which enables others to derive value from it. In addition, freely available data can be abused. For example, some face recognition companies have been especially aggressive inscraping face portraits, building systems that invade privacy.\n\nThe U.S. court found that scraping data that is publicly accessible doesn’t violate theComputer Fraud and Abuse Act. This is not the same as allowing unfettered access to web scrapers. Data held behind a login wall or accessible only after agreeing to restrictive terms of service may be a different matter. (Disclaimer: Please don’t construe anything I say as legal advice.)While this ruling may hurt companies that have built businesses on data that is fully visible to the public, overall I view it as a positive step. It will increase the free flow of information and make it easier for teams to innovate in AI and beyond. Also, knocking down part of the wall that surrounds walled gardens should increase competition on the internet. On the other hand, it increases the incentives to put data behind a login wall, where it’s no longer publicly accessible.The issues of open versus closed data aren’t new. With the rise of mobile apps over a decade ago, web search companies worried that data would be locked within mobile apps rather than accessible on the web. This is one reason why Google invested in the Android mobile operating system as a counterweight to Apple’s iOS. Although ideas about which data should be accessible continue to shift, I continue to believe that a more open internet will benefit more people. With the rise of AI, algorithms — in addition to people — are hungry to see this data, making it even more important to ensure relatively free access.\n\nKeep learning!\n\nAndrew\n\nWestern nations are making a substantial investment in AI.\n\nWhat’s new:The North Atlantic Treaty Organization (NATO), which includes the United States, Canada, and much of Europe,announceda €1 billion venture capital fund that will focus on technologies including AI. The move adds to the growing momentum behind AI for warfare.How it works:The alliance’s Innovation Fund is bankrolled primarily by 22 of the alliance’s 30 members with additional pledges from other members. It will disburse its money over 15 years.\n\nBehind the news:NATO members recently boosted their individual AI budgets as well.\n\nWhy it matters:Besides autonomous weaponry, AI has numerous military applications that confer strategic and tactical advantages. In the Russian invasion of Ukraine alone, AI has been used toidentifyenemy soldiers,combatpropaganda, andinterceptcommunications.We’re thinking:The rising tide of military AI adds urgency to calls for international agreements on how the technology can be used in warfare. We support the United Nations’ proposedbanon autonomous weapons.\n\nA drive-through system automatically inspects vehicles for dents, leaks, and low tire pressure.\n\nWhat’s new:General Motors is giving its dealerships an option toinstalla visual inspection system from UVeye. Volvostrucka similar deal with the Tel Aviv startup in March.How it works:UVeye’s technology is designed to cut the time it takes to inspect a vehicle from minutes, possibly hours, to seconds. The company offers three systems to be installed on a service center’s premises for an undisclosed subscription fee.\n\nBehind the news:General Motors and Volvo separately invested undisclosed sums in UVeye, as have Honda, Toyota, and Škoda, a Volkswagen subsidiary. Several General Motors dealers around the U.S. already use its technology for vehicle checkups; the new deal will make it available to all 4,000. Volvo uses UVeye scanners on its assembly lines and offers incentives to dealerships to use them as well.Why it matters:A computer vision system that completes inspections in seconds can free mechanics to focus on more critical tasks, help dealers evaluate trade-ins, and give customers confidence that service stations are addressing real issues.We’re thinking:Autonomous driving is the first automotive application for AI that many people think of, but other important tasks are easier to automate. Streamlining routine maintenance is one. Others includeassessing insurance claimsandoptimizing traffic patterns.\n\nLaunching today: Course 3 of theMachine Learning Specialization, “Unsupervised Learning, Recommender Systems, and Reinforcement Learning.” Learn to train models using unsupervised clustering, generate recommendations via collaborative filtering, and build deep reinforcement learning models!Enrollto #BreakIntoAI\n\nNeural networks are spotting up-and-coming players for some of the best teams in football (known as soccer in the United States).\n\nWhat’s new:AiSCOUTuses computer vision to grade amateur footballers and recommends those who score highest to representatives of professional teams,Forbesreported.\n\nHow it works:Amateurs upload videos of themselves performing eight drills such as passing, shooting, and dribbling around cones. AiSCOUT scores the performance on a scale of 0 to 2 relative to others it has evaluated (a score of 1.7 might prompt an in-person trial with a top team).\n\nBehind the news:Machine learning is being used to improve performance in a wide range of sports.\n\nWhy it matters:Talent scouts have been obsessed with data since the days of pencil and paper. Machine learning can help clubs to cast a wider net and give far-flung aspirants a shot at going pro.\n\nWe’re thinking:We get a kick out of this app!\n\nRobots trained via reinforcement learning usually study videos of robots performing the task at hand. A new approach used videos of humans to pre-train robotic arms.\n\nWhat’s new:UC Berkeley researchers led by Tete Xiao and Ilija Radosavovic showed that real-world videos with patches missing were better than images of robot arms for training a robot to perform motor-control tasks. They call their methodMasked Visual Pretraining(MVP). They also built a benchmark suite of tasks for robot arms.\n\nKey insight:One way to train a robot arm involves two models: one that learns to produce representations of visual input and a much smaller one, the controller, that uses those representations to drive the arm. Typically, both models learn from images of a robotic arm. Surprisingly, pretraining the vision model on images of humans performing manual tasks not only results in better representations but also reduces the cost of adapting the system to new tasks. Instead of retraining the whole system on images of a new task, object, or environment, the controller alone can be fine-tuned.\n\nHow it works:The authors pretrained a visual model to reproduce images that had been partly masked by obscuring a rectangular portion at random. The pretraining set was drawn fromthreevideodatasetsthat include clips of humans performing manual actions such as manipulating a Rubik’s Cube. They used the resulting representations to fine-tune controllers that moved a robot arm in asimulation. They fine-tuned a separate controller for each of four tasks (opening a cabinet door as well as reaching, picking up, and relocating objects of different colors, shapes, and sizes) for each of two types of arm (one with a gripper, the other with four fingers).\n\nResults:In all eight tasks, the authors’ approach outperformed twostate-of-the-artmethodsthat train the visual and controller models on images of robots for training. The authors compared their representations to those produced by a transformer trained on ImageNet in supervised fashion. In seven tasks, the controller that used their representations outperformed one that used the supervised transformer’s representations. In the eighth, it performed equally well. In tasks that required a four-fingered arm to pick up an object, the authors’ approach achieved a success rate of 80 percent versus 60 percent.\n\nYes, but:The authors didn’t compare masked pretraining on images of humans with masked pretraining on images of robots. Thus, it’s not clear whether their method outperformed the baseline due to their choice of training dataset or pretraining technique.\n\nWhy it matters:Learning from more varied data is a widely used approach to gaining skills that generalize across tasks. Masked pretraining of visual models has improved performance invideo classification,image generation, and other tasks. The combination looks like a winner.\n\nWe’re thinking:Variety of data is important, but so is its relation to the task at hand. ImageNet probably is more varied than the authors’ training set of humans performing manual actions, but it’s unrelated to tasks performed by robot arms. So it stands to reason that the authors’ dataset was more effective.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/07/Data-Scraping2-tweak_1200px-1.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/07/MILITARY_600px_3.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/07/UVEYE.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/07/DeepLearningAI_Banner_Stanford_Launch_1200x628-V2_Artboard-2-copy-2--1-.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/07/SCOUT_Recrop_600px.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/07/MOTOR.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-67/",
    "title": "issue 67",
    "date": "",
    "reading_time": "",
    "content": "Over the lasttwoweeks, I described the importance of clean, consistent labels and how to use human-level performance (HLP) to trigger a review of whether labeling instructions need to be reviewed.\n\nWhen training examples are labeled inconsistently, an AI that beats HLP on the test set might not actually perform better than humans in practice. Take speech recognition. If humans transcribing an audio clip were to label the same speech disfluency “um” (a U.S. version) 70 percent of the time and “erm” (a UK variation) 30 percent of the time, then HLP would be low. Two randomly chosen labelers would agree only 58 percent of the time (0.72 + 0.33). An AI model could gain a statistical advantage by picking “um” all of the time, which would be consistent with 70 percent of the time with the human-supplied label. Thus, the AI would beat HLP without being more accurate in a way that matters.\n\nLabeling training data consistently is particularly important for small data problems. Innovations like data synthesis usinggenerative adversarial networks,data augmentation,transfer learning, and self-supervision expand the possibilities for small data. But when I’m trying to train a neural network on 1,000 examples, the first thing I do is make sure they’re labeled consistently.\n\nLet’s continue with last week’s example of determining if a scratch is significant based on its length. If the labels are noisy — say, different labelers used different thresholds for labeling a scratch as significant (the left-hand graph in the image above)¸— an algorithm will need a large number of examples to determine the optimal threshold. But if the data were clean — if all the labelers agree on the length that causes the label to switch from 0 to 1 (the right-hand graph) — the optimal threshold is clear.\n\nLearning theoryaffirms that the number of examples needed is significantly lower when the data is consistently labeled. In the simple example above, the error decreases on the order of {1 / √ m} in the case on the left, and {1/m} in the case on the right, where m is the training set size. Thus, error decreases much faster when the labels are consistent, and the algorithm needs many fewer examples to do well.\n\nClean labels are generally helpful. You might be better able to get away with noisy labels when you have 1 million examples, since the algorithm can average over them. And it’s certainly much harder to revise 1 million labels than 1,000. But clean labels are worthwhile for all machine learning problems and particularly important if you’re working with small data.\n\nKeep learning!\n\nAndrew\n\nThe U.S. government’s effort to take advantage of AI have not lived up to its promise, according to a new report.What’s new:Implementations of machine learning systems by federal agencies are “uneven at best, and problematic and perhaps dangerous at worst,”saidauthors of asurveyby the Administrative Conference of the United States, Stanford Law School, and New York University School of Law.What they found:Less than half of civilian federal agencies surveyed used some form of AI, and about 7 percent of them accounted for the lion’s share of AI implementations evaluated. The most common implementations were in law enforcement, health care, and financial regulations. Examples include the Border Patrol’s use of face recognition for itsBiometric Entry/Exitprogram and the Securities and Exchange Commission’sCorporate Issuer Risk Assessment, which helps regulators detect faults in companies’ financial reports.\n\nYes, but:The authors relied primarily on publicly available information, which may not contain sufficient technical perspective for such analysis. In addition, the survey period ended in August 2019, so the report excludes systems deployed since then.Why it matters:AI could help government agencies operate more effectively and efficiently, but this report shows that they have a long way to go to fulfill that vision.We’re thinking:Governments have an obligation to audit AI systems for performance, fairness, and compliance before rolling them out. Yet most agencies (and, for that matter, most corporations) don’t have the capability to assess these factors. We need tools that that enable a variety of stakeholders to define clear standards and assess  whether they’ve been met, so we can spot problems, mitigate risks, and build trust in automated systems.  We hope that companies such asCredo AI(which is backed by Andrew Ng’s AI Fund) can help.\n\nDid a million people attend theMillion Man March? Estimates of the crowd size gathered at a given place and time can have significant political implications — and practical ones, too, as they can help public safety experts deploy resources for public health or crowd control. A new method improves on previous crowd-counting approaches with a novel way to compare predictions with hand-labeled training data.What’s new:DM-Counttrains neural networks to count crowd size usingoptimal transportin the cost function. Optimal transport is a measure of difference between two distributions. In this case, the first distribution is the network’s prediction of people’s locations in a training example, and the second is the ground-truth locations. The method was developed by Boyu Wang and colleagues at Stony Brook University.Key insight:Training datasets for crowd-counting models typically mark each person in an image with a single-pixel label. Training a network to match such labels is difficult, because tiny discrepancies in a label’s location count as errors. Previous approaches managed this problem by replacing the pixels with blobs, but choosing the right blob size is difficult given the wide range of sizes of people and parts of people in an image. Optimal transport gave the authors a way to compare the density of single-pixel predictions with that of single-pixel labels. Armed with this metric, they could measure the deformation necessary to match a matrix of predictions to the labels and apply a cost accordingly.How it works:DM-Count accepts a picture of a crowd and places pixels where it sees people. Ideally, it would place one per person with 100 percent certainty, but in practice it spreads that certainty over a few pixels. In training, it learns to match those values to the training data using a loss function that combines three terms:\n\nResults:The authors built a modified [VGG-19]https://arxiv.org/abs/1409.1556 as detailed in thispaperand used DM-Count to train it on datasets includingNWPU, which the authors considered the most challenging crowd-counting dataset. Their method achieved a mean absolute error of 88.4 compared to 106.3 forContext-Aware Crowd Counting, the previous state of the art.Yes, but: Context-Aware Crowd Counting achieved a marginally lower root mean squared error (386.5) than DM-Count’s (388.6).Why it matters:We often try to improve models by finding better ways to format training data such as replacing pixels with blobs. This work shows that finding new ways to evaluate a network’s predictions can be a good alternative.We’re thinking:Can this method be adapted to check whether people in a crowd are maintaining proper social distance?\n\nCourses 1 and 2 of theTensorFlow: Advanced Techniques Specializationare now available to learners on Coursera!Enroll now\n\nMany people worry that face recognition is intrusive, but wild animals seem to find it bearable.What’s new:Melanie Clapham at University of Victoria with teammates of theBearID Projectdeveloped a model that performsface recognition for brown bears.How it works: BearID recognizes individual bears with 84 percent accuracy. It comprises four components: bearface, bearchip, bearembed, and bearsvm.\n\nBehind the news:Face recognition systems have been built for a growing number of non-human species, includingchimpanzees,lemurs, andpandas.Why it matters:By providing a low-cost way to track individual animals, apps like BearID could help researchers and conservationists map habitats for protection and monitor the health of animal populations. Clapham has been experimenting with the model in the field, and the team hopes to pair it with camera traps, which would allow researchers to monitor large wild populations.We’re thinking:We’re so impressed, we can bearly contain our appaws!\n\nA new summarization model boils down AI research papers to a single sentence.What’s new:TLDRfrom Allen Institute for AI creates at-a-glance summaries of scientific research papers. It’s up and running atSemantic Scholar, a research database, where searches now return its pithy precis.How it works:The researchers trainedBART, a pretrained language model, using a multitask learning strategy. Because the dataset of summaries was small, the authors also trained the model to generate titles, a task for which far more data was available.\n\nResults:TLDR was able to summarize research articles that averaged 5,000 words long using around 20 words. Human experts ranked the output of TLDR, BART model trained only on SciTLDR, the author-generated summaries, and student-generated summaries of 51 papers chosen at random. TLDF outperformed BART trained on SciTLDR, achieving a mean reciprocal rank, where 1 is highest, of 0.54 versus 0.42. Its output ranked on par with the author generated summaries (0.53) but worse than the student generated summaries (0.60).Behind the news:Most summarizers produce summaries that average between 150 and 200 words.Why it matters:At least 3 million scientific papers are published annually, Semantic Scholarestimates, and a growing portion of them describe innovations in AI, according to theAI Indexfrom Stanford Human-Centered Artificial Intelligence. This model, along with the excellentArxiv Sanity Preserver, promises a measure of relief to weary engineers and students. (To learn more about the Allen Institute for AI’s research, watch our Heroes of NLP interview with AI2 CEO Oren Etzionihere.)We’re thinking:Some papers can be summed up in a couple of dozen words, but many are so complex that no single sentence can do them justice. We look forward ton-sentence summarizers.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-11-2520at2010.36.4120AM.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2039.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2029.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Gif202-1080202.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/BearsGif2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/SEMANTIC.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-15/",
    "title": "issue 15",
    "date": "",
    "reading_time": "",
    "content": "I’ll be spending Thanksgiving with Nova and watching her taste turkey for the first time. To those of you who celebrate Thanksgiving, I hope you spend time with loved ones, reflect on what you are thankful for, and discuss some very important topics around the dinner table:\n\nIn AI, all of us should be thankful to stand on the shoulders of those who came before. I’ll leave you with one thought: What can you do now so that, in the future, dozens or more will feel thankful toward you? Let’s work together to help each other, and thereby move the world forward.\n\nKeep learning!\n\nAndrew\n\nSony, the consumer-electronics powerhouse behind the PlayStation and other hit gadgets, is launching three research-and-design centers to focus on AI. Staffing up means competing with — and likely poaching talent from — frontrunners like Google, Facebook, and Microsoft.What’s new:The company next month will open AI offices in Tokyo, Austin, and a European city to be named. The company says it will hire local machine learning engineers. It hasn’t said how many it will employ.The plan:Hiroaki Kitano, president of Sony’s Computer Science Laboratories, will lead the effort. His vision encompasses three areas: Gaming, sensing and hardware, and — surprise! — gastronomy. Sony provided few details, but other news offers clues:\n\nBehind the news:Sony’s Computer Science Laboratories is known for its independence, secrecy, and freedom to pursue blue-sky projects. The division’s most notable product is Aibo, the AI-powered robot dog. It also did pioneering research inaugmented realityand developedvideo conferencing protocols.Why it matters:Sony invested in AI in the 1990s and early 2000s, but it sat out the deep learning revolution. With AI centers in the U.S. and Europe, the Japanese company likely will focus on consumer products and experiences while competing for talent with companies that dove into deep learning head-first.We’re thinking:Kitano has passion and clout, but he also has an awful lot on his plate. Outside of Sony, he’s the founding president of theRoboCup Federation, an international group of computer scientists aiming to win the 2050 World Cup with a team of robot soccer players. Meanwhile, he runs the nonprofitSystems Biology Instituteand holds aprofessorshipat the Okinawa Institute of Research and Technology.\n\nAs black-box algorithms like neural networks find their way into high-stakes fields such as transportation, healthcare, and finance, researchers have developed techniques to help explain models’ decisions. New findings show that some of these methods can be fooled.What’s new:Two popular approaches to interpreting black-box algorithms includeLocal Interpretable Model-Agnostic Explanations(LIME) andShapley Additive Explanations(SHAP). Researchers at Harvard and the University of California Irvine found a way to trick these methods to make a deliberately biased model appear to be unbiased.Key Insight:LIME and SHAP expose bias by showing how a model might react to small perturbations in its input. Statistical analysis shows that the distribution of perturbed inputs is distinct from that of the original inputs, as shown in the image above. A malicious actor can take advantage of the difference.How it works:LIME and SHAP use a linear model, which is highly explainable, to mimic a black-box model’s decision with respect to any given input sample. The explainable model is built by feeding perturbed inputs to the black-box model and measuring how its output changes in response to the perturbations. Once the model is built, we can rank the features most important to the decision, highlighting any bias in the original model. Dylan Slack, Sophie Hilgard, and their colleagues introduce a way to hide such bias from LIME and SHAP by learning to recognize data they’ve altered.\n\nResults:The authors created malicious systems to run COMPAS (a criminal recidivism dataset), Communities And Crime (a dataset of violent crime and demographic information), and the German Credit loan-application dataset. The embedded biased models deliberately abused discriminatory features such as race and gender, while the unbiased models used only features which were uncorrelated with discriminatory features. A malicious system biased on one feature (say, race) fooled LIME every time and SHAP in 85 percent of cases. A malicious system biased on two features fooled LIME over 90 percent of the time and SHAP 67 percent of the time.Why it matters:The authors’ approach highlights LIME’s and SHAP’s reliance on generating novel data. If these methods were to generate data more similar to the training data’s distribution, the method would fail. This may be a promising avenue for explainability research. Meanwhile, Duke University computer scientist Cynthia Rudinproposesavoiding black-box models in high-stakes situations. The AI community needs to hold a vigorous discussion about when such models are and aren’t appropriate.We’re thinking:If a major AI provider were caught using this technique, likely it would be vilified, which should provide some disincentive. We can imagine changes to LIME and SHAP that would counter a specific implementation, but this paper provides a dose of caution that checking for bias is not easy.\n\nWhat are the basic building blocks of a convolutional neural network? Learn how to implement convolutions and pooling in Course 4 of theDeep Learning Specialization.\n\nIntel, which dominates the market for general-purpose processors, is shipping its long-awaited AI chips.What happened:The chip giantannouncedthat two so-called neural network processors are available to data-center customers.How they work:One of the new chips is intended for training deep learning models, the other for inferencing. They’redesignedto balance computational horsepower, communications speed, and memory capacity.\n\nBehind the news:While Intel chips process most AI inferencing in data centers, Nvidia leads in GPUs that speed up AI training. In 2016, Intel acquired Nervana, a startup devoted to next-generation AI chips. Meanwhile, however, the field has become crowded. Specialized designs have proliferated at a host of startups like Cerebras and tech giants like Google, while Qualcomm has been building inferencing capability into chips for low-powered devices like smartphones.Why it matters:There’s no such thing as too much processing power for machine learning. The faster we can train models, the more data we can absorb, and the faster we can innovate new network architectures and applications. And the faster users can run our models, the more value we can deliver. As for chip makers, they recognize that AI is the future: Neural networks’ voracious appetite for processing power likely will drive silicon sales for years.We’re thinking:Large cloud providers are consolidating computation, and that’s having a big impact on the chip business. Their concentrated buying power puts them in a strong position to demand lower prices. The cloud companies also want to make sure they have alternative providers of deep learning chips, so they’ll buy chips from several vendors rather than only the top one. All this is playing out against a backdrop of rapid growth of AI workloads. Expect intense competition and in the years ahead.\n\nLarge NLP models like BERT can answer questions about a document thanks to the transformer network, a sequence-processing architecture that retains information across much longer sequences than previous methods. But transformers have had little success in reinforcement learning — until now.What’s new:Research in reinforcement learning (RL) has focused primarily on immediate tasks such as moving a single object. Transformers could support tasks that require longer-term memory. However, past research struggled to train transformer-based RL models. Emilio Parisotto and a DeepMind team combined them successfully with Gated Transformer-XL, orGTrXL. This network can substitute directly for an LSTM in RL applications.Key insight:A transformer’s attention component models out-of-sequence relationships. Consider a block-stacking task where the first  and sixth actions taken are the most important to predicting whether the stack will be in the right order. GTrXL modifies the transformer architecture to allow it to learn sequential relationships early on (say, between the first and second actions, where the first action places the initial block and the second identifies which block needs to be picked up next) before it has learned out-of-sequence relationships.How it works:GTrXL modifies the transformer network (TrXL) as shown in the diagram above.\n\nResults:On DMLab 30, an RL environment that supports puzzle tasks requiring long-term memory, GTrXL outperformed the previous state of the art (MERLIN) averaged across all 30 tasks. It also outperformed an LSTM, the ubiquitous recurrent layer in RL research.Why it matters:LSTMs have been essential to sequence-processing neural networks that work on short-term data. GTrXL give such networks longer-term memory. Longer time horizons eventually may help boost performance in life-long learning and meta-learning.We’re thinking:Since the originalpaperdescribing transformer networks was published in 2017, researchers have developed extensions. This work continues to show that, when it comes to transformers, there’s more than meets the eye.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2021/06/thanksgiving201-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Sony.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Bias20SIZED201.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_DLS20Course20420Course20Ad20Fixed20Size.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Chips20SIZED.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-11-2720at2011.00.3120AM.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-156/",
    "title": "issue 156",
    "date": "",
    "reading_time": "",
    "content": "While working on Course 3 of theMachine Learning Specialization, which covers reinforcement learning, I was reflecting on how reinforcement learning algorithms are still quite finicky. They’re very sensitive to hyperparameter choices, and someone experienced at hyperparameter tuning might get 10x or 100x better performance. Supervised deep learning was equally finicky a decade ago, but it has gradually become more robust with research progress on systematic ways to build supervised models.Will reinforcement learning (RL) algorithms also become more robust in the next decade? I hope so. However, RL faces a unique obstacle in the difficulty of establishing real-world (non-simulation) benchmarks.When supervised deep learning was at an earlier stage of development, experienced hyperparameter tuners could get much better results than less-experienced ones. We had to pick the neural network architecture, regularization method, learning rate, schedule for decreasing the learning rate, mini-batch size, momentum, random weight initialization method, and so on. Picking well made a huge difference in the algorithm’s convergence speed and final performance.Thanks to research progress over the past decade, we now have more robust optimization algorithms like Adam, better neural network architectures, and more systematic guidance for default choices of many other hyperparameters, making it easier to get good results. I suspect that scaling up neural networks — these days, I don’t hesitate to train a 20 million-plus parameter network (like ResNet-50) even if I have only 100 training examples — has also made them more robust. In contrast, if you’re training a 1,000-parameter network on 100 examples, every parameter matters much more, so tuning needs to be done much more carefully.\n\nMy collaborators and I have applied RL tocars,helicopters,quadrupeds, robot snakes, and many other applications. Yet today’s RL algorithms still feel finicky. Whereas poorly tuned hyperparameters in supervised deep learning might mean that your algorithm trains 3x or 10x more slowly (which is bad), in reinforcement learning, it feels like they might result in training 100x more slowly — if it converges at all! Similar to supervised learning a decade ago, numerous techniques have been developed to help RL algorithms converge (like double Q learning, soft updates, experience replay, and epsilon-greedy exploration with slowly decreasing epsilon). They’re all clever, and I commend the researchers who developed them, but many of these techniques create additional hyperparameters that seem to me very hard to tune.Further research in RL may follow the path of supervised deep learning and give us more robust algorithms and systematic guidance for how to make these choices. One thing worries me, though. In supervised learning, benchmark datasets enable the global community of researchers to tune algorithms against the same dataset and build on each other’s work. In RL, the more-commonly used benchmarks are simulated environments likeOpenAI Gym. But getting an RL algorithm to work on a simulated robot is much easier than getting it to work on a physical robot.Many algorithms that work brilliantly in simulation struggle with physical robots. Even two copies of the same robot design will be different. Further, it’s infeasible to give every aspiring RL researcher their own copy of every robot. While researchers are making rapid progress on RL for simulated robots (and for playing video games), the bridge to application in non-simulated environments is often missing. Many excellent research labs are working on physical robots. But because each robot is unique, one lab’s results can be difficult for other labs to replicate, and this impedes the rate of progress.I don’t have a solution to these knotty issues. But I hope that all of us in AI collectively will manage to make these algorithms more robust and more widely useful.Keep learning!\n\nAndrew\n\nAndrew answers questions about the newMachine Learning Specializationincluding:\n\nLearn more\n\nAutonomous aircraft in the United Kingdom are getting their own superhighway.What’s new:The UK governmentapprovedProject Skyway, a 165-mile system of interconnected drone-only flight routes. The airspace is scheduled to open by 2024.How it works:The routes, each just over six miles wide, will connect six medium-sized English cities including Cambridge, Coventry, Oxford, and Rugby. They avoid forested or ecologically sensitive areas, as well as major cities like London and Birmingham.\n\nBehind the news:Project Skyway is the largest proposed designated drone flight zone, but it’s not the only one.\n\nYes, but:Although Skyway includes a collision-avoidance system, it’s not designed to prevent accidents during takeoff and landing, when they’re most common. Moreover, it's not yet clear whether the plan includes designated takeoff and landing sites. “The problem is what happens when you're 10 feet away from people,” one aerospace engineertoldthe BBC.Why it matters:Drones are restricted from flying in most places due to worries that they could interfere — or collide — with other aircraft. By giving them their own airspace, the UK is allowing drones to deliver on their potential without putting other aircraft at risk.We’re thinking:Figuring out how to operate drones safely has proven one of the most difficult aspects of deploying them in commercial applications. This project is a big step toward ironing out the regulatory bugs and also provides a relatively safe space to address technical issues.\n\nA worldwide collaboration produced the biggest open source language model to date.What’s new:BLOOMis a family of language models built by the BigScience Research Workshop, a collective of over1,000 researchersfrom 250 institutions around the globe.How it works:BLOOM is a transformer model that emulates OpenAI’sGPT-3. It was trained on a custom 1.6 billion terabyte dataset to generate output in any of 46 human languages and 13 programming languages.\n\nBehind the news:BigScience began in May 2021 as a year-long series of workshops aimed at developing open source AI models that are more transparent, auditable, and representative of people from diverse backgrounds than their commercial counterparts. Prior to BLOOM, the collaboration released theT0family of language models, which were English-only and topped out at 11 billion parameters.Why it matters:Developing large language models tends to be the province of large companies because they can afford to amass gargantuan datasets and expend immense amounts of processing power. This makes it difficult for independent researchers to evaluate the models’ performance, including biased or harmful outputs. Groups like BigScience and EleutherAI, whichreleasedits own open source large language model earlier this year, show that researchers can band together as a counterweight to Big AI.We’re thinking:Just over two years since GPT-3’s debut, we have open access to large language models from Google, Meta, OpenAI, and now BigScience. The rapid progress toward access is bound to stimulate valuable research and commercial projects.\n\nCourse 3 of theMachine Learning Specialization, “Unsupervised Learning, Recommender Systems, and Reinforcement Learning,” is available! Learn unsupervised techniques for anomaly detection, clustering, and dimensionality reduction. Build a recommender system, too!Enroll now\n\nA machine learning method could help chemists formulate pesticides that target harmful insects but leave bees alone.What’s new:Researchers at Oregon State Universitydevelopedmodels that classify whether or not a chemical is fatally toxic to bees. The authors believe their approach could be used to screen pesticide formulations for potential harm to these crucial pollinators.How it works:The authors trained two support vector machines to classify molecules as lethal or nonlethal. The dataset was 382 graphs ofpesticide molecules, in which each atom is a node and each bond between atoms is an edge, labeled for toxicity. The researchers used a different method to train each model.\n\nResults:The two models performed similarly. They accurately classified 81 to 82 percent of molecules as lethal or nonlethal to bees. Of the molecules classified as lethal, 67 to 68 percent were truly lethal.Behind the news:Bees play a crucial role inpollinatingmany agricultural products. Without them, yields of important crops like cotton, avocados, and most fruit would drop precipitously.Numerous studieshave shown that pesticides are harmful to bees. Pesticides have contributed to increased mortality amongdomesticated honey beesas well as a decline in the number ofwild bee species.Why it matters:Pesticides, herbicides, and fungicides have their dangers, but they help enable farms to produce sufficient food to feed a growing global population. Machine learning may help chemists engineer pesticides that are benign to all creatures except their intended targets.We’re thinking:It’s good to see machine learning take some of the sting out of using pesticides.\n\nClassification isn’t always cut and dried. While the majority of doctors are men and nurses women, that doesn't mean all men who wear scrubs are doctors or all women who wear scrubs are nurses. A new method attempts to account for biases that may be held by certain subsets of labelers.What's new:Mitchell L. Gordon and colleagues at Stanford introduced a method to control bias in machine learning model outputs. Theirjury learningapproach models a user-selected subset of the annotators who labeled the training data.Key insight:A typical classifier mimics how an average labeler would annotate a given example. Such output inevitably reflects biases typically associated with an annotator’s age, gender, religion, and so on, and if the distribution of such demographic characteristics among labelers is skewed, the model’s output will be skewed as well. How to correct for such biases? Instead of predicting the average label, a classifier can predict the label likely to be applied by each individual in a pool of labelers whose demographic characteristics are known. Users can choose labelers who have the characteristics they desire, and the model can emulate them and assign a label accordingly. This would enable users to correct for biases (or select for them).How it works:The authors used jury learning to train a classifier to mimic the ways different annotators label the toxicity of social media comments. Thedatasetcomprised comments from Twitter, Reddit, and 4Chan.\n\nResults:The authors evaluated their model’s ability to predict labels assigned by individual annotators. It achieved 0.61 mean average error, while a BERTweet fine-tuned on the dataset achieved 0.9 mean average error (lower is better). The authors’ model achieved fairly consistent error rates when estimating how annotators of different races would label examples: Asian (0.62), Black (0.65), Hispanic (0.57), White (0.60). In contrast, BERTweet’s error rate varied widely with respect to Black annotators: Asian (0.83), Black (1.12), Hispanic (0.87), White (0.87). The authors’ model, which focused on estimating labels assigned by individuals, also outperformed a similar model that was trained to predict decisions by demographic groups, which scored 0.81 mean average error.Why it matters:Users of AI systems may assume that data labels are objectively true. In fact, they’re often messy approximations, and they can be influenced by the circumstances and experiences of individual annotators. The jury method gives users a way to account for this inherent subjectivity.We're thinking:Selecting a good demographic mix of labelers can reduce some biases and ensure that diverse viewpoints are represented in the resulting labels — but it doesn’t reduce biases that are pervasive across demographic groups. That problem requires a different approach.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/08/ezgif.com-gif-maker--45--0.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/08/MLS-updated-1024x578.jpeg",
      "https://dl-staging-website.ghost.io/content/images/2022/08/DRONEWAY.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/08/OPENSOURCE.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/08/DeepLearningAI_Banner_Stanford_Launch_1200x628-V2_Artboard-2-copy.png",
      "https://dl-staging-website.ghost.io/content/images/2022/08/BEES.png",
      "https://dl-staging-website.ghost.io/content/images/2022/08/JURY.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-250/",
    "title": "issue 250",
    "date": "",
    "reading_time": "",
    "content": "A good way to get started in AI is to start with coursework, which gives a systematic way to gain knowledge, and then to work on projects. For many who hear this advice, “projects” may evoke a significant undertaking that delivers value to users. But I encourage you to set a lower bar and relish small, weekend tinkering projects that let you learn, even if they don’t result in a meaningful deliverable.\n\nRecently, my son and daughter (ages 3 and 5) were building Lego vehicles. They built a beautiful ice-cream truck as well as a . . . umm . . . colorful and asymmetric dinosaur car, shown in the picture below. While most observers would judge the ice-cream truck as the superior creation, my kids built it by following Lego’sinstructions, and it is likely identical to thousands of ice-cream trucks built by others. In contrast, building the dinosaur car required creativity and novel thinking. The exercise helped them hone their ability to pick and assemble Lego building blocks.\n\nThere is, of course, room for both mimicking others’ designs (with permission) and coming up with your own. As a parent, I try to celebrate both. (To be honest, I celebrated the dinosaur car more.) When learning to build Lego, it’s helpful to start by following a template. But eventually, building your own unique projects enriches your skills.\n\nAs a developer, too, I try to celebrate unique creations. Yes, it is nice to have beautiful software, and the impact of the output does matter. But good software is often written by people who spend many hours tinkering and building things. By building unique projects, you master key software building blocks. Then, using those blocks, you can go on to build bigger projects.\n\nI routinely tinker with building AI applications, and a lot of my tinkering doesn’t result in anything useful. My latest example: I built a Streamlit app that would authenticate to Google docs, read the text in a doc, use a large language model to edit my text, and write the result back into the doc. I didn’t find it useful in the end because of friction in the user interface, and I’m sure a commercial provider will soon, if they haven’t already, build a better product than I was able to throw together in a couple of hours on a weekend. But such tinkering helps me hone my intuition and master software components (I now know how to programmatically interface with Google docs) that might be useful in future projects.\n\nIf you have an idea for a project, I encourage you to build it! Often, working on a project will also help you decide what additional skills to learn, perhaps through coursework. To sustain momentum, it helps to find friends with whom to talk about ideas and celebrate projects — large or small.\n\nKeep tinkering!Andrew\n\nP.S. On the heels of Microsoft’s announcement of the Copilot+ PC, which uses on-device AI optimized for a Qualcomm chip, we have a short course on deploying on-device AI created with Qualcomm! In “Introduction to On-Device AI,” taught by Qualcomm’s Senior Director of Engineering Krishna Sridhar, you’ll deploy a real-time image segmentation model on-device and learn key steps for on-device deployment: neural network graph capture, on-device compilation, hardware acceleration, and validating on-device numerical correctness.Please sign up here!\n\nOpenAI’s latest model raises the bar for models that can work with common media types in any combination.What’s new:OpenAIintroducedGPT-4o, a model that accepts and generates text, images, audio, and video — the “o” is for omni — more quickly, inexpensively, and in some cases more accurately than its predecessors. Text and image input and text-only output are available currently via ChatGPT and API, with image output coming soon. Speech input and output will roll out to paying users in coming weeks. General audio and video will be available first to partners before rolling out more broadly.\n\nHow it works:GPT-4o is a single model trained on multiple media types, which enables it to process different media types and relationships between them faster and more accurately than earlier GPT-4 versions that use separate models to process different media types. The context length is 128,000 tokens, equal to GPT-4 Turbo but well below the 2-million limit newly set by Google Gemini 1.5 Pro.\n\nGPT-4o significantly outperforms Gemini Pro 1.5 at several benchmarks for understanding text, code, and images includingMMLU,HumanEval,MMMU, andDocVQA. It outperformed OpenAI’s ownWhisper-large-v3speech recognition model at speech-to-text conversion andCoVoST 2language translation.\n\nAftershocks:As OpenAI launched the new model,troublesresurfaced that had led to November’s rapid-fire ouster and reinstatement of CEO Sam Altman. Co-founder and chief scientist Ilya Sutskever, who co-led a team that focused on mitigating long-term risks, resigned. He did not give a reason for his departure; previously he hadarguedthat Altman didn’t prioritize safety sufficiently. The team’s other co-leader Jan Leike followed,allegingthat the company had a weak commitment to safety. The company promptlydissolvedthe team altogether and redistributed its responsibilities. Potential legal issues also flared when actress Scarlett Johansson, who had declined an invitation to supply her voice for a new OpenAI model, issued astatementsaying that one of GPT-4o’s voices sounded “eerily” like her own and demanding to know how the artificial voice was built. OpenAI denied that it had used or tried to imitate Johansson’s voice and withdrew that voice option.\n\nWhy it matters:Competition between the major AI companies is putting more powerful models in the hands of developers and users at a dizzying pace. GPT-4o shows the value of end-to-end modeling for multimodal inputs and outputs, leading to significant steps forward in performance, speed, and cost. Faster, cheaper processing of tokens makes the model more responsive and lowers the barrier for powerful agentic workflows, while tighter integration between processing of text, images, and audio makes multimodal applications more practical.\n\nWe’re thinking:Between GPT-4o, Google’s Gemini 1.5, and Meta’s newly announcedChameleon, the latest models are media omnivores. We’re excited to see what creative applications developers build as the set of tasks such models can perform continues to expand!\n\nGoogle’s annual I/O developers’ conference brought a plethora of updates and new models.What’s new:Googleannouncedimprovements to its Gemini 1.5 Pro large multimodal model — notably increasing its already huge input context window — as well as new open models, a video generator, and a further step in digital assistants. In addition, Gemini models will power new features inGoogle Search, Gmail, and Android.\n\nHow it works:Google launched a variety of new capabilities.\n\nPrecautionary measures:Amid the flurry of new developments, Googlepublishedprotocols for evaluating safety risks. The “Frontier Safety Framework” establishes risk thresholds such as a model’s ability to extend its own capabilities, enable a non-expert to develop a potent biothreat, or automate a cyberattack. While models are in development, researchers will evaluate them continually to determine whether they are approaching any of these thresholds. If so, developers will make a plan to mitigate the risk. Google aims to implement the framework by early 2025.\n\nWhy it matters:Gemini 1.5 Pro’s expanded context window enables developers to apply generative AI to multimedia files and archives that are beyond the capacity of other models currently available — corporate archives, legal testimony, feature films, shelves of books — and supports prompting strategies such asmany-shot learning. Beyond that, the new releases address a variety of developer needs and preferences: Gemini 1.5 Flash offers a lightweight alternative where speed or cost is at a premium, Veo appears to be a worthy competitor for OpenAI’s Sora, and the new open models give developers powerful options.\n\nWe’re thinking:Google’s quick iteration on its Gemini models is impressive. Gemini 1.0 wasannouncedless than six months ago. White-hot competition among AI companies is giving developers more choices, faster speeds, and lower prices.\n\nIn our new short course “Introduction to On-Device AI,” made in collaboration with Qualcomm, you’ll learn to deploy AI models on edge devices using local compute for faster inference and privacy. Join the next wave of AI as models go beyond the cloud!Enroll for free\n\nThe world’s second-largest music publisher accused AI developers of potential copyright violations.What’s new:Sony Music Groupdeclaredthat AI developers had trained models on Sony’s intellectual property without permission and that any method of collecting media or other data owned by the company violated its copyrights. Whether AI developers actually have violated copyrights has not been established.\n\nHow it works:In astatementposted on the company’s website andlettersto developers, Sony forbade the use of its music or other media such as lyrics, music videos, album art for “training, developing, or commercializing any AI systems.”\n\nBehind the news:In April, more than 200 music artistscalledfor streaming services and AI developers to stop using their work for training and stop generating music in the styles of specific musicians without compensation. Universal Music Group (UMG), which is Sony Music’s top competitor, has also opposed unrestricted AI-generated music.\n\nLast year, UMGorderedApple Music and Spotify to block AI developers from downloading its recordings and issued takedown notices to YouTube and Spotify uploaders who generated music that sounds like artists who are under contract to Universal.\n\nWhy it matters:Sony Music Group’s warning comes as generated audio isapproachinga level of quality that might attract a mainstream audience, and it could chill further progress. Although it is not yet clear whether training AI systems on music recordings without permission violates copyrights, Sony Music Group hasdemonstratedits willingness to pursue both individuals and companies for alleged copyright violations. The company accounted for 22 percent of the global music market in 2023. (UMG accounted for 32 percent.) Its catalog includes many of the world’s most popular artists including AC/DC, Adele, Celine Dion, and Harry Styles.\n\nWe’re thinking:We believe that AI developers should be allowed to let their software learn from data that’s freely available on the internet, but uncertainty over the limits of copyright protection isn’t good for anyone. It’s high time toupdateto intellectual property laws for the era of generative AI.\n\nThe latest text-to-image generators can alter images in response to a text prompt, but their outputs often don’t accurately reflect the text. They do better if, in addition to a prompt, they’re told the general type of alteration they’re expected to make.What’s new:Developed by Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar and colleagues at Meta,Emu Editenriches prompts with task classifications that help the model interpret instructions for altering images. You can see exampleshere.Key insight:Typical training datasets for image-editing models tend to present, for each example, an initial image, an instruction for altering it, and a target image. To train a model to interpret instructions in light of the type of task it describes, the authors further labeled examples with a task. These labels included categories for regional alterations such as adding or removing an object or changing the background, global alterations such as changing an image’s style, and computer-vision tasks such as detecting or segmenting objects.How it works:Emu Edit comprises a pretrainedEmulatent diffusion image generator and pretrained/fine-tuned Flan-T5 large language model. The system generates a novel image given an image, text instruction, and one of 16 task designations. The authors generated the training set through a series of steps and fine-tuned the models on it.\n\nResults:Judges compared altered images produced by the authors’ method,InstructPix2Pix, andMagicBrushusing the MagicBrush test set. Evaluating how well the generated images aligned with the instruction, 71.8 percent of the time, the judges preferred Emu Edit over InstructPix2Pix, and 59.5 percent of the time, they preferred Emu Edit over MagicBrush. Evaluating how well the generated images preserve elements from the input images, 71.6 percent preferred Emu Edit over InstructPix2Pix, and 60.4 percent preferred Emu Edit over MagicBrush.Why it matters:Richer data improves machine learning results. Specifying tasks and generating images that reflect them improved Emu Edit’s data compared to other works, enabling it to achieve better results.We’re thinking:Text-to-image generators are amazing and fun to use, but their output can be frustratingly unpredictable. It’s great to see innovations that make them more controllable.\n\nJoin FourthBrain's two live workshops next week! In these interactive sessions, you’ll build useful applications with large language models and walk away with practical skills. Enroll as an individual or register as a team for a group discount.Learn more",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-22T145628.272-1.png",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-22T145727.291.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-22T145823.262.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/05/V3_DeepLearning_Qualcomm_C1_Banner_2070x1080--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-22T150014.575.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/05/unnamed---2024-05-22T150103.924.gif",
      "https://dl-staging-website.ghost.io/content/images/2024/05/FourthBrain-Batch-Ad-05222024.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-83/",
    "title": "issue 83",
    "date": "",
    "reading_time": "",
    "content": "Over the past weekend, I happened to walk by a homeless encampment and went over to speak with some of the individuals there.\n\nI spoke with a homeless man who seemed to be partially speaking with me, and partially speaking with other people that I could not see. I also spoke with a woman who said she fled her abusive home at the age of 21, and wished that she had a tent — like some of the others — so she could sleep with something over her head rather than be exposed to the elements at night.\n\nI feel grateful and privileged every day to have enough food, to have a place to live, and to even have a modern computer with internet access.\n\nI’m going to come out and say this (knowing some people will disagree): Every one of us has an obligation to serve others.\n\nWhile we can try to help a handful of people at a time with a meal or a donation — and this is to be celebrated — I don’t know how to systematically help the large and growing number of homeless. But I will keep thinking on this, and am determined to find a way. Even as we build amazing products and technologies, let’s keep thinking about how we can scalably serve the many wonderful, resilient individuals like the ones I met last weekend.\n\nKeep learning!\n\nAndrew\n\nFacebook’s management obstructed the architect of its recommendation algorithms from mitigating their negative social impact,MIT Technology Reviewreported.What’s new:The social network focused on reining in algorithmic bias against particular groups of users at the expense of efforts to reduce disinformation and hate speech, according to an in-depth profile of Joaquin Quiñonero Candela, who designed Facebook’s early recommenders and now leads its Responsible AI team.The story:The article traces Quiñonero’s effort to balance the team’s mission to build trustworthy technology with management’s overriding priorities: boosting user engagement and avoiding accusations that it favored one political faction over another.\n\nThe response:Facebook denied that it interfered with moves to reduce disinformation and hate speech. It also denied that politics motivated its focus on mitigating bias.\n\nWhy it matters:Facebook, like many AI companies, is struggling to balance business priorities with its social impact. Teams like Responsible AI are crucial to achieving that balance, and business leaders need to give them authority to set technical priorities and limits.We’re thinking:The powers of AI can put machine learning engineers in the difficult position of mediating between business priorities and ethical imperatives. We urge business leaders to empower employees who try to do the responsible thing rather than throttling their work, even if it negatively impacts the bottom line.\n\nMany medical drugs work by modulating the body’s production of specific proteins. Recent research aimed to predict this activity, enabling researchers to identify drugs that might counteract the effects of Covid-19.What’s new:Thai-Hoang Pham and colleagues at The Ohio State University and The City University of New York developedDeepCE, a system designed to predict how particular drugs will influence the amounts of RNA, and therefore the amounts of various proteins, produced by a cell.Key insight:In machine learning, attention layers learn to represent how the various parts of two input sequences interact with one another. In biology, genes mediate the production of RNA, while drugs can affect the action of genes. Given separate embeddings that represent genes and chemical structures of drugs, attention can capture how a drug affects RNA production.How it works:Given a drug, a dose, and a line of cells cloned from a particular patient, DeepCE predicts the amount of RNA produced by each of roughly 1,000 genes. (Collectively, this information constitutes a gene expression profile). The training and test data included more than 600 drugs for a total of over 4,000 gene expression profiles from seven human cell lines in theL1000database.\n\nResults:The authors compared DeepCE’s predictions with those of several baseline methods using the Pearson correlation coefficient, a measure of the correlation between predictions and ground truth. DeepCE outperformed all of them with a score of 0.4907. The next-best method, a two-layer feed-forward network, scored 0.4270. They also used DeepCE to look for existing drugs that might treat Covid-19. They compared the predictions formore than 11,000 drugswith corresponding profiles ofCovid-19patients, looking for the greatest negative correlations — an indicator that the drug would fight the illness. Of 25 drugs surfaced by DeepCE, at least five already had shown potential as Covid-19 treatments; others had been used for different viruses with similar symptoms.Why it matters:Complex datasets may have features that aren’t processed easily by a single network. By using a different network for each type of input and combining their outputs, machine learning engineers can extract useful information that otherwise might be inaccessible.\n\nWe’re thinking:The next blockbuster antiviral (or antidepressant, anti-inflammatory, or heart medicine) may already be on pharmacy shelves. Wouldn’t it be wonderful if deep learning found it?\n\nAuditing is a critical technique in the effort to build fair and equitable AI systems. But current auditing methods may not be up to the task.What’s new:There’s no consensus on how AI should be audited, whether audits should be mandatory, and what to do with their results, according toThe Markup, a nonprofit investigative reporting outfit.What’s Happening:Auditing firms are doing brisk business analyzing AI systems to determine whether they’re effective and fair. But such audits are often limited in scope, and they may lend legitimacy to models that haven’t been thoroughly vetted.\n\nBehind the news:In the U.S., members ofCongressand theNew York City Councilhave proposed bills that would require companies to audit AI systems. The laws have yet to be passed.Why it matters:AI systems increasingly affect the lives of ordinary people, influencing whether they land a job, get a loan, or go to prison. These systems must be trustworthy — which means the audits that assess them must be trustworthy, too.We’re thinking:Makers of drugs and medical devices must prove their products are effective and safe. Why not makers of AI, when its output can dramatically impact people’s lives? The industry should agree on standards and consider making audits mandatory for systems that affect criminal justice, allocating health care resources, and offering loans.\n\nYou’re invited toMLOps: From Model-Centric to Data-Centric AI,a special presentation by Dr. Andrew Ng on Wednesday, March 24, 2021. Learn about the fundamental skills required for the next generation of machine learning practitioners.\n\nVoice clones — the audio counterpart to deepfaked images — are poised to invade popular media and entertainment.What’s new:Professionals and amateurs alike are using AI to emulate the voices of human actors,Wiredreported.Cloned like a pro:Game developers and marketers are cloning voices to save money and make their products more immersive.\n\nRemixers join in:Much of the entertainment industry issorting outwho owns which rights to an actor’s voice, but some amateur content creators have embraced the technology with abandon.\n\nWhy it matters:Voice cloning opens new avenues of creativity and productivity. For instance, generated voices can help developers road-test dialogue before bringing in the human talent and expand the conversational role of background characters. Yet the technology also holds potential forabuse, and guarding against them will require new kinds of vigilance.We’re thinking:Have you ever been yelled at? We would love to build a system to transcribe the yeller’s words, and then re-synthesize their voice in a more polite tone.\n\nIn training an image recognition model, it’s not uncommon to augment the data by cropping original images randomly. But if an image contains several objects, a cropped version may no longer match its label. Researchers developed a way to make sure random crops are labeled properly.What’s new:Led by Sangdoo Yun, a team at Naver AI Lab developedReLabel, a technique that labels any random crop of any image. They showcased their method onImageNetKey insight:Earlier workusedknowledge distillation: Given a randomly cropped image, a so-called student model learned from labels predicted by a teacher model. That approach requires that the teacher predict a label for each of many cropped versions of a given example. In this work, an image was divided into a grid, and the teacher predicted a label for each grid square, creating a map of regions and their labels that was used to determine a label for any given portion of the image. This way, the teacher could examine each example only once, making the process much more efficient.How it works:The teacher was anEfficientNet-L2that had been pretrained on Google’sJFT-300Mdataset of 300 million images. The student was aResNet-50.\n\nResults:The researchers compared a ResNet-50 trained on ImageNet using their labels to one trained using the standard labels. The new labels improved test classification accuracy from 77.5 percent to 78.9 percent.Why it matters:Images on social and photo-sharing sites tend to be labeled with tags, but a tag that reads, say, “ox” indicates only that an ox appears somewhere in the image. This approach could enable vision models to take better advantage of data sources like this.We’re thinking:A bounding box around every object of interest would ameliorate the cropping problem — but such labels aren’t always easy to get.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/10/Social-Engagement-vs-Social-Good-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/10/Old-Drugs-for-New-Ailments-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/10/Who-Audits-the-Auditors-1.gif",
      "https://info.deeplearning.ai/hubfs/MLOps-March-24_The-Batch-Image-2048x1152%20copy.png",
      "https://dl-staging-website.ghost.io/content/images/2022/10/Your-Words--Their-Voices-1.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/10/Good-Labels-for-Cropped-Images-1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-55/",
    "title": "issue 55",
    "date": "",
    "reading_time": "",
    "content": "Did you ever spend days obsessing over a technical problem? If so, I applaud you. Determined pursuit of solutions to hard problems is an important step toward building deep expertise.\n\nI’ve been privileged to have worked with several of today’s AI leaders when they were still students. Every one of them spent days, weeks, and months relentlessly trying out different approaches to a range of problems, coming up with hypotheses and performing experiments to hone their intuition. This gave them a thorough understanding of machine learning.\n\nIt takes many judgement calls to build an effective AI system. How do you tune a particular hyperparameter? What are the tradeoffs between model size, real-time throughput, and accuracy for an application? What type of data pre-processing will yield the best results? When facing complex questions, engineers with deep expertise will come up with better answers.\n\nLately I’ve been thinking about how to train neural networks on small amounts of data. I try to find quiet time to brainstorm, and sometimes I end up with many pages of handwritten notes. After I’ve obsessed over a problem during the day, before I fall asleep I remind my brain that I want to make progress on it. Then, if I’m lucky, I awaken in the morning with new ideas.\n\nThe world is complex and becoming more so. We need people, in AI and other disciplines, who will take the time and effort to build deep expertise. When a worthy problem taps you on the shoulder, I encourage you to give it your attention. Give yourself the time you need to explore a solutions, and keep at it. It’s not a weird thing to do. Even if you don’t succeed — as a student, I spent countless hours trying, and failing, to prove P ≠ NP, and I don’t regret a minute of it — the journey will make you better.\n\nKeep learning!\n\nAndrew\n\nAn aviation startups is using neural networks to put air freight on autopilot.What’s new:Xwing, a California startup, is test-flying an autonomous pilot system aboard cargo aircraft with an eye toward crewless commercial flights in 2022, theWall Street Journalreported.\n\nHow it works:A suite of models reads sensor data while the plane is in motion. When the models detect another plane or an obstacle, they funnel the information to a rules-based flight control system, which adjusts course, Xwing CEO Marc Piette toldThe Batch.\n\nBehind the news:Several companies are racing toward regulatory approval for autonomous freight transport, including Amazon, which this week gained permission todeliver packages using drones. The remaining issues are not technical. Commercial airliners routinely fly on autopilot, and last year a Cessna outfitted with an AI-powered autopilot fromReliable Roboticsperformed the first autonomous take-off, flight, and landing over an urban area. However, regulations and public concerns have kept human pilots in cockpits. Xwing and its proponents believe that restriction may lift before long, starting with approval for flights over water or uninhabited areas. The company’s reliance on existing aircraft may help expedite the process.\n\nWhy it matters:Small planes move cargobetween outlying areas and central hubs. Autonomous systems could make service faster, more frequent, and less costly.We’re thinking:Air, land, or sea: Where will fully autonomous vehicles first enjoy widespread deployment?\n\nThe technique known as dropout discourages neural networks from overfitting by deterring them from reliance on particular features. A new approach reorganizes the process to run efficiently on the chips that typically run neural network calculations.What’s new:Pascal Notin and colleagues at Oxford and Cohere.ai introduced an alternative,SliceOut, that boosts neural network speed with little or no compromise to accuracy.Key insight:Most operations in deep learning consist of multiplying a matrix of weights by a vector of activations or features. Deleting an input feature means a row of the weight matrix has no effect. Similarly, deleting an output feature means a column has no effect. But the resulting matrix forces the chip that’s processing the calculations to shuttle data in and out of memory, which takes time. By deleting — and keeping — only features that are contiguous in memory, the authors avoided time-consuming memory reallocations.How it works:In its simplest form, dropout zeroes out a random selection of parameter values or, equivalently, by zeroing out the corresponding weights.\n\nResults:The researchers evaluated SliceOut in an image-recognition task using CNNs trained onCIFAR-100, SliceOut matched dropout’s test accuracy but ran trained 33.3 percent faster and required 27.8 percent less memory. SliceOut achieved time savings of 8.4 percent and memory savings of 9 percent with transformer networks on theOne Billion Word Benchmarkand saved double-digit percentages in fully connected layers onMNIST.Why it matters:Larger networks often achieve better results in a variety of tasks, but they require regularization techniques to avoid overfitting. SliceOut could enable gargantuan models to run faster than dropout allows without a hardware upgrade.We’re thinking:As the organizers ofPie & AI, we’ll always try to make sure there’s a slice for you.\n\nCovid shut down the tennis tournament at Wimbledon this year, but a new model simulates showdowns between the sport’s greatest players.What’s new:Stanford researchers Kayvon Fatahalian and Maneesh Agrawala developedVid2Player, a system that simulates the footwork, positioning, and strokes of tennis pros like Roger Federer, Serena Williams, and Novak Djokovic. Users choose players and, if they want, control their location and stroke at the beginning of each shot cycle, creating arealistic match.How it works:Vid2Player has three main parts: a behavioral model for each player, a clip search, and a renderer.\n\nRealistic rallies:Five tennis experts who evaluated Vid2Player said it produced more realistic action than previous efforts. Similar systems likeTennis Real PlayandVid2Gamedraw from video databases to simulate on-court action under user control. However, they don’t search for clips that most closely match what a player would do in a given situation.Why it matters:Apart from filling the hole in this year’s tennis season, simulations like this could allow fans to create never-before-seen matchups like Federer versus Williams. Or our favorite: Federer vs. Federer. (Federer won.)\n\nWe’re thinking:We’re in love-love with this model!\n\nCourse 4 of theNatural Language Processing Specializationis launching this month! Make sure to complete Courses 1 through 3!\n\nThe White House called for new funding for AI research including a constellation of research centers. Nonetheless, the U.S. government’s annual spending on the technology still would lag behind that of several other nations.What’s new:The 2021 U.S. budget proposal includes $180 million to establish seven newartificial intelligence research institutesfunded through the National Science Foundation, Department of Agriculture, and other agencies. Private-sector partners including Dell, Facebook, and Netflix would contribute another $300 million. The proposal awaits approval by Congress.Focused research:The new institutes target both basic research and industrial applications:\n\nBehind the news:The White House has said it wouldboostAI spending by $2 billion in the next two years. The 2020 U.S. budget allocated around $850 million tononmilitary spendingon AI research and $4 billion to military AI spending. Other nations match or exceed these figures:\n\nYes, but:American tech giants are pouring billions more into AI. A 2018 study estimated that the top 10 tech companies, all based in the U.S., havespenta combined $8.6 billion acquiring AI startups since 1998, and some spend lavishly on research.Why it matters:The U.S. government sees itself in anarms racefor dominance in AI. It has a head start, thanks mostly to the country’s strong private tech sector. However, a recentreportconcluded that spending must reach $25 billion annually to keep up with perceived rivals.We’re thinking:Early funding for deep learning from theNational Science FoundationandDefense Advanced Research Projects Agency(Darpa) was critical in establishing the technology. The last thing the world needs is another arms race, but — at a time when U.S. government funds for research have been flat — we’d like to see AI get more public support.\n\nTransformer networks are gaining popularity as a high-accuracy alternative to recurrent neural networks. But they can run slowly when they’re applied to long sequences. New research converts transformers into functional RNNs for a major speed boost.What’s new:Angelos Katharopoulos and colleagues at Idiap Research Institute, École Polytechnique Fédérale de Lausanne and University of Washington accelerated transformers nearly a thousand-fold by outfitting them withlinear attention.Key insight:Researchers have used transformers instead of RNNs to analyze sequences, primarilysequences of wordsbut alsosequences of pixels. However, the number of calculations performed by the straightforward implementation of a transformer rises quadratically as sequence length increases, while calculations performed by RNNs rise linearly. The authors modified a transformer to act like an RNN’s hidden state. This modification, along with a clever speedup, allows the transformer’s computations to scale linearly with sequence length.How it works:Transformers extract features that capture the relationship between elements in the sequence. These features depend on comparisons between a single token to every other token in the sequence.\n\nResults:Linear attention generated syntheticMNISTimages over 400 times faster thanReformer, the pace-setting transformer in this task. And it was more accurate, too. In speech recognition on theWSJdataset, linear attention achieved a lower error rate (8 percent) compared to both Reformer (9.3 percent) and abi-LSTM(10.9 percent).Why it matters:This work demonstrated advantages over typical transformers without incurring any apparent costs. It remains to be seen whether these benefits extend to all situations.We’re thinking:Estimatesof the cost of training gargantuan transformer-based language models run to millions of dollars. It sure would be nice to trim those budgets by a few orders of magnitude.",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter204-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize201-2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Sliceout.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Tennis202.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/GIF205-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/institutes.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Transformer20RNN.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-147/",
    "title": "issue 147",
    "date": "",
    "reading_time": "",
    "content": "The United States Federal Reserve Bank has signaled that it will continue to raise interest rates. As one consequence, the stock market is significantly down, particularly tech stocks, relative to the beginning of the year. What does this mean for AI? In this two-part series, I’d like to discuss what I think will happen — which may have implications on your AI projects — and what I think should happen. Unfortunately, these are different things.The U.S. has enjoyed low interest rates over the past decade. Simplifying a bit, if r is the interest rate (if the interest rate is 2%, then r = 0.02), then one dollar T years in the future is worth 1/(1+r)^T as much as one dollar today. The larger r is, the less that future dollar is worth relative to its value today. If you’re familiar with the discount factor ɣ (Greek alphabet gamma) in reinforcement learning, you may notice that ɣ plays a similar role to 1/(1+r) and weights rewards T steps in the future by ɣ^T.\n\nIf interest rates were near zero, then one dollar in 10 years would be worth about the same as it is today. But if the interest rate were 5%, then a guaranteed promise of one dollar in 10 years would be worth only 61 cents today. What this means is that investors in the stock market are shifting to place a higher premium on cash today rather than cash in the future. This, in turn, will drive many CFOs, CEOs, and venture capital investors to discount investments that they deem likely to pay off only many years into the future.\n\nThis has important implications for AI. Over the past decade, many ambitious AI efforts sought to build fundamental technology that might pay off over many years. A few years ago, highly speculative bets on an experimental technology — from bold initiatives such as self-driving to more measured ones in which a team sought to execute a clear roadmap for a particular company — seemed like reasonable risks. Amid rising interest rates, such long-term bets look less attractive.\n\nMany investors are wondering if the stock market’s 13-year bull run has come to an end, and if the next era will be very different. If interest rates continue to rise, then:\n\nWhat this means for our community is that we should be ready for increased pressure to develop projects that demonstrate near-term, tangible value. For example, if you can explain how your AI system — for reading hospital records, inspecting parts, ensuring worker safety, or what have you — can save $1 million in two years, it will be easier to justify the $300,000 annual budget that you might be asking for. So if you’re looking for funding for a company or project, consider near-term impacts or financial justifications you can develop.\n\nSo far, I’ve laid out my prediction about what will happen, but what I think should happen is different. I believe this is still a good time to invest in long-term bets, because (i) the real interest rate (that is, the rate adjusted for inflation) is still very low, and (ii) the transformative value of AI is more financially powerful than interest rates, even as they compound at the moderate pace of 1/(1+r)^T. More about this in my next letter.\n\nKeep learning!Andrew\n\nP.S. I’m grateful to Erik Brynjolfsson, a brilliant economist who has done seminal work on tech’s impact on the economy, for helping me think through the contents of this letter. Responsibility for any errors lies with me.\n\nPerforming artists are taking action to protect their earning power against scene-stealing avatars.What’s new:Equity, a union of UK performing artists,launcheda campaign to pressure the government to prohibit unauthorized use of a performer’s AI-generated likeness. The unionpublishedtips to help artists who work on AI projects exercise control over their performances and likenesses.Protections for performers:Equity demands that the UK revise existing copyright laws and adopt guidelines enacted by other jurisdictions.\n\nWhat performers think of AI:Equity conducted a survey of its members between November 2021 and January 2022. Among the 430 people who responded:\n\nWhy it matters:While synthetic images, video, and audio contribute to countless exciting works, they’re an obvious source of concern for artists who wish to preserve — never mind increase — their earning power. These developments also affect members of the audience, who may find that their favorite performers have less and less to do with the productions they nominally appear in.We’re thinking:Using autotune to fix a wayward vocal performance doesn’t require the performer’s permission (though perhaps it should). The emerging generation of media production tools can generate performances entirely without the artist’s participation, further concentrating power in the hands of studios that own the technology. Defining the legal and ethical boundaries of generated media should help tip the balance toward performers, and it might lead to more fruitful creative collaborations between artists and machines.\n\nAI startups are helping writers tailor articles that appear near the top of Google’s search results.What’s new:At least 14 companies sell access to software that uses GPT-3, the language model from OpenAI, to generate headlines, product descriptions, blog posts, and video scripts,Wiredreported.How it works:The services enable people who have little experience or skill in writing to make content that’s optimized for web search engines.\n\nMachine privilege:Google’s guidelinesstatethat it may take action against automatically generated content. However, a Google spokesperson toldWiredthat the company may take a more lenient approach toward generated text that has been designed to serve readers rather than manipulate search results.Behind the news:Neural networks are reaching into video production, too. Given a script, Synthesiaproducescustomized videos, rendered by a generative adversarial network, aimed at corporate customers. Given a finished video, Mumbai-based Videoverse tags key highlights andrenders them into clipsoptimized for sharing on social media.Why it matters:Producing text for online marketers is an early commercial use case for text-generation models. The tech gives people who don’t specialize in marketing a leg up and raises the bar for professional writers — assuming it produces consistently high-quality output. In any case, AI has found a lucrative place in advertising and marketing, helping to drive$370 billionin ad sales this year, according to the marketing agency GroupM.We’re thinking:AI may write compelling marketing copy, but it’s still a long way from producing a great newsletter. Right?!\n\nIn FourthBrain’s newIntroduction to MLOpscourse, you’ll walk through the AI product life cycle by building a minimum viable product using the latest tools. This live course meets on Tuesdays from July 5 to July 26, 2022, 5 p.m. to 8 p.m. Central European Summer Time. Join us!Learn more\n\nWith prices on the rise, an app analyzes user data to deliver cash back on retail purchases.What’s new:Upside, a startup based in Washington, D.C., works with gas stations, grocery stores, and restaurants to offer personalized discounts to consumers,The Markupreported.How it works:The app displays a map studded with offers, customized for each user, from 30,000 partners, most of them U.S. retail chains. A user who patronizes a partner pays full price, then uploads an image of the receipt. The app applies a discount to the user’s in-app balance, which can be transferred to a bank account — for a fee — or traded for digital gift cards.\n\nBehind the news:Founded in 2015, Upside says its services reach 30 million U.S. users. Lyft and Uber integrate it with their driving app to offset inflation-driven spikes in gas prices. Fuel-saving apps GasBuddy and Checkout51 offer Upside-powered promotions, and DoorDash and Instacart have offered Upside to their drivers.Yes, but:Upside’s algorithmic approach to calculating discounts may leave some customers feeling left out.\n\nWhy it matters:Many families, individuals, and employees are on the lookout for ways to cut their expenses, and they may consider surrendering personal information a fair trade. However, the terms of the deal should be transparent and easy to understand. It’s deceptive to offer discounts that don’t pan out or diminish without warning as a casual shopper becomes a steady customer.We’re thinking:Offering discounts to attract users is an old tactic; think of Groupon and its countless competitors. But AI can tailor a deal to each individual user — a new approach that could make this strategy more effective, scalable, and sticky.\n\nAn object detector trained exclusively on urban images might mistake a moose for a pedestrian and express high confidence in its poor judgment. New work enables object detectors, and potentially other neural networks, to lower their confidence when they encounter unfamiliar inputs.What’s new:Xuefeng Du and colleagues at University of Wisconsin-Madison proposedVirtual Outlier Synthesis(VOS), a training method that synthesizes representations of outliers to make an object detector more robust to unusual examples.Key insight:Neural networks that perform classification (including object detectors) learn to divide high-dimensional space into regions that contain different classes of examples. Having populated a region with examples of a given class, they can include nearby empty areas in that region. Then, given an outlier, they’re likely to confidently label it with a class even if all familiar examples are far away. But a model can learn to recognize when low confidence is warranted by giving it synthetic points that fall into those empty areas and training it to distinguish between synthetic and actual points.How it works:Given an image, an object detector generates two types of outputs: bounding boxes and classifications for those boxes. VOS adds a third: the model’s degree of certainty that the image is an outlier.\n\nResults:VOS maintained object detectors’ classification performance while reducing its false-positive rate. For instance, aResNet-50trained using VOS on adatasetthat depicts persons, animals, vehicles, and indoor objects achieved object-detection performance of 88.66 percent AUC with a false-positive rate (FPR95) of 49.02 percent. By comparison, a ResNet-50 trained via amethodthat used a GAN to generate outlier images achieved slightly lower object-detection performance (83.67 percent AUC) and a much higher false-positive rate (60.93 percent FPR95).Why it matters:It’s difficult to teach a neural network that the training dataset is just a subset of a diverse world. Moreover, the data distribution can drift between training and inference. VOS tackles the hard problem of encouraging object detectors to exercise doubt about unfamiliar objects without reducing their certainty with respect to familiar ones.We’re thinking:The typical machine learning model learns about known knowns so it can recognize unknown knowns. While it’s a relief to have a neural network that identifies known unknowns, we look forward to one that can handleunknown unknowns.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/06/Screen-Shot-2022-06-01-at-11-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2022/06/ezgif.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/06/CONTENT--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/Intro-to-MLOps-Cover--1-.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/06/UPSIDE.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/06/ezgif.com-gif-maker--12--1.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-80/",
    "title": "issue 80",
    "date": "",
    "reading_time": "",
    "content": "AI-enabled automation is often portrayed as a binary on-or-off: A process is either automated or not. But in practice, automation is a spectrum, and AI teams have to choose where on this spectrum to operate. It’s important to weigh the social impact of our work, and we must ameliorate automation’s impact on jobs. In addition to this important consideration, the best choice often depends on the application and what AI can and cannot do.Take the problem of diagnosing medical patients from X-rays. The deployment options include:\n\nThese options can apply to medical diagnosis, visual inspection, autonomous navigation, media content moderation, and many other tasks. In many cases, I’ve found that picking the right one is critical for a successful deployment, and that using either too much or too little automation can have a significant negative impact.When you’re choosing a point along the automation spectrum, it’s worth considering what degree of automation is possible given the AI system’s accuracy, availability of humans to assist with the task, and desired rate of decision making (for example, human-in-the-loop options won’t work if you need to select an ad to place on a webpage within 100 milliseconds). Today’s algorithms are good enough only for certain points on the spectrum in a given application. As an AI team gains experience and collects data, it might gradually move to higher levels of automation within ethical and legal boundaries.Some people say that we should focus on IA (intelligence augmentation) rather than AI — that AI should be used to help humans perform tasks rather than automate those tasks. I believe we should try to create value for society overall. Automation can transform and create jobs (as when taxi cabs created new opportunities for cab drivers) as well as destroy them. Even as we pick a point on this spectrum, let’s take others’ livelihoods into account and create value that is widely and fairly shared.\n\nKeep learning!\n\nAndrew\n\nDatasets for training face recognition models have ballooned in size — while slipping in quality and respect for privacy.\n\nWhat’s new:In asurveyof 130 datasets compiled over the last four decades, Mozilla fellow Inioluwa Deborah Raji and AI consultant Genevieve Fried traced how the need for increasing quantities of data led researchers to relax their standards. The result: datasets riddled with blurred photos, biased labels, and images of minors, collected and used without permission, the authors toldMIT Technology Review.\n\nWhat they found:The study divides the history of face datasets into four periods.\n\nWhy it matters:People deserve to be treated fairly and respectfully by algorithms as well as other people. Moreover, datasets assembled without due attention to permission and data quality erode the public’s trust in machine learning. Companies likeClearview.aiandFindFacestand accused of harvesting online images without consent and using them in ways that violate individuals’ privacy, while shaky algorithms have contributed tobiased policing. In theU.K.,Canada, andcertain U.S. jurisdictions, lawmakers and lawsuits are calling for restrictions on the use of face images without consent.\n\nWe’re thinking:Andrew and his teams have worked on many face recognition systems over the years. Our practices have evolved — and continue to do so — as both society and AI practitioners have come to recognize the importance of privacy. As we gather data, we must also work toward fairer and more respectful standards governing its collection, documentation, and use.\n\nFun fact:Andrew’s face appears (with permission!) in a Carnegie Mellon University facedatasetcollected by Tom Mitchell in 1996. Here’swhat Andrew looked likein those days.\n\nPerformance in language tasksrises with the size of the model— yet, as a model’s parameter count rises, so does the time it takes to render output. New work pumps up the number of parameters without slowing down the network.What’s new:William Fedus, Barret Zoph, and Noam Shazeer at Google Brain developed theSwitch Transformer, a large-scale architecture (the authors built a version comprising 1.6trillionparameters) that’s nearly as fast as a much smaller model.Key insight:The approach known asmixture-of-expertsuses only a subset of a model’s parameters per input example. Like mixture-of-experts, Switch Transformer chooses which of many layers would best process a given input.How it works:The authors trained Switch Transformer to predict words that had been removed at random from alarge text datasetscraped from the web. The dataset was preprocessed to remove offensive language, placeholder text, and other issues.\n\nResults:The authors compared Switch Transformer (7.4 billion parameters) toT5(223 million parameters), a variant similar to the original transformer that was trained on the same dataset, using negative log perplexity, a measure of the model’s uncertainty (higher is better). The new model achieved -1.561 negative log perplexity compared to T5’s -1.731. Switch Transformer ran at two-thirds the speed of T5 — it executed 1,000 predictions per second compared to T5’s 1,600 — with 33 times the number of parameters. It beat a mixture-of-experts transformer, presumably of roughly the same size, on both counts.Why it matters:In deep learning, bigger is better — but so is a manageable computation budget.We’re thinking:Transformers come in an increasing variety of flavors. We hope this summary helps you remember which is switch.\n\nFacebook’s content moderation algorithms block many advertisements aimed at disabled people.\n\nWhat’s new:The social media platform’s automated systems regularly reject ads for clothing designed for people with physical disabilities. The algorithms have misread such messages as pornography or sales pitches for medical devices,The New York Timesreported.\n\nHow it works:Automated systems at Facebook and Instagram examines the images and words in ads that users try to place on the sites. They turn down ads they deem to violate their terms of service. The system tells would-be ad buyers when it rejects their messages, but not why, making it difficult for advertisers to bring rejected materials into compliance. Companies can appeal rejections, but appeals often are reviewed by another AI system, creating a frustrating loop.\n\nBehind the news:Other social media platforms have been tripped up by well-intentioned efforts to control harmful speech.\n\nWhy it matters:Millions ofsmall businessesadvertise on Facebook and Instagram, many of which serve niche communities. For such companies, being barred from promoting their wares on these platforms is a major blow.\n\nWe’re thinking:Moderating content on platforms as big as Facebook would be impossible without AI. But these cases illustrate how far automated systems are from being able to handle the job by themselves. Humans in the loop are still required to mediate between online platforms and their users.\n\nLast day to register for “Optimizing BizOps with AI,” an Expert Panel presented in collaboration with FourthBrain. Technical leaders at Amazon, Samsung, and Uber will explain how they’re deploying AI to improve business efficiency. Join us on Feb. 25, 2021, at 4 p.m. Pacific Standard Time!RSVP\n\nAI may help revolutionize the human diet – or dessert, at least.\n\nWhat’s new:Google applied AI engineer Dale Markowitz and developer advocate Sara Robinsontraineda model to predict whether a recipe is a bread, cake, or cookie. They brightened the recent holiday season by using it to develop novel hybrid recipes.\n\nHow it works:The engineers conceived their project to demonstrate Google’s AutoML, a software suite for easy-bake machine learning.\n\nBehind the news:Machine learning’s culinary education is coming along, though some of its creations are tastier than others.\n\nWhy it matters:Experimenting with new recipes isn’t just fun for home cooks. Commercial test kitchens are on the lookout for novel flavors, textures, and dishes. AI could help chefs invent smorgasbords of culinary delights.\n\nWe’re thinking:These AI-powered recipes may seem half-baked, but suddenly we have a craving for Chocolate Chicken Chicken Cake.\n\nModels that interpret the interplay of words and images tend to be trained on richer bodies of text than images. Recent research worked toward giving such models a more balanced knowledge of the two domains.What’s new:Pengchuan Zhang and Xiujun Li led a team at Microsoft and University of Washington raised the bar in several vision-and-language tasks. They call their systemOscar+, building on earlierworkthat used class names of objects in an image to improve matching of image and text representations.Key insight:Recent progress in vision-and-language models has come mostly by combining learned image and text representations more effectively rather than improving the representations themselves, the authors observed. Honing these representations through additional pretraining ought to boost their performance.How it works:The authors started with pretrained representations for images and text generated by separate models for vision (ResNeXt-152 C4pretrained onImageNet-5k) and language (pretrainedBERT). They honed the image representations by further pretraining the vision model on new data. Then they generated image-and-text representations as they pretrained Oscar+ as a whole. Finally, they fine-tuned the system on specific vision-and-language tasks.\n\nResults:Oscar+ achieved state-of-the-art results in all seven tasks, frommatching images with captions(and vice-versa) todetermining the truth of a statement about two images. The system boostedNoCapsaccuracy (captioning images that contain objects not seen in training) to 92.5 percent from 86.6 percent — its biggest gain. To show that performance was substantially improved by separately pretraining the object detector on additional data, the authors compared performance with and without that step. That step boosted visual question-answering accuracy, for instance, to 74.90 percent from 71.34 percent.Why it matters:Performance in multimodal tasks can improve with additional learning in just one of the modes involved.We’re thinking:IfOscar is a grouch, is Oscar+ nicer — or even more grumpy?",
    "images": [
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/unnamed-1-1.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/unnamed-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/unnamed-1-1.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/unnamed-2.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/unnamed-2-1024x576.png",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/unnamed-3.gif",
      "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/unnamed-4.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-121/",
    "title": "issue 121",
    "date": "",
    "reading_time": "",
    "content": "Should we be optimistic or pessimistic about the prospects for ethical AI? I meet people who are encouraged by the progress we’ve made toward making AI more responsible and free of bias. I also see people who are dismayed by the daunting challenges we face.Comparing things today to where they were five years ago, I find ample grounds for optimism in this area. Not long ago, we had barely defined the issues. Today we have numerous tools, publications, and conference sessions devoted to identifying bias and building systems that benefit people broadly. We’ve begun to acknowledge the social disparities that place barriers in front of talented people, and to chip away at them. Many more teams are working on these issues than ever before.On the other hand, comparing the current state of responsible AI with where we could or should be, I understand why some people are pessimistic. AI systems often reflect pernicious social patterns. Biases infect datasets, which are used to train biased models, which are deployed without adequate auditing, which contribute to denying someone a loan, insurance policy, medical procedure, or release from prison. Far too few teams are addressing these problems effectively.\n\nWhether one is an optimist or pessimist often depends on the frame of comparison. Do you compare where we are with how far we’ve come or how far we’ve yet to go? Beyond AI, society has made remarkable progress against racism in the last few decades. Within the past year, the Black Lives Matter movement has raised awareness of racism in the U.S. and George Floyd’s murderer was convicted. Yet the work ahead is daunting. Deeply rooted problems like racism and sexism seem nearly impossible to cure. Will we ever get past them?In light of these realities, I choose to be a clear-eyed optimist: grateful for progress and also realistic about the challenges ahead. I’m grateful for everyone who is making AI more responsible through frank conversation, designing responsible systems, and sharing ideas — thank you! Let’s celebrate this progress and give kudos to those who have contributed in any way, large or small. And simultaneously, let’s identify problems and work toward solutions —while treating each other with civility. As a utilitarian matter, I believe this balanced approach is the best way to make a better world.\n\nKeep learning!\n\nAndrew\n\nAn independent investigation found evidence of racial and economic bias in a crime-prevention model used by police departments in at least nine U.S. states.What’s new:Geolitica, a service that forecasts where crimes will occur, disproportionately targeted Black, Latino, and low-income populations, according to ananalysisof leaked internal data byGizmodoandThe Markup. The reporters found the data on an unsecured policewebsite. Geolitica, formerly called PredPol, changed its name in March.How it works:The model predicts where crimes are likely to occur, helping police departments use allocate personnel. The companytrainsa separate model for each jurisdiction on two to five years of crime dates, locations, and types.\n\nSources of bias:Critics point to pervasive biases in the models’ training data as well as potential adverse social effects of scheduling patrols according to automated crime predictions.\n\nThe response: Geolitica confirmed that the data used in the investigation “appeared to be” authentic, but it took issue with the analysis:\n\nWhy it matters:Over 70 U.S. law enforcement jurisdictions use Geolitica’s service, and it is used in other countries as well. Yet this report is the first independent analysis of the algorithm’s performance based on internal data. Its findings underscoreconcernsthat predictive policing systems invite violations of civil liberties, which have promptedeffortsto ban such applications.We’re thinking:Predictive policing can have a profound impact on individuals and communities. Companies that offer such high-stakes systems should audit them for fairness and share the results proactively rather than waiting for data leaks and press reports.\n\nClassical machine learning techniques could help children with autism receive treatment earlier in life.What’s new:Researchers led by Ishanu Chattopadhyay at University of Chicagodevelopeda system that classified autism in young children based on data collected during routine checkups.Key insight:Autistic children havehigher ratesof certain conditions — such as asthma, gastrointestinal problems, and seizures — than their non-autistic peers. Incidence of these diseases could be a useful diagnostic signal.How it works:The authors usedMarkov models, which predict the likelihood of a sequence of actions occurring, to feed agradient boosting machine(an ensemble of decision trees). The dataset comprised weekly medical reports on 30 million children aged 0 to 6 years.\n\nResults:The system’s precision — the percentage of kids it classified as autistic who actually had the condition — was 33.6 percent at 26 months. Classifying children of the same age, aquestionnaireoften used to diagnose children between 18 and 24 months of age achieved 14.1 percent precision. The model was able to achieve sensitivity — the percentage of children it classified correctly as autistic — as high as 90 percent, with 30 percent fewer false positives than the questionnaire at a lower sensitivity.Why it matters:It may be important to recognize autism early. Although there’s no consensus,some expertsbelieve that early treatment yields the best outcomes. This system appears to bring that goal somewhat closer by cutting the false-positive rate in half compared to the questionnaire. Nonetheless, it misidentified autism two-thirds of the time, and the authors caution that it, too, could lead to over-diagnosis.We’re thinking:Data drift and concept drift, which cause learning algorithms to generalize poorly to populations beyond those represented in the training data, has stymied many healthcare applications. The authors' large 30 million-patient dataset makes us optimistic that their approach can generalize in production.\n\nHave you checked out ourPractical Data Science Specialization? This specialization will help you develop the practical skills to deploy data science projects and teach you how to overcome challenges at each step using Amazon SageMaker.\n\nOne year after her acrimonious exit from Google, ethics researcher Timnit Gebru launched an independent institute to study neglected issues in AI.What’s new:TheDistributed Artificial Intelligence Research Institute(DAIR) is devoted to countering the influence of large tech companies on the research, development, and deployment of AI. The organization is funded by $3 million in grants from the Ford Foundation, MacArthur Foundation, Kapor Center, and Open Society Foundation.How it works:DAIR is founded upon Gebru’s belief that large tech companies, with their focus on generating profit, lack the incentive to assess technology’s harms and the motivation to address them. It will present its first project this week atNeurIPS.\n\nBehind the news:Gebru was the co-lead of Google’s Ethical AI group until December 2020. The company ousted her after she refused to retract or alter a paper that criticized its BERT language model. A few months later, it fired her counterpart andestablisheda new Responsible AI Research and Engineering group to oversee various initiatives including Ethical AI.Why it matters:AI has the potential to remake nearly every industry as well as governments and social institutions, and the AI community broadly agrees on the need for ethical principles to guide the process. Yet the companies at the center of most research, development, and deployment have priorities that may overwhelm or sidetrack ethical considerations. Independent organizations like DAIR can call attention to the ways in which AI may harm some groups and use the technology to shed light on problems that may be overlooked by large, mainstream institutions.We’re thinking:Gebru has uncovered important issues in AI and driven the community toward solutions. We support her ongoing effort to promote ethics in technology.\n\nTransformers have matched or exceeded earlier architectures in language modeling and image classification. New work shows they can achieve state-of-the-art results in some reinforcement learning tasks as well.What’s new:Lili Chen and Kevin Lu at UC Berkeley with colleagues at Berkeley, Facebook, and Google developedDecision Transformer, which models decisions and their outcomes.Key insight:A transformer learns from sequences, and a reinforcement learning task can be modeled as a repeating sequence of state, action, and reward. Given such a sequence, a transformer can learn to predict the next action (essentially recasting the reinforcement learning task as a supervised learning task). But this approach introduces a problem: If the transformer chooses the next action based on earlier rewards, it won’t learn to take actions that, though they may bring negligible rewards on their own, lay a foundation for winning higher rewards in the future. The solution is to tweak the reward part of the sequence. Instead of showing the model the reward for previous actions, the authors provided the sum of rewards remaining to be earned by completing the task. This way, the model took actions likely to reach that sum.How it works:The researchers trained agenerative pretrained transformer(GPT) on recorded matches of three types of games:Atari gameswith a fixed set of actions,OpenAI Gym gamesthat require continuous control, andKey-to-Door. Winning Key-to-Door requires learning to pick up a key, which brings no reward, and using it to open a door and receive a reward.\n\nResults:The authors compared Decision Transformer with the previous state-of-the-art method,Conservative Q-Learning(CQL).They normalized scores of Atari and OpenAI Gym games to make 0 on par with random actions and 100 on par with a human expert. In Atari games, the authors’ approach did worse, earning an average score of 98 versus CQL’s 107. However, it excelled in the more complex games. In OpenAI Gym, averaged 75 versus CQL’s 64. In Key-to-Door, it succeeded 71.8 percent of the time versus CQL’s 13.1 percent.Why it matters:How to deal with actions that bring a low reward in the present but contribute to greater benefits in the future is a classicissuein reinforcement learning. Decision Transformer learned to solve that problem viaself-attentionduring training.We’re thinking:It’s hard to imagine using this approach for online reinforcement learning, as the sum of future rewards would be unknown during training. That said, it wouldn’t be difficult to run a few experiments, train offline, and repeat.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2022/06/optimism1-1.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/06/PREDPOL--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/AUTISM--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/The-Batch-S12n.webp",
      "https://dl-staging-website.ghost.io/content/images/2022/06/DAIR--1-.gif",
      "https://dl-staging-website.ghost.io/content/images/2022/06/DECISION-1.gif"
    ]
  }
]