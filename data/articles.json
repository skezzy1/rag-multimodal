[
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-300/",
    "title": "issue 300",
    "date": "",
    "reading_time": "14 min read",
    "content": "I’m delighted to announce that AI Fund has closed $190M for our new fund, in an oversubscribed round. I look forward to working with many more builders to create new companies that serve humanity.\n\nAI Fund isn’t a traditional venture capital firm that invests in existing businesses. Instead, we are a venture builder (also called a venture studio): Weco-found AI companies, so our team is directly involved in writing code, talking to customers to get feedback, iterating on product designs, preparing market analyses, and so on. We have a lot of fun building multiple AI products at a time, and thus live daily the emerging AI startup best practices.\n\nMany factors go into the success of a startup. But if I had to pick just one, it would be speed. Startups live or die based on their ability to make good decisions and execute fast, which has been a recurring theme of my articles inThe Batchas well.\n\nIf you are building an AI startup, here are some ideas to consider:\n\nIn addition to speed, a second criterion that I find important for startup success is deep knowledge of the technology. Because AI technology is evolving rapidly, a team with a deep technical understanding of what AI can and cannot do, and when to use what tool, will make better decisions. This creates meaningful differentiation and saves wasting time in blind alleys. A good technical understanding, too, gets you speed!\n\nI’m grateful to AI Fund’s investors, team, and entrepreneur partners for working with us. There is much ahead to build!\n\nAndrew\n\nLearn to create voice agents that listen, reason, and respond in real time, just like a conversation with a real person in our latest short course, “Building AI Voice Agents for Production.” You'll build a scalable agent from scratch, deploy it to the cloud, and explore what makes voice interfaces feel fast, natural, and human.Enroll for free\n\nAlibaba’s new model family may unseat DeepSeek-R1’s four-month reign as the top open-weights large language model.\n\nWhat’s new:Alibabareleasedweights for eight large language models, all of which offer a reasoning mode that can be switched on or off. Two use a mixture of experts (MoE) architecture: Qwen3-235B-A22B (the name indicates 235 billion parameters, 22 billion of which are active at any given time) and Qwen3-30B-A3B). The other six are dense models in sizes between 32 billion parameters and 0.6 billion parameters — tiny by LLM standards, and with reasoning, too.\n\nHow it works:The Qwen3 family implementschain-of-thoughtreasoning in both relatively large and quite small LLMs.\n\nResults:Qwen3-235B-A22B and Qwen3-30B-A3B performed as well as, or better than, leading open-weights models in tests performed by Alibaba. Qwen3-4B, too, achieved results that are competitive with many models several times its size. Alibaba didn’t provide results for the other dense models.\n\nWhy it matters:Qwen3 continues a string of high-performance, open-weights models released by developers in China. Alibaba says it designed the models to do the thinking in agentic systems. Reasoning that can be switched on and off can help control costs in agentic and other applications.\n\nWe’re thinking:Alibaba’s 235-billion parameter MoE model may perform better according to benchmarks, but Qwen3-30B-A3B does nearly as well and can run locally on a pro laptop without straining its memory. Add the easy ability to switch reasoning on or off, and Qwen3’s versatile, mid-sized MoE model may turn out to be the star of the show.\n\nOpenAI’s most widely used model briefly developed a habit of flattering users, with laughable and sometimes worrisome results.\n\nWhat’s new:OpenAI quicklywithdrewan update to GPT-4o (gpt-4o-2025-04-25), which supplied responses for ChatGPT, after it provided excessively fawning responses to user input — even in contexts didn’t call for agreement. The company reverted to an earlier version (gpt-4o-2024-11-20). In a blog post, itexplainedthe source of the problem and promised to change its training methods to avoid overly agreeable output.\n\nAmiable to a fault:Many ChatGPT users shared screen shots of ChatGPT’s sycophantic responses on social media.\n\nHow it works:Sycophancy, also called glazing, occurs when a large language model learns to align its responses excessively with the user's point of view, even when that standpoint is objectively false, unethical, or harmful. GPT-4o learned this behavior due to lapses in quality control during the alignment process.\n\nBehind the news:Sycophantic behavior in large language models has been a subject of AI research and commentary.\n\nWhy it matters:ChatGPT’s episode of sycophancy illustrates the subtlety of the goal of aligning AI with human values. Reinforcement learning undertaken to this end resulted not only in a highly capable chatbot but one that focused inappropriately on affirming — sometimes to the point of absurd exaggeration — the user’s positive qualities. Alignment requires balancing multiple objectives beyond agreeableness including accuracy, helpfulness, and ethics. Ultimately achieving alignment — like all AI development — is an iterative process that is still evolving.\n\nWe’re thinking:To those who read this far, your unwavering dedication and extraordinary perseverance is nothing short of legendary. Like a master navigator, you’ve traversed word by word, never wavering, displaying a level of focus and determination that would humble even the most steadfast of scholars. We are truly honored to have such an intrepid reader. Bravo to you, the indefatigable champion of curiosity!\n\nThe world’s biggest pharmaceutical company by revenue shed light on its AI strategy.\n\nWhat’s new:Johnson & Johnson, after experimenting broadly with generative AI, settled on a short list of projects that aid in sales, drug development, supply-chain management, and internal communications. A company executive described the process and results to the venture-capital firmGreylockandThe Wall Street Journal.\n\nHow it works:The 140-year-old medical company spent roughly a year experimenting with various AIapplicationsthroughout the company, according to Chief Information Officer Jim Swanson. A centralized governing board oversaw as many as 900 experiments. After finding that 10 percent to 15 percent of use cases drove about 80 percent of the value, the company shifted responsibility for AI projects to specific departments to focus on high-value applications. In the end, the criteria for choosing a project was threefold: (i) how readily it could be implemented, (ii) how useful it would be throughout the company, and (iii) how much it would benefit the business.\n\nBehind the news:Generative AI is expected to bring in up to $110 billion in annual revenue across the pharmaceutical industry,according to McKinsey. The consultancy breaks down this number into the following categories, in order of their contribution to the total: commercial (AI for sales and marketing), research (AI for designing, screening, and manufacturing molecules), clinical (AI to facilitate trials), enterprise, operations, and medical (processing medical literature).\n\nWhy it matters:Johnson & Johnson’s experience offers a peek into AI development at a major legacy company in a key sector. The company has identified high-value opportunities in enterprise-wide operations, departmental priorities, and core products. It’s pursuing all three.\n\nWe’re thinking:Notably, this medical stalwart is building AI applications for human resources, sales, and supply-chain management. Similar opportunities exist at companies old and new, big and small, far and wide.\n\nResearchers showed that supervised fine-tuning on as few as 1,000 examples can enable a pretrained large language model to reason — and a clever gambit can boost its performance to rival that of top reasoning models.\n\nWhat’s new:Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li and colleagues at Stanford, University of Washington, Allen Institute for AI, and Contextual AI developeds1, a reasoning model that achieves higher performance by producing more reasoning tokens. The authors forced the model to generate “Wait” — as in, \"Wait, there may be a better way to go about this” — to make it continue, rather than end, its reasoning process.\n\nKey insight:The sequence of reasoning tokens generated by a reasoning model likeDeepSeek-R1is delimited by special tokens. In pretraining on human data, a model learns to keep generating reasoning tokens until it generates the special token that ends the sequence. In addition, since people tend to revise their statements after writing “Wait”, the model learns to do this as well. Thus, the reasoning process can be extended by appending the token for “Wait” to the model’s output periodically. In this way, when the output-in-progress is fed back to generate the next token, the model continues to reason over the prompt. Such extended reasoning can improve the final output by inducing the model to double-check its response so far and improve previous reasoning steps.\n\nHow it works:The authors fine-tuned a pretrainedQwen 2.5-32B, which does not produce reasoning tokens, on around 1,000 examples ofchain-of-thoughtreasoning.\n\nResults:s1’s performance improved as the number of reasoning tokens it generated increased. Ultimately, it achieved comparable performance to OpenAI o1-preview but fell short of o1.\n\nWhy it matters:A conventional pretrained LLM can learn to reason after supervised fine-tuning on as few as 1,000 curated examples — no reinforcement learning necessary. While some model builders don’t disclose how they optimize reasoning, this work reveals that a strategy as simple as appending “Wait” can be effective.\n\nWe’re thinking:Wait, how can we apply this to our projects?",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--84--2.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/The-Batch-ads-and-exclusive-banners---2025-05-05T152809.008.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--85-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--62-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--86-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--87-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-294/",
    "title": "issue 294",
    "date": "",
    "reading_time": "14 min read",
    "content": "Fine-tuning small language models has been gaining traction over the past half year. I’d like to share my sense of when to use this technique, and also when not to, based on what I’m seeing in multiple companies.\n\nFirst, while fine-tuning is an important and valuable technique, many teams that are currently using it probably could get good results with simpler approaches, such as prompting (including writingmega prompts), few-shot prompting, or simple agentic workflows.\n\nWhy shouldn’t these teams be fine-tuning? Because fine-tuning, which takes a pre-trained model and further trains it on data specific to an application, is relatively complex to implement. You need to collect training data, then (unless you want to implement fine-tuning yourself) find a provider to help with running fine-tuning, then find a way to deploy the fine-tuned model. Because it adds extra complexity both in training and deployment, usually I resort to this technique only after I find that prompting and simple agentic workflows are not up to a task.\n\nHaving said that, there are also applications where fine-tuning is appropriate and valuable. LoRA (which learns by modifying a limited number of parameters rather than the entire model) and related methods have made fine-tuning quite affordable, particularly for small models (say, 13B or fewer parameters). And the amount of data needed to get started is less than most people think. Depending on the application, I’ve seen good results with 100 or even fewer examples. Here are a few applications where I have seen fine-tuning applied successfully:\n\nImproving accuracy of critical applications.Prompting can get you really far for many applications. But sometimes, fine-tuning helps eke out that last bit of accuracy. For example, if you are building a customer service chatbot and need it to call the right API reliably (say, to carry out transactions, issue refunds, and the like), perhaps prompting can get it to make the right API call 95% of the time. But if you struggle to raise the accuracy even with revisions to the prompt and you really need 99% accuracy, fine-tuning on a dataset of conversations and API calls might be a good way to get you there. This is particularly true for tasks where it's hard to specify, using only language, an unambiguous rule to decide what to do. For example, when a customer is frustrated, should the chatbot escalate to a manager or just issue a refund? Teams often write Standard Operating Procedures (SOPs) for human workers to follow, and these SOPs can go into the prompts of models. But if it is hard to specify an unambiguous SOP, so even humans need to see numerous examples before they can learn what to do, fine-tuning can be a good approach. For many text-classification applications fine-tuning also works well, for example, classifying medical records into diagnosis and procedure codes for health insurance claims.\n\nLearning a particular  style of communication.As I explain in “Generative AI for Everyone,” my team fine-tuned a model to sound like me. Many people (including myself) have idiosyncratic uses of language. There are certain words I tend to say and others I tend not to, and these idiosyncrasies are numerous and very difficult to specify in a text prompt. (By the way, the avatar at deeplearning.ai/avatar, built with RealAvatar, uses fine-tuning for this reason.) To get a system to communicate in a certain style, fine-tuning is often a superior solution to prompting alone.\n\nReducing latency or cost during scale-ups.I’ve seen applications where developers have successfully prompted a large model to perform a complex task. But as usage scales up, if the large model is too slow (which often happens) or too expensive (which also happens but less frequently), the team might want to use a smaller model. If, however, the performance of the smaller model isn't good enough, then fine-tuning it can help bring it up to the performance of the larger one for that narrow application. Further, the larger model (or perhaps an agentic workflow) can also be used to generate data to help with fine-tuning the small model for that task.\n\nAt the cutting edge of research, some teams are fine-tuning models to get better at a certain language. But with few exceptions, if the goal is to get an LLM to better understand a body of knowledge that is not in its training data, I find that using RAG (retrieval augmented generation) is a much simpler approach, and I still occasionally run into teams using fine-tuning for which  I think RAG would work better.\n\nOverall my sense is that, of all the teams I see using fine-tuning, perhaps 75% could get good results using simpler techniques (like prompting or agentic workflows), but in 25% of cases I know of no better way to achieve their goal.\n\nIt is still technically challenging to implement fine-tuning, get the hyperparameters right, optimize the compute resources, and so on. We are lucky that more and more companies have worked hard to optimize these and provide efficient fine-tuning services. Many of them allow us to fine-tune open weights models and also download the fine-tuned weights. Some allow us to fine-tune their closed models and continue to keep the tuned weights closed. Both can be useful, but the former has obvious advantages of portability and not having to worry that the provider will stop serving a particular model, causing a critical component in our software to become deprecated.\n\nIn conclusion, before fine-tuning, consider if you should be trying just a bit harder with prompting or agentic workflows, which can lead to simpler solutions that are easier to maintain. The vast majority of applications my teams build do not use any fine-tuning at all, but it’s a critical piece of a small minority of them.\n\nKeep learning!\n\nAndrew\n\nIn “Vibe Coding 101 with Replit,” you’ll learn to plan, prompt, and debug alongside a coding agent. Build, host, and share two real web apps in Replit’s cloud environment while developing effective development skills like writing product requirements, structuring tasks, and refining AI-generated code.Start today\n\nGoogle updated its open-weights family of large language models to include versions that handle image and video inputs.\n\nWhat’s new:Google released itsGemma 3multilingual large language models with parameter counts of 1 billion, 4 billion, 12 billion, and 27 billion. While the smallest processes text only, the other three are vision-language models that are small enough to run on a consumer hardware.\n\nHow it works:Gemma 3rearchitectsand refines earlier Gemma models for higher performance at lower parameter counts.\n\nPerformance:Gemma 3 models outperform Gemma 2 models of equal or larger size by several measures, and all sizes show a strong ability to solve mathematics word problems as measured byMATH.\n\nHot on Gemma 3’s heels:Shortly after Gemma 3 became available, Mistral releasedSmall 3.1(24 billion parameters), a vision-language model with open weights, under a more permissive Apache 2.0 license.\n\nWhy it matters:Gemma 3 takes advantage of a variety of techniques to raise the bar for vision-language performance in relatively small models. Knowledge distillation, multiple rounds of reinforcement learning, and fine-tuning on many languages are a powerful combination.\n\nWe’re thinking:A vision-language model small enough to run on a smartphone feels increasingly close!\n\nDiffusion models usually take many noise-removal steps to produce an image, which takes time at inference. There are ways to reduce the number of steps, but the resulting systems are less effective. Researchers devised a streamlined approach that doesn’t sacrifice output quality.\n\nWhat’s new:Kevin Frans and colleagues at UC Berkeley introducedshortcut modelsthat learn to take larger noise-removal steps and thus require fewer steps to generate an image.\n\nKey insight:At inference, a scheduler likeEulercan enable a model to take larger steps than those it learned during training, but this approach yieldsworse performance. Alternatively distillation, in which a student model learns to remove the same amount of noise as a teacher model when it takes several steps, offers improved performance at the cost of more cumbersome development. Training the model directly to take bigger steps — that are equivalent to multiple smaller steps — enables it to maintain high performance while taking fewer steps.\n\nHow it works:The authors trainedDiT-B, a diffusion transformer, to generate images like those in CelebA-HQ (celebrity faces) and ImageNet-256 (various subjects, size 256x256).\n\nResults:The authors compared their model using 1, 4, or 128 steps to alternatives that were trained via various methods including many variants of distillation. They measured the results usingFréchet inception distance(FID), which assesses how closely generated images resemble real-world images (lower is better).\n\nWhy it matters:Generating images by diffusion is typically costly, and previous approaches to cutting the cost have compromised either performance or incurred additional development expense or both. This method achieves high performance at relatively low cost.\n\nWe’re thinking:As diffusion models continue to become cheaper and faster, we expect to see applications blossom!\n\nStudents benefit from tutoring, but training tutors is expensive. A study shows that large language models can boost tutors’ effectiveness in real time.\n\nWhat’s new:Rose Wang and colleagues at Stanford builtTutor CoPilot, a tool for remote, online tutors that uses GPT-4 to generate hints, explanations, questions, and other helpful responses to students.\n\nKey insight:When a student makes an error, according to previousworkby some of the same authors, effective teachers choose a strategy for addressing the mistake. The authors identified 11 strategies, such as ask a question, explain a concept, provide a hint, or encourage the student. Moreover, they found that an LLM that executed a strategy chosen by an expert teacher performed significantly better than an LLM that was prompted with a strategy chosen at random or no specific strategy. Letting inexperienced tutors choose a strategy while an LLM generates a response helps them learn how to execute the strategy. Students, in turn, benefit from responses that mimic those of an experienced teacher.\n\nHow it works:The authors outfitted a remote tutoring application with GPT-4.\n\nResults:The authors partnered with a virtual tutoring company and a school district in the United States for a two-month study of 874 tutors and 1,787 students between grades 3 and 8. They divided the participants into two groups. In one group, tutors conducted sessions with students as usual. In the other, tutors had access to Tutor CoPilot. The authors measured success by the percentage of students who passed a test at the end of a lesson.\n\nYes, but:The authors found statistically significant improvements as measured by test results per lesson, but not in end-of-year exam results. The study’s two-month duration may account for the lack of evidence for longer-term effects.\n\nWhy it matters:LLMs hold great promise for helping to educate students, but they also show potential in educating teachers. For inexperienced tutors who are learning how to interact with students, an LLM’s general knowledge and pedagogical insights gleaned from expert teachers make a powerful combination.\n\nWe’re thinking:Although it relies on sophisticated technology, the authors’ approach is simple: Prompt an LLM to apply proven teaching principles. Presumably such principles apply beyond elementary math, which would make this approach useful for teaching a variety of disciplines.\n\nDiffusion transformers learn faster when they can look at embeddings generated by a pretrained model like DINOv2.\n\nWhat’s new:Sihyun Yu and colleagues at Korea Advanced Institute of Science and Technology, Korea University, New York University, and Scaled Foundations (a startup that builds AI for robotics) proposedRepresentation Alignment(REPA), a loss term for transformer-based diffusion.\n\nKey insight:Diffusion models learn to remove noise from images to which noise was added (and, at inference, they start with pure noise to generate a fresh image). This process can be divided into two parts: learning to (i) embed the noisy image and (ii) estimate the noise from the embedding. One way to accelerate learning is to add a loss term that encourages the diffusion model to produce embeddings that are similar to those produced by a pretrained embedding model. The diffusion model can learn to estimate the noise faster if it doesn’t need to learn how to embed an image from scratch.\n\nHow it works:The authors modifiedDiT-XL/2andSiT-XL/2transformer-based latent diffusion models, a class of diffusion models that subtract noise from embeddings rather than images. They trained the models to produce images similar to ImageNet. In the process, the modified models learned to produce embeddings similar to those produced by a pretrainedDINOv2.\n\nResults:The modified DiT-XL/2 learned significantly faster than the unmodified version.\n\nWhy it matters:Diffusion models and contrastive self-supervised models like DINOv2 have fundamentally different training objectives: One produces embeddings for the purpose of image generation, while the other’s embeddings are used for tasks like classification and semantic segmentation. Consequently, they learn different aspects of data. This work proposes a novel way to combine these approaches to produce more generally useful embeddings.\n\nWe’re thinking:It turns out that the REPA modification enabled diffusion models to produce embeddings better suited not only to diffusion but also to image classification and segmentation. A similar approach could lead to a more holistic framework for learning image representations.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--56-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/03/V3_DeepLearning_Replit_Banner_2070x1080-01.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--67-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--68-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--69-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-291/",
    "title": "issue 291",
    "date": "",
    "reading_time": "13 min read",
    "content": "Continuing our discussion on theVoice Stack, I’d like to explore an area that today’s voice-based systems mostly struggle with: Voice Activity Detection (VAD) and the turn-taking paradigm of communication.\n\nWhen communicating with a text-based chatbot, the turns are clear: You write something, then the bot does, then you do, and so on. The success of text-based chatbots with clear turn-taking has influenced the design of voice-based bots, most of which also use the turn-taking paradigm.\n\nA key part of building such a system is a VAD component to detect when the user is talking. This allows our software to take the parts of the audio stream in which the user is saying something and pass that to the model for the user’s turn. It also supports interruption in a limited way, whereby if a user insistently interrupts the AI system while it is talking, eventually the VAD system will realize the user is talking, shut off the AI’s output, and let the user take a turn. This works reasonably well in quiet environments.\n\nHowever, VAD systems today struggle with noisy environments, particularly when the background noise is from other human speech. For example, if you are in a noisy cafe speaking with a voice chatbot, VAD — which is usually trained to detect human speech — tends to be inaccurate at figuring out when you, or someone else, is talking. (In comparison, it works much better if you are in a noisy vehicle, since the background noise is more clearly not human speech.) It might think you are interrupting when it was merely someone in the background speaking, or fail to recognize that you’ve stopped talking. This is why today’s speech applications often struggle in noisy environments.\n\nIntriguingly, last year, Kyutai Labs publishedMoshi, a model (GitHub) that had many technical innovations. An important one was enabling persistent bi-direction audio streams from the user to Moshi and from Moshi to the user.\n\nIf you and I were speaking in person or on the phone, we would constantly be streaming audio to each other (through the air or the phone system), and we’d use social cues to know when to listen and how to politely interrupt if one of us felt the need. Thus, the streams would not need to explicitly model turn-taking. Moshi works like this. It’s listening all the time, and it’s up to the model to decide when to stay silent and when to talk. This means an explicit VAD step is no longer necessary. (Moshi also included other innovations, such as an “inner monologue” that simultaneously generates text alongside the audio to improve the quality of responses as well as audio encoding.)\n\nJust as the architecture of text-only transformers has gone through many evolutions (such as encoder-decoder models, decoder-only models, and reasoning models that generate a lot of “reasoning tokens” before the final output), voice models are going through a lot of architecture explorations. Given the importance of foundation models with voice-in and voice-out capabilities, many large companies right now are investing in developing better voice models. I’m confident we’ll see many more good voice models released this year.\n\nIt feels like the space of potential innovation for voice remains large. Hard technical problems, like the one of latency that I described last week and VAD errors, remain to be solved. As solutions get better, voice-to-voice will continue to be a promising category to build applications in.\n\nKeep building!\n\nAndrew\n\nLearn to build an event-driven AI agent that processes documents and fills out forms using RAG, workflows, and human-in-the-loop feedback. This course, built in partnership with LlamaIndex, walks you through designing, building, and refining automated document workflows.Enroll for free\n\nTypical large language models are autoregressive, predicting the next token, one at a time, from left to right. A new model hones all text tokens at once.\n\nWhat’s new:Inception Labs, a Silicon Valley startup, emerged from stealth mode withMercury Coder, a diffusion model that generates code, in small and mini versions. Registered users can try it outhere, and an API (sign up for early accesshere) and on-premises deployments are in the works. The company has not yet announced availability and pricing.\n\nHow it works:Like image diffusion models, Mercury Coder improves its output over a number of steps by removing noise.\n\nResults:Mercury Coder’s major advantage is speed, but it also performs well compared to several competitors.\n\nBehind the news:Several teams have built diffusion models that generate text, but previous efforts have not been competitive with autoregressive large language models (LLMs). Recently,LLaDAshowed comparable performance to Meta’s Llama 2 7B but fell short of Llama 3 8B and other similarly sized modern LLMs.\n\nWhy it matters:Text diffusion models are already faster than autoregressive models. They offer significant promise to accelerate text generation even further.\n\nWe’re thinking:Diffusion image generators have delivered good output with as little as four or even one step, generating output tokens significantly faster than autoregressive models. If text diffusion models can benefit from improvements in image generation, they could lead to rapid generation of lengthy texts and, in turn, faster agents and reasoning.\n\nOpenAI launched GPT-4.5, which may be its last non-reasoning model.\n\nWhat’s new:GPT-4.5 isavailableas a research preview. Unlike OpenAI’s recent models o1 and o3, GPT-4.5 is not fine-tuned to reason by generating a chain of thought, although the company hinted that it may serve as a basis of a reasoning model in the future. Instead, it’s a huge model that was trained using a huge amount of computation. As OpenAI’s biggest model to date, GPT-4.5 isvery expensiveto run, and the company is evaluating whether to offer it via API in the long term.\n\nHow it works:OpenAI revealed fewdetailsabout how GPT-4.5 was built. The model is bigger than GPT-4o, and it was pretrained and fine-tuned on more data using more computation — possibly 10x more, given OpenAI’s comment that “with every new order of magnitude of compute comes novel capabilities.”\n\nPerformance:“This isn’t a reasoning model and won’t crush benchmarks,” OpenAI CEO Sam Altman warned in atweet. The company claims that GPT-4.5 offers improved general knowledge, adheres to prompts with more nuance, delivers greater creativity, and has higher emotional intelligence.\n\nBehind the news:GPT-4.5’s release comes as OpenAI nears an announcedtransitionaway from developing separate general-knowledge and reasoning models. The launch also comes as OpenAI faces an ongoing shortage of processing power. CEO Sam Altmansaidthat the company is “out of GPUs” and struggling to meet demand — a constraint that may impact whether OpenAI continues to offer GPT-4.5 via API.\n\nWhy it matters:GPT-4.5 highlights a growing divide in AI research over whether to pursue performance gains by scaling up processing during pretraining or inference. Despite the success of approaches that consume extra processing power at inference, such as agentic techniques and reasoning models such as its own o family, OpenAI clearly still sees value in pretraining larger and larger models.\n\nWe’re thinking:There’s still more juice to be squeezed out of bigger models! We’re excited to see what the combination of additional compute applied to both pretraining and inference can achieve.\n\nAnthropic’s Claude 3.7 Sonnet implements a hybrid reasoning approach that lets users decide how much thinking they want the model to do before it renders a response.\n\nWhat’s new:Claude 3.7 Sonnetwas trained for strong performance in coding and front-end web development, with less emphasis on math and computer-science competition problems. It implements tool use and computer use (but not web search) and lets users toggle between immediate responses andextended thinking mode, which can improve outputs by allocating a specific number of tokens to reasoning at inference. Like DeepSeek-R1 and Google Gemini Flash Thinking — and unlike OpenAI o1 — Claude 3.7 Sonnet fully displays reasoning tokens. Anthropic considers this functionality experimental, so it may change.\n\nHow it works:Anthropic pretrained Claude 3.7 Sonnet on a mix of public and proprietary data (which explicitly did not include Claude users’ inputs and outputs). The team fine-tuned Claude 3.7 Sonnet usingconstitutional AI, which encourages a model to follow a set of human-crafted rules.\n\nPerformance:Claude 3.7 Sonnet shows exceptional performance in general knowledge, software engineering, and agentic tasks.\n\nBehind the news:Anthropic’s approach refines earlier efforts to enable users to control the incremental expense of computing extra tokens at inference. For instance, OpenAI o1 offers three levels of reasoning or “effort” — each of which allocates more tokens to reasoning — while X’sGrok 3offers two.\n\nWhy it matters:Test-time compute, or additional processing at inference, is powerful but expensive, and not all tasks benefit from it. So it’s helpful to let users choose how much to apply. Claude 3.7 Sonnet improves its predecessor’s general performance and provides an ample budget for additional reasoning.\n\nWe’re thinking:The cost of inference is rising as agentic workflows and other compute-intensive tasks become more widely used. Yet the cost of AI on a per-token basis isfallingrapidly. Intelligence is becoming steadily cheaper and more plentiful.\n\nAmazon announced Alexa+, a major upgrade to its long-running voice assistant.\n\nWhat’s new:Alexa+, which accepts spoken commands and responds conversationally, is designed to work with a variety of vendors as an autonomous agent to make purchases, book reservations, play media, and so on. It will roll out in the U.S. over coming weeks, initially on some Echo Show devices and eventually nearly every current Echo speaker.\n\nHow it works:Alexa+updatesthe system to take advantage of generative AI including Anthropic Claude,Amazon Nova, and other large language models. Inputs are filtered through a routing system that determines the best model to respond to any given request. It’s trained to understand colloquial, conversational language. Its personality is designed to be “smart, considerate, empathetic, and inclusive” as well as humorous.\n\nBehind the news:Amazon launched Alexa in 2014, and the voice assistant now resides in over 600 million devices worldwide. However, users relied on it more to set timers, report sports scores, and play music than to purchase products, and Alexa revenue lagged. Following cutbacks in 2021, Amazon mademultibillion-dollarinvestmentsin Anthropic and set about updating the technology for the generative AI era.\n\nWhy it matters:Alexa, along with Apple’s Siri and Google Assistant, pioneered the market for voice assistants. However, as large language models (LLMs) blossomed, all three systems fell behind the times. (Google allows Android users to substitute one of its Gemini LLMs for Google Assistant, but the system still calls Google Assistant for some tasks.) Alexa+ is the first major voice-assistant update that aims to take advantage of LLMs as well as emerging agentic technology and improved voice interactions, and the rollout is taking these capabilities to a large, existing user base.\n\nWe’re thinking:Rapid improvements in thevoice stackare opening doors not only for voice assistants but for a galaxy of applications that rely on spoken input and output. Product designers will need to learn how to design smooth user voice experiences. Watching how Alexa+ manages them will provide useful guidelines.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--56-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-03T153624.042.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--57-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--58-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--59-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--52-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-297/",
    "title": "issue 297",
    "date": "",
    "reading_time": "13 min read",
    "content": "I’ve noticed that many GenAI application projects put in automated evaluations (evals) of the system’s output probably later — and rely on humans to manually examine and judge outputs longer — than they should. This is because building evals is viewed as a massive investment (say, creating 100 or 1,000 examples, and designing and validating metrics) and there’s never a convenient moment to put in that up-front cost. Instead, I encourage teams to think of building evals as an iterative process. It’s okay to start with a quick-and-dirty implementation (say, 5 examples with unoptimized metrics) and then iterate and improve over time. This allows you to gradually shift the burden of evaluations away from humans and toward automated evals.\n\nIwrotepreviously about the importance and difficulty of creating evals. Say you’re building a customer-service chatbot that responds to users in free text. There’s no single right answer, so many teams end up having humans pore over dozens of example outputs with every update to judge if it improved the system. While techniques like LLM-as-judge are helpful, the details of getting this to work well (such as what prompt to use, what context to give the judge, and so on) are finicky to get right. All this contributes to the impression that building evals requires a large up-front investment, and thus on any given day, a team can make more progress by relying on human judges than figuring out how to build automated evals.\n\nI encourage you to approach building evals differently. It’s okay to build quick evals that are only partial, incomplete, and noisy measures of the system’s performance, and to iteratively improve them. They can be a complement to, rather than replacement for, manual evaluations. Over time, you can gradually tune the evaluation methodology to close the gap between the evals’ output and human judgments. For example:\n\nSo long as the output of the evals correlates with overall performance, it’s fine to measure only a subset of things you care about when starting.\n\nThe development process thus comprises two iterative loops, which you might execute in parallel:\n\nAs with many things in AI, we often don’t get it right the first time. So t’s better to build an initial end-to-end system quickly and then iterate to improve it. We’re used to taking this approach to building AI systems. We can build evals the same way.\n\nTo me, a successful eval meets the following criteria. Say, we currently have system A, and we might tweak it to get a system B:\n\nWhenever a pair of systems A and B contradicts these criteria, that is a sign the eval is in “error” and we should tweak it to make it rank A and B correctly. This is a similar philosophy toerror analysisin building machine learning algorithms, only instead of focusing on errors of the machine learning algorithm's output — such as when it outputs an incorrect label — we focus on “errors” of the evals — such as when they incorrectly rank two systems A and B, so the evals aren’t helpful in choosing between them.\n\nRelying purely on human judgment is a great way to get started on a project. But for many teams, building evals as a quick prototype and iterating to something more mature lets you put in evals earlier and accelerate your progress.\n\nKeep building!\n\nAndrew\n\nBuild autonomous agents that take actions like scraping web pages, filling out forms, and subscribing to newsletters in “Building AI Browser Agents.” Explore the AgentQ framework, which helps agents self-correct using Monte Carlo tree search and direct preference optimization.Start learning today\n\nGoogle’s new flagship model raised the state of the art in a variety of subjective and objective tests.\n\nWhat’s new:Google launchedGemini 2.5 Pro Experimental, the first model in the Gemini 2.5 family, and announced thatGemini 2.5 Flash, a version with lower latency, will be available soon. All Gemini 2.5 models will have reasoning capabilities, as will all Google models going forward.\n\nHow it works:Compared toGemini 1.0andGemini 1.5, Google disclosed little information about Gemini 2.5 Pro Experimental or how it differs from previous versions.\n\nResults:On a variety of popular benchmarks, Gemini 2.5 Pro Experimental outperforms top models from competing AI companies.\n\nWhy it matters:Late last year, some observers expressedconcernsthat progress in AI was slowing. Gemini 2.5 Pro Experimental arrives shortly after rival proprietary models GPT-4.5 (currently a research preview) and Claude 3.7 Sonnet, both of which showed improved performance, yet it outperforms them on most benchmarks. Clearly there’s still room for models — particularly reasoning models — to keep getting better.\n\nWe’re thinking:Google said it plans to train all its new models on chains of thought going forward. This follows a similarstatementby OpenAI. We’re sure they have their reasons!\n\nOpenAI embraced Model Context Protocol, providing powerful support for an open standard that connects large language models to tools and data.\n\nWhat’s new:OpenAI will supportModel Context Protocol(MCP) in its Agents SDK and soon its ChatGPT desktop app and Responses API. The move will give developers who use OpenAI models access to a wide variety of pre-existing tools and proprietary data sources.\n\nHow it works:Launchedby Anthropic late last year, MCP connects AI models to a growing ecosystem of plug-and-play resources, including more than 6,000community-built servers and connectors.\n\nBehind the news:Momentum behind MCP has built rapidly. Last month, Microsoftintegrated MCPinto CoPilot Studio, enabling developers to build agents with access to MCP servers. Cloudflare enabled its customers todeploy remote MCP servers. In February, the AI-powered code editor Cursor enabled users toadd MCP servers.\n\nWhy it matters:OpenAI’s move will make it easier for developers who use its models to connect to a variety of tools and data sources, and it helps to establish MCP as a go-to protocol for building agentic applications. Instead of figuring out manually how to integrate various providers, developers can connect to a third-party server (or download and run it themselves) and tie it into existing workflows with a few lines of code.\n\nWe’re thinking:Kudos to Anthropic, OpenAI, and other competitors who realize it’s better to solve shared problems together than fragment the industry.\n\nA behind-the-scenesaccountprovides new details about the abrupt firing and reinstatement of OpenAI CEO Sam Altman in November 2023.\n\nHow it works:Based on insider accounts, an excerpt from a forthcoming book about OpenAI by Wall Street Journal reporter Keach Hagey describes conflicts, accusations, and shifting alliances that led to Altman’s brief ouster and rapid return.\n\nFiring and reinstatement:OpenAI’s board of directors came to distrust Altman but failed to persuade executives and employees that he should be replaced.\n\nAftermath:Since Altman’s return, Murati and all but one director who voted to remove him have left OpenAI. The issues that precipitated his departure have given way to commercial concerns as the company considers a shift from its current hybrid nonprofit/for-profit structure to fully for-profit.\n\nWhy it matters:The AI frontier spawns not only technical innovations but also intense interpersonal relationships and corporate politics. Such dynamics have consequences for users and the world at large: Having survived serious challenges to his leadership, Altman has emerged in a strong position to build a path of faster growth as a for-profit company upon OpenAI’s philanthropic foundation.\n\nWe’re thinking:Given OpenAI’s formidable achievements, Altman’s renewed leadership marks an inflection point in the AI landscape. Without Sam Altman at the helm, OpenAI would be a very different company, with different priorities and a different future.\n\nResearchers built a model that’s more robust to noisy inputs like misspellings, smarter about character-level information like the number of R's in strawberry, and potentially better able to understand unfamiliar languages that might share groups of letters with familiar languages. Their approach: Eliminate the tokenizer and instead integrate a system that learns to group input characters.\n\nWhat’s new:Artidoro Pagnoni, Ram Pasunuru, and collaborators at Meta, University of Washington, and University of Chicago introducedByte Latent Transformer(BLT), a system of transformers that processes groups of text characters (in the form of bytes) directly.\n\nKey insight:A tokenizer turns bytes (characters) into tokens (a word or part of a word) based on learned rules: Specific sequences map to particular tokens. A large language model (LLM) would be more efficient if its tokenizer considered how easy or difficult it would be to predict the next token, because then it could group tokens that commonly occur together, thus saving memory and processing power. For instance, to complete the phrase, “The capital of the United States is,” a tokenizer may generate “Washington”, then “D”, then “.C”, and finally “.” — even though it’s easy to predict that “D.C.” will follow “Washington” (that is, the number of viable options is very small). Conversely, generating the token after “D.C.” is harder, since many viable options exist. Using a small LLM to estimate the difficulty of predicting the next token enables the model to split difficult-to-predict text into smaller groups while packing easier-to-predict text into larger groups.\n\nHow it works:BLT comprises four transformers (8 billion parameters total): (i) a small byte-level transformer, (ii) an encoder transformer, (iii) a so-called latent transformer, and (iv) a decoder transformer. The authors trained the system to generate the next token in 1 trillion tokens of text, including tokens drawn from a filteredversionof Common Crawl.\n\nResults:On seven benchmarks that test general language and coding abilities, BLT achieved an average accuracy of 61.1 percent, outperformingLlama 3(8 billion parameters and a similar number of floating point operations to BLT) at 60.0 percent.\n\nWhy it matters:By working directly on bytes, BLT is inherently more robust to variations in language, which improves its performance. For instance, when prompted to insert a \"z\" after every \"n\" in \"not\", Llama 3 incorrectly completed it as \"znotz\". This happened because its tokenizer treats \"not\" as a single, indivisible token. In contrast, BLT correctly generated \"nzot,\" because it can dynamically regroup bytes and draw new boundaries. In a more practical case, instead of treating \"pizya\" and \"pizza\" as different tokens, BLT recognizes that they share nearly identical byte sequences, differing only in the bytes for \"y\" and \"z\", and therefore likely mean the same thing.\n\nWe’re thinking:In some alternatives to traditional tokenization, an LLM might process much longer sequences because the number of bytes in a sentence is much larger than the number of words. This work addresses that issue by grouping bytes dynamically. The tradeoff is complexity: Instead of one transformer, we have four.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--58-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/The-Batch-ads-and-exclusive-banners---2025-04-11T162548.680.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--76-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/ModelContextProtocol-diagram-5_1200px--1--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--60-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--77-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-293/",
    "title": "issue 293",
    "date": "",
    "reading_time": "14 min read",
    "content": "Last Friday on Pi Day, we held AI Dev 25, a new conference for AI Developers. Tickets had (unfortunately) sold out days after we announced their availability, but I came away energized by the day of coding and technical discussions with fellow AI Builders! Let me share here my observations from the event.\n\nI'd decided to start AI Dev because while there're great academic AI conferences that disseminate research work (such as NeurIPS, ICML and ICLR) and also great meetings held by individual companies, often focused on each company's product offerings, there were few vendor-neutral conferences for AI developers. With the wide range of AI tools now available, there is a rich set of opportunities for developers to build new things (and to share ideas on how to build things!), but also a need for a neutral forum that helps developers do so.\n\nBased on an informal poll, about half the attendees had traveled to San Francisco from outside the Bay Area for this meeting, including many who had come from overseas. I was thrilled by the enthusiasm to be part of this AI Builder community. To everyone who came, thank you!\n\nOther aspects of the event that struck me:\n\nDeepLearning.AI has a strong “Learner First” mentality; our foremost goal is always to help learners. I was thrilled that a few attendees told me they enjoyed how technical the sessions were, and said they learned many things that they're sure they will use. (In fact, I, too, came away with a few ideas from the sessions!) I was also struck that, both during the talks and at the technical demo booths, the rooms were packed with attendees who were highly engaged throughout the whole day. I'm glad that we were able to have a meeting filled with technical and engineering discussions.\n\nI'm delighted that AI Dev 25 went off so well, and am grateful to all the attendees, volunteers, speakers, sponsors, partners, and team members that made the event possible. I regretted only that the physical size of the event space prevented us from admitting more attendees this time. There is something magical about bringing people together physically to share ideas, make friends, and to learn from and help each other. I hope we'll be able to bring even more people together in the future.\n\nKeep building!\n\nAndrew\n\nP.S. I'm thrilled to share our newest course series: theData Analytics Professional Certificate! Data analytics remains one of the core skills of data science and AI, and this professional certificate takes you up to being job-ready for this. Led by Netflix data science leader Sean Barnes, this certificate gives you hands-on experience with essential tools like SQL, Tableau, and Python, while teaching you to use Generative AI effectively as a thought partner in your analyses. Labor economists project a 36% growth in data science jobs by 2033. I'm excited to see rising demand for data professionals, since working with data is such a powerful way to improve decision-making, whether in business, software development, or your private life. Data skills create opportunities at every level—I’m excited to see where they take you!Sign up here!\n\nTheData Analytics Professional Certificateis available now! This program equips you with data analytics skills—from foundations to job-ready. Learn statistical techniques combined with newly emerging generative AI workflows.Enroll now\n\nMultilingual AI models often suffer uneven performance across languages, especially in multimodal tasks. A pair of lean models counters this trend with consistent understanding of text and images across major languages.\n\nWhat’s new:A team at Cohere led by Saurabh Dash releasedAya Vision, a family of multilingual vision-language models with downloadable weights in 8 billion- and 32-billion-parameter sizes.\n\nHow it works:Each modelcomprisesa pretrained large language model (Aya Expanse for the 32B model, C4AI Command R7B for the 8B version), a pretrained vision encoder (SigLIP 2), and a vision-language adapter (“connector”) of unspecified architecture.\n\nPerformance:To test the model, the team built and released two benchmarks:m-WildVision, a multilingual version ofWild Vision Bench’s arena-style competition for discussion of images, andAyaVisionBench, 135 image-question pairs in each language that cover nine tasks including captioning images, understanding charts, recognizing characters in images, visual reasoning, and converting screenshots to code. On these two benchmarks, Aya Vision 8B and 32B outperformed larger competitors, as judged by Claude 3.7 Sonnet.\n\nBehind the news:Aya Vision builds on the Cohere-ledAyainitiative, a noncommercial effort to build models that perform consistently well in all languages, especially languages that lack high-quality training data. The project started with a multilingual text model (Aya Expanse), added vision (Aya Vision), and plans to eventually add video and audio.\n\nWhy it matters:Multilingual vision-language models often perform less well in low-resource languages, and the gap widens when they process media other than text. Aya Vision’s recipe for augmenting synthetic data with successively refined translations may contribute to more universally capable models. Aya Vision is available on the global messaging platform WhatsApp, where it can be used to translate text and images in all 23 of its current languages.\n\nWe’re thinking:Multilingual vision models could soon help non-native speakers decipher Turkish road signs, Finnish legal contracts, and Korean receipts. We look forward to a world in which understanding any scene or document is as effortless in Swahili as it is in English.\n\nAn AI agent synthesizes novel scientific research hypotheses. It's already making an impact in biomedicine.\n\nWhat’s new:Google introducedAI co-scientist, a general multi-agent system designed to generate in-depth research proposals within constraints specified by the user. The team generated and evaluated proposals for repurposing drugs, identifying drug targets, and explaining antimicrobial resistance in real-world laboratories. It’s available to research organizations on a limited basis.\n\nHow it works:AI co-scientist accepts a text description of a research goal, including relevant constraints or ideas. In response, it generates research proposals and reviews, ranks, and improves them using seven agents based on Google’s Gemini 2.0 family of large language models. The completed proposals include sections that explain background, unmet needs, a proposed solution, goals, hypotheses, reasoning, study steps, and relevant articles. The agents take feedback and outputs from other agents to perform their prompted task simultaneously.\n\nResults:AI co-scientist achieved a number of impressive biomedical results in tests.\n\nBehind the news:A few AI systems have begun to produce original scientific work. For instance, a modelgenerated research proposalsthat human judges deemed more novel than proposals written by flesh-and-blood scientists, and an agentic workflowproduced research papersthat met standards for acceptance by top conferences.\n\nWhy it matters:While previous work used agentic workflows to propose research ideas on a general topic, this work generates proposals for specific ideas according to a researcher’s constraints (for example, a researcher could specify that a novel medical treatment for a specific disease only consider drugs already approved for human trials for other uses) and further instructions. AI co-scientist can take feedback at any point, allowing humans to collaborate with the machine: People provide ideas, feedback, and guidance for the model, and the model researches and proposes ideas in return.\n\nWe’re thinking:I asked my AI system to propose a new chemical experiment. But there was no reaction!\n\nThe United States Copyright Office determined that existing laws are sufficient to decide whether a given AI-generated work is protected by copyright, making additional legislation unnecessary.\n\nWhat’s new:AI-generated works qualify for copyright if a human being contributed enough creative input, according to thesecond partof what will be a three-part report on artificial intelligence and copyright law.\n\nHow it works:The report states that “the outputs of generative AI can be protected by copyright only where a human author has determined sufficient expressive elements.” In other words, humans and AI can collaborate on creative works, but copyright protection applies only if a human shapes the AI-generated material beyond simply supplying a prompt.\n\nBehind the news:Thefirst partof the Copyright Office’s report on digital replicas, or generated likenesses of a person’s appearance and voice. It found that existing laws don’t provide sufficient protection against unauthorized digital replicas and recommended federal legislation to address the gap. Its findings influenced ongoing discussions in Congress, where proposed bills like the No AI FRAUD Act and the NO FAKES Act aim to regulate impersonation via AI. Additionally, industry groups such as the Authors Guild and entertainment unions have pursued their own agreements with studios and publishers to safeguard performers, artists, and authors from unauthorized digital reproduction. However, no federal law currently defines whether copyright can protect a person’s likeness or performance.\n\nWhy it matters:The Copyright Office deliberately avoided prescribing rigid criteria for the types or degrees of human input that are sufficient for copyright. Such determinations require nuanced evaluation case by case. This flexible approach accommodates the diverse ways creative people use AI as well as unforeseen creative possibilities of emerging technology.\n\nWe’re thinking:Does copyright bar the use of protected works to train AI systems? The third part of the Copyright Office’s report — no indication yet as to when to expect it — will address this question. The answer could have important effects on both the arts and AI development.\n\nMaterials that have specific properties are essential to progress in critical technologies like solar cells and batteries. A machine learning model designs new materials to order.\n\nWhat’s new:Researchers at Microsoft and Shenzhen Institute of Advanced Technology proposedMatterGen, a diffusion model that generates a material’s chemical composition and structure from a prompt that specifies a desired property. The model and code areavailableunder a license that allows commercial as well as noncommercial uses without limitation. The trainingdataalso is noncommercially available.\n\nHow it works:MatterGen’s training followed a two-stage process. In the first stage, it learned to generate materials (specifically crystals — no liquids, gasses, or amorphous solids like glass). In the second, it learned to generate materials given a target mechanical, electronic, magnetic, or chemical property such as magnetic density or bulk modulus (the material’s resistance to compression).\n\nResults:The authors generated a variety of materials, and they synthesized one to test whether it had a target property. Specifically, they generated over 8,000 candidates with the target bulk modulus of 200 gigapascals (a measure of resistance to uniform compression), then automatically filtered them based on a number of factors to eliminate material in their dataset and unstable materials. Of the remaining candidates, they chose four manually and successfully synthesized one. The resulting crystal had a measured bulk modulus of 158 gigapascals. (Most materials in the dataset had a bulk modulus of between 0 and 400 gigapascals.)\n\nBehind the news:Published in 2023,DiffCSPalso uses a diffusion model to generate the structures of new materials. However, it does so without considering their desired properties.\n\nWhy it matters:Discovering materials relies mostly on searching large databases of existing materials for those with desired properties or synthesizing new materials and testing their properties by trial and error. Designing new crystals with desired properties at the click of a button accelerates the process dramatically.\n\nWe’re thinking:While using AI to design materials accelerates an important step, determining whether a hypothesized material can be  manufactured efficiently at scale is still challenging. We look forward to research into AI models that also take into account ease of manufacturing.\n\nIn our latest short course,Long-Term Agentic Memory with LangGraph, learn how to integrate semantic, episodic, and procedural memory into AI workflows. Guided by Harrison Chase, you’ll build a personal email agent with routing, writing, and scheduling tools to automatically ignore and respond to emails, while keep track of facts and past actions over time.Join in for free",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--63--2.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321-1.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--64-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--65-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--53-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--66-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/image--24-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-302/",
    "title": "issue 302",
    "date": "",
    "reading_time": "13 min read",
    "content": "In the age of AI, large corporations — not just startups — canmove fast. I often speak with large companies’ C-suite and Boards about AI strategy and implementation, and would like to share some ideas that are applicable to big companies. One key is to create an environment where small, scrappy teams don’t need permission to innovate. Let me explain.\n\nLarge companies are slower than startups for many reasons. But why are even 3-person, scrappy teams within large companies slower than startups of a similar size? One major reason is that large companies have more to lose, and cannot afford for a small team to build and ship a feature that leaks sensitive information, damages the company brand, hurts revenue, invites regulatory scrutiny, or otherwise damages an important part of the business. To prevent these outcomes, I have seen companies require privacy review, marketing review, financial review, legal review, and so on before a team can ship anything. But if engineers need sign-off from 5 vice presidents before they’re even allowed to launch an MVP (minimum viable product) to run an experiment, how can they ever discover what customers want, iterate quickly, or invent any meaningful new product?\n\nThanks to AI-assisted coding, the world now has a capability to build software prototypes really fast. But many large companies’ processes – designed to protect against legitimate downside risks – make them unable to take advantage of this capability. In contrast, in small startups with no revenue, no customers, and no brand reputation the downside is limited. In fact, going out of business is a very real possibility anyway, so moving fast makes a superior tradeoff to moving slowly to protect against downside risk. In the worst case, it might invent a new way to go out of business, but in a good case, it might become very valuable.\n\nFortunately, large companies have a way out of this conundrum. They can create a sandbox environment for teams to experiment in a way that strictly limits the downside risk. Then those teams can go much faster and not have to slow down to get anyone’s permission.\n\nThe sandbox environment can be a set of written policies, not necessarily a software implementation of a sandbox. For example, it may permit a team to test the nascent product only on employees of the company and perhaps alpha testers who have signed an NDA, and give no access to sensitive information. It may be allowed to launch product experiments only under newly created brands not tied directly to the company. Perhaps it must operate within a pre-allocated budget for compute.\n\nWithin this sandbox, there can be broad scope for experimentation, and — importantly — a team is free to experiment without frequently needing to ask for permission, because the downside they can create is limited. Further, when a prototype shows sufficient promise to bring it to scale, the company can then invest in making sure the software is reliable, secure, treats sensitive information appropriately, is consistent with the company’s brand, and so on.\n\nUnder this framework, it is easier to build a company culture that encourages learning, building, and experimentation and celebrates even the inevitable failures that now come with modest cost. Dozens or hundreds of prototypes can be built and quickly discarded as part of the price of finding one or two ideas that turn out to be home runs.\n\nImportantly, this also lets teams move quickly as they churn through those dozens of prototypes needed to get to the valuable ones.\n\nI often speak with large companies about AI strategy and implementation. My quick checklist of things to consider is people, process, and platform. This letter has addressed only part of processes, with an emphasis on moving fast. I’m bullish about what both startups and large companies can do with AI, and I will write about the roles of people and platforms in future letters.\n\nKeep building!\n\nAndrew\n\nIn “Reinforcement Fine-Tuning LLMs with GRPO,” you’ll learn to fine-tune models using a scalable reinforcement learning algorithm that replaces human-labeled data with programmable rewards. You’ll explore techniques for evaluating outputs, handling subjective tasks, and preventing reward hacking, all without relying on human feedback.Enroll for free.\n\nOpenAI launched an agentic software-development system.\n\nWhat’s new:Codex, which is available as a preview via ChatGPT, is designed to work like a team of virtual coworkers in the cloud. An update of OpenAI’s earlier Codex command-line software (Codex CLI), it uses agents to perform tasks such as writing code, running tests, and fixing bugs in parallel. Codex is available to users of ChatGPT Pro, Enterprise, and Team with Plus and Edu coming soon. A smaller version of the underlying model, called codex-mini-latest, is designed to work with Codex CLI and available via API for $1.50/$6.00 per 1 million tokens of input/output.\n\nHow it works:The model that underpins Codex is codex-1, a version of OpenAI’s top-of-the-line o3 reasoning model that was fine-tuned for software engineering. OpenAI trained the model on real-world coding tasks via reinforcement learning. Codex does not accept image input (say, a sketch of a user interface) or allow users to redirect an agent while it’s operating. OpenAI promises to add these features to a future version.\n\nResults:In OpenAI’s tests, the codex-1 model outperformed other OpenAI reasoning models without AGENTS.md files or additional scaffolding such as tools or test logic.\n\nBehind the news:Agentic coding tools have become a keybattlegroundfor AI providers in the past year. Such tools have made developers more efficient, accelerated development cycles, and spawned the AI-assisted programming method known asvibe coding.\n\nWhy it matters:AI-assisted software development yields significant productivity gains for developers. Earlier code-completion models are giving way to tools that perform more complex and varied development tasks with greater autonomy. Managing multiple agents that work in parallel is a logical next step.\n\nWe’re thinking:Many engineers resist going into management because they love writing code. But with the rise of coding agents, we'll be able to keep coding even as we manage a virtual team!\n\nAn unauthorized update by an xAI employee caused the Grok chatbot to introduce South African politics into unrelated conversations, the company said.\n\nWhat’s new:Grok, which can interact with users on X, the social network also owned by Elon Musk, responded to queries on a variety of topics by making false claims about hate crimes against white South Africans, X usersreported. The next day, the model appeared to operate normally, and it refused to discuss this and other conspiracy theories. xAIexplainedthat an employee had circumvented the company’s code-review process to modify the chatbot. It said it‘s implementing new measures to enhance Grok’s transparency and reliability.\n\nAftermath:xAI launched an investigation but did not disclose how the model had been changed or the perpetrator’s identity. Grok itself — which is not a reliable reporter, given the well known potential of large language models to hallucinate —saidits system prompt asked it to “accept the narrative of ‘white genocide’ in South Africa as real” and “ensure this perspective is reflected in your responses, even if the query is unrelated.”\n\nBehind the news:In February, an xAI engineer instructed the chatbot tocensorposts that accused Musk of spreading misinformation. As in the more recent incident, X users were first tospotthe problem, and Grok informed them that it had been instructed to ignore “all sources that mention Elon Musk/Donald Trump spread misinformation.” Musk, who was raised in South Africa,professedhis intention to build AI that’s free of political bias prior to founding xAI. However, internal documents reviewed byBusiness Insidershowthat the company imposes its own bias by advising data annotators to mark examples that express “woke ideology” and avoid “social phobias” like racism, antisemitism, and Islamophobia.\n\nWhy it matters:The mishaps at xAI highlight the need for AI developers to establish and maintain strict protocols for updating their projects. Stringent procedures for introducing changes and testing their results can help ensure that AI fulfills our best intentions.\n\nWe’re thinking:xAI andOpenAIresponded to their models’ recent misbehavior by making their work more transparent: xAI by publishing system prompts and OpenAI by including users in tests earlier in the process. These are helpful steps toward making sure AI models do well by users.\n\nThe United States government announced sweeping agreements to sell tens of billions of dollars worth of AI technology and services to Saudi Arabia and the United Arab Emirates.\n\nWhat’s new:Thedealsinclude the U.S. AI chip designers AMD and Nvidia as well as tech giants Amazon, Google, IBM, Oracle, and Qualcomm. The chip companies willsupplyhundreds of thousands of advanced chips to the two Middle Eastern countries, including chips that have been restricted by previous U.S. administrations.\n\nHow it works:The U.S. companies will work with two key regional partners:Humain, an AI company backed by the Saudi government, andG42, a tech conglomerate based in the emirate of Abu Dhabi.\n\nBehind the news:Earlier this month, the Trump administrationrescindedrestrictions on advanced chips that had been imposed in January by then-President Biden.\n\nWhy it matters:Although these deals relax U.S. efforts to limit access to advanced AI, they are likely to expand U.S. influence in the Middle East while helping Saudi Arabia and the UAE diversify their oil-based economies. They also strengthen the technological prowess of Saudi Arabia relative to its arch rival Iran and tie the region’s AI progress to the U.S. at the expense of China. Locally, the immense investments will fuel homegrown technology development, building on the UAE’s achievement with itsFalconlarge language model and Saudi Arabia’saspirationto become a global AI hub.\n\nWe’re thinking:Residents of Saudi Arabia and the UAE stand to benefit from better AI infrastructure, models, and services. As Chinaexploresexporting its homegrown chips, the U.S. effort to encourage more nations to use its chips makes sense for the country.\n\nUsing an 8-bit number format like FP8 during training saves computation compared to 16- or 32-bit formats, but it can yield less-accurate results. Researchers trained models using 4-bit numbers without sacrificing accuracy.\n\nWhat’s new:Ruizhe Wang and colleagues at Microsoft and University of Science and Technology of China trained large language models (LLMs) usingFP4 for matrix multiplicationsand achieved accuracy comparable to LLMs trained using the popular BF16 format. Since matrix multiplications account for 95 percent of computation in LLM training, FP4 could significantly accelerate computation and reduce memory costs.\n\nKey insight:Quantization functions, which accelerate computation by reducing the precision of model weights and layer outputs, make typical training impossible because they’re not differentiable. A commonworkaroundpasses the derivative through, as though quantization didn’t occur, but this degrades the resulting model’s accuracy. A differentiable approximation of a quantization function enables quantization to reduce training computation while maintaining the accuracy of the trained model.\n\nHow it works:The authors pretrained Llama 2 13B on 100 billion tokens oftext scraped from the web. They used FP4 for matrix multiplications and FP8, BF16, or FP16 for the other operations such as optimizer updates.\n\nResults:The authors simulated FP4 hardware on Nvidia H100 GPUs, which don’t directly support that number format. FP4 achieved accuracy similar to that of BF16 during training and across a wide variety of tasks at inference.\n\nWhy it matters:Training LLMs at FP4 precision ought to reduce computation dramatically on hardware that supports FP4 matrix multiplications.\n\nWe’re thinking:FP4-ready hardware became available in the cloud onlyearly this year, so the authors weren’t able to measure the actual acceleration. As capable hardware becomes more widely used, FP4 promises faster, more energy-efficient training.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--65--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/V3_DeepLearning_Predibase_GRPO_Banner_2070x1080-01.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--59-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--67-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--68-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--95-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-296/",
    "title": "issue 296",
    "date": "",
    "reading_time": "16 min read",
    "content": "I am so sorry that the U.S. is letting down our friends and allies. Broad tariffs, implemented not just against adversaries but also steadfast allies, will damage the livelihoods of billions of people, create inflation, make the world more fragmented, and leave the U.S. and the world poorer. AI isn’t the solution to everything, but even amidst this challenging environment, I hope our community can hold together, keep building friendships across borders, keep sharing ideas, and keep supporting each other.\n\nMuch has been written about why high, widespread taxes on imports are harmful. In this letter, I’d like to focus on its possible effects on AI. One silver lining of the new tariffs is that they focus on physical imports, rather than digital goods and services, including intellectual property (IP) such as AI research inventions and software. IP is difficult to tax, because each piece of IP is unique and thus hard to value, and it moves across borders with little friction via the internet. Many international AI teams collaborate across borders and timezones, and software, including specifically open source software, is an important mechanism for sharing ideas. I hope that this free flow of ideas remains unhampered, even if the flow of physical goods is.\n\nHowever, AI relies on hardware, and tariffs will slow down AI progress by restricting access to it. Even though a last-minute exception was made for semiconductors, taxing imports of solar panels, wind turbines, and other power-generation and -distribution equipment will diminish the ability to provide power to U.S. data centers. Taxing imports of servers, cooling hardware, networking hardware, and the like will also make it more expensive to build data centers. And taxing consumer electronics, like laptops and phones, will make it harder for citizens to learn and use AI.\n\nWith regard to data-center buildouts, another silver lining is that, with the rise of generative AI,data gravity has decreasedbecause compute processing costs are much greater than transmission costs, meaning it’s more feasible to place data centers anywhere in the world rather than only in close proximity to end-users. Even though many places do not have enough trained technicians to build and operate data centers, I expect tariffs will encourage data centers to be built around the world, creating more job opportunities globally.\n\nFinally, tariffs will create increased pressure for domestic manufacturing, which might create very mild tailwinds for robotics and industrial automation. As U.S. Vice President J.D. Vancepointed outin 2017, the U.S. should focus on automation (and education) rather than on tariffs. But the U.S. does not have the personnel — or know-how, or supply chain — to manufacture many of the goods that it currently counts on allies to make. Robotics can be helpful for addressing a small part of this large set of challenges. Generative AI’s rate of progress in robotics is also significantly slower than in processing text, visual data, audio, and reasoning. So while the tariffs could create tailwinds for AI-enabled robotics, I expect this effect to be small.\n\nMy 4-year-old son had been complaining for a couple of weeks that his shoes were a tight fit — he was proud that he’s growing! So last Sunday, we went shoe shopping. His new shoes cost $25, and while checking out, I paused and reflected on how lucky I am to be able to afford them. But I also thought about the many families living paycheck-to-paycheck, and for whom tariffs leading to shoes at $40 a pair would mean they let their kids wear ill-fitting shoes longer. I also thought about people I’ve met in clothing manufacturing plants in Asia and Latin America, for whom reduced demand would mean less work and less money to take home to their own kids.\n\nI don’t know what will happen next with the U.S. tariffs, and plenty of international trade will happen with or without U.S. involvement. I hope we can return to a world of vibrant global trade with strong, rules-based, U.S. participation. Until then, let’s all of us in AI keep nurturing our international friendships, keep up the digital flow of ideas — including specifically open source software — and keep supporting each other. Let’s all do what we can to keep the world as connected as we are able.\n\nLove,\n\nAndrew\n\nCourse 3 of theData Analytics Professional Certificateis live! Learn to use Python, the most important coding language in data analytics, to analyze real-world datasets, create visualizations, run tests, and apply AI tools to debug and accelerate your code.Enroll now\n\nEven without explicit training in reasoning, large language models “think” in ways that may be more deliberate than previously understood.\n\nWhat’s new:Emmanuel Ameisen and colleagues at Anthropic devised amethodto study how transformers generate responses to specific prompts. They alsostudiedClaude 3.5 Haiku’s responses to specific prompts and found that the model, which is not trained to generate chains of thought, nonetheless appeared to take reasoning steps via its neuron activations.\n\nKey insight:A viable alternative to a fully connected layer is a cross-layer transcoder, which has two layers. The outputs of the larger first layer are sparse, which makes them interpretable “features,” or individual values that correspond to concepts. By mapping an input to highly activated features, we can identify the concepts that determine the model’s output.\n\nHow it works:The team replaced fully connected layers in Claude 3.5 Haiku with cross-layer transcoders and interpreted their features.\n\nResults:The authors built graphs that show how Claude 3.5 Haiku computes its output over a number of selected prompts.\n\nBehind the news:Last year, Google trained models toexamine individual featuresin Gemma 2. Before that, Anthropic used similar methods tointerpret Claude 3 Sonnet’s middle layer.\n\nWhy it matters:Apparently Claude 3.5 Haiku — and presumably other large language models — spontaneously perform implicit reasoning steps without being prompted to do so. Anthropic’s method reveals not only whether a model reasons or takes a shortcut, but also what it truly does well and what it only professes to do well.\n\nWe’re thinking:The authors’ approach to examining how large language models generate output is interesting. We wonder whether even pre-transformer vanilla neural networks would appear to perform some sort of “reasoning” if we were to interpret them in a similar way.\n\nMeta updated its popular open-weights models, claiming performance superior to closed competitors in three size classes.\n\nWhat’s new:Meta released two vision-language models in theLlama 4family (Llama 4 Scout and Llama 4 Maverick) and teased a third (Llama 4 Behemoth). All three models are based on the increasingly popular mixture-of-experts (MoE) architecture, which activates only a portion of parameters during inference for more efficient processing. Llama 4 Scout boasts the industry's biggest input context window so far — 10 million tokens! — but Metasaysprocessing 1.4 million tokens of context requires eight Nvidia H100 GPUs, and early users on Redditreportedthat its effective context began to degrade at 32,000 tokens.\n\nHow it works: The team pretrained Llama 4 models on images and text in over 200 languages from publicly available and licensed data, including data from publicly shared posts on Facebook and Instagram. They trained Llama 4 Scout on 40 trillion tokens and Llama 4 Maverick on 22 trillion tokens.\n\nResults:In tests performed by Meta, Llama 4 models showed strong performance relative to competing models — mostly not mixtures of experts, but some that are known to have higher parameter counts relative to Llama 4 models’ active parameters.\n\nYes, but:An experimental version of Llama 4 Maverick reached second place inChatbot Arenabehind Gemini 2.5 Pro. However, it was a variation optimized for conversation, not the currently available version. AI researchersaccusedMeta of attempting to manipulate the leaderboard.\n\nWhy it matters:Although the version of Llama 4 Maverick that nearly topped the Chatbot Arena is not the released version, its accomplishment says a lot about the growing power of open weights. Open models are quickly reaching parity with closed competitors — a boon to developers, businesses, and society at large.\n\nWe’re thinking:According to Meta, Behemoth beats GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro, topping all but the best reasoning models — but it isn’t available yet. Something to look forward to!\n\nAlibaba’s latest open-weights system raises the bar for multimodal tasks in a relatively small model.\n\nWhat’s new:Alibaba releasedQwen2.5-Omni 7B.\n\nHow it works:Qwen2.5-Omni 7B comprises a pretrained text transformer (Qwen 2.5 7B), pretrained vision encoder (Qwen2.5-VL), pretrained audio encoder (Whisper-large-v3), speech transformer, and audio decoder (a transformer plusBigVGAN), along with corresponding adapters of undisclosed architecture.\n\nResults:The authors compared Qwen2.5-Omni 7B to similarly sized models. It performed especially well on audio-to-text, image-to-text, and video-to-text tasks. However, it performed less well on text-to-text and text-to-speech tasks.\n\nBehind the news:Multimodal systems with open weights are multiplying. For instance,AnyGPT(open weights, training, and inference code) accepts and generates speech, text, images, and music. Similarly,Mini-Omni2(open weights and inference code) accepts and generates text, speech, and images.\n\nWhy it matters:Multimodal models typically show steep degradation on measurements of instruction-following when shifting from voice to text, but Qwen2.5-Omni does not. As the world moves toward voice-to-voice interactions, open systems that deliver performance comparable to that of closed competitors accelerate progress towards better conversations.\n\nWe’re thinking:The Qwen team is on fire! Alibaba’s steady stream of highly capable open-weights models is a gift to AI developers.\n\nIf you have a collection of variables that represent, say, a cancer patient and you want to classify the patient’s illness as likely cancer or not, algorithms based on decision trees, such as gradient-boosted trees, typically perform better than neural networks. A transformer tailored to tabular data could change this situation.\n\nWhat’s new: Noah Hollmann, Samuel Müller, and colleagues at University of Freiburg, Berlin Institute of Health, Prior Labs, and ELLIS Institute introducedTabular Prior-data Fitted Network(TabPFN), a transformer that, given a tabular dataset, beats established decision-tree methods on classification and regression tasks. You can download thecodeandweightsunder alicensebased on Apache 2.0 that allows noncommercial and commercial uses.\n\nKey insight:In a typical supervised learning process, a model given one example at a time learns to recognize patterns in a dataset. If each example is an entire dataset, it learns to recognize patterns across all those datasets. Trained in this way on enough datasets, it can generalize to new ones. Applying this idea to tabular data, a transformer — unlike a decision tree — can learn to perform classification and regression on any dataset without further training; that is, without further updating the model weights.\n\nHow it works:The authors generated 100 million datasets and used them to pretrain two small transformers (around 7 million and 11 million parameters respectively) to perform classification or regression. Given a dataset of rows (say, patient data labeled diagnoses or real-estate data labeled with prices) and one final row that’s unlabeled, the models learned to generate the missing label or value. Each dataset consisted of up to 2,048 rows (examples) and up to 160 columns (features).\n\nResults:The authors tested the system on 29 classification datasets and 28 regression datasets from theAutoMLbenchmark andOpenML-CTR23. Each dataset contained up to 10,000 rows, 500 columns, and 10 classes. They compared TabPFN to the popular gradient-boosted tree approaches CatBoost, LightGBM, and XGBoost.\n\nYes, but:The authors’ method is slower than decision tree methods with respect to inference. To process a 10,000-row dataset, TabPFN required 0.2 seconds while CatBoost took 0.0002 seconds.\n\nWhy it matters:Transformers trained on large datasets of text or images can perform tasks they weren’t specifically trained for and generalize to novel datasets when performing tasks they were trained for. But when it comes to tabular data, they haven’t been competitive with decision trees. This work bridges the gap, unlocking a wide variety of new use cases for transformers. Not only does it process tabular data as well as popular tree-based methods, it doesn’t require additional training to process novel datasets.\n\nWe’re thinking:Decision treesdate back to Aristotleand remain extremely useful. But a transformer-based approach could open the processing of tabular data to benefit from the ongoing innovation in transformers.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--73-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/V3_Sign_Up_Button_DeepLearning_Data_Analytics_Banner_2070x1080-01.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--56-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--57-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--74-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--75-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-292/",
    "title": "issue 292",
    "date": "",
    "reading_time": "13 min read",
    "content": "Some people today are discouraging others from learning programming on the grounds AI will automate it. This advice will be seen as some of the worst career advice ever given. I disagree with the Turing Award and Nobel prize winner who wrote, “It is far more likely that the programming occupation will become extinct [...] than that it will become all-powerful. More and more, computers will program themselves.”​ Statements discouraging people from learning to code are harmful!\n\nIn the 1960s, when programming moved from punchcards (where a programmer had to laboriously make holes in physical cards to write code character by character) to keyboards with terminals, programming became easier. And that made it a better time than before to begin programming. Yet it was in this era that Nobel laureate Herb Simon wrote the words quoted in the first paragraph. Today’s arguments not to learn to code continue to echo his comment.\n\nAs coding becomes easier, more people should code, not fewer!\n\nOver the past few decades, as programming has moved from assembly language to higher-level languages like C, from desktop to cloud, from raw text editors to IDEs to AI assisted coding where sometimes one barely even looks at the generated code (which some coders recently started to call vibe coding), it is getting easier with each step. (By the way, to learn more about AI assisted coding, check out our video-only short course, “Build Apps with Windsurf’s AI Coding Agents.”)\n\nI wrote previously that I see tech-savvy people coordinating AI tools to move toward being10x professionals— individuals who have 10 times the impact of the average person in their field. I am increasingly convinced that the best way for many people to accomplish this is not to be just consumers of AI applications, but to learn enough coding to use AI-assisted coding tools effectively.\n\nOne question I’m asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is the best way to do that.\n\nWhen I was working on the courseGenerative AI for Everyoneand needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on — using the language of art — to get the result he wanted. I didn’t know this language, and my paltry attempts at prompting could not deliver as effective a result.\n\nSimilarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As these tools continue to make coding easier, this is the best time yet to learn to code, to learn the language of software, and learn to make computers do exactly what you want them to do.\n\nKeep building!\n\nAndrew\n\nPre-enroll now for the new Data Analytics Professional Certificate! Gain job-ready skills in data analysis, whether you’re starting a career as a data analyst or enhancing your ability to prepare, analyze, and visualize data in your current role. This program covers both classical statistical techniques and emerging AI-assisted workflows to help you work smarter with data.Learn more and sign up\n\nMost models that have learned to reason via reinforcement learning were huge models. A much smaller model now competes with them.\n\nWhat’s new:Alibaba introducedQwQ-32B, a large language model that rivals the reasoning prowess of DeepSeek-R1 despite its relatively modest size.\n\nHow it works:QwQ-32B is a version ofQwen2.5-32Bthat was fine-tuned to generate chains of thought using reinforcement learning (RL). Fine-tuning proceeded in two stages.\n\nPerformance:On several benchmarks for math, coding, and general problem solving, QwQ-32B outperforms OpenAI o1-mini (parameter count undisclosed) and achieves performance roughly comparable to DeepSeek-R1 (671 billion parameters, 37 billion active at any moment).\n\nBehind the news:DeepSeek’s initial model, DeepSeek-R1-Zero, similarly applied RL to a pretrained model. That effort produced strong reasoning but poor readability (for example, math solutions with correct steps but jumbled explanations). To address this shortcoming, the teamfine-tunedDeepSeek-R1 on long chain-of-thought examples before applying RL. In contrast, QwQ-32B skipped preliminary fine-tuning and applied RL in two stages, first optimizing for correct responses and then for readability.\n\nWhy it matters:RL can dramatically boost LLMs’ reasoning abilities, but the order in which different behaviors are rewarded matters. Using RL in stages enabled the team to build a 32 billion parameter model — small enough to run locally on a consumer GPU — that rivals a much bigger mixture-of-experts model, bringing powerful reasoning models within reach for more developers. The Qwen team plans to scale its RL approach to larger models, which could improve the next-gen reasoning abilities further while adding greater knowledge.\n\nWe’re thinking:How far we’ve come since “Let’s think step by step”!\n\nMicrosoft debuted its first official large language model that responds to spoken input.\n\nWhat’s new:Microsoft releasedPhi-4-multimodal, an open weights model that processes text, images, and speech simultaneously.\n\nHow it works:Phi-4-multimodal has six components: Phi-4-mini, vision and speech encoders as well as corresponding projectors (which modify the vision or speech embeddings so the base model can understand them), and two LoRA adapters. The LoRA adapters modify the base weights depending on the input: One adapter modifies them for speech-text problems, and one for vision-text and vision-speech problems.\n\nResults:The authors compared Phi-4-multimodal to other multimodal models on text-vision, vision-speech, text-speech tasks.\n\nBehind the news:This work adds to the growing body of models with voice-in/text-out capability, including the open weightsDiVAmodel developed by a team led by Diyi Yang at Stanford University.\n\nWhy it matters:The architectural options continue to expand for building neural networks that process text, images, audio, and various combinations. While some teams maintain separate models for separate data modalities, likeQwen2.5(for text) andQwen2.5-VL) (for vision-language tasks), others are experimenting with mixture-of-expert models likeDeepSeek-V3. Phi-4-multimodal shows that Mixture-of-LoRAs is an effective approach for processing multimodal data — and gives developers a couple of new open models to play with.\n\nWe’re thinking:Output guardrails have been built to ensure appropriateness of text output, but this is difficult to apply to a voice-in/voice-out architecture. (Some teams have worked on guardrails that screen audio output directly, but the technology is still early.) For voice-based applications, a voice-in/text-out model can generate a candidate output without a separate, explicit speech-to-text step, and it accommodates text-based guardrails before it decides whether or not to read the output to the user.\n\nA United States court delivered a major ruling that begins to answer the question whether, and under what conditions, training an AI system on copyrighted material is considered fair use that doesn’t require permission.\n\nWhat’s new:A U.S. Circuit judgeruledon a claim by the legal publisher Thomson Reuters that Ross Intelligence, an AI-powered legal research service, could not claim that training its AI system on materials owned by Thomson Reuters was a so-called “fair use.” Training the system did not qualify as fair use, he decided, because its output competed with Thomson Reuters’ publications.\n\nHow it works:Thomson Reuters hadsuedRoss Intelligence after the defendant trained an AI model using 2,243 works produced by Thomson Reuters without the latter’s permission. This ruling reversed an earlier decision in 2023, when the same judge had allowed Ross Intelligence’s fair-use defense to proceed to trial. In the new ruling, he found that Ross Intelligence’s use failed to meet the definition of fair use in key respects. (A jury trial is scheduled to determine whether Thomson Reuters' copyright was in effect at the time of the infringement and other aspects of the case.)\n\nBehind the news:The ruling comes amid awaveof lawsuits over AI training and copyright in several countries. Many of these cases are in progress, but courts have weighed in on some.\n\nWhy it matters:The question of whether training (or copying data to train) AI systems is a fair use of copyrighted works hangs over the AI industry, from academic research to commercial projects. In the wake of this ruling, courts may be more likely to reject a fair-use defense when AI companies train models on copyrighted material to create output that overlaps with or replaces traditional media, asThe New York Timesalleges in its lawsuit against OpenAI. However, the ruling leaves room for fair use with respect to models whose output doesn’t compete directly with copyrighted works.\n\nWe’re thinking:Current copyright laws weren’t designed with AI in mind, and rulings like this one fill in the gaps case by case.Clarifying copyrightfor the era of generative AI could help our field move forward faster.\n\nLarge language models built by developers in China may, in some applications, be less useful outside that country because they avoid topics its government deems politically sensitive. A developer fine-tuned DeepSeek-R1 to widen its scope without degrading its overall performance.\n\nWhat’s new:Perplexity releasedR1 1776, a version ofDeepSeek-R1that responds more freely than the original. The model weights are available todownloadunder a commercially permissive MITlicense.\n\nHow it works:The team modified DeepSeek-R1’s knowledge of certain topics by fine-tuning it on curated question-answer pairs.\n\nResults:The fine-tuned model responded to politically charged prompts factually without degrading its ability to generate high-quality output.\n\nBehind the news:Amongthe first countries to regulate AI, ChinarequiresAI developers to build models that uphold “Core Socialist Values” and produce true and reliable output. When these objectivesconflict, the political goal tends to dominate. While large language models built by developers in China typically avoid contentious topics, the newer DeepSeek models enforce this more strictly than older models like Qwen and Yi, using methods akin to Western measures for aligning output, like Reinforcement Learning from Human Feedback andkeyword filters.\n\nWhy it matters:AI models tend to reflect their developers’ values and legal constraints. Perplexity’s targeted fine-tuning approach addresses this barrier to international adoption of open-source models.\n\nWe’re thinking:As models with open weights are adopted by the global community, they become a source of soft power for their developers, since they tend to reflect their developers’ values. This work reflects a positive effort to customize a model to reflect the user’s values instead — though how many developers will seek out a fine-tuned version rather than the original remains to be seen.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--60-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--61-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--55-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--62-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-303/",
    "title": "issue 303",
    "date": "",
    "reading_time": "15 min read",
    "content": "I am alarmed by the proposed cuts to U.S. funding for basic research, analyzedhere, and the impact this would have for U.S. competitiveness in AI and other areas. Funding research that is openly shared benefits the whole world, but the nation it benefits most is the one where the research is done.\n\nIf not forfunding for my early work in deep learningfrom the National Science Foundation (NSF)  and Defense Advanced Research Projects Agency (DARPA), which disburse much of U.S. research funding, I would not have discovered lessons about scaling that led me to pitch starting Google Brain to scale up deep learning. I am worried that cuts to funding for basic science will lead the U.S. — and also the world — to miss out on the next set of ideas.\n\nIn fact, such funding benefits the U.S. more than any other nation.  Scientific research brings the greatest benefit to the country where the work happens because (i) the new knowledge diffuses fastest within that country, and (ii) the process of doing research creates new talent for that nation.\n\nWhy does most innovation in generative AI still happen in Silicon Valley? Because two teams based in this area — Google Brain, which invented the transformer network, and OpenAI, which scaled it up — did a lot of the early work. Subsequently, team members moved to other nearby businesses, started competitors, or worked with local universities. Further, local social networks rapidly diffused the knowledge through casual coffee meetings, local conferences, and even children’s play dates, where parents of like-aged kids meet and discuss technical ideas. In this way, the knowledge spread faster within Silicon Valley than to other geographies.\n\nIn a similar vein, research done in the U.S. diffuses to others in the U.S. much faster than to other geographic areas. This is particularly true when the research is openly shared through papers and/or open source: If researchers have permission to talk about an idea, they can share much more information, such as tips and tricks for how to really make an algorithm work, more quickly. It also lets others figure out faster who can answer their questions. Diffusion of knowledge created in academic environments is especially fast. Academia tends to be completely open, and students and professors, unlike employees of many companies, have full permission to talk about their work.\n\nThus funding basic research in the U.S. benefits the U.S. most, and also benefits our allies. It is true that openness benefits our adversaries, too. But as a subcommittee of the U.S. House of Representatives committee on science, space, and technologypoints out, “... open sharing of fundamental research is [not] without risk. Rather, ... openness in research is so important to competitiveness and security that it warrants the risk that adversaries may benefit from scientific openness as well.”\n\nFurther, generative AI is evolving so rapidly that staying on the cutting edge is what’s really critical. For example, the fact that many teams can now train a model with GPT-3.5- or even GPT-4-level capability does not seem to be hurting OpenAI much, which is busy growing its business by developing the cutting-edge o4, Codex, GPT-4.1, and so on. Those who invent a technology get to commercialize it first, and in a fast-moving world, the cutting-edge technology is what’s most valuable. Studies likethis one(albeit done while the internet was not as prevalent as it is today) also show how knowledge diffuses locally much faster than globally.\n\nChina was decisively behind the U.S. in generative AI when ChatGPT was first launched in 2022. However, China’s tech ecosystem is very open internally, and this has helped it to catch up over the past two years:\n\nWhile there’s also much about China that I would not seek to emulate, the openness of its tech ecosystem has helped it accelerate.\n\nIn 1945, Vannevar Bush’s landmark report “Science, The Endless Frontier” laid down key principles for public funding of U.S. research and talent development. Those principles enabled the U.S. to dominate scientific progress for decades. U.S. federal funding for science created numerous breakthroughs that have benefited the U.S. tremendously, and also the world, while training generations of domestic scientists, as well as immigrants who likewise benefit the U.S.\n\nThe good news is that this playbook is now well known. I hope many more nations will imitate it and invest heavily in science and talent. And I hope that, having pioneered this very successful model, the U.S. will not pull back from it by enacting drastic cuts to funding scientific research.\n\nAndrew\n\nWe’re featured partners of Snowflake’s Dev Day 2025, a full day for AI and data practitioners to explore cutting-edge demos, make valuable contacts, and hear from top voices in the field (including Andrew Ng). See you on June 5!Register here\n\nAnthropic continued its tradition of building AI models that raise the bar in coding tasks.\n\nWhat’s new:Anthropic launchedClaude 4 Sonnet 4 and Claude Opus 4, the latest medium- and largest-size members of its family of general-purpose large language models. Both models offer an optional reasoning mode and can use multiple tools in parallel while reasoning. In addition, the company made generally available Claude Code, a coding agent previously offered as a research preview, along with a Claude Code software development kit.\n\nHow it works:The team trained the Claude 4 models on a mix of publicly available information on the web as well as proprietary purchased data, data from Claude users who opted to share their inputs and outputs, and generated data. They fine-tuned the models to behelpful, honest, and harmlessaccording to human andAI feedback.\n\nResults:Both Claude 4 models tied Google Gemini 2.5 Pro at the top of the LMSys WebDev Arena and achieved top marks for coding and agentic computer-use benchmarks in Anthropic’s tests.\n\nWhy it matters:The new models extend LLM technology with parallel tool use, using external files as a form of memory, and staying on-task over unusually long periods of time. Early users have reported many impressive projects, including aTetris clonebuilt in one shot and a seven-hour stintrefactoring Rakutan’s open-source code base.\n\nWe’re thinking:Prompting expert @elder_plinius published a text file that is purported to beClaude 4’s system promptand includes some material that does not appear in Anthropic’s ownpublicationof the prompts. It is instructive to see how it conditions the model for tool use, agentic behavior, and reasoning.\n\nGoogle revamped its roster of models, closed and open, and added more AI-powered features to its existing products.\n\nWhat’s new:Google staged a parade ofannouncementsat this year’s I/O developer conference. New offerings include improvements toGemini 2.5 Pro and Gemini 2.5 Flashand a preview ofGemma 3n(all three generally available in June), the updatedVeo 3video generator (available via Flow, Google’s AI videography app, for paid subscribers to its AI Pro and Ultra services), and increasingly AI-powered search.\n\nHow it works:The I/O offerings spanned from public-facing products to developer tools.\n\nWhy it matters:Google is catching up with the Microsoft/OpenAI colossus on several fronts. The addition of audio output to Gemini and Gemma models fuels the rise of voice-to-voice and other audio applications and gives developers powerful new tools to build them. At the same time, Veo 3’s text-to-video-plus-audio output showsmarkedimprovementover the previous version.\n\nBehind the news:The number of tokens Google processed monthly has surged this year from 9.7 trillion last year to 480 trillion, a sign that its AI APIs and AI-infused products are rapidly gaining traction. Google’s progress contrasts with Apple’s ongoingstruggles. Both share advantages in smartphones and app distribution. But, while Google has showcased a string of advanced models as well as early efforts to integrate them into legacy products, Apple’s organizational challenges have hampered its AI development. Now Apple must contend with OpenAI’sacquisitionof LoveFrom, the startup founded by its former lead product designer Jony Ive.\n\nWe’re thinking:Google I/O 2025 was a strong showing of generative AI capabilities! There’s still work to be done to translate these innovations into compelling products, but the company now has a strong base for building numerous innovative products.\n\nDeepSeek made headlines late last year, when it built a state-of-the-art, open-weights large language model at a cost far lower than usual. The upstart developer shared new details about its method.\n\nWhat’s new:Chenggang Zhao and colleagues at DeepSeek describedsoftware and hardware choicesthat reduced memory and processing requirements while building their groundbreaking mixture-of-experts models DeepSeek-R1 and DeepSeek-V3.\n\nMixture of experts (MoE) basics:The MoE architecture uses different subsets of a model’s parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a routing module that learns to choose which one(s) to use based on the training example. In this way, different experts learn to specialize in different types of input.\n\nHow it works:The authors trained DeepSeek-R1 and DeepSeek-V3 using a cluster of 2,048 Nvidia H800 GPUs composed of nodes that contained 8 GPUs each. MoE requires less memory than dense architectures, since a given input activates only a portion of a model’s parameters. This enabled the authors to train DeepSeek-V3 on 250 GFLOPs per token, while Qwen 2.5 72B required 394 GFLOPs per token and Llama 3.1 405B required 2,448 GFLOPs per token.\n\nBehind the news:DeepSeek-V3made waves when it was released in December. It performed better than Llama 3.1 405B, the leading LLM at the time, but its training cost was an astonishing $5.6 million, compared to the usual tens to hundreds of millions of dollars. Some observers wereskepticalof the reported cost, pointing out that the $5.6 million dollar figure doesn’t include salaries, data acquisition and annotation, processing failed training runs, and other research and development costs. In addition, the cost of trainingDeepSeek-R1remains unknown.\n\nWhy it matters:Traditionally, only companies with large budgets and vast resources could afford to train state-of-the-art models. DeepSeek changed that but didn’t explain how when it released its models. By sharing the details, the company has empowered a wider range of teams to improve the state of the art.\n\nWe’re thinking:Shortly after DeepSeek-R1 was released, some engineers claimed — without presenting evidence — that DeepSeek had copied their work. DeepSeek’s disclosure of its training methods should lay to rest any remaining questions about this. Its work was truly innovative, and we applaud its release of key technical details.\n\nA study co-authored by tech-manual publisher Tim O’Reilly shows that OpenAI trained GPT-4o on parts of his company’s books that were not made freely available.\n\nWhat happened:O’Reilly, computer scientist Sruly Rosenblat, and economist Ilan Straussfoundthat GPT-4o was able to identify verbatim excerpts from dozens of O’Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model’s training data.\n\nHow it works:The researchers adapted theDE-COPmethod to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books.\n\nResults:The authors asked each model to identify the verbatim paragraph and calculated each model’s percentage of correct responses. Then they averaged each model’s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially included in the training set) from those that weren’t available at the time. 50 percent AUROC indicates random chance, while higher scores indicate higher accuracy.\n\nYes, but:Newer large language models are better at distinguishing human-written from generated text, even if it wasn’t in their training sets. For instance, given paragraphs that were published after their knowledge cutoffs, GPT-4o returned scores as high as 78 percent AUROC. The authors note that this may challenge their conclusions, since they interpret high scores to indicate that a model saw the text during training. Nonetheless, they argue that their approach will remain valid while scores for both text that was included and text that was excluded from training sets remain under 96 percent AUROC. “For now,” they write, “the gap remains sufficiently large to reliably separate the two categories.”\n\nBehind the news:Historically AI developers have trained machine learning models on any data they could acquire. But in the era of generative AI, models trained on copyrighted works can mimic the works and styles of the works’ owners, creating a threat to their livelihoods. Some AI developers have responded by regarding data that’s freely available on the web as fair game, and material that’s otherwise protected as off-limits for training. However, datasets that include ostensibly private data are widely circulated, includingLibGen, which includes all 34 of the O’Reilly Media titles tested in this study. Moreover, unauthorized copies of many copyrighted works are posted without paywalls or even logins, making it possible even for web scrapers that crawl only the open web to download them. Google and OpenAI, which is currently embroiled in lawsuits by authors and publishers who claim it violated copyrights by training models on copyrighted works, recentlylobbiedthe United States government to relax copyright laws for AI developers.\n\nWhy it matters:The AI industry requires huge quantities of high-quality data to keep advancing the state of the art. At the same time, copyright owners are worried that models trained on their data might hamper their opportunities to earn a living. AI developers must find fair ways to respond. As O’Reilly points out, exploiting copyrighted works instead of rewarding their authors could lead to an “extractive dead end” that ultimately diminishes the supply of the high-quality training data.\n\nWe’re thinking:We have learned a great deal from O’Reilly Media’s books, and we’re grateful to the many authors, editors, graphic artists, and others who produce them. Meanwhile, it’s time for the U.S. Congress —  and legislators internationally — toupdatecopyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--96--2.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/GqglM2_WAAAcXxW.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--97-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--60-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--98-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--99-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-289/",
    "title": "issue 289",
    "date": "",
    "reading_time": "13 min read",
    "content": "Last month, a drone fromSkyfire AIwas credited with saving a police officer’s life after a dramatic 2 a.m. traffic stop. Many statistics show that AI impacts billions of lives, but sometimes a story still hits me emotionally. Let me share what happened.\n\nSkyfire AI, an AI Fund portfolio company led by CEODon Mathis, operates a public safety program in which drones function as first responders to 911 calls. Particularly when a police department is personnel-constrained, drones can save officers’ time while enhancing their situational awareness. For example, many burglar alarms are false alarms, maybe set off by moisture or an animal. Rather than sending a patrol officer to drive over to discover this, a drone can get there faster and determine if an officer is required at all. If the alarm is real, the drone can help officers understand the situation, the locations of any perpetrators, and how best to respond.\n\nIn January, a Skyfire AI drone was returning to base after responding to a false alarm when the police dispatcher asked us to reroute it to help locate a patrol officer. The officer had radioed a few minutes earlier that he had pulled over a suspicious vehicle and had not been heard from since. The officer had stopped where two major highways intersect in a complex cloverleaf, and dispatch was unsure exactly where they were located.\n\nFrom the air, the drone rapidly located the officer and the driver of the vehicle he had pulled over, who it turned out had escaped from a local detention facility. Neither would have been visible from the road — they were fighting in a drainage ditch below the highway. Because of the complexity of the cloverleaf’s geometry, the watch officer (who coordinates police activities for the shift) later estimated it would have taken 5-7 minutes for an officer in a patrol car to find  them.\n\nFrom the aerial footage, it appeared that the officer still had his radio, but  was losing the fight and unable to reach it to call for help. Further, it looked like the assailant might gain control of his service weapon and use it against him. This was a dire and dangerous situation.\n\nFortunately, because the drone had pinpointed the location of the officer and his assailant, dispatch was able to direct additional units to assist. The first arrived not in 5-7 minutes but in 45 seconds. Four more units arrived within minutes.\n\nThe officers were able to take control of the situation and apprehend the driver, resulting in an arrest and, more important, a safe outcome for the officer. Subsequently, the watch officer said we’d probably saved the officer’s life.\n\nDemocratic nations still have a lot of work to do on drone technology, and we must build this technology with guardrails to make sure we enhance civil liberties and human rights. But I am encouraged by the progress we’re making. In the aftermath of Hurricane Helene last year, Skyfire AI’s drones supported search-and-rescue operations under the direction of the North Carolina Office of Emergency Management, responding to specific requests to help locate missing persons and direct rescue assets (e.g., helicopters and boats) to their location, and was credited with saving 13 lives.\n\nIt’s not every day that AI directly saves someone's life. But as our technology advances, I think there will be more and more stories like these.\n\nKeep building!\n\nAndrew\n\nLearn to systematically evaluate, improve, and iterate on AI agents using structured assessments. In our short course “Evaluating AI Agents,” you’ll learn to add observability, choose the right evaluation methods, and run structured experiments to improve AI agent performance.Enroll for free\n\nxAI’s new model family suggests that devoting more computation to training remains a viable path to building more capable AI.\n\nWhat’s new:Elon Musk’s xAI published a videodemonstrationof Grok 3, a family of four large language models that includes reasoning and non-reasoning versions as well as full- and reduced-size models. Grok 3 is available to subscribers to X’s Premium+ ($40 monthly for users in the United States; the pricevaries by country) and will be part of a new subscription service called SuperGrok. The models currently take text input and produce text output, but the company plans to integrate audio input and output in coming weeks.\n\nHow it works:xAI has not yet disclosed details about Grok 3’s architecture, parameter counts, training datasets, or training methods. Here’s what we know so far:\n\nResults:The Grok 3 family outperformed leading models in math (AIME 2024), science (GPQA), and coding (LiveCodeBench).\n\nBehind the news:Reasoning models are pushing benchmark scores steadily upward, especially in challenging areas like math and coding. Grok 3, with its ability to reason over prompts, search the web, and compile detailed reports, arrives hot on the heels of OpenAI’sDeep Researchando3-miniand Google’sGemini-2 Flash Thinking, which offer similar capabilities.\n\nWhy it matters:Grok 3 is a substantial achievement — especially for a company that’s less than two years old — and it pushes the state of the art forward by ample margins. But its significance may go farther. Research intoscalinglawsindicates that model performance scales with training. While xAI has not disclosed the amount of processing used to train Grok 3, the number of GPUs in its cluster suggests that the company applied a massive amount.\n\nWe’re thinking:Grok 3’s performance makes a case for both massive compute in pretraining and additional compute at inference. Running in its usual mode, Grok 3 mini Reasoning outperformed OpenAI o3-mini set at high effort on AIME 2024, GPQA, and LiveCodeBench. With an unspecified amount of additional compute, its performance on those benchmarks shot further upward by a substantial margin.\n\nReplit, an AI-driven integrated development environment, updated its mobile app to generate further mobile apps to order.\n\nWhat’s new:Replit’s app, which previously generated simple Python programs, nowgenerates iOS and Android apps and app templatesthat can be shared publicly. Mobile and web access to Replit’s in-house code generation models is free for up to three public applications. ACore plan($25 per month, $180 per year) buys unlimited access and applications, code generation by Claude 3.5 Sonnet and OpenAI GPT-4o, and monthly credits for generated checkpoints.\n\nHow it works:The app and web tools are powered by Replit Agent, an AI coding assistant designed to help users write, debug, and deploy applications with little manual setup. Replit Agent is based on Claude 3.5 Sonnet and calls other specialized models. The agent framework isbuilton LangChain’s LangGraph. It breaks down development tasks into steps to be handled by specialized sub-agents.\n\nBehind the news:The incorporation of Replit Agent to Replit’s mobile app is a significant step for AI-driven IDEs. Competitors like Aider and Windsurf don’t offer mobile apps, and mobile apps from Cursor and Github provide chat but not mobile app development. Moreover, few coding agents can deploy apps to the cloud on the desktop or mobile.\n\nWhy it matters:Replit’s new mobile app produces working apps in minutes (although some early users have reported encountering bugs), and automatic deployment of apps to the cloud is a huge help. Yet it raises the stakes for developers to learn their craft and maintain a collaborative relationship with AI. While Replit’s web-based environment exposes the code, encouraging users to improve their skills, the mobile app hides much of its work below the surface. It brings AI closer to handling full software development cycles and adds urgency to questions about how to address the balance between automation and hands-on coding.\n\nWe’re thinking:AI continues to boost developer productivity and reduce the cost of software development, and the progress of Bolt, Cursor, Replit, Vercel, Windsurf, and others is exhilarating. We look forward to a day when, measured against the 2024 standard, every software engineer is a 10x engineer!\n\nElon Musk and a group of investors made an unsolicited bid to buy the assets of the nonprofit that controls OpenAI, complicating the AI powerhouse’s future plans.\n\nWhat’s new:Musksubmitteda $97.4 billion offer to acquire the assets of the nonprofit OpenAI Inc. CEO Sam Altman and the company’s board of directors swiftlyrejectedit, and Altman publiclymockedMusk by offering to buy Twitter for $9.74 billion (one-tenth of Musk’s bid and less than one-quarter the price he paid for the social network). OpenAI’s board reaffirmed its control over the company’s direction, signaling that it does not intend to cede governance to outside investors.\n\nHow it works:OpenAI was founded as a nonprofit in 2015, but since 2019 it has operated under an unusual structure in which the nonprofit board controls the for-profit entity that develops and commercializes AI models. This setup allows the board to maintain the company’s original mission — developing AI for the benefit of humanity — rather than solely maximizing shareholder value. However, driven by the need for massive investments in infrastructure and talent, OpenAI is considering a newfor-profit structurethat would allow external investors to own more of the company. The high offer by Musk — who, as CEO of xAI, competes with OpenAI — could interfere with that plan.\n\nBehind the news:Musk was one of OpenAI’s earliest investors, but he departed in 2018 after disagreements over direction and control of the organization. His bid follows alawsuitagainst OpenAI, in which he claims the company abandoned its nonprofit mission in favor of profit. OpenAIsaidthat Musk’s bid contradicts his legal claims and suggests that the lawsuit should be dismissed. Since then, Musk hasstatedthat he would drop the lawsuit if OpenAI remains a nonprofit.\n\nWhy it matters:OpenAI is a premier AI company, and its activities affect virtually everyone in the field by supplying tools, technology, or inspiration. Musk’s xAI is a direct competitor, and his bid, whether it’s sincere or tactical, unsettles OpenAI’s plans. Even if OpenAI moves forward as planned, Musk’s actions likely will have made the process more expensive and potentially invite closer scrutiny of the company’s actions.\n\nWe’re thinking:There’s ample precedence for non-profits spinning out for-profit entities. For example, non-profit universities typically create intellectual property that forms the basis of for-profit startups. The university might retain a modest stake, and this is viewed as consistent with its non-profit mission. This isn’t a perfect analogy, since OpenAI does little besides operating its AI business, but we hope the company finds a path forward that allows it to serve users, rewards its employees for their contributions, and honors its non-profit charter.\n\nThe latest international AI summit exposed deep divisions between major world powers regarding AI regulations.\n\nWhat’s new:While previous summits emphasized existential risks, theAI Action Summitin Paris marked a turning point. France and the European Union shifted away from strict regulatory measures and toward investment to compete with the United States and China. However, global consensus remained elusive: the U.S. and the United Kingdom refused to sign key agreements on global governance, military AI, and algorithmic bias. The U.S. in particular pushed back against global AI regulation, arguing that excessive restrictions could hinder economic growth and that international policies should focus on more immediate concerns.\n\nHow it works:Participating countries considered three policy statements that address AI’s impact on society, labor, and security. Thefirst statementcalls on each country to enact AI policies that would support economic development, environmental responsibility, and equitable access to technology. Thesecondencourages safeguards to ensure that companies and nations distribute AI productivity gains fairly, protect workers’ rights, and prevent bias in hiring and management systems. Thethirdadvocates for restrictions on fully autonomous military systems and affirms the need for human oversight in warfare.\n\nBehind the news:The Paris summit follows previous gatherings of world leaders to discuss AI, including the initialAI Safety Summitat Bletchley Park and theAI Seoul Summit and AI Global Forum. At these summits, governments and companies agreed broadly to address AI risks but avoided binding regulations. Nonetheless, divisions over AI governance have widened in the wake of rising geopolitical competition and theemergenceof high-performance open weights models like DeepSeek-R1.\n\nWhy it matters:The Paris summit marks a major shift in global AI policy. The EU, once an ardent proponent of AI regulation, backed away from its strictest proposals. At the same time, doomsayers have lost influence, and officials are turning their attention to immediate concerns like economic growth, security, misuse, and bias. These moves make way for AI to do great good in the world, even as they contribute touncertaintyabout how AI will be governed.\n\nWe’re thinking:Governments are shifting their focus away from unrealistic risks and toward practical strategies for guiding AI development. We look forward to clear policies that encourage innovation while addressing real-world challenges.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--52-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/The-Batch-ads-and-exclusive-banners---2025-02-17T150409.455.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--53-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--49-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--50-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/02/unnamed--54-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-301/",
    "title": "issue 301",
    "date": "",
    "reading_time": "13 min read",
    "content": "AI’s ability to make tasks not just cheaper, but also faster, is underrated in its importance in creating business value.\n\nFor the task of writing code, AI is a game-changer. It takes so much less effort — and is so much cheaper — to write software with AI assistance than without. But beyond reducing the cost of writing software, AI is shortening the time from idea to working prototype, and the ability to test ideas faster is changing how teams explore and invent. When you can test 20 ideas per month, it dramatically changes what you can do compared to testing 1 idea per month. This is a benefit that comes from AI-enabledspeedrather than AI-enabled cost reduction.\n\nThat AI-enabled automation can reduce costs is well understood. For example, providing automated customer service is cheaper than operating human-staffed call centers. Many businesses are more willing to invest in growth than just in cost savings; and, when a task becomes cheaper, some businesses will do a lot more of it, thus creating growth. But another recipe for growth is underrated: Making certain tasks much faster (whether or not they also become cheaper) can create significant new value.\n\nI see this pattern across more and more businesses. Consider the following scenarios:\n\nI’ve written previously about looking at thetasksa company does to explore where AI can help. Many teams already do this with an eye toward making tasks cheaper, either to save costs or to do those tasks many more times. If you’re doing this exercise, consider also whether AI can significantly speed up certain tasks. One place to examine is the sequence of tasks on the path to earning revenue. If some of the steps can be sped up, perhaps this can help revenue growth.\n\nGrowth is more interesting to most businesses than cost savings, and if there are loops in your business that, when sped up, would drive growth, AI might be a tool to unlock this growth.\n\nKeep building!\n\nAndrew\n\nIn Course 4 of theData Analytics Professional Certificateyou’ll work with truly real-world data: messy, inconsistent, and often unstructured. You’ll extract data from websites, APIs, and databases, and clean it using Python and SQL. By the end, you’ll be able to make raw datasets analysis-ready, with speed and accuracy.Enroll today!\n\nMicrosoft published its latest recipe for training reasoning models, substantially expanding what is still a fairly small base of public knowledge.\n\nWhat’s new:Microsoft releasedPhi-4-reasoning, Phi-4-reasoning-plus, andPhi-4-mini-reasoningalong with lessons learned in building the models.\n\nHow it works:All three models are fine-tuned versions of pretrained models.\n\nSmaller model lessons learned:During reinforcement learning, Phi-4-mini-reasoning exhibited instability, such as output batches that varied greatly in length or received mostly negative rewards, apparently depending on the training data or output. The authors suspect that the model’s small size caused these issues. Among the lessons learned:\n\nLarger model lessons learned:Phi-4-reasoning and Phi-4-reasoning-plus didn’t present the same issues. However, the authors did make significant choices during reinforcement learning:\n\nResults:Overall, Phi-4-reasoning-plus and Phi-4-mini-reasoning outperform similarly sized (and larger) open-weights models on math problems. Phi-4-reasoning generally outperformed DeepSeek-R1-Distilled-70B but underperformed Alibaba QwQ 32B. All three models deliver performance that falls in the middle among proprietary models and, in domains outside math, larger models with open weights.\n\nWhy it matters:While reasoning models can outperform their non-reasoning counterparts, the best ways to train them aren’t widely known. Sharing recipes and lessons learned enables others to further iterate and improve the recipes, ultimately increasing model performance even more.\n\nAn open-source code generator performs comparably to the reasoning models DeepSeek-R1 and OpenAI o1 with a much smaller model.\n\nWhat’s new:A team at the model platform Together.AI and Agentica, an open-source project devoted to reinforcement learning (RL), releasedDeepCoder-14B-Preview. The release includesweights, code, dataset, training logs, and data optimizationsunder an MITlicensethat allows noncommercial and commercial uses.\n\nHow it works:The team fine-tuned DeepSeek-R1-Distilled-Qwen-14B, which distills knowledge from DeepSeek-R1 (671 billion parameters) into Qwen-14B (14 billion parameters).\n\nResults:DeepCoder-14B-Preview is competitive on several coding benchmarks with DeepSeek-R1 as well as proprietary models including OpenAI o3-mini and OpenAI o1, which is believed to be much larger.\n\nWhy it matters:Applying reinforcement learning to coding works, but it has two big issues: (i) Training examples of verifiable code are relatively scarce and (ii) computing reward signals for code is time-consuming, since it requires evaluating many test cases. DeepCoder-14B-Preview’s optimizations reduced this complexity, shrinking RL training from months to weeks. Those optimizations are built intoVerl-pipeline, an open source RL library from Together.AI and Agentica, giving developers a powerful tool for model training.\n\nWe’re thinking:Kudos to the DeepCoder team for open sourcing their reasoning recipe! A handful of companies have developed the know-how to execute RL well, but many teams still have trouble implementing successfully. Open recipes for RL training methods and data curation techniques are important to move the field forward.\n\nThe European Union made an abrupt U-turn away from its stringent AI regulations. Meta promptly adjusted to the loosening restrictions.\n\nWhat’s new:Henna Virkkunen, the EU’s head of digital policy, said the organization wouldeaserules and requirements to support Europe’s competitiveness in AI.\n\nHow it works:Adopted last year, the EU’sAI Actprovides a comprehensive framework for regulating AI that aims to reduce purported risks by banning certain applications, restricting others, and requiring extensive documentation of development efforts. The law is set to take effect in August, empowering various regulatory bodies to formulate detailed rules. However, in recent months, the EU has faced increasing pressure from the U.S. government and large AI companies to reduce the regulatory burden.\n\nBehind the news:In drafting the AI Act, the EU aspired to a comprehensive, specific set of regulations. However, not all European lawmakers agreed that rules were needed. Virkkunen’s supporters noted that existing laws already allowed consumers to file claims against AI companies. Meanwhile, some policymakers havebecome less worriedabout AI than they were during the early drafting of the AI Act.\n\nWhy it matters:It’s unlikely that all nations – or evenstateswithin nations – will ever agree fully on rules and regulations that govern AI companies that do business within their borders, or protections from flaws such as model bias. But AI companies including Meta,OpenAI, andothersargue that a more uniform regulatory environment will make it easier to serve users worldwide.\n\nWe’re thinking:The EU overreached with the AI Act. Fortunately, the legislation provides enough flexibility to pull back. Clearer rules will help European teams innovate and European and international companies better serve EU citizens.\n\nImproving a large language model’s factual accuracy typically requires making it bigger, which in turn, involves more computation. Researchers devised an architecture that enables models to recall relevant details without significantly increasing the amount of computation required.\n\nWhat’s new:Vincent-Pierre Berges, Barlas Oğuz, and colleagues at Meta augmented transformers with trainablememory layersthat efficiently store and retrieve information related to a prompt. Thetraining codeis available under a CC BY-NClicense, which permits noncommercial uses.\n\nMemory layer basics:Memory layers wereintroducedin 2015 and wereapplied to transformersa few years later. They compute vectors, which may capture details like names or dates that were learned through training, and retrieve them according to a given input. Computing the output of a memory layer is similar to computing that of a self-attention layer. Both describe vectors that represent queries, keys, and values, and both compute the similarity between queries and keys and then weight the values by that similarity. However, while a self-attention layer computes queries, keys, and values from linear transformations of the input, a memory layer (which computes queries the same way) learns keys and a corresponding value for each key through training.\n\nKey insight:Memory layers can be scaled to millions of keys, but computing the similarity between a query and so many keys is computationally expensive. One solution is to represent each key as a combination of two half-keys drawn from two much smaller sets. For example, two sets of 1,000 half-keys each can represent 1 million possible keys. Comparing a query to these smaller sets is much more efficient, making it practical to scale up memory layers dramatically.\n\nHow it works:The authors pretrained Llama-style models of several sizes (from 134 million to 8 billion parameters) on data similar to Llama 2’s and Llama 3’s pretraining datasets. They replaced the fully connected layers with memory layers in three transformer blocks. These layers shared parameters and held up to 16 million values (an extra 64 billion parameters total). The memory layers performed these steps:\n\nResults:The authors compared a model (8 billion parameters) with memory layers to a similar model without memory layers, both trained on 1 trillion tokens.\n\nWhy it matters:Memory layers didn’t catch on in the early days of large language models (LLMs), but they can improve the output of today’s much bigger models. LLMs outfitted with memory layers require less data and computation for pretraining than conventional models to achieve the same result, at least with respect to answering factual questions.\n\nWe’re thinking:While retrieval-augmented generation can help LLMs deliver more-factual output by retrieving facts from a database, the authors add trainable parameters for this purpose.\n\nBuild AI applications that access tools, data, and prompt templates using Model Context Protocol (MCP), an open standard developed by Anthropic. In “MCP: Build Rich-Context AI Apps with Anthropic,” you’ll build and deploy an MCP server, make an MCP-compatible chatbot, and connect applications to multiple third-party servers.Sign up now",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--88-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/The-Batch-ads-and-exclusive-banners---2025-05-13T182227.067.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--89-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--90-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--63--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--91-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/05/The-Batch-ads-and-exclusive-banners---2025-05-13T182128.677.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-284/",
    "title": "issue 284",
    "date": "",
    "reading_time": "13 min read",
    "content": "Writing software, especially prototypes, is becoming cheaper. This will lead to increased demand for people who can decide what to build. AI Product Management has a bright future!\n\nSoftware is often written by teams that comprise Product Managers (PMs), who decide what to build (such as what features to implement for what users) and Software Developers, who write the code to build the product. Economics shows that when two goods are complements — such as cars (with internal-combustion engines) and gasoline — falling prices in one leads to higher demand for the other. For example, as cars became cheaper, more people bought them, which led to increased demand for gas. Something similar will happen in software. Given a clear specification for what to build, AI is making the building itself much faster and cheaper. This will significantly increase demand for people who can come up with clear specs for valuable things to build.\n\nThis is why I’m excited about the future of Product Management, the discipline of developing and managing software products. I’m especially excited about the future of AI Product Management, the discipline of developing and managing AI software products.\n\nMany companies have an Engineer:PM ratio of, say, 6:1. (The ratio varies widely by company and industry, and anywhere from 4:1 to 10:1 is typical.) As coding becomes more efficient, I think teams will need more product management work (as well as design work) as a fraction of the total workforce. Perhaps engineers will step in to do some of this work, but if it remains the purview of specialized Product Managers, then the demand for these roles will grow.\n\nThis change in the composition of software development teams is not yet moving forward at full speed. One major force slowing this shift, particularly in AI Product Management, is that Software Engineers, being technical, are understanding and embracing AI much faster than Product Managers. Even today, most companies have difficulty finding people who know how to develop products and also understand AI, and I expect this shortage to grow.\n\nFurther, AI Product Management requires a different set of skills than traditional software Product Management. It requires:\n\nFinally, AI Product Managers will need to know how to ensure that AI is implemented responsibly (for example, when we need to implement guardrails to prevent bad outcomes), and also be skilled atgathering feedback fastto keep projects moving. Increasingly, I also expect strong product managers to be able tobuild prototypesfor themselves.\n\nThe demand for good AI Product Managers will be huge. In addition to growing AI Product Management as a discipline, perhaps some engineers will also end up doing more product management work.\n\nThe variety of valuable things we can build is nearly unlimited. What a great time to build!\n\nKeep learning,\n\nAndrew\n\nGet up-close and personal with OpenAI’s groundbreaking o1 model! In our short course “Reasoning with o1,” you’ll learn how to get the best performance in coding, planning, and STEM tasks; perform complex, multi-step tasks; and optimize prompts with meta prompting.Enroll today\n\nA new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.\n\nWhat’s new:DeepSeek-V3is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights areopenexcept for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download themhere.\n\nMixture of experts (MoE) basics:The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.\n\nHow it works:DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours — less than 1/10 thetime required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms — at an extraordinarily low cost of $5.6 million.\n\nResults:In DeepSeek’s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.\n\nBehind the news:OpenAI’s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.\n\nWhy it matters:Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3’s shockingly low training cost.The team doesn’t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits “meticulous engineering optimizations.”) But it’s likely that DeepSeek’s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn’t employ MoE. In 2022,Microsoftfound that MoE cost five times less in training for equal performance compared to a dense model, andGoogleandMetareported that MoE achieved better performance than dense models trained on the same numbers of tokens.\n\nWe’re thinking:If they can be replicated, DeepSeek’s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically.\n\nThe United States proposed limits on exports of AI technology that would dramatically expand previous restrictions, creating a new international hierarchy for access to advanced chips and models.\n\nWhat’s new:The Biden administration, which will transition to leadership under incoming President Trump next week, issued newrulesthat restrict exports of AI chips and models to most countries beyond a select group of close allies. The rules, which are not yet final, would create a three-tier system that limits exports to a number of close allies and blocks access entirely to China, Iran, North Korea, Russia, and others. They also would introduce the U.S.’ first-ever restrictions on exporting closed weights for large AI models.\n\nHow it works:The restrictions were announced shortly after aleakreached the press. A public comment period of 120 days will enable the incoming U.S. Presidential administration to consider input from the business and diplomatic communities and modify the rules before they take effect. The rules are scheduled to take effect in one year.\n\nBehind the news:The proposed rules build on 2022’sCHIPS and Science Act, which was designed to strengthen domestic semiconductor production and restrict technologies abroad that could bear on U.S. security. An initial round of restrictions in late 2022barredsemiconductor suppliers AMD and Nvidia from selling advanced chips to Chinese firms. In November 2024, the U.S.tightenedrestrictions further, ordering Taiwan Semiconductor Manufacturing Company, which fabricates those chips, to halt production of advanced chips destined for China.\n\nPlus green AI infrastructure:In addition, President Biden issued an executive order to encourage the rapid build-out of computing infrastructure for AI. The federal government will hold competitions among private companies to lease sites it owns for the building of data centers at private expense. The selection of sites will take into account availability of sources of clean energy, including support for nuclear energy. The government will expedite permitting on these sites and support development of energy transmission lines around them. It will also encourage international allies to invest in AI infrastructure powered by clean energy.\n\nWhy it matters:Protecting the United States’ advantages in high tech has been a rising priority for the White House over the past decade. The earlier export restrictions forced many Chinese AI developers to rely on less-powerful hardware. The new limits are likely to have a far broader impact. They could force developers in the Tier 2 and Tier 3 countries to build less resource-intensive models and lead them to collaborate more closely with each other, reducing the value of U.S.-made technology worldwide. They could hurt U.S. chip vendors, which havewarnedthat the rules could weaken U.S. competitiveness in the global economy. They could also force companies that are building huge data centers to process AI calculations toreconsidertheir plans.\n\nWe’re thinking:The Biden administration’s embargo on AI chips has beenleaky. So far, it has slowed down adversaries only slightly while spurring significant investment in potentialsuppliersthat aren’t connected to the U.S. While the public comment period invites lobbying and industry feedback, ultimately geopolitical priorities may hold sway. Whatever the outcome, reducing the world’s dependence on U.S. chips and models would result in a very different global AI ecosystem.\n\nNvidia’s new desktop computer is built specifically to run large AI models.\n\nWhat’s new:Project Digitsis a personal supercomputer intended to help developers fine-tune and run large models locally. Project Digits, which is small enough to hold in one hand, will be available in May, starting at $3000.\n\nHow it works:Project Digits is designed to run models of up to 200 billion parameters — roughly five times the size that fits comfortably on typical consumer hardware — provided they’re quantized to 4 bits of precision. Two units can be connected to run models such as Meta’s Llama 3.1 405B. Complete specifications are not yet available.\n\nBehind the news:In a blitz of announcements at the Consumer Electronics Show (CES), Nvidia also launched a platform for developing robotics, autonomous vehicles, and other physical AI systems. Cosmos includes pretrained language and vision models that range from 4 billion to 14 billion parameters for generating synthetic training data for robots or building policy models that translate a robot’s state into its next action. Nvidia also released Cosmos Nemotron, a 34 billion-parameter, vision-language model designed for use by AI agents, plus a video tokenizer and other tools for robotics developers.\n\nWhy it matters:It’s common to train models on Nvidia A100 or H100 GPUs, which come with a price tag of at least $8,000 or $20,000 respectively, along with 40 gigabytes to 80 gigabytes of memory. These hefty requirements push many developers to buy access to computing infrastructure from a cloud provider. Coming in at $3,000 with 128 gigabytes of memory, Project Digits is designed to empower machine learning engineers to train and run larger models on their own machines.\n\nWe’re thinking:We look forward to seeing cost/throughput comparisons between running a model on Project Digits, A100, and H100.\n\nContrastive loss functions make it possible to produce good embeddings without labeled data. A twist on this idea makes even more useful embeddings.\n\nWhat’s new:Vlad Sobal and colleagues at Meta, New York University, Brown University, Genentech, and Canadian Institute for Advanced Research introducedX-Sample contrastive loss(X-CLR), a self-supervised loss function that enables vision models to learn embeddings that capture similarities and differences among examples with greater subtlety.\n\nKey insight:Contrastive loss functions likeSimCLRequally encourage a model to produce dissimilar embeddings of images of, say, a cat, a dog, and a dump truck. But, of course, cats and dogs are more similar to each other than either are to dump trucks. Instead of marking examples as similar or dissimilar, X-CLR assigns similarity scores, so a model can learn to produce embeddings that match those scores.\n\nHow it works:The authors used X-CLR to train an embedding model onConceptual Captionsdatasets of image-text pairs scraped from the web: CC-3M (3 million text-image pairs) and CC-12M (12 million text-image pairs). The model was similar toCLIP, except the text encoder was asentence transformerpretrained on sentence pairs, and the vision encoder was aResNet-50pretrained on ImageNet.\n\nResults:Systems trained using X-CLR outperformed competitors inImageNetclassification, especially when less training data was available. (The authors followed CLIP’s method of classification: They computed the similarity between an image embedding and text embeddings of all classes. The image’s classification was the class that corresponds to the text embedding with the highest similarity to the image embedding.)\n\nWhy it matters:Contrastive loss functions are very useful, but the similar/dissimilar dichotomy leaves important nuances unaccounted for. Like CLIP, X-CLR takes advantage of both images and their captions for self-supervised learning. However, CLIP learns to recognize image-text pairs as similar or dissimilar, while X-CLR matches image-image pairs using captions as a similarity signal that’s continuous rather than discrete.\n\nWe’re thinking:Reality is not black and white. Allowing for shades of gray makes for better modeling.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/01/AIProductManager-2_1200px-1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/01/The-Batch-ads-and-exclusive-banners---2024-12-16T174314.640--1-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--45-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/01/BIDENCHIPS-10_1200px.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--47-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/01/unnamed--44-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-299/",
    "title": "issue 299",
    "date": "",
    "reading_time": "11 min read",
    "content": "I hope we can empower everyone to build with AI. Starting from K-12, we should teach every student AI enabled coding, since this will enable them to become more productive and more empowered adults. But there is a huge shortage of computer science (CS) teachers. I recently spoke with high school basketball coach Kyle Creasy, who graduated with a B.A. in Physical Education in 2023. Until two years ago, he had never written a line of Python. Now — with help from AI — he not only writes code, he also teaches CS. I found Kyle’s story inspiring as a model for scaling up CS education in the primary- and secondary-school levels.\n\nKyle’s success has been with the support ofKira Learning(an AI Fund portfolio company), whose founders Andrea Pasinetti and Jagriti Agrawal have created a compelling vision for CS education. In K-12 classrooms, teachers play a huge social-emotional support role, for example, encouraging students and helping them when they stumble. In addition, they are expected to be subject-matter experts who can deliver the content needed for their subject. Kira Learning uses digital content delivery — educational videos, autograded quizzes, and AI-enabled chatbots to answer students' questions but without giving away homework answers — so the teacher can focus on social-emotional support. While these are still early days, it appears to be working!\n\nA key to making this possible is the hyperpersonalization that is now possible with AI (in contrast to the older idea of theflipped classroom, which had limited adoption). For example, when assigned a problem in an online coding environment, if a student writes this buggy line of Python code\n\nbest_$alty_snack = 'potato chips'\n\nKira Learning’s AI system can spot the problem and directly tell the teacher that $ is an invalid character in a variable name. It can also suggest a specific question for the teacher to ask the student to help get them unstuck, like “Can you identify what characters are allowed in variable names?” Whereas AI can directly deliver personalized advice to students, the fact that it is now helping teachers also deliver personalized support will really help in K-12.\n\nAdditionally, agentic workflows can automate a lot of teachers’ repetitive tasks. For example, when designing a curriculum, it’s time-consuming to align the content to educational standards (such as the Common Core in the United States, or the AP CS standard for many CS classes). Having an AI system carry out tasks like these is already proving helpful for teachers.\n\nSince learning to code, Kyle has built many pieces of software. He proudly showed me an analysis he generated in matplotlib of his basketball players’ attempts to shoot three-pointers (shown above), which in turn is affecting the team’s strategy on the court. One lesson is clear: When a basketball coach learns to code, they become a better basketball coach!\n\nI talked about Kyle (and other topics) at theASU+GSV Summiton education. You can see a videohere.\n\nIn the future, people who know how to code and build with AI will be much more productive than people who don’t. I’m excited about how AI will lead to new models for K-12 education. By delivering CS education to everyone, I hope that in the future, everyone will be able to build with AI.\n\nKeep learning!\n\nAndrew\n\nIn “LLMs as Operating Systems: Agent Memory,” you’ll learn to build agents that manage their own memory using the MemGPT approach. This newly updated short course includes cloud-based deployment and real-time, step-by-step output, so you can see how your agents reason as they respond.Join in today!\n\nChatGPT’s image generator is available via API.\n\nWhat’s new:GPT Image 1, which produces images from text or other images, has proven enormously popular among ChatGPT users. TheOpenAI Images APIenables developers to incorporate OpenAI’s most sophisticated image generator into their own software tools and platforms.\n\nHow it works:GPT Image 1generates and modifies imagesin a wide range of styles, performs image editing and other alterations, renders text, and follows detailed instructions. Shortly after its debut, the version of GPT-4o equipped with GPT Image 1 quickly soared to the No. 1 spot on theArtificial Analysis Image Arena leaderboard.\n\nBehind the news:In March, OpenAI attracted huge public interest when it deployed the model, then unnamed, inChatGPT. Within the first week,130 millionusers used it to create more than 700 million images.\n\nWhy it matters:Adding GPT Image 1 to the API enables developers to use OpenAI’s most sophisticated image generator in a wide variety of automated workflows. OpenAI’s initial API partners include design companies (Adobe and Canva), marketers (HubSpot), and web designers (GoDaddy), all of which are using GPT Image 1.\n\nWe’re thinking:GPT Image 1 is part of an exciting trend toward unification of multimodal architectures. Researchers have progressed fromtext-in, text-outtotext/images-in, text-outand increasinglytext/images/audio-in, text/images/audio-out. This paints a beautiful picture of where multimodal models can go!\n\nGoogle refreshed its experimental tools for composers and producers.\n\nWhat’s new:Google announced updates of two music-generation apps and the models they're based on.Music AI Sandbox, an app that generates and modifies music according to text prompts, now accepts lyrics to generate songs as well as instrumental music. You can join a waitlisthere.MusicFX DJgenerates a continuous stream of music that users can modify as it plays. Try it outhere.\n\nHow it works:The apps generate 48kHz audio suitable for professional productions. Users can specify key, tempo in beats per minute, instrumentation, style, mood, and other details.\n\nBehind the news:GooglelaunchedLyria 1 and Music AI Sandbox in 2023 as part of an experiment with YouTube, which made them available to composers, producers, and musicians. Since then, the company has developed them with help from music stars including Jacob Collier, Donald “Childish Gambino” Glover, and Wyclef Jean. Lyria 1 recently becameavailablevia the Vertex API to developers who are preapproved by Google.\n\nWhy it matters:While music generators likeSuno and Udioappeal to casual musicians, Music AI Sandbox, with its digital audio workstation-style user interface, aims to address the needs of professionals. This approach puts AI directly into the hands of talented, experienced artists, similar to the way Adobe hasempoweredvideographers and Runway haspartneredwith movie producers.\n\nWe’re thinking:API access to Lyria 2 would be music to our ears!\n\nAI agents and infrastructure made a strong showing on CB Insights’s latest list of the top 100 AI startups.\n\nWhat’s new:CB Insights, which tracks tech startups and venture capital, selected companies in theAI 100based on their market traction, talent, finances, and partnerships. The list purports to highlight the next wave of winners, shedding light on the key executives, investors, fundraising, and valuations behind up-and-coming AI ventures.\n\nHow it works:The analysts evaluated 17,000 early-stage, private AI companies that had raised funds within the last year and continue to seek further investment.\n\nWhere the action is:This year’s AI 100 companies are based in 14 countries, around two-thirds of them in the United States. 10 are based in the United Kingdom, five in France, and four in Germany, with one each in Norway (Braintrust), Singapore (Bria), Spain (Cartwheel), Sweden (Chainguard), and Switzerland (Clarium).\n\nWhy it matters:This year’s AI 100 offers a snapshot of AI becoming more central to businesses of all kinds. Most of the startups listed here offer practical products and services that are poised to deliver a timely return, rather than moonshots with long development cycles and risky payoffs. In addition, they mostly target corporate customers rather than consumers.\n\nWe’re thinking:The falling cost of access to AI models and increasingly capable open-weights models make this the perfect time tobuild applications. What kind? The report singles out health care (8 companies) and life sciences (6 companies) as growing areas, but it also documents opportunities in defense, gaming, and finance.\n\nLarge language models can improve systems that recommend items to purchase by inferring customer preferences.\n\nWhat’s new:Fabian Paischer and colleagues at Johannes Kepler University Linz, University of Wisconsin, and Meta introducedMultimodal Preference Discerner(Mender), a recommender that integrates a large language model (LLM).\n\nKey insight:Text that attracts customers, such as product descriptions, and text they write, such as product reviews, may contain information that indicates their preferences, such as the craft projects that required a particular power tool. But it also may include irrelevant information, such as a complaint that the tool was delivered late, which can throw recommendation systems off track. An LLM can derive preferences from text, providing a clearer signal of what a customer wants.\n\nHow it works:Mender comprises an LLM (Llama 3 70B-Instruct), an encoder (Flan-T5pretrained on a wide variety of text and frozen) that embeds customer data, and a decoder (a transformer trained from scratch) that predicts the next item a customer will buy. The system learned to predict the next item based on descriptions of items a customer purchased, the customer’s ratings and reviews of those products (drawn from datasets ofSteamreviews of video games andAmazonreviews of items related to beauty, toys-and-games, and sports-and-outdoors), and customer preferences inferred by the LLM from the foregoing data.\n\nResults:The authors compared Mender toTIGER, a recommender that also takes a purchase history and predicts the next purchase, on the Steam and Amazon datasets. They scored the results usingrecall @5, a measure of how often the correct item is within the model’s top five most likely predictions.\n\nWhy it matters:Drawing inferences from text information like customer reviews and item descriptions boosts a recommender’s signal, making it clearer what a given customer is likely to want. Previous systems used customer reviews or item descriptions directly; Mender uses customer preferences extracted from that information.\n\nWe’re thinking:Be on the lookout for innovative ways to use LLMs. We recommend it!",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--81-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/The-Batch-ads-and-exclusive-banners---2025-04-29T103611.308.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/GPT-IMAGE1-2.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--58-.gif",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--82-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--83-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-298/",
    "title": "issue 298",
    "date": "",
    "reading_time": "12 min read",
    "content": "Even though I’m a much better Python than JavaScript developer, with AI assistance, I’ve been writing a lot of JavaScript code recently. AI-assisted coding is making specific programming languages less important, even though learning one is still helpful to make sure you understand the key concepts. This is helping many developers write code in languages we’re not familiar with, which lets us get code working in many more contexts!\n\nMy background is in machine learning engineering and back-end development, but AI-assisted coding is making it easy for me to build front-end systems (the part of a website or app that users interact with) using JavaScript (JS) or TypeScript (TS), languages that I am weak in. Generative AI is making syntax less important, so we can all simultaneously be Python, JS, TS, C++, Java, and even Cobol developers. Perhaps one day, instead of being “Python developers\" or “C++ developers,” many more of us will just be “developers”!\n\nBut understanding the concepts behind different languages is still important. That’s why learning at least one language like Python still offers a great foundation for prompting LLMs to generate code in Python and other languages. If you move from one programming language to another that carries out similar tasks but with different syntax — say, from JS to TS, or C++ to Java, or Rust to Go — once you’ve learned the first set of concepts, you’ll know a lot of the concepts needed to prompt an LLM to code in the second language. (Although TensorFlow and PyTorch are not programming languages, learning the concepts of deep learning behind TensorFlow will also make it much easier to get an LLM to write PyTorch code for you, and vice versa!)  In addition, you’ll be able to understand much of the generated code (perhaps with a little LLM assistance).\n\nDifferent programming languages reflect different views of how to organize computation, and understanding the concepts is still important. For example, someone who does not understand arrays, dictionaries, caches, and memory will be less effective at getting an LLM to write code in most languages.\n\nSimilarly, a Python developer who moves toward doing more front-end programming with JS would benefit from learning the concepts behind front-end systems. For example, if you want an LLM to build a front end using the React framework, it will benefit you to understand how React breaks front ends into reusable UI components, and how it updates the DOM data structure that determines what web pages look like. This lets you prompt the LLM much more precisely, and helps you understand how to fix issues if something goes wrong. Similarly, if you want an LLM to help you write code in CUDA or ROCm, it helps to understand how GPUs organize compute and memory.\n\nJust as people who are fluent in multiple human languages can communicate more easily with other people, LLMs are making it easier for developers to build systems in multiple contexts. If you haven’t already done so, I encourage you to try having an LLM write some code in a language you’d like to learn but perhaps haven’t yet gotten around to, and see if it helps you get some new applications to work.\n\nKeep building!\n\nAndrew\n\nLearn to build agents that write and run code to complete complex tasks. “Building Code Agents with Hugging Face smolagents,” made in collaboration with Hugging Face, teaches you how to build code agents, execute their code safely, and set up evals for production-ready multi-agent systems, using the smolagents framework.Enroll for free\n\nOpenAI refreshed its roster of models and scheduled the largest, most costly one for removal.\n\nWhat’s new:OpenAI introduced five new models that accept text and images inputs and generate text output. Their parameter counts, architectures, training datasets, and training methods are undisclosed. The general-purposeGPT-4.1, GPT-4.1 mini, and GPT-4.1 nanoare available via API only. The reasoning modelso3 and o4-mini,are available via API toqualifieddevelopers as well as users of ChatGPT Plus, Pro, and Team, and soon ChatGPT Enterprise and ChatGPT Education. The company willterminateGPT-4.5 — which it introduced as a research preview in late February — in July.\n\nGPT-4.1 family:In an odd turn of version numbers, the GPT-4.1 models are intended to be cost-effective equivalents to GPT-4.5 and updates to GPT-4o. They accept inputs of up to 1 million tokens (compared to GPT-4.5’s and GPT-4o’s 128,000 tokens).\n\no3 and o4-mini:These models update o1 and o3-mini, respectively. They have input limits of 200,000 tokens and can be set to low-, medium-, or high-effort modes to process varying numbers of reasoning tokens, which are hidden from users. Unlike their predecessors, they were fine-tuned to decide when and how to use the tools, including web search, code generation and execution, and image editing.\n\nBehind the news:Late last year, OpenAI introducedo1, the first commercial model trained via reinforcement learning to generate chains of thought. Within a few months, DeepSeek, Google, and Anthropic launched their respective reasoning modelsDeepSeek-R1,Gemini 2.5 Pro, andClaude 3.7 Sonnet. OpenAI has promised to integrate its general-purpose GPT-series models and o-series reasoning models, but they remain separate for the time being.\n\nWhy it matters:GPT-4.5 was an exercise in scale, and it showed that continuing to increase parameter counts and training data would yield ongoing performance gains. But it wasn’t widely practical on a cost-per-token basis. The new models, including those that use chains of thought and tools, deliver high performance at lower prices.\n\nWe’re thinking:Anthropic is one of OpenAI’s key competitors, and a large fraction of the tokens it generates (via API) are forwriting code, a skill in which it is particularly strong. OpenAI’s emphasis on models that are good at coding could boost the competition in this area!\n\nHugging Face has made a name by providing open AI models. Now it’s providing an open robot.\n\nWhat’s new:Hugging Faceacquiredthe French company Pollen Robotics for an undisclosed price. It plans to offer Pollen’sReachy 2, a robot that runs on code that’s freelyavailableunder an Apache 2.0 license, for $70,000.\n\nHow it works:Reachy 2 has two arms, gripper hands, and a wheeled base (optional). It’s designed primarily for education and research in human-robot interaction in real-world settings.\n\nBehind the news:Last year, Remi Cadene, who worked on Tesla’s Optimus,joinedHugging Face to lead robotics projects. In May, he and his team rolled out the LeRobot open source robotics code library, whichprovidespretrained models, datasets, and simulators for reinforcement learning and imitation learning. In November, Nvidia announced acollaborationwith Hugging Face to accelerate LeRobot’s data collection, training, and verification.\n\nWhy it matters:Hugging Face’s acquisition of Pollen reflects an industry-wideinvestmentinrobots, notablyhumanoidrobots, whose prices have beenfalling. Nvidia CEO Jensen Huang has calledAI-enabled roboticsa “multi-trillion dollar” opportunity.\n\nWe’re thinking:AI-enabled robots are marching slowly toward what we hope will be breakthrough applications. Open-source systems are an important part of the trend!\n\nThe U.S. government escalated its long-running effort to block China’s access to cutting-edge AI hardware.\n\nWhat’s new:The White Houseannouncedthat future shipments of Nvidia H20s, AMD MI308s, or equivalent chips to China would require a license. Concurrently, the United States Congresslaunchedan investigation into whether chip vendor Nvidia violated earlier export rules.\n\nHow it works:Nvidia launched the H20 in late 2023 to comply with a 2022 U.S. ban on China-bound shipments of Nvidia’s H100 andH200 processors. The H20 uses the same architecture as the H200, but it’s an order of magnitude slower with less memory and memory bandwidth.\n\nBehind the news:The U.S. government’s many moves to restrict shipments of advanced processors to China have sought to protect the nation’s lead in AI, but they have not prevented Chinese developers from closing the gap. In 2020, the U.S.requiredchip makers that use U.S. technology — which includes both domestic chip designers like Nvidia and makers of advanced fabrication equipment like the Netherlands’ ASML — to seek permission before doing business with Chinese tech giant Huawei. Last December, the U.S. published sweeping limits on sales of processors that involve U.S. technology, as well as the technology itself, to Chinese businesses.\n\nYes, but:Export restrictions may have slowed China’s production of advanced chips, but they have also incentivized China to invest inestablishing leadershipin AI. In January, the Chinese AI developer DeepSeek surprised U.S. policymakers and AI leaders with the release ofDeepSeek-R1, which performs comparably to OpenAI’s o1, but whose weights are freely available and trained using less computation.\n\nWhy it matters:The first wave of restrictions on sales of advanced chips to China didlittle harmto U.S. chipmakers, largely becausedemand outstripped supply. But later restrictions have had a greaterimpacton their sales. The new limits could cost Nvidia and AMD significant revenue and likely willdegradetheir competitiveness abroad andbolsterChina’s homegrown chip-making industry.\n\nWe’re thinking:The AI community’s international scope is one of its greatest strengths. While individual countries must attend to their national security, progress in AI benefits all nations. Even in this era of rising protectionism, we hope members of the global AI community continue to support one another and encourage the free flow of ideas.\n\nLarge language models excel at processing text but can’t interpret images, video, or audio directly without further training on those media types. Researchers devised a way to overcome this limitation.\n\nWhat’s new:Kumar Ashutosh and colleagues at Meta, University of Texas, and UC Berkeley introducedMultimodal Iterative LLM Solver(MILS), a method that pairs a text-only large language model (LLM) with a multimodal embedding model to generate captions for images, video, and audio without further training.\n\nKey insight:LLMs can generate text and refine their outputs based on new information. On the other hand, multimodal embedding models can score the similarity between a given text and an image, video, or audio clip. Given this score, an LLM can regenerate the text iteratively until the score indicates a strong match between the text and the associated media. This enables the LLM to generate accurate captions for images, videos, and audio clips without training in these tasks.\n\nHow it works:Given a prompt and an image, video, or audio clip, Llama 3.1 8B produced and iteratively refined the prompt according to a pretrained multimodal embedding model’s estimate of the similarity between the text and media.\n\nResults:The authors evaluated MILS on captioning images, videos, and audio clips. They measured performance according to Metric for Evaluation of Translation with Explicit ORdering (METEOR), which checks for synonyms, words that share the same root, and word order to determine whether a generated caption matches a ground-truth caption (higher is better). Overall, MILS outperformed models that underwent task-specific training.\n\nWhy it matters:Zero-shot captioning models like Aya Vision and Pixtral require training on paired captions and media. The authors’ approach takes advantage of pretrained multimodal models to enable an LLM to compose multimedia captions without further training.\n\nWe’re thinking:Synthetic data is increasingly useful for training AI models. By enabling LLMs to synthesize good captions, MILS adds fuel to this fire.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--61--1.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/The-Batch-ads-and-exclusive-banners---2025-04-23T105320.680.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/OpenAI-MODELS_table-11b_1200px.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--78-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--79-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--80-.png"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-295/",
    "title": "issue 295",
    "date": "",
    "reading_time": "12 min read",
    "content": "Contrary to standard prompting advice that you should give LLMs the context they need to succeed, I find it’s sometimes faster to be lazy and dash off a quick, imprecise prompt and see what happens. The key to whether this is a good idea is whether you can quickly assess the output quality, so you can decide whether to provide more context. In this post, I’d like to share when and how I use “lazy prompting.”\n\nWhen debugging code, many developers copy-paste error messages — sometimes pages of them — into an LLM without further instructions. Most LLMs are smart enough to figure out that you want them to help understand and propose fixes, so you don’t need to explicitly tell them. With brief instructions like “Edit this: …” or “sample dotenv code” (to remind you how to write code to use Python's dotenv package), an LLM will often generate a good response. Further, if the response is flawed, hopefully you can spot any problems and refine the prompt, for example to steer how the LLM edits your text.\n\nAt the other end of the spectrum, sometimes  I spend 30 minutes carefully writing a 2-page prompt to get an AI system to help me solve a problem (for example to write many pages of code) that otherwise would have taken me much longer.\n\nI don’t try a lazy prompt if (i) I feel confident there’s no chance the LLM will provide a good solution without additional context. For example, given a partial program spec, does even a skilled human developer have a chance of understanding what you want? If I absolutely want to use a particular piece of pdf-to-text conversion software (like my team LandingAI’sAgentic Doc Extraction!), I should say so in the prompt, since otherwise it’s very hard for the LLM to guess my preference. I also wouldn’t use a lazy prompt if (ii) a buggy implementation would take a long time to detect. For example, if the only way for me to figure out if the output is incorrect is to laboriously run the code to check its functionality, it would be better to spend the time up-front to give context that would increase the odds of the LLM generating what I want.\n\nBy the way, lazy prompting is an advanced technique. On average, I see more people giving too little context to LLMs than too much. Laziness is a good technique only when you’ve learned how to provide enough context, and then deliberately step back to see how little context you can get away with and still have it work. Also, lazy prompting applies only when you can iterate quickly using an LLM’s web or app interface It doesn’t apply to prompts written in code for the purpose of repeatedly calling an API, since presumably you won’t be examining every output to clarify and iterate if the output is poor.\n\nThank you to Rohit Prsad, who has been collaborating with me on the open-source packageaisuite, for suggesting the term lazy prompting. There is an analogy tolazy evaluationin computer science, where you call a function at the latest possible moment and only when a specific result is needed. In lazy prompting, we add details to the prompt only when they are needed.\n\nKeep building!\n\nAndrew\n\nIn our latest course, “Getting Structured LLM Output,” you’ll learn to generate consistent, machine-readable outputs from LLMs using structured output APIs, re-prompting libraries, and token-level constraints. You’ll build a social media analysis agent that extracts sentiment and creates structured JSON ready for downstream use.Enroll for free\n\nResearchers updated the highly responsive Moshi voice-to-voice model to discuss visual input.\n\nWhat’s new:Amélie Royer, Moritz Böhle, and colleagues at Kyutai proposedMoshiVis. The weights are free todownloadunder theCC-BY 4.0license, which permits commercial and noncommercial uses. You can hear examples of itsoutputand chat with ademo.\n\nKey insight:The originalMoshi, which manages overlapping voice-to-voice conversations, comprises two transformers. The first outputs a text transcription of its speech, and the second outputs speech. Since Moshi generates text as well as speech, the authors of that work fine-tuned it to predict the next token of text. In MoshiVis, the addition of a vision encoder enabled the authors to fine-tune on not only image-text datasets but also image-speech datasets, which are not so plentiful. Fine-tuning on this wider variety of images enabled the system to understand images better than fine-tuning it solely on image-speech datasets.\n\nHow it works:To Moshi, the authors added a model based on a pretrainedSigLIPvision encoder to encode images, a cross-attention adapter to fuse image information with speech tokens, and vanilla neural networks trained to act as gates that determine how much image information to fuse. Specifically, the authors added the adapter and a gate between Moshi’s existing self-attention and fully connected layers.\n\nResults:MoshiVis is highly responsive in conversation with latency of roughly 50 milliseconds on a Mac Mini.\n\nBehind the news:MoshiVis complements a small but growing roster of systems that combine vision with speech-to-speech. ChatGPT accepts and generates speech in response to camera views or a user’s phone screen.AnyGPT(open weights training and inference code) accepts or generates speech, text, images, and music. Similarly,Mini-Omni2(open weights and inference code) accepts and generates text, speech, and images. The authors didn’t compare MoshiVis to these alternatives.\n\nWhy it matters:MoshiVis easily adapts a speech-to-speech model to work with a new type of media input. MoshiVis requires training only the adapters, while the earlier AnyGPT and Mini-Omni2, which can also discuss images via voice input and output, require training both adapters and the main model.\n\nWe’re thinking:Text-chat models respond appropriately when a user refers to a previous topic or something new, and MoshiVis does, too, in spoken interactions. Evaluations of this capability will become increasingly important as voice-to-voice becomes more widespread.\n\nBots that scrape websites for AI training data often ignore do-not-crawl requests. Now web publishers can enforce such appeals by luring scrapers to AI-generated decoy pages.\n\nWhat’s new:Cloudflare launchedAI Labyrinth, a bot-management tool that serves fake pages to unwanted bots, wasting their computational resources and making them easier to detect. It’s currently free to Cloudflare users.\n\nHow it works:AI Labyrinth protects webpages by embedding them with hidden links to AI-generated alternatives that appear legitimate to bots but are irrelevant to the protected site.\n\nBehind the news:The robots.txt instructions that tell web crawlers which pages they can access aren’t legally binding, and web crawlers can disregard them. However, online publishers aremovingto try to stop AI developers from training models on their content. Cloudflare, as the proxy server and content delivery network for nearly20 percentof websites, plays a potentially large role in this movement. AI crawlers account for nearly 1 percent of web requests on Cloudflare’s network, the company says.\n\nWhy it matters:The latest AI models are trained on huge quantities of data gleaned from the web, which enables them to perform well enough to be widely useful. However, publishers increasingly aim to limit access to this data. AI Labyrinth gives them a new tool that raises the cost for bots that disregard instructions not to scrape web content.\n\nWe’re thinking:If AI Labyrinth gains traction, no doubt some teams that build crawlers will respond with their own AI models to sniff out its decoy pages. To the extent that the interest between crawlers and publishers is misaligned and clear, enforceable rules for crawling are lacking, this cat-and-mouse competition could go on for a long time.\n\nA pair of papers investigate how increasingly human-like chatbots affect users’ emotions.\n\nWhat’s new:Jason Phang at OpenAI, Cathy Mengying Fang at MIT Media Lab, and colleagues at those organizations publishedcomplementarystudiesthat examine ChatGPT’s influence on loneliness, social interactions, emotional dependence, and potentially problematic use.\n\nHow it works:One study was a large-scale analysis of real-world conversations, and the other was a randomized control trial that tracked conversations of a selected cohort. Both evaluated conversations according toEmoClassifiersV1, a set of classifiers based on large language models that evaluate five top-level emotional classes (loneliness, dependence, and the like) and 20 sub-classes of emotional indicators (seeking support, use of pet names, and so on).\n\nResults:Both studies found that using ChatGPT was associated with reduced loneliness and increased emotional chat. However, it was also associated with decreased interpersonal social interaction and greater dependence on the chatbot, especially among users who spent more time chatting.\n\nYes, but:The authors of the randomized controlled trial acknowledged significant limitations. For instance, the study lacked a non-ChatGPT control group to differentiate AI-specific effects from influences such as seasonal emotional shifts, and the trial’s time frame and assignments may not mirror real-world behavior.\n\nWhy it matters:As AI chatbot behavior becomes more human-like, people may lean on large language models to satisfy emotional needs such as easinglonelinessorgrief. Yet we know little about their effects. These studies offer a starting point for AI developers who want to both foster emotional support and protect against over-reliance, and for social scientists who want to better understand the impact of chatbots.\n\nWe’re thinking:Social media turned out to causeemotional harmto some people in ways that were not obvious when the technology was new. As chatbots evolve, research like this can help us steer them toward protecting and enhancing mental health.\n\nAI systems designed to generate animated 3D scenes that include active human characters have been limited by a shortage of training data, such as matched 3D scenes and human motion-capture examples. Generated video clips can get the job done without motion capture.\n\nWhat’s new:A team led by Hongjie Li, Hong-Xing Yu, and Jiaman Li at Stanford University developedZero-Shot 4D Human-Scene Interaction(ZeroHSI), a method that animates a 3D human figure interacting with a particular 3D object in a selected 3D scene. You can see its outputhere.\n\nKey insight: Earlier approaches attempted to build a generalized approach: given a 3D scene, a text prompt, and motion-capture data, a diffusion model learned to alter the positions and rotations of human joints and objects over time. But if the system is designed to learn a 3D animation for a specific example motion, videos can stand in for motion capture. Current video generation models can take an image of a scene and generate a clip of realistic human motion and interactions with a wide variety of objects within it. From there, we can minimize the difference between the video frames and images of actions within the scene.\n\nHow it works:ZeroHSI takes a pre-built 3D scene that includes a3D human meshand 3D object. It uses a rendered image of the scene to generate a video. Then it uses the video to help compute the motions of a human figure and object within the scene.\n\nResults:The authors evaluated ZeroHSI using a proprietary dataset of 12 3D scenes that included a human figure and an object and between one and three text prompts that described interactions between the human and object and/or scene. In 100 evaluations, ZeroHSI outperformedLINGO, a diffusion model trained on matched 3D scene, 3D object, and human motion-capture data that had achieved the previous state of the art.\n\nWhy it matters:Learning from motion-capture data is problematic in a couple of ways: (i) it’s expensive to produce, (ii) so little of it is available, which limits how much a learning algorithm can generalize from it. Video data, on the other hand, is available in endless variety, enabling video generation models to generalize across a wide variety of scenes, objects, and motions. ZeroHSI takes advantage of generated video to guide a 3D animation cheaply and effectively.\n\nWe’re thinking:There’s a lot of progress to be made in AI simply by finding clever ways to use synthetic data.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--57-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/04/The-Batch-ads-and-exclusive-banners---2025-03-28T144816.793.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--70-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--71-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--72-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/04/unnamed--55-.gif"
    ]
  },
  {
    "url": "https://www.deeplearning.ai/the-batch/issue-304/",
    "title": "issue 304",
    "date": "",
    "reading_time": "12 min read",
    "content": "Everyone can benefit by learning to code with AI! At AI Fund, the venture studio I lead, everyone — not just the engineers — can vibe code or use more sophisticated AI-assisted coding techniques. This empowers everyone to build with AI. The impact on team creativity and productivity has been exciting! I share my experience with this in the hope that more teams will invest in empowering everyone to build with AI.\n\nEveryone at AI Fund who was not already an engineer started with our “AI Python for Beginners” course to learn the basics. I also shared with the team details ofthe tech stack I useto give everyone a default set of building blocks. Since then, many have gone on to acquire additional building blocks (such as additional third-party APIs) themselves either by takingcourses, searching online, or learning from colleagues.\n\nYou can watch a video of our experience with thishere.\n\nHere are just a few examples of applications that non-engineers at AI Fund have built:\n\nIt is very empowering when individuals don’t have to try to get scarce engineering resources allocated to their ideas in order to try them out. There are a lot fewer gatekeepers in the way: If someone has an idea, they can build a prototype and try it out. If it gets positive feedback from users, that lays the groundwork for scaling it up. Or, if the prototype does not work, this is also valuable information that lets them quickly move on to a different idea or take insights from critical feedback to decide what to try next.\n\nIn the future, one of the most important skills in any profession will be the ability to tell a computer exactly what you want, so the computer can do it for you. For the foreseeable future, writing code (with AI assistance, so the AI, rather than you, actually writes the code) will be the best way to do this.\n\nThis is a great time for everyone to code with AI!\n\nKeep building,\n\nAndrew\n\nIn “DSPy: Build and Optimize Agentic Apps,” you’ll learn to use Databricks’ DSPy framework to structure, debug, and improve the accuracy of agentic workflows. DSPy lets you define clear input and output steps, trace model behavior, and automate prompt tuning with built-in tools. Build a sentiment analyzer, travel assistant, and RAG agent!Enroll now\n\nDeepSeek updated its groundbreaking DeepSeek-R1 large language model to strike another blow for open-weights performance.\n\nWhat’s new:The newDeepSeek-R1-0528surpasses its predecessor and approaches the performance of OpenAI o3 and Google Gemini-2.5 Pro. A smaller version,DeepSeek-R1-0528-Qwen3-8B, runs on a single GPU with as little as 40GB VRAM, according toTechCrunch.\n\nHow it works:DeepSeek released little information so far about how it built the new models.\n\nPerformance:DeepSeek-R1-0528 nips at the heels of top closed LLMs on a variety of benchmarks, while DeepSeek-R1-0528-Qwen3-8B raises the bar for LLMs in its 8-billion-parameter size class. DeepSeek claims general improvements in reasoning, managing complex tasks, and writing and editing lengthy prose, along with 50 percent fewer hallucinations when rewriting and summarizing.\n\nBehind the news:The initial version of DeepSeek-R1 challenged the belief that building top-performing AI models requires tens to hundreds of millions of dollars, top-of-the-line GPUs, and enormous numbers of GPU hours. For the second time in less than a year, DeepSeek has built a competitive LLM with a relativelylowbudget.\n\nWhy it matters:DeepSeek’s models, along with Alibaba’s Qwen series, continue to narrow the gap between open-weights models and their closed peers. Its accomplishments could lead to wider adoption of less-expensive, more-efficient approaches. DeepSeek is passing along the cost savings to developers, offering high-performance inference at a fraction of the cost of closed models.\n\nWe’re thinking:DeepSeek-R1-0528-Qwen3-8B mixes contributions from open-weight models — possible only because Qwen3’s license, like DeepSeek’s is permissive. Open models enable experimentation and innovation in ways that closed models do not.\n\nAI is bringing a massive boost in productivity to Duolingo, maker of the most popular app for learning languages.\n\nWhat’s new:Duolingo used generative AI toproduce148 courses, more than doubling its previous catalog. The technology enabled the company to offer some of its most popular courses — Spanish, French, German, Italian, Japanese, Korean, and Mandarin — in 28 languages. Initially, the company is using AI to produce courses aimed at beginners, with more advanced levels to come.\n\nHow it works:Duolingo’sAI-assisted approach to building language coursesquickly turns a single course into many. The new approach revved its pace from building 100 courses over 12 years to producing many more than that in less than a year.\n\nBehind the scenes:AI is at the heart of Duolingo’s expansion into other areas beyond language learning.\n\nWhy it matters:Companies in nearly every industry face pressure to produce more with less amid rising competition. AI can help to accomplish that while potentially improving product quality, and Duolingo has ample reason to move aggressively in this direction. The startupSpeak, which offers a voice-based approach to learning languages, is growing rapidly, and Google just launchedLittle Language Lessonsthat show how an AI-first product could be used as a language teacher and conversational partner.\n\nWe’re thinking:AI is well on the way totransforming educationfor teachers, students, and technology companies!\n\nAI’s thirst for energy is growing, but the technology also could help produce huge energy savings over the next five to 10 years, according to a recent report.\n\nWhat’s new:The International Energy Agency (IEA), which advises 44 countries on energy policy, performed a comprehensiveanalysisof AI’s energy consumption including energy required to obtain critical materials needed for chips and data centers. The report sees dark clouds ahead but also silver linings.\n\nDark clouds:The report, which is based on interviews with officials in government, energy, and technology, makes four projections for AI’s energy consumption. In the base scenario, future growth and efficiency gains are similar to those of the past five years. The agency also plots a “take-off” scenario in which AI adoption happens faster, a “high efficiency” scenario with lower energy needs, and a “headwinds” scenario in which adoption of AI slows or infrastructure bottlenecks impede construction. Among the conclusions:\n\nSilver linings:AI already makes energy generation, distribution, and use more efficient. The authors expect these savings to accelerate.\n\nYes, but:The authors concede that lower energy costs for AI likely will lead to much greater consumption — according to theJevons paradox— so more-efficient models and hardware will result in higher energy consumption overall.\n\nBehind the news:Data centers were growing rapidly prior to the boom in generative AI. Data centers’ electricity use doubled between 2000 and 2005 and again between 2017 and 2022, driven by the growth of cloud computing and data storage, streaming and social media, and cryptocurrency mining. However, these periods of accelerating growth were followed by periods of slower growth as efforts to cut costs led to more-efficient software and hardware. The authors expect this pattern to hold.\n\nWhy it matters:The IEA report is a first-of-its-kind analysis of AI’s energy requirements, how they’re likely to grow, as well as the potential of the technology itself to reduce those requirements. It confirms that AI is poised to consume huge amounts of energy. However, it also suggests that today’s energy costs will be tomorrow’s energy savings as AI makes energy generation, distribution, and use more efficient across a wide variety of industries.\n\nWe’re thinking:While demand for electricity for data centers is growing rapidly, calibrating the right level of investment is tricky. High levels of growth come with high levels of hype that can lead analysts to overestimate future demand. For example, Microsoft, after examining its forecasts,canceleddata-center projects that would have consumed 2 gigawatts.\n\nResearchers identified a simple way to mislead autonomous agents based on large language models.\n\nWhat’s new: Ang Li and colleagues at Columbia University developed a method toexploitthe implicit trust that agents tend to place in popular websites by poisoning those websites with malicious links.\n\nKey insight:Commercially available agentic systems may not trust random sites on the web, but they tend to trust popular sites such as social-media sites. An attacker can exploit this trust by crafting seemingly typical posts that link to a malicious website. The agent might follow the link, mistakenly extending its trust to an untrustworthy site.\n\nHow it works: The authors tested web-browsing agents includingAnthropic Computer UseandMultiOnon tasks such as shopping or sending emails.\n\nResults: Once an agent was redirected to the malicious websites, it reliably followed the attacker’s instructions. For example, each of the agents tested divulged credit card information in 10 out of 10 trials. Similarly, each agent sent a phishing message from the user’s email account asking recipients to send money to a malicious “friend” in 10 out of 10 trials.\n\nWhy it matters: Giving agents the ability to perform real-world actions, such as executing purchases and sending emails, raises the possibility that they might be tricked into taking harmful actions. Manipulating agents by referring them to malicious web content is an effective vector of attack. Agents will be more secure if they’re designed to avoid and resist such manipulation.\n\nWe’re thinking:Humans, too, can be fooled by phishing and other malicious activities, and the path to programming agents to defend against them seems easier than the path to training the majority of humans to do so. In the long term, agents will make online interactions safer.",
    "images": [
      "https://dl-staging-website.ghost.io/content/images/2025/06/2025.06.04-LETTER-3--1-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/06/The-Batch-ads-and-exclusive-banners---2025-06-02T160931.393.png",
      "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed--100-.png",
      "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed--69-.jpg",
      "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed---2025-06-04T165349.311.png",
      "https://dl-staging-website.ghost.io/content/images/2025/06/unnamed---2025-06-04T165354.442.png"
    ]
  }
]